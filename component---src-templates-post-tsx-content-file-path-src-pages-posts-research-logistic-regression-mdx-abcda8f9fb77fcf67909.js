"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[3246],{41326:function(e,t,n){n.r(t),n.d(t,{Head:function(){return T},PostTemplate:function(){return z},default:function(){return _}});var a=n(28453),i=n(96540),l=n(61992),r=(n(62087),n(90548));function s(e){const t=Object.assign({p:"p",hr:"hr",h2:"h2",a:"a",span:"span",h3:"h3",br:"br",ul:"ul",li:"li",strong:"strong",ol:"ol"},(0,a.RP)(),e.components),{Image:n}=t;return n||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),i.createElement(i.Fragment,null,"\n",i.createElement("br"),"\n","\n",i.createElement(t.p,null,'Logistic regression is a cornerstone technique in supervised machine learning for tackling binary classification tasks. Although it is called a "regression," it is used primarily for classification, making it a prime example of how linear models can be adapted to classify data. Historically, linear models have been central to both statistics and machine learning (e.g., Nelder and Wedderburn\'s work on generalized linear models in 1972), and logistic regression stands as one of their most widely recognized forms in predictive analytics.'),"\n",i.createElement(t.p,null,"In a typical classification setting, we have data points represented by feature vectors ",i.createElement(r.A,{text:"\\( x \\in \\mathbb{R}^d \\)"})," and an associated label ",i.createElement(r.A,{text:"\\( y \\in \\{0, 1\\} \\)"}),". Our goal is to learn a function that can correctly map new, unseen ",i.createElement(r.A,{text:"\\( x \\)"})," to a label 0 or 1. For a linear model, one might initially think of extending linear regression by simply thresholding its output. However, such a direct approach can produce values outside the range ",i.createElement(r.A,{text:"\\([0,1]\\)"})," and is not ideally suited for probability estimation."),"\n",i.createElement(t.p,null,"This is where logistic regression enters the scene: it transforms the output of a linear function into a probability via the logistic (sigmoid) function, ensuring that the output always lies within ",i.createElement(r.A,{text:"\\([0,1]\\)"}),". Although the resulting decision boundary is still linear in the input space, this probability-based interpretation is extremely powerful."),"\n",i.createElement(t.p,null,"Nonetheless, logistic regression does have certain limitations, especially when dealing with complex, highly non-linear decision boundaries. Newer, more flexible models (e.g., kernel-based SVMs, neural networks) can capture richer representations. Still, logistic regression remains popular due to its interpretability, low computational cost, and the natural probabilistic interpretation of its outputs."),"\n",i.createElement(t.hr),"\n",i.createElement(t.h2,{id:"intuition-behind-logistic-regression",style:{position:"relative"}},i.createElement(t.a,{href:"#intuition-behind-logistic-regression","aria-label":"intuition behind logistic regression permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Intuition behind logistic regression"),"\n",i.createElement(t.h3,{id:"transition-from-linear-regression-to-logistic-regression",style:{position:"relative"}},i.createElement(t.a,{href:"#transition-from-linear-regression-to-logistic-regression","aria-label":"transition from linear regression to logistic regression permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Transition from linear regression to logistic regression"),"\n",i.createElement(t.p,null,'If we take a naive step from linear regression to classification by simply saying "output class 1 if ',i.createElement(r.A,{text:"\\(w^T x + b\\)"}),' exceeds some threshold," we run into problems of interpretability and unbounded outputs. While linear regression tries to fit:',i.createElement(t.br),"\n",i.createElement(r.A,{text:"\\( \\hat{y} = w^T x + b \\)"}),",",i.createElement(t.br),"\n","this value can exceed 1 or be negative, so it is not a valid probability. Logistic regression solves this by mapping the linear output to the interval ",i.createElement(r.A,{text:"\\([0,1]\\)"}),"."),"\n",i.createElement(t.h3,{id:"the-sigmoid-function",style:{position:"relative"}},i.createElement(t.a,{href:"#the-sigmoid-function","aria-label":"the sigmoid function permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"The sigmoid function"),"\n",i.createElement(t.p,null,"The heart of logistic regression is the sigmoid (a.k.a. logistic) function:",i.createElement(t.br),"\n",i.createElement(r.A,{text:"\\( \\sigma(z) = \\frac{1}{1 + e^{-z}} \\)"}),",",i.createElement(t.br),"\n","where ",i.createElement(r.A,{text:"\\( z = w^T x + b \\)"}),"."),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"For large positive ",i.createElement(r.A,{text:"\\(z\\)"})," (i.e., ",i.createElement(r.A,{text:"\\(w^T x + b \\gg 0\\)"}),"), ",i.createElement(r.A,{text:"\\(\\sigma(z)\\)"})," approaches 1."),"\n",i.createElement(t.li,null,"For large negative ",i.createElement(r.A,{text:"\\(z\\)"})," (i.e., ",i.createElement(r.A,{text:"\\(w^T x + b \\ll 0\\)"}),"), ",i.createElement(r.A,{text:"\\(\\sigma(z)\\)"})," approaches 0."),"\n"),"\n",i.createElement(t.p,null,"Thus, the sigmoid neatly squeezes any real value into the ",i.createElement(r.A,{text:"\\([0,1]\\)"})," range, making it suitable as an estimated probability ",i.createElement(r.A,{text:"\\( \\hat{p} = \\sigma(w^T x + b) \\)"}),"."),"\n",i.createElement(n,{alt:"Sigmoid function plot",path:"",caption:"The sigmoid function <Latex text='S(z) = 1/(1 + e^{-z})'/> always outputs a value in [0,1].",zoom:"false"}),"\n",i.createElement(t.h3,{id:"log-odds-interpretation",style:{position:"relative"}},i.createElement(t.a,{href:"#log-odds-interpretation","aria-label":"log odds interpretation permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Log-odds interpretation"),"\n",i.createElement(t.p,null,"One of logistic regression's most appealing characteristics is its direct log-odds interpretation. Specifically:"),"\n",i.createElement(r.A,{text:"\\[\n\\log \\left(\\frac{p}{1-p}\\right) = w^T x + b,\n\\]"}),"\n",i.createElement(t.p,null,"where ",i.createElement(r.A,{text:"\\(p\\)"})," is the predicted probability that ",i.createElement(r.A,{text:"\\(y=1\\)"}),". The left-hand side ",i.createElement(r.A,{text:"\\( \\log \\big(\\frac{p}{1 - p}\\big) \\)"}),' is called the "log-odds" or "logit." This means a one-unit change in a particular feature ',i.createElement(r.A,{text:"\\(x_j\\)"})," corresponds to an additive shift in the log-odds by ",i.createElement(r.A,{text:"\\(w_j\\)"}),", making the model coefficients directly interpretable in terms of odds ratios."),"\n",i.createElement(t.h3,{id:"comparison-to-other-classification-methods",style:{position:"relative"}},i.createElement(t.a,{href:"#comparison-to-other-classification-methods","aria-label":"comparison to other classification methods permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Comparison to other classification methods"),"\n",i.createElement(t.p,null,"While advanced classifiers — such as random forests, gradient boosting, or neural networks — often outperform logistic regression in highly complex tasks, logistic regression's simplicity and interpretability remain key advantages. In regulated industries like healthcare or finance, understanding why a model makes a certain decision is sometimes paramount, and logistic regression is often more transparent than black-box models."),"\n",i.createElement(t.hr),"\n",i.createElement(t.h2,{id:"cost-function-and-log-loss",style:{position:"relative"}},i.createElement(t.a,{href:"#cost-function-and-log-loss","aria-label":"cost function and log loss permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Cost function and log loss"),"\n",i.createElement(t.p,null,"Because logistic regression outputs probabilities, a suitable loss function should heavily penalize wrong predictions made with high confidence. The most common choice is the cross-entropy loss, also known as log loss in the binary setting:"),"\n",i.createElement(r.A,{text:"\\[\nJ(w, b) = - \\frac{1}{m} \\sum_{i=1}^{m} \\Big[y^{(i)} \\log(\\hat{p}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{p}^{(i)})\\Big],\n\\]"}),"\n",i.createElement(t.p,null,"where ",i.createElement(r.A,{text:"\\( m \\)"})," is the number of training samples, ",i.createElement(r.A,{text:"\\( \\hat{p}^{(i)} \\)"})," is the predicted probability ",i.createElement(r.A,{text:"\\( \\sigma(w^T x^{(i)} + b) \\)"}),", and ",i.createElement(r.A,{text:"\\( y^{(i)} \\)"})," is the true label in ",i.createElement(r.A,{text:"\\{0,1\\}"}),"."),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Interpretation of terms:")),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(r.A,{text:"\\(y^{(i)} \\log(\\hat{p}^{(i)})\\)"}),": If the true label is 1, we want ",i.createElement(r.A,{text:"\\(\\hat{p}^{(i)}\\)"})," to be close to 1 to avoid a large penalty."),"\n",i.createElement(t.li,null,i.createElement(r.A,{text:"\\((1 - y^{(i)}) \\log(1 - \\hat{p}^{(i)})\\)"}),": If the true label is 0, we want ",i.createElement(r.A,{text:"\\(\\hat{p}^{(i)}\\)"})," to be near 0."),"\n"),"\n",i.createElement(t.p,null,"Because the logarithm grows large in magnitude (negatively) as ",i.createElement(r.A,{text:"\\(\\hat{p}\\)"})," approaches 0, the model is heavily penalized for confident but incorrect predictions. Log loss is convex in terms of ",i.createElement(r.A,{text:"\\(w\\)"})," and ",i.createElement(r.A,{text:"\\(b\\)"}),", making it more tractable to optimize than many other losses."),"\n",i.createElement(t.hr),"\n",i.createElement(t.h2,{id:"parameter-estimation",style:{position:"relative"}},i.createElement(t.a,{href:"#parameter-estimation","aria-label":"parameter estimation permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Parameter estimation"),"\n",i.createElement(t.h3,{id:"gradient-descent-approach",style:{position:"relative"}},i.createElement(t.a,{href:"#gradient-descent-approach","aria-label":"gradient descent approach permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Gradient descent approach"),"\n",i.createElement(t.p,null,"A widely used method to find optimal parameters ",i.createElement(r.A,{text:"\\( (w, b) \\)"})," that minimize the log loss is gradient descent. We iterate:"),"\n",i.createElement(r.A,{text:"\\[\nw := w - \\alpha \\frac{\\partial J}{\\partial w}, \\quad b := b - \\alpha \\frac{\\partial J}{\\partial b},\n\\]"}),"\n",i.createElement(t.p,null,"where ",i.createElement(r.A,{text:"\\( \\alpha \\)"})," is the learning rate, and ",i.createElement(r.A,{text:"\\(\\partial J / \\partial w\\)"})," and ",i.createElement(r.A,{text:"\\(\\partial J / \\partial b\\)"})," are computed based on the cross-entropy loss. For each sample ",i.createElement(r.A,{text:"\\( (x^{(i)}, y^{(i)}) \\)"}),", we have:"),"\n",i.createElement(t.p,null,i.createElement(r.A,{text:"\\( \\hat{p}^{(i)} = \\sigma\\big(w^T x^{(i)} + b\\big)\\)"}),","),"\n",i.createElement(t.p,null,"and"),"\n",i.createElement(r.A,{text:"\\[\n\\frac{\\partial J}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^m \\Big(\\hat{p}^{(i)} - y^{(i)}\\Big) x^{(i)}.\n\\]"}),"\n",i.createElement(t.p,null,"Each update step moves ",i.createElement(r.A,{text:"\\(w\\)"})," and ",i.createElement(r.A,{text:"\\(b\\)"})," in the direction that locally decreases the loss."),"\n",i.createElement(t.h3,{id:"newtons-method-and-other-optimization-techniques",style:{position:"relative"}},i.createElement(t.a,{href:"#newtons-method-and-other-optimization-techniques","aria-label":"newtons method and other optimization techniques permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Newton's method and other optimization techniques"),"\n",i.createElement(t.p,null,"For logistic regression, the cross-entropy loss has a nice structure that can also be tackled by second-order optimization methods, such as Newton's method. Newton's method uses curvature information (the Hessian matrix) to converge in fewer steps, though each step can be more computationally expensive. Libraries like statsmodels (in Python) often implement this approach, giving very precise parameter estimates."),"\n",i.createElement(t.p,null,"Other methods include quasi-Newton approaches (e.g., L-BFGS) or coordinate descent (especially when regularization is involved). In practice, both first-order (gradient descent variants) and second-order methods can be effective depending on dataset size and constraints."),"\n",i.createElement(t.h3,{id:"convergence-and-computational-considerations",style:{position:"relative"}},i.createElement(t.a,{href:"#convergence-and-computational-considerations","aria-label":"convergence and computational considerations permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Convergence and computational considerations"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Learning rate")," (",i.createElement(r.A,{text:"\\( \\alpha \\)"}),"): Too large can cause divergence; too small can slow convergence."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Number of features")," (",i.createElement(r.A,{text:"\\( d \\)"}),"): When ",i.createElement(r.A,{text:"\\( d \\)"})," is massive, specialized optimizers or mini-batch gradient methods can improve performance."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Batch vs. stochastic updates"),": In large-scale problems, we often use stochastic gradient descent (SGD) or mini-batch SGD for efficiency and faster iteration cycles."),"\n"),"\n",i.createElement(t.hr),"\n",i.createElement(t.h2,{id:"properties-of-logistic-regression",style:{position:"relative"}},i.createElement(t.a,{href:"#properties-of-logistic-regression","aria-label":"properties of logistic regression permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Properties of logistic regression"),"\n",i.createElement(t.h3,{id:"probabilistic-interpretation",style:{position:"relative"}},i.createElement(t.a,{href:"#probabilistic-interpretation","aria-label":"probabilistic interpretation permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Probabilistic interpretation"),"\n",i.createElement(t.p,null,"A prominent appeal of logistic regression is its ability to produce probabilities. Rather than merely classifying samples as 0 or 1, the model outputs ",i.createElement(r.A,{text:"\\( \\hat{p} \\)"})," — the probability of belonging to the positive class — enabling well-calibrated decision rules. This is useful in domains (e.g., medical diagnosis) where you might need to act on a predicted probability instead of a hard label."),"\n",i.createElement(t.h3,{id:"decision-boundaries-and-interpretability",style:{position:"relative"}},i.createElement(t.a,{href:"#decision-boundaries-and-interpretability","aria-label":"decision boundaries and interpretability permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Decision boundaries and interpretability"),"\n",i.createElement(t.p,null,"Despite producing probability estimates, the final classification boundary in logistic regression is still linear: it is the set of ",i.createElement(r.A,{text:"\\( x \\)"})," such that ",i.createElement(r.A,{text:"\\( w^T x + b = 0 \\)"}),". Points on one side of the hyperplane yield probabilities above 0.5, and on the other side, below 0.5."),"\n",i.createElement(t.p,null,"Logistic regression remains one of the most interpretable classifiers:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Coefficients ",i.createElement(r.A,{text:"\\( w \\)"})),": Show how each feature contributes to the log-odds of the outcome."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Intercept ",i.createElement(r.A,{text:"\\( b \\)"})),": Shifts the decision boundary globally."),"\n"),"\n",i.createElement(t.h3,{id:"overfitting-and-the-role-of-regularization-l1-l2",style:{position:"relative"}},i.createElement(t.a,{href:"#overfitting-and-the-role-of-regularization-l1-l2","aria-label":"overfitting and the role of regularization l1 l2 permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Overfitting and the role of regularization (L1, L2)"),"\n",i.createElement(t.p,null,"When the number of features is large compared to the number of samples, logistic regression can overfit. Regularization combats this by penalizing large parameter values:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"L2 regularization")," (",i.createElement(r.A,{text:"\\( \\|\\!w\\|\\!_2^2 \\)"}),' penalty) tends to shrink coefficients smoothly, often used in practice (a.k.a. "Ridge" regularization).'),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"L1 regularization")," (",i.createElement(r.A,{text:"\\( \\|\\!w\\|\\!_1 \\)"}),' penalty) promotes sparsity, driving some coefficients exactly to zero ("Lasso" style).'),"\n"),"\n",i.createElement(t.p,null,"Regularization affects both interpretability (L1 can yield sparse models that are easier to interpret) and the shape of the decision boundary."),"\n",i.createElement(t.h3,{id:"feature-scaling-and-its-impact",style:{position:"relative"}},i.createElement(t.a,{href:"#feature-scaling-and-its-impact","aria-label":"feature scaling and its impact permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Feature scaling and its impact"),"\n",i.createElement(t.p,null,"Because logistic regression is sensitive to the relative scale of features, applying standardization or normalization typically accelerates convergence. This is especially relevant if gradient-based methods are used. Feature scaling ensures all features contribute comparably to the loss gradient, preventing large-scale features from dominating the learning process."),"\n",i.createElement(t.hr),"\n",i.createElement(t.h2,{id:"multiclass-logistic-regression",style:{position:"relative"}},i.createElement(t.a,{href:"#multiclass-logistic-regression","aria-label":"multiclass logistic regression permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Multiclass logistic regression"),"\n",i.createElement(t.p,null,"Although logistic regression was originally formulated for binary classification, there are straightforward methods to extend it to multi-class problems."),"\n",i.createElement(t.h3,{id:"one-vs-all-ova-strategy",style:{position:"relative"}},i.createElement(t.a,{href:"#one-vs-all-ova-strategy","aria-label":"one vs all ova strategy permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"One-vs-all (OvA) strategy"),"\n",i.createElement(t.p,null,'Sometimes called "one-vs-rest," the OvA strategy trains a separate binary logistic classifier for each class ',i.createElement(r.A,{text:"\\(k\\)"})," by labeling all samples in class ",i.createElement(r.A,{text:"\\(k\\)"})," as 1 and all others as 0. For an ",i.createElement(r.A,{text:"\\(N\\)"}),"-class problem, you end up with ",i.createElement(r.A,{text:"\\(N\\)"})," classifiers. A new sample is assigned to the class whose corresponding classifier outputs the highest probability."),"\n",i.createElement(n,{alt:"One-vs-all classification",path:"",caption:"Each class receives its own binary classifier distinguishing that class from the rest.",zoom:"false"}),"\n",i.createElement(t.h3,{id:"one-vs-one-ovo-strategy",style:{position:"relative"}},i.createElement(t.a,{href:"#one-vs-one-ovo-strategy","aria-label":"one vs one ovo strategy permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"One-vs-one (OvO) strategy"),"\n",i.createElement(t.p,null,"In one-vs-one, a separate classifier is trained for every pair of classes. For ",i.createElement(r.A,{text:"\\(N\\)"})," classes, this leads to ",i.createElement(r.A,{text:"\\(N(N-1)/2\\)"}),' classifiers. When predicting, each pairwise classifier "votes" for one of the two classes it distinguishes, and the final class is the one with the most votes. This approach can be more efficient if your data is balanced among classes, but it can become cumbersome when ',i.createElement(r.A,{text:"\\(N\\)"})," is large."),"\n",i.createElement(t.h3,{id:"softmax-multinomial-logistic-regression-approach",style:{position:"relative"}},i.createElement(t.a,{href:"#softmax-multinomial-logistic-regression-approach","aria-label":"softmax multinomial logistic regression approach permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Softmax (multinomial logistic regression) approach"),"\n",i.createElement(t.p,null,'Alternatively, the multinomial logistic regression (also called "softmax regression") generalizes the sigmoid to multiple classes by computing an exponentiated linear score for each class:'),"\n",i.createElement(r.A,{text:"\\[\np(y = k \\mid x) = \\frac{\\exp\\big(w_k^T x + b_k\\big)}{\\sum_{j=1}^N \\exp\\big(w_j^T x + b_j\\big)}.\n\\]"}),"\n",i.createElement(t.p,null,"This approach uses a cross-entropy loss over all classes simultaneously. It is elegantly handled by many machine learning libraries (e.g., scikit-learn's ",i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">LogisticRegression</code>'}})," with ",i.createElement(t.span,{dangerouslySetInnerHTML:{__html:"<code class=\"language-text\">multi_class='multinomial'</code>"}}),")."),"\n",i.createElement(t.hr),"\n",i.createElement(t.h2,{id:"applications-and-practical-tips",style:{position:"relative"}},i.createElement(t.a,{href:"#applications-and-practical-tips","aria-label":"applications and practical tips permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Applications and practical tips"),"\n",i.createElement(t.p,null,"Logistic regression is ubiquitous across industries due to its reliability, interpretability, and relative ease of training. Common examples include:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Customer churn prediction"),": Estimating the probability that a customer will stop using a service."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Medical diagnosis"),": Predicting the probability of having a certain disease based on biomarkers and patient history."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Credit scoring"),": Assessing the likelihood of default."),"\n"),"\n",i.createElement(t.p,null,"When working with logistic regression in practice, keep these tips in mind:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Data preprocessing"),": Clean your data and scale numeric features. Deal with missing values appropriately."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Regularization"),": Use ",i.createElement(r.A,{text:"\\( \\ell_2 \\)"})," or ",i.createElement(r.A,{text:"\\( \\ell_1 \\)"})," to avoid overfitting, especially with many features."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Imbalanced datasets"),": If your positive class is rare, consider oversampling, undersampling, or using class-weight adjustments."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Interpretability"),": Logistic regression coefficients are straightforward to interpret, but consider advanced explainability methods (e.g., SHAP, LIME) if needed for more complex use cases."),"\n"),"\n",i.createElement(t.h3,{id:"simple-python-implementation-example",style:{position:"relative"}},i.createElement(t.a,{href:"#simple-python-implementation-example","aria-label":"simple python implementation example permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Simple Python implementation example"),"\n",i.createElement(t.p,null,'Below is a very basic illustration of how one might implement logistic regression "from scratch" in Python using batch gradient descent. In practice, libraries such as scikit-learn, statsmodels, or PyTorch/TensorFlow are preferred for efficient and robust implementations.'),"\n",i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">&lt;Code text={`\nimport numpy as np\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\ndef compute_loss_and_gradients(X, y, w, b):\n    m = X.shape[0]\n    # Predicted probabilities\n    z = X.dot(w) + b\n    p = sigmoid(z)\n    \n    # Cross-entropy loss\n    loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\n    \n    # Gradients\n    dw = (1/m) * X.T.dot(p - y)\n    db = np.mean(p - y)\n    \n    return loss, dw, db\n\ndef train_logistic_regression(X, y, lr=0.01, num_iters=1000):\n    np.random.seed(42)\n    w = np.random.randn(X.shape[1]) * 0.01\n    b = 0.0\n    \n    for i in range(num_iters):\n        loss, dw, db = compute_loss_and_gradients(X, y, w, b)\n        w -= lr * dw\n        b -= lr * db\n        \n        if i % 100 == 0:\n            print(f"Iteration {i}, Loss: {loss:.4f}")\n    \n    return w, b\n\n# Example usage:\n# X_train, y_train = ... # Load or generate data\n# w, b = train_logistic_regression(X_train, y_train, lr=0.1, num_iters=1000)\n# print("Trained weights:", w)\n# print("Trained bias:", b)\n`}/></code></pre></div>'}}),"\n",i.createElement(t.p,null,"In this small script:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(l.A,null,"sigmoid")," implements ",i.createElement(r.A,{text:"\\( \\sigma(z) \\)"}),"."),"\n",i.createElement(t.li,null,"The function ",i.createElement(l.A,null,"compute_loss_and_gradients")," calculates the cross-entropy loss and its gradients."),"\n",i.createElement(t.li,null,i.createElement(l.A,null,"train_logistic_regression")," iteratively updates parameters ",i.createElement(r.A,{text:"\\( w \\)"})," and ",i.createElement(r.A,{text:"\\( b \\)"})," using gradient descent."),"\n"),"\n",i.createElement(t.p,null,"Logistic regression's simplicity keeps such implementations concise and readable."),"\n",i.createElement(t.p,null,"You now have a deeper, more theoretical understanding of logistic regression: from its origins as a linear classifier with a probabilistic twist, through the details of the cost function, up to its optimization, properties, and multi-class extensions. Logistic regression remains a mainstay in modern data science and machine learning pipelines — especially for interpretable, moderate-scale problems where a straightforward, well-grounded model is invaluable."))}var o=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,a.RP)(),e.components);return t?i.createElement(t,e,i.createElement(s,e)):s(e)};var c=n(54506),m=n(88864),h=n(58481),d=n.n(h),p=n(5984),g=n(43672),u=n(27042),f=n(72031),b=n(81817),v=n(27105),E=n(17265),y=n(2043),w=n(95751),x=n(94328),S=n(80791),A=n(78137);const H=e=>{let{toc:t}=e;if(!t||!t.items)return null;return i.createElement("nav",{className:S.R},i.createElement("ul",null,t.items.map(((e,t)=>i.createElement("li",{key:t},i.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const n=t.replace("#",""),a=document.getElementById(n);a&&a.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&i.createElement(H,{toc:{items:e.items}}))))))};function z(e){let{data:{mdx:t,allMdx:l,allPostImages:r},children:s}=e;const{frontmatter:o,body:m,tableOfContents:h}=t,f=o.index,S=o.slug.split("/")[1],z=l.nodes.filter((e=>e.frontmatter.slug.includes(`/${S}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),_=z.findIndex((e=>e.frontmatter.index===f)),T=z[_+1],k=z[_-1],C=o.slug.replace(/\/$/,""),M=/[^/]*$/.exec(C)[0],I=`posts/${S}/content/${M}/`,{0:L,1:N}=(0,i.useState)(o.flagWideLayoutByDefault),{0:V,1:B}=(0,i.useState)(!1);var P;(0,i.useEffect)((()=>{B(!0);const e=setTimeout((()=>B(!1)),340);return()=>clearTimeout(e)}),[L]),"adventures"===S?P=E.cb:"research"===S?P=E.Qh:"thoughts"===S&&(P=E.T6);const O=d()(m).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,$=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),n=e%60;return n<=30?`~${t}${n>0?".5":""} h`:`~${t+1} h`}(Math.ceil(O/P)+(o.extraReadTimeMin||0)),G=[{flag:o.flagDraft,component:()=>Promise.all([n.e(5850),n.e(9833)]).then(n.bind(n,49833))},{flag:o.flagMindfuckery,component:()=>Promise.all([n.e(5850),n.e(7805)]).then(n.bind(n,27805))},{flag:o.flagRewrite,component:()=>Promise.all([n.e(5850),n.e(8916)]).then(n.bind(n,78916))},{flag:o.flagOffensive,component:()=>Promise.all([n.e(5850),n.e(6731)]).then(n.bind(n,49112))},{flag:o.flagProfane,component:()=>Promise.all([n.e(5850),n.e(3336)]).then(n.bind(n,83336))},{flag:o.flagMultilingual,component:()=>Promise.all([n.e(5850),n.e(2343)]).then(n.bind(n,62343))},{flag:o.flagUnreliably,component:()=>Promise.all([n.e(5850),n.e(6865)]).then(n.bind(n,11627))},{flag:o.flagPolitical,component:()=>Promise.all([n.e(5850),n.e(4417)]).then(n.bind(n,24417))},{flag:o.flagCognitohazard,component:()=>Promise.all([n.e(5850),n.e(8669)]).then(n.bind(n,18669))},{flag:o.flagHidden,component:()=>Promise.all([n.e(5850),n.e(8124)]).then(n.bind(n,48124))}],{0:R,1:W}=(0,i.useState)([]);return(0,i.useEffect)((()=>{G.forEach((e=>{let{flag:t,component:n}=e;t&&n().then((e=>{W((t=>[].concat((0,c.A)(t),[e.default])))}))}))}),[]),i.createElement(u.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},i.createElement(b.A,{postNumber:o.index,date:o.date,updated:o.updated,readTime:$,difficulty:o.difficultyLevel,title:o.title,desc:o.desc,banner:o.banner,section:S,postKey:M,isMindfuckery:o.flagMindfuckery,mainTag:o.mainTag}),i.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},o.otherTags.map(((e,t)=>i.createElement("span",{key:t,className:`noselect ${A.MW}`,style:{margin:"0 5px 5px 0"}},e)))),i.createElement("div",{className:"postBody"},i.createElement(H,{toc:h})),i.createElement("br",null),i.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},i.createElement(u.P.button,{className:`noselect ${x.pb}`,id:x.xG,onClick:()=>{N(!L)},whileTap:{scale:.93}},i.createElement(u.P.div,{className:w.DJ,key:L,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},L?"Switch to default layout":"Switch to wide layout"))),i.createElement("br",null),i.createElement("div",{className:"postBody",style:{margin:L?"0 -14%":"",maxWidth:L?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},i.createElement("div",{className:`${x.P_} ${V?x.Xn:x.qG}`},R.map(((e,t)=>i.createElement(e,{key:t}))),o.indexCourse?i.createElement(y.A,{index:o.indexCourse,category:o.courseCategoryName}):"",i.createElement(p.Z.Provider,{value:{images:r.nodes,basePath:I.replace(/\/$/,"")+"/"}},i.createElement(a.xA,{components:{Image:g.A}},s)))),i.createElement(v.A,{nextPost:T,lastPost:k,keyCurrent:M,section:S}))}function _(e){return i.createElement(z,e,i.createElement(o,e))}function T(e){var t,n,a,l,r;let{data:s}=e;const{frontmatter:o}=s.mdx,c=o.titleSEO||o.title,h=o.titleOG||c,d=o.titleTwitter||c,p=o.descSEO||o.desc,g=o.descOG||p,u=o.descTwitter||p,b=o.schemaType||"BlogPosting",v=o.keywordsSEO,E=o.date,y=o.updated||E,w=o.imageOG||(null===(t=o.banner)||void 0===t||null===(n=t.childImageSharp)||void 0===n||null===(a=n.gatsbyImageData)||void 0===a||null===(l=a.images)||void 0===l||null===(r=l.fallback)||void 0===r?void 0:r.src),x=o.imageAltOG||g,S=o.imageTwitter||w,A=o.imageAltTwitter||u,H=o.canonicalURL,z=o.flagHidden||!1,_=o.mainTag||"Posts",T=o.slug.split("/")[1]||"posts",{siteUrl:k}=(0,m.Q)(),C={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:k},{"@type":"ListItem",position:2,name:_,item:`${k}/${o.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${k}${o.slug}`}]};return i.createElement(f.A,{title:c+" - avrtt.blog",titleOG:h,titleTwitter:d,description:p,descriptionOG:g,descriptionTwitter:u,schemaType:b,keywords:v,datePublished:E,dateModified:y,imageOG:w,imageAltOG:x,imageTwitter:S,imageAltTwitter:A,canonicalUrl:H,flagHidden:z,mainTag:_,section:T,type:"article"},i.createElement("script",{type:"application/ld+json"},JSON.stringify(C)))}},90548:function(e,t,n){var a=n(96540),i=n(7978);t.A=e=>{let{text:t}=e;return a.createElement(i.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-tsx-content-file-path-src-pages-posts-research-logistic-regression-mdx-abcda8f9fb77fcf67909.js.map