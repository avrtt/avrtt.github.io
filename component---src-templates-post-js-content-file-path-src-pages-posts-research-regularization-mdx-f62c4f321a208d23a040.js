"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[6964],{3962:function(e,t){t.A="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iaXNvLTg4NTktMSI/Pgo8IS0tIEdlbmVyYXRvcjogQWRvYmUgSWxsdXN0cmF0b3IgMTYuMC4wLCBTVkcgRXhwb3J0IFBsdWctSW4gLiBTVkcgVmVyc2lvbjogNi4wMCBCdWlsZCAwKSAgLS0+CjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+CjxzdmcgdmVyc2lvbj0iMS4xIiBpZD0iQ2FwYV8xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB4PSIwcHgiIHk9IjBweCIKCSB3aWR0aD0iNDE2Ljk3OXB4IiBoZWlnaHQ9IjQxNi45NzlweCIgdmlld0JveD0iMCAwIDQxNi45NzkgNDE2Ljk3OSIgc3R5bGU9ImVuYWJsZS1iYWNrZ3JvdW5kOm5ldyAwIDAgNDE2Ljk3OSA0MTYuOTc5OyIKCSB4bWw6c3BhY2U9InByZXNlcnZlIj4KPGc+Cgk8cGF0aCBkPSJNMzU2LjAwNCw2MS4xNTZjLTgxLjM3LTgxLjQ3LTIxMy4zNzctODEuNTUxLTI5NC44NDgtMC4xODJjLTgxLjQ3LDgxLjM3MS04MS41NTIsMjEzLjM3OS0wLjE4MSwyOTQuODUKCQljODEuMzY5LDgxLjQ3LDIxMy4zNzgsODEuNTUxLDI5NC44NDksMC4xODFDNDM3LjI5MywyNzQuNjM2LDQzNy4zNzUsMTQyLjYyNiwzNTYuMDA0LDYxLjE1NnogTTIzNy42LDM0MC43ODYKCQljMCwzLjIxNy0yLjYwNyw1LjgyMi01LjgyMiw1LjgyMmgtNDYuNTc2Yy0zLjIxNSwwLTUuODIyLTIuNjA1LTUuODIyLTUuODIyVjE2Ny44ODVjMC0zLjIxNywyLjYwNy01LjgyMiw1LjgyMi01LjgyMmg0Ni41NzYKCQljMy4yMTUsMCw1LjgyMiwyLjYwNCw1LjgyMiw1LjgyMlYzNDAuNzg2eiBNMjA4LjQ5LDEzNy45MDFjLTE4LjYxOCwwLTMzLjc2Ni0xNS4xNDYtMzMuNzY2LTMzLjc2NQoJCWMwLTE4LjYxNywxNS4xNDctMzMuNzY2LDMzLjc2Ni0zMy43NjZjMTguNjE5LDAsMzMuNzY2LDE1LjE0OCwzMy43NjYsMzMuNzY2QzI0Mi4yNTYsMTIyLjc1NSwyMjcuMTA3LDEzNy45MDEsMjA4LjQ5LDEzNy45MDF6Ii8+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPC9zdmc+Cg=="},66501:function(e,t,a){a.d(t,{A:function(){return l}});var n=a(96540),i=a(3962),r="styles-module--tooltiptext--a263b";var l=e=>{let{text:t,isBadge:a=!1}=e;const{0:l,1:o}=(0,n.useState)(!1),s=(0,n.useRef)(null);return(0,n.useEffect)((()=>{function e(e){s.current&&!s.current.contains(e.target)&&o(!1)}return document.addEventListener("click",e),()=>{document.removeEventListener("click",e)}}),[]),n.createElement("span",{className:"styles-module--tooltipWrapper--75ebf",ref:s},n.createElement("img",{id:a?"styles-module--infoBadge--e3d66":"styles-module--info--26c1f",src:i.A,alt:"info",onClick:e=>{e.stopPropagation(),o((e=>!e))}}),n.createElement("span",{className:l?`${r} styles-module--visible--c063c`:r},t))}},95063:function(e,t,a){a.r(t),a.d(t,{Head:function(){return C},PostTemplate:function(){return H},default:function(){return _}});var n=a(54506),i=a(28453),r=a(96540),l=a(66501),o=a(16886),s=(a(46295),a(96098));function c(e){const t=Object.assign({p:"p",h2:"h2",a:"a",span:"span",ul:"ul",li:"li",h3:"h3",strong:"strong",ol:"ol",hr:"hr"},(0,i.RP)(),e.components);return r.createElement(r.Fragment,null,"\n",r.createElement("br"),"\n","\n",r.createElement(t.p,null,"Regularization is a fundamental concept in machine learning aimed at preventing models from becoming overly tailored to training data. This tailoring, typically referred to as ",r.createElement(o.A,null,"overfitting"),", happens when a model memorizes the noise or random fluctuations in the training dataset rather than learning the underlying signal. Regularization methods limit a model's capacity by penalizing overly large parameters or adding constraints, which in turn promotes better generalization and performance on unseen data."),"\n",r.createElement(t.p,null,"In the context of linear models, such as linear or logistic regression, traditional unconstrained optimization can yield solutions that fit the training data perfectly but fail to capture the broader patterns. The power of regularization lies in curbing model complexity — or, in probabilistic terms, in discouraging extreme parameter values. Several modern machine learning systems, ranging from ",r.createElement(o.A,null,"deep neural networks")," to ",r.createElement(o.A,null,"support vector machines"),", rely heavily on regularization techniques to achieve competitive and stable results. Before discussing the mechanics of common regularization techniques (Ridge, Lasso, ElasticNet), we will briefly touch on the phenomenon of overfitting/underfitting and the bias-variance tradeoff, which together form the motivation for using these methods."),"\n",r.createElement(t.h2,{id:"overfitting-and-underfitting",style:{position:"relative"}},r.createElement(t.a,{href:"#overfitting-and-underfitting","aria-label":"overfitting and underfitting permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Overfitting and underfitting"),"\n",r.createElement(t.p,null,"Overfitting manifests when a model fits the ",r.createElement(o.A,null,"noise")," or random idiosyncrasies in the training dataset, leading to impressive training performance but poor performance on new data. Underfitting represents the opposite extreme: the model is too simple to capture the complexity of the data, and it performs poorly even on the training set."),"\n",r.createElement(t.p,null,"A common illustration of this is polynomial regression. A very high-degree polynomial might pass through nearly every training point, thereby minimizing training error but capturing noise rather than the true underlying function (overfitting). Meanwhile, a straight line might be too simple to capture the curvature present in the data (underfitting)."),"\n",r.createElement(t.p,null,"In practice, the cost of overfitting can be severe. A highly overfitted model might:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"Fail to generalize to real-world scenarios."),"\n",r.createElement(t.li,null,"Provide misleadingly low training errors, inspiring false confidence."),"\n",r.createElement(t.li,null,"Require more complex computational resources to maintain unnecessarily large numbers of parameters."),"\n"),"\n",r.createElement(t.p,null,"Balancing model complexity is therefore critical. This is where regularization steps in: it adds additional terms or constraints to the optimization objective, effectively nudging model parameters toward simpler (often smaller in magnitude) solutions."),"\n",r.createElement(t.h2,{id:"gradient-descent-intuition",style:{position:"relative"}},r.createElement(t.a,{href:"#gradient-descent-intuition","aria-label":"gradient descent intuition permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Gradient descent intuition"),"\n",r.createElement(t.p,null,"Regularization can be incorporated into most optimization methods, including the very commonly used gradient descent. Let's recall the cost function and how gradient descent updates typically work."),"\n",r.createElement(t.p,null,"When training a parametric model — such as a linear regressor with parameters ",r.createElement(s.A,{text:"\\( \\boldsymbol{\\theta} \\)"})," — we often define a cost function ",r.createElement(s.A,{text:"\\( J(\\boldsymbol{\\theta}) \\)"})," based on the sum of squared errors or negative log-likelihood, depending on the task. Gradient descent finds a (local) minimum of this cost by iteratively updating each parameter:"),"\n",r.createElement(s.A,{text:"\\[\n\\boldsymbol{\\theta} := \\boldsymbol{\\theta} - \\eta \\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta}),\n\\]"}),"\n",r.createElement(t.p,null,"where ",r.createElement(s.A,{text:"\\( \\eta \\)"})," is the learning rate and ",r.createElement(s.A,{text:"\\( \\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta}) \\)"})," is the gradient of the cost with respect to ",r.createElement(s.A,{text:"\\( \\boldsymbol{\\theta} \\)"}),"."),"\n",r.createElement(t.h3,{id:"role-of-gradients-in-parameter-updates",style:{position:"relative"}},r.createElement(t.a,{href:"#role-of-gradients-in-parameter-updates","aria-label":"role of gradients in parameter updates permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Role of gradients in parameter updates"),"\n",r.createElement(t.p,null,"The gradient ",r.createElement(s.A,{text:"\\( \\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta}) \\)"})," points in the direction of steepest ascent of the cost function; hence, subtracting it moves us toward a (local) minimum. In unregularized linear regression, this cost function might be:"),"\n",r.createElement(s.A,{text:"\\[\nJ(\\boldsymbol{\\theta}) = \\frac{1}{m} \\sum_{i=1}^{m} \\bigl(h_{\\boldsymbol{\\theta}}(\\mathbf{x}_i) - y_i\\bigr)^2,\n\\]"}),"\n",r.createElement(t.p,null,"where ",r.createElement(s.A,{text:"\\( m \\)"})," is the number of training samples, ",r.createElement(s.A,{text:"\\( \\mathbf{x}_i \\)"})," is the feature vector for the ",r.createElement(s.A,{text:"\\( i \\)"}),"-th sample, and ",r.createElement(s.A,{text:"\\( h_{\\boldsymbol{\\theta}}(\\mathbf{x}_i) \\)"})," is the model's prediction."),"\n",r.createElement(t.h3,{id:"limitations-when-models-have-too-many-parameters",style:{position:"relative"}},r.createElement(t.a,{href:"#limitations-when-models-have-too-many-parameters","aria-label":"limitations when models have too many parameters permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Limitations when models have too many parameters"),"\n",r.createElement(t.p,null,"When the number of parameters is large and the data is insufficient to constrain them properly, gradient descent (and other optimization algorithms) can converge to parameter configurations that appear optimal for the training data but do not generalize. This is especially prevalent when using complex models (e.g., high-degree polynomials, deep neural networks, or very flexible kernels in SVM). Regularization modifies the cost function to mitigate this effect."),"\n",r.createElement(t.h2,{id:"bias-variance-tradeoff",style:{position:"relative"}},r.createElement(t.a,{href:"#bias-variance-tradeoff","aria-label":"bias variance tradeoff permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Bias-variance tradeoff"),"\n",r.createElement(t.p,null,"The bias-variance tradeoff is a cornerstone of understanding how regularization works. ",r.createElement(o.A,null,"Bias")," corresponds to the difference between the average model prediction and the true data distribution; ",r.createElement(o.A,null,"variance")," refers to how sensitive the model is to fluctuations in the training set."),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"High bias"),": The model is oversimplified and does not capture the structure of the data (underfitting)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"High variance"),": The model is too complex and fits random fluctuations in the training data rather than the true pattern (overfitting)."),"\n"),"\n",r.createElement(t.p,null,'By penalizing large parameter values, regularization generally reduces variance but may increase bias slightly. The net effect is often beneficial, as slightly higher bias is a small price to pay for drastically lower variance. Modern best practices, including cross-validation, help practitioners choose a regularization strength that balances bias and variance effectively (Hastie and gang, "The Elements of Statistical Learning").'),"\n",r.createElement(t.h2,{id:"ridge-l2-regularization",style:{position:"relative"}},r.createElement(t.a,{href:"#ridge-l2-regularization","aria-label":"ridge l2 regularization permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Ridge (l2) regularization"),"\n",r.createElement(t.p,null,"Ridge regularization (sometimes known as Tikhonov regularization in statistics) adds an ",r.createElement(s.A,{text:"\\( L_2 \\)"})," penalty on parameter magnitude to the cost function."),"\n",r.createElement(t.h3,{id:"definition-and-mathematical-formulation",style:{position:"relative"}},r.createElement(t.a,{href:"#definition-and-mathematical-formulation","aria-label":"definition and mathematical formulation permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Definition and mathematical formulation"),"\n",r.createElement(t.p,null,"The ridge regularization term is:"),"\n",r.createElement(s.A,{text:"\\[\n\\lambda \\sum_{j=1}^n \\theta_j^2,\n\\]"}),"\n",r.createElement(t.p,null,"where ",r.createElement(s.A,{text:"\\( \\lambda \\)"})," (lambda) is a hyperparameter that controls the strength of regularization. If our original cost function was ",r.createElement(s.A,{text:"\\( J(\\boldsymbol{\\theta}) \\)"}),", then the ridge-regularized cost function can be written as:"),"\n",r.createElement(s.A,{text:"\\[\nJ_{ridge}(\\boldsymbol{\\theta}) = J(\\boldsymbol{\\theta}) + \\lambda \\sum_{j=1}^n \\theta_j^2.\n\\]"}),"\n",r.createElement(t.p,null,"Here, ",r.createElement(s.A,{text:"\\( n \\)"})," is the number of parameters (excluding, typically, a bias/intercept term which we do not penalize in many implementations)."),"\n",r.createElement(t.h3,{id:"effect-on-model-parameters-shrinkage",style:{position:"relative"}},r.createElement(t.a,{href:"#effect-on-model-parameters-shrinkage","aria-label":"effect on model parameters shrinkage permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Effect on model parameters (shrinkage)"),"\n",r.createElement(t.p,null,"The ",r.createElement(s.A,{text:"\\( L_2 \\)"}),' penalty encourages smaller parameter values, effectively "shrinking" them toward zero. Unlike ',r.createElement(s.A,{text:"\\( L_1 \\)"})," methods, ridge does not enforce exact zeros in the coefficients but instead spreads out the penalty more smoothly. This shrinkage can alleviate the problem of high variance and produce a model that generalizes well."),"\n",r.createElement(t.h3,{id:"ridge-penalty-in-gradient-descent",style:{position:"relative"}},r.createElement(t.a,{href:"#ridge-penalty-in-gradient-descent","aria-label":"ridge penalty in gradient descent permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Ridge penalty in gradient descent"),"\n",r.createElement(t.p,null,"Incorporating ridge regularization into gradient descent modifies the gradient as follows:"),"\n",r.createElement(s.A,{text:"\\[\n\\nabla_{\\boldsymbol{\\theta}} J_{ridge}(\\boldsymbol{\\theta}) = \\nabla_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}) + 2 \\lambda \\boldsymbol{\\theta}.\n\\]"}),"\n",r.createElement(t.p,null,"The term ",r.createElement(s.A,{text:"\\( 2 \\lambda \\boldsymbol{\\theta} \\)"})," (or sometimes ",r.createElement(s.A,{text:"\\( \\lambda \\boldsymbol{\\theta} \\)"})," depending on the constant factors used in definitions) systematically reduces parameter magnitudes at each step, helping to control overfitting."),"\n",r.createElement(t.h3,{id:"practical-considerations-in-hyperparameter-tuning",style:{position:"relative"}},r.createElement(t.a,{href:"#practical-considerations-in-hyperparameter-tuning","aria-label":"practical considerations in hyperparameter tuning permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Practical considerations in hyperparameter tuning"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(s.A,{text:"\\( \\lambda \\)"})," is chosen via model selection methods such as ",r.createElement(o.A,null,"cross-validation"),"."),"\n",r.createElement(t.li,null,"If ",r.createElement(s.A,{text:"\\( \\lambda \\)"})," is too large, the model parameters are shrunk too aggressively, which may underfit."),"\n",r.createElement(t.li,null,"If ",r.createElement(s.A,{text:"\\( \\lambda \\)"})," is too small, ridge behaves much like ordinary least squares, risking overfitting."),"\n"),"\n",r.createElement(t.p,null,"Below is a simple Python snippet illustrating the use of ridge regression in scikit-learn:"),"\n",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">&lt;Code text={`\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\nX = np.random.rand(100, 5)\ny = 2.5*X[:, 0] - 1.3*X[:, 1] + 0.5*np.random.randn(100)\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n\nparam_grid = {\'alpha\': [0.001, 0.01, 0.1, 1, 10, 100]}\nridge = Ridge()\ngrid = GridSearchCV(ridge, param_grid, cv=5)\ngrid.fit(X_train, y_train)\n\nprint("Best alpha:", grid.best_params_)\nprint("Validation score:", grid.best_score_)\n`}/></code></pre></div>'}}),"\n",r.createElement(t.h2,{id:"lasso-l1-regularization",style:{position:"relative"}},r.createElement(t.a,{href:"#lasso-l1-regularization","aria-label":"lasso l1 regularization permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Lasso (l1) regularization"),"\n",r.createElement(t.p,null,"Lasso regularization, short for ",r.createElement(o.A,null,"Least Absolute Shrinkage and Selection Operator"),", adds an ",r.createElement(s.A,{text:"\\( L_1 \\)"})," penalty on the parameter vector. Lasso is particularly well-known for its ability to produce sparse solutions (Tibshirani, 1996)."),"\n",r.createElement(t.h3,{id:"definition-and-mathematical-formulation-1",style:{position:"relative"}},r.createElement(t.a,{href:"#definition-and-mathematical-formulation-1","aria-label":"definition and mathematical formulation 1 permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Definition and mathematical formulation"),"\n",r.createElement(t.p,null,"The lasso penalty is:"),"\n",r.createElement(s.A,{text:"\\[\n\\lambda \\sum_{j=1}^n |\\theta_j|.\n\\]"}),"\n",r.createElement(t.p,null,"Incorporating it into the cost function:"),"\n",r.createElement(s.A,{text:"\\[\nJ_{lasso}(\\boldsymbol{\\theta}) = J(\\boldsymbol{\\theta}) + \\lambda \\sum_{j=1}^n |\\theta_j|.\n\\]"}),"\n",r.createElement(t.h3,{id:"sparsity-and-feature-selection-properties",style:{position:"relative"}},r.createElement(t.a,{href:"#sparsity-and-feature-selection-properties","aria-label":"sparsity and feature selection properties permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Sparsity and feature selection properties"),"\n",r.createElement(t.p,null,"A prominent effect of ",r.createElement(s.A,{text:"\\( L_1 \\)"})," regularization is that some parameters tend to become exactly zero if ",r.createElement(s.A,{text:"\\( \\lambda \\)"})," is sufficiently large. This characteristic naturally selects features by eliminating those that do not contribute meaningfully to the predictive performance — making lasso popular in high-dimensional settings such as genomics or natural language processing."),"\n",r.createElement(t.h3,{id:"l1-penalty-in-gradient-descent",style:{position:"relative"}},r.createElement(t.a,{href:"#l1-penalty-in-gradient-descent","aria-label":"l1 penalty in gradient descent permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"L1 penalty in gradient descent"),"\n",r.createElement(t.p,null,"During gradient descent, the absolute value function ",r.createElement(s.A,{text:"\\( |\\theta_j| \\)"})," introduces a discontinuity at zero, making it non-differentiable at that point. In practice, implementations often use subgradient methods or specialized optimizers (e.g., coordinate descent) to handle the ",r.createElement(s.A,{text:"\\( L_1 \\)"})," term."),"\n",r.createElement(t.p,null,"A coordinate descent procedure for lasso systematically updates each parameter by solving a one-dimensional minimization problem that leads to a ",r.createElement(l.A,{text:"Soft thresholding operator"})," effect on the parameters."),"\n",r.createElement(t.h3,{id:"practical-considerations-in-hyperparameter-tuning-1",style:{position:"relative"}},r.createElement(t.a,{href:"#practical-considerations-in-hyperparameter-tuning-1","aria-label":"practical considerations in hyperparameter tuning 1 permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Practical considerations in hyperparameter tuning"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"Lasso's sparsity effect can be highly beneficial when you suspect many redundant or irrelevant features."),"\n",r.createElement(t.li,null,"As ",r.createElement(s.A,{text:"\\( \\lambda \\)"})," increases, more coefficients are driven to zero."),"\n",r.createElement(t.li,null,"Strong regularization can lead to an underfit model if set too aggressively."),"\n",r.createElement(t.li,null,"In scikit-learn, ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">Lasso</code>'}})," can be used similarly to ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">Ridge</code>'}})," with a range of alpha values tested via cross-validation."),"\n"),"\n",r.createElement(t.h2,{id:"elasticnet",style:{position:"relative"}},r.createElement(t.a,{href:"#elasticnet","aria-label":"elasticnet permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"ElasticNet"),"\n",r.createElement(t.p,null,"ElasticNet (Zou and Hastie, 2005) combines both ",r.createElement(s.A,{text:"\\( L_1 \\)"})," and ",r.createElement(s.A,{text:"\\( L_2 \\)"})," penalties. It aims to leverage the benefits of lasso's sparsity and ridge's stability."),"\n",r.createElement(t.h3,{id:"combining-l1-and-l2-penalties",style:{position:"relative"}},r.createElement(t.a,{href:"#combining-l1-and-l2-penalties","aria-label":"combining l1 and l2 penalties permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Combining l1 and l2 penalties"),"\n",r.createElement(t.p,null,"The ElasticNet regularization term is typically written as:"),"\n",r.createElement(s.A,{text:"\\[\n\\lambda \\Bigl(\\alpha \\sum_{j=1}^n |\\theta_j| + (1 - \\alpha) \\sum_{j=1}^n \\theta_j^2\\Bigr),\n\\]"}),"\n",r.createElement(t.p,null,"where ",r.createElement(s.A,{text:"\\( 0 \\le \\alpha \\le 1 \\)"})," controls the ratio between the ",r.createElement(s.A,{text:"\\( L_1 \\)"})," and ",r.createElement(s.A,{text:"\\( L_2 \\)"})," components, and ",r.createElement(s.A,{text:"\\( \\lambda \\)"})," scales the overall strength of regularization."),"\n",r.createElement(t.h3,{id:"advantages-of-the-elasticnet-approach",style:{position:"relative"}},r.createElement(t.a,{href:"#advantages-of-the-elasticnet-approach","aria-label":"advantages of the elasticnet approach permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Advantages of the ElasticNet approach"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(s.A,{text:"\\( L_2 \\)"})," part helps group correlated features, distributing the coefficient weights among them more evenly rather than zeroing one feature arbitrarily."),"\n",r.createElement(t.li,null,r.createElement(s.A,{text:"\\( L_1 \\)"})," part still promotes sparsity, though often less aggressively than a pure lasso."),"\n",r.createElement(t.li,null,"Works well in scenarios where you have many correlated predictors."),"\n"),"\n",r.createElement(t.h3,{id:"selecting-the-mixing-parameter",style:{position:"relative"}},r.createElement(t.a,{href:"#selecting-the-mixing-parameter","aria-label":"selecting the mixing parameter permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Selecting the mixing parameter"),"\n",r.createElement(t.p,null,"The parameter ",r.createElement(s.A,{text:"\\( \\alpha \\)"})," is often chosen through grid search in conjunction with ",r.createElement(s.A,{text:"\\( \\lambda \\)"}),". Researchers frequently pick a few candidate values for ",r.createElement(s.A,{text:"\\( \\alpha \\)"})," (e.g., 0.1, 0.5, 0.9) and then search across a range of ",r.createElement(s.A,{text:"\\( \\lambda \\)"})," values."),"\n",r.createElement(t.h3,{id:"when-to-use-elasticnet",style:{position:"relative"}},r.createElement(t.a,{href:"#when-to-use-elasticnet","aria-label":"when to use elasticnet permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"When to use ElasticNet"),"\n",r.createElement(t.p,null,"ElasticNet is a strong choice when:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"You suspect your data contains multiple correlated features."),"\n",r.createElement(t.li,null,"Pure lasso might arbitrarily drop one of the correlated features (or keep them all if ",r.createElement(s.A,{text:"\\( \\lambda \\)"})," is small)."),"\n",r.createElement(t.li,null,"You still want some feature selection (or partial feature selection) but with added stability over lasso alone."),"\n"),"\n",r.createElement(t.h2,{id:"regularization-in-matrix-form",style:{position:"relative"}},r.createElement(t.a,{href:"#regularization-in-matrix-form","aria-label":"regularization in matrix form permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Regularization in matrix form"),"\n",r.createElement(t.p,null,"For linear regression, an elegant view of regularization arises by revisiting the normal equation."),"\n",r.createElement(t.h3,{id:"revisiting-the-normal-equation",style:{position:"relative"}},r.createElement(t.a,{href:"#revisiting-the-normal-equation","aria-label":"revisiting the normal equation permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Revisiting the normal equation"),"\n",r.createElement(t.p,null,"Recall that in ordinary least squares (OLS), we solve:"),"\n",r.createElement(s.A,{text:"\\[\n\\boldsymbol{\\theta} = (X^\\top X)^{-1} X^\\top \\mathbf{y},\n\\]"}),"\n",r.createElement(t.p,null,"where ",r.createElement(s.A,{text:"\\( X \\)"})," is an ",r.createElement(s.A,{text:"\\( m \\times n \\)"})," design matrix and ",r.createElement(s.A,{text:"\\( \\mathbf{y} \\)"})," is the vector of observations. This solution can be ill-defined or numerically unstable if ",r.createElement(s.A,{text:"\\( X^\\top X \\)"})," is close to singular (often due to highly correlated features or ",r.createElement(l.A,{text:"Multicollinearity"}),")."),"\n",r.createElement(t.h3,{id:"adding-the-regularization-term-to-the-cost-function",style:{position:"relative"}},r.createElement(t.a,{href:"#adding-the-regularization-term-to-the-cost-function","aria-label":"adding the regularization term to the cost function permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Adding the regularization term to the cost function"),"\n",r.createElement(t.p,null,"Ridge regression modifies the OLS cost function by adding ",r.createElement(s.A,{text:"\\( \\lambda \\|\\boldsymbol{\\theta}\\|^2 \\)"}),":"),"\n",r.createElement(s.A,{text:"\\[\n\\min_{\\boldsymbol{\\theta}} \\bigl\\|X \\boldsymbol{\\theta} - \\mathbf{y}\\bigr\\|^2 + \\lambda \\|\\boldsymbol{\\theta}\\|^2.\n\\]"}),"\n",r.createElement(t.h3,{id:"deriving-the-regularized-normal-equation",style:{position:"relative"}},r.createElement(t.a,{href:"#deriving-the-regularized-normal-equation","aria-label":"deriving the regularized normal equation permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Deriving the regularized normal equation"),"\n",r.createElement(t.p,null,"Taking derivatives and setting them to zero yields:"),"\n",r.createElement(s.A,{text:"\\[\n\\boldsymbol{\\theta} = \\bigl(X^\\top X + \\lambda I\\bigr)^{-1} X^\\top \\mathbf{y}.\n\\]"}),"\n",r.createElement(t.p,null,"Here, ",r.createElement(s.A,{text:"\\( \\lambda I \\)"}),' is a diagonal matrix that "stabilizes" ',r.createElement(s.A,{text:"\\( X^\\top X \\)"})," by shifting its eigenvalues. This improved conditioning helps avoid exploding coefficients and reduces variance."),"\n",r.createElement(t.h3,{id:"computational-considerations",style:{position:"relative"}},r.createElement(t.a,{href:"#computational-considerations","aria-label":"computational considerations permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Computational considerations"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"Adding ",r.createElement(s.A,{text:"\\( \\lambda I \\)"})," ensures that the matrix is invertible (or at least significantly better conditioned)."),"\n",r.createElement(t.li,null,"In certain high-dimensional applications, direct matrix inversion remains expensive. Iterative methods like gradient descent or coordinate descent are often used instead."),"\n"),"\n",r.createElement(t.h2,{id:"practical-guidelines-and-considerations",style:{position:"relative"}},r.createElement(t.a,{href:"#practical-guidelines-and-considerations","aria-label":"practical guidelines and considerations permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Practical guidelines and considerations"),"\n",r.createElement(t.h3,{id:"choosing-the-right-regularization-technique",style:{position:"relative"}},r.createElement(t.a,{href:"#choosing-the-right-regularization-technique","aria-label":"choosing the right regularization technique permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Choosing the right regularization technique"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Ridge (",r.createElement(s.A,{text:"\\( L_2 \\)"}),"):")," Tends to shrink coefficients smoothly. Effective when you have many features of roughly similar importance."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Lasso (",r.createElement(s.A,{text:"\\( L_1 \\)"}),"):")," Encourages feature selection by driving some coefficients to zero. Particularly helpful if you suspect a number of irrelevant or redundant predictors."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"ElasticNet:")," A flexible approach if you want both the stability of ",r.createElement(s.A,{text:"\\( L_2 \\)"})," and the feature selection effect of ",r.createElement(s.A,{text:"\\( L_1 \\)"}),"."),"\n"),"\n",r.createElement(t.h3,{id:"cross-validation-for-hyperparameter-tuning",style:{position:"relative"}},r.createElement(t.a,{href:"#cross-validation-for-hyperparameter-tuning","aria-label":"cross validation for hyperparameter tuning permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Cross-validation for hyperparameter tuning"),"\n",r.createElement(t.p,null,"Selecting hyperparameters ",r.createElement(s.A,{text:"\\( \\lambda \\)"})," (and ",r.createElement(s.A,{text:"\\( \\alpha \\)"})," in ElasticNet) via cross-validation is common practice. One typical strategy is:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,"Split the data into training and validation folds."),"\n",r.createElement(t.li,null,"Train multiple models with different values of ",r.createElement(s.A,{text:"\\( \\lambda \\)"})," (and ",r.createElement(s.A,{text:"\\( \\alpha \\)"})," for ElasticNet)."),"\n",r.createElement(t.li,null,"Evaluate each model on the validation fold (or folds)."),"\n",r.createElement(t.li,null,"Choose the hyperparameters that yield the best performance metric (e.g., mean squared error)."),"\n"),"\n",r.createElement(t.h3,{id:"impact-on-interpretability",style:{position:"relative"}},r.createElement(t.a,{href:"#impact-on-interpretability","aria-label":"impact on interpretability permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Impact on interpretability"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Ridge:")," May preserve all features' coefficients, but each is dampened. Interpretability remains straightforward if the coefficient magnitudes are not too large."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Lasso:")," Produces sparse coefficients, simplifying interpretation. A zero coefficient means the corresponding feature is effectively removed."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"ElasticNet:")," Balances interpretability and stability; you may still get sparse solutions, but correlated features are handled more smoothly."),"\n"),"\n",r.createElement(t.p,null,"Sometimes, especially in scientific applications, the interpretability of coefficients is as important as raw predictive performance. If interpretability and dimensionality reduction are priorities, lasso or ElasticNet are often recommended."),"\n",r.createElement(t.hr),"\n",r.createElement(t.p,null,"In summary, regularization is a versatile and vital tool for controlling complexity in machine learning models, mitigating overfitting, and improving generalization. By penalizing large weights (either in ",r.createElement(s.A,{text:"\\( L_1 \\)"})," or ",r.createElement(s.A,{text:"\\( L_2 \\)"})," form, or a combination of both), these methods ensure a balance between fitting power and generalization. With careful hyperparameter tuning and awareness of each approach's strengths and weaknesses, practitioners can unlock models that are both accurate and robust — a pillar of modern data science and machine learning."))}var m=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,i.RP)(),e.components);return t?r.createElement(t,e,r.createElement(c,e)):c(e)},h=a(36710),d=a(58481),u=a.n(d),g=a(36310),p=a(87245),f=a(27042),v=a(59849),y=a(5591),b=a(61122),E=a(9219),w=a(33203),x=a(95751),z=a(94328),S=a(80791),L=a(78137);const M=e=>{let{toc:t}=e;if(!t||!t.items)return null;return r.createElement("nav",{className:S.R},r.createElement("ul",null,t.items.map(((e,t)=>r.createElement("li",{key:t},r.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const a=t.replace("#",""),n=document.getElementById(a);n&&n.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&r.createElement(M,{toc:{items:e.items}}))))))};function H(e){let{data:{mdx:t,allMdx:l,allPostImages:o},children:s}=e;const{frontmatter:c,body:m,tableOfContents:h}=t,d=c.index,v=c.slug.split("/")[1],S=l.nodes.filter((e=>e.frontmatter.slug.includes(`/${v}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),H=S.findIndex((e=>e.frontmatter.index===d)),_=S[H+1],C=S[H-1],A=c.slug.replace(/\/$/,""),N=/[^/]*$/.exec(A)[0],T=`posts/${v}/content/${N}/`,{0:I,1:j}=(0,r.useState)(c.flagWideLayoutByDefault),{0:k,1:V}=(0,r.useState)(!1);var B;(0,r.useEffect)((()=>{V(!0);const e=setTimeout((()=>V(!1)),340);return()=>clearTimeout(e)}),[I]),"adventures"===v?B=E.cb:"research"===v?B=E.Qh:"thoughts"===v&&(B=E.T6);const P=u()(m).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,D=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),a=e%60;return a<=30?`~${t}${a>0?".5":""} h`:`~${t+1} h`}(Math.ceil(P/B)+(c.extraReadTimeMin||0)),O=[{flag:c.flagDraft,component:()=>Promise.all([a.e(3231),a.e(8809)]).then(a.bind(a,28809))},{flag:c.flagMindfuckery,component:()=>Promise.all([a.e(3231),a.e(2471)]).then(a.bind(a,67709))},{flag:c.flagRewrite,component:()=>Promise.all([a.e(3231),a.e(6764)]).then(a.bind(a,62002))},{flag:c.flagOffensive,component:()=>Promise.all([a.e(3231),a.e(2443)]).then(a.bind(a,17681))},{flag:c.flagProfane,component:()=>Promise.all([a.e(3231),a.e(8048)]).then(a.bind(a,53286))},{flag:c.flagMultilingual,component:()=>Promise.all([a.e(3231),a.e(4069)]).then(a.bind(a,78831))},{flag:c.flagUnreliably,component:()=>Promise.all([a.e(3231),a.e(3417)]).then(a.bind(a,8179))},{flag:c.flagPolitical,component:()=>Promise.all([a.e(3231),a.e(5195)]).then(a.bind(a,30433))},{flag:c.flagCognitohazard,component:()=>Promise.all([a.e(3231),a.e(3175)]).then(a.bind(a,8413))},{flag:c.flagHidden,component:()=>Promise.all([a.e(3231),a.e(9556)]).then(a.bind(a,14794))}],{0:R,1:G}=(0,r.useState)([]);return(0,r.useEffect)((()=>{O.forEach((e=>{let{flag:t,component:a}=e;t&&a().then((e=>{G((t=>[].concat((0,n.A)(t),[e.default])))}))}))}),[]),r.createElement(f.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},r.createElement(y.A,{postNumber:c.index,date:c.date,updated:c.updated,readTime:D,difficulty:c.difficultyLevel,title:c.title,desc:c.desc,banner:c.banner,section:v,postKey:N,isMindfuckery:c.flagMindfuckery,mainTag:c.mainTag}),r.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},c.otherTags.map(((e,t)=>r.createElement("span",{key:t,className:`noselect ${L.MW}`,style:{margin:"0 5px 5px 0"}},e)))),r.createElement("div",{class:"postBody"},r.createElement(M,{toc:h})),r.createElement("br"),r.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},r.createElement(f.P.button,{class:"noselect",className:z.pb,id:z.xG,onClick:()=>{j(!I)},whileTap:{scale:.93}},r.createElement(f.P.div,{className:x.DJ,key:I,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},I?"Switch to default layout":"Switch to wide layout"))),r.createElement("br"),r.createElement("div",{class:"postBody",style:{margin:I?"0 -14%":"",maxWidth:I?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},r.createElement("div",{className:`${z.P_} ${k?z.Xn:z.qG}`},R.map(((e,t)=>r.createElement(e,{key:t}))),c.indexCourse?r.createElement(w.A,{index:c.indexCourse,category:c.courseCategoryName}):"",r.createElement(g.Z.Provider,{value:{images:o.nodes,basePath:T.replace(/\/$/,"")+"/"}},r.createElement(i.xA,{components:{Image:p.A}},s)))),r.createElement(b.A,{nextPost:_,lastPost:C,keyCurrent:N,section:v}))}function _(e){return r.createElement(H,e,r.createElement(m,e))}function C(e){var t,a,n,i,l;let{data:o}=e;const{frontmatter:s}=o.mdx,c=s.titleSEO||s.title,m=s.titleOG||c,d=s.titleTwitter||c,u=s.descSEO||s.desc,g=s.descOG||u,p=s.descTwitter||u,f=s.schemaType||"BlogPosting",y=s.keywordsSEO,b=s.date,E=s.updated||b,w=s.imageOG||(null===(t=s.banner)||void 0===t||null===(a=t.childImageSharp)||void 0===a||null===(n=a.gatsbyImageData)||void 0===n||null===(i=n.images)||void 0===i||null===(l=i.fallback)||void 0===l?void 0:l.src),x=s.imageAltOG||g,z=s.imageTwitter||w,S=s.imageAltTwitter||p,L=s.canonicalURL,M=s.flagHidden||!1,H=s.mainTag||"Posts",_=s.slug.split("/")[1]||"posts",{siteUrl:C}=(0,h.Q)(),A={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:C},{"@type":"ListItem",position:2,name:H,item:`${C}/${s.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${C}${s.slug}`}]};return r.createElement(v.A,{title:c+" - avrtt.blog",titleOG:m,titleTwitter:d,description:u,descriptionOG:g,descriptionTwitter:p,schemaType:f,keywords:y,datePublished:b,dateModified:E,imageOG:w,imageAltOG:x,imageTwitter:z,imageAltTwitter:S,canonicalUrl:L,flagHidden:M,mainTag:H,section:_,type:"article"},r.createElement("script",{type:"application/ld+json"},JSON.stringify(A)))}},96098:function(e,t,a){var n=a(96540),i=a(7978);t.A=e=>{let{text:t}=e;return n.createElement(i.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-js-content-file-path-src-pages-posts-research-regularization-mdx-f62c4f321a208d23a040.js.map