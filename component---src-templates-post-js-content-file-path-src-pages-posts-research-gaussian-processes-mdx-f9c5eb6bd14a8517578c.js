"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[9029],{17935:function(e,t,n){n.r(t),n.d(t,{Head:function(){return T},PostTemplate:function(){return H},default:function(){return z}});var a=n(54506),i=n(28453),r=n(96540),l=n(16886),o=n(46295),s=n(96098);function c(e){const t=Object.assign({p:"p",ul:"ul",li:"li",h2:"h2",a:"a",span:"span",h3:"h3",ol:"ol",strong:"strong"},(0,i.RP)(),e.components),{Image:n}=t;return n||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),r.createElement(r.Fragment,null,"\n",r.createElement("br"),"\n","\n","\n",r.createElement(t.p,null,"Gaussian processes (GPs) are a powerful framework for modeling functions in a Bayesian, non-parametric way, providing a principled mechanism to handle uncertainty. Instead of prespecifying a fixed number of parameters — like the weights of a neural network — Gaussian processes place a distribution over functions directly. Any finite set of points evaluated from a GP follows a multivariate normal distribution, governed by a mean function and a covariance (kernel) function."),"\n",r.createElement(t.p,null,"Because of their flexibility, GPs have been successfully used in regression, classification, optimization (e.g., Bayesian optimization), and beyond, including active learning, hyperparameter tuning, and even aspects of reinforcement learning. GPs are especially appealing in situations where data is not abundant or one needs interpretable uncertainty estimates. Historically, they gained significant traction in machine learning research following foundational works such as Radford Neal (1996) and the influential text by Rasmussen and Williams (2006)."),"\n",r.createElement(t.p,null,"In this article, we will dive deep into Gaussian processes, focusing particularly on:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"Their motivation and key advantages over classic parametric models."),"\n",r.createElement(t.li,null,"The theoretical foundations of how they handle data and encode uncertainty."),"\n",r.createElement(t.li,null,"How to specify and tune covariance kernels."),"\n",r.createElement(t.li,null,"How to derive the posterior predictive distribution in regression tasks."),"\n",r.createElement(t.li,null,"How to implement a GP from scratch."),"\n",r.createElement(t.li,null,"How to implement a GP using the modern gpytorch library."),"\n",r.createElement(t.li,null,"Pitfalls, initialization strategies, and advanced considerations."),"\n"),"\n",r.createElement(t.p,null,"Throughout, we will maintain an emphasis on balancing clarity with theoretical depth, using American English in an informal yet precise style suitable for scientists and professionals in machine learning who desire a deeper grasp of Gaussian processes."),"\n",r.createElement(t.h2,{id:"basic-concepts",style:{position:"relative"}},r.createElement(t.a,{href:"#basic-concepts","aria-label":"basic concepts permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"basic concepts"),"\n",r.createElement(t.p,null,"Gaussian processes can be viewed as distributions over functions that allow us to reason directly about the function space rather than about parameters such as weights in a parametric model. Here, we introduce the most essential concepts and notations:"),"\n",r.createElement(t.h3,{id:"observed-data-and-missing-inputs",style:{position:"relative"}},r.createElement(t.a,{href:"#observed-data-and-missing-inputs","aria-label":"observed data and missing inputs permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"observed data and missing inputs"),"\n",r.createElement(t.p,null,"Let ",r.createElement(s.A,{text:"\\( \\{(x_i, y_i)\\}_{i=1}^n \\)"})," be a set of observed data points, where ",r.createElement(s.A,{text:"\\( x_i \\)"})," typically belongs to some input space (e.g., ",r.createElement(s.A,{text:"\\( \\mathbb{R}^d \\)"}),") and ",r.createElement(s.A,{text:"\\( y_i \\)"})," are the observed outputs. Often, we think of these outputs as real-valued (for regression problems), but Gaussian processes can also be extended to classification and other domains through more advanced likelihood models (albeit requiring approximate inference in many such cases)."),"\n",r.createElement(t.p,null,"An especially appealing aspect of GPs is that they handle missing inputs naturally. If we have large gaps between observed inputs — or if we want to forecast beyond observed intervals — GPs automatically quantify increased uncertainty in those regions, a property strongly tied to the kernel function."),"\n",r.createElement(t.h3,{id:"prior-functions-and-posterior-functions",style:{position:"relative"}},r.createElement(t.a,{href:"#prior-functions-and-posterior-functions","aria-label":"prior functions and posterior functions permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"prior functions and posterior functions"),"\n",r.createElement(t.p,null,"In the GP framework, we place a prior directly on the space of functions. Concretely, suppose ",r.createElement(s.A,{text:"\\( f(x) \\)"})," is a function we wish to model. Declaring ",r.createElement(s.A,{text:"\\( f(x) \\)"})," to be drawn from a Gaussian process with mean function ",r.createElement(s.A,{text:"\\( m(x) \\)"})," and covariance function ",r.createElement(s.A,{text:"\\( k(x,x') \\)"})," is written as:"),"\n",r.createElement(s.A,{text:"\\[\nf(x) \\sim \\mathcal{GP}\\bigl(m(x), k(x,x')\\bigr).\n\\]"}),"\n",r.createElement(t.p,null,"This statement means that for any finite collection of points ",r.createElement(s.A,{text:"\\( x_1, x_2, \\ldots, x_n \\)"}),", the random variables ",r.createElement(s.A,{text:"\\( f(x_1), f(x_2), \\ldots, f(x_n) \\)"})," follow a multivariate Gaussian distribution whose mean vector and covariance matrix are determined by ",r.createElement(s.A,{text:"\\( m(\\cdot) \\)"})," and ",r.createElement(s.A,{text:"\\( k(\\cdot,\\cdot) \\)"}),"."),"\n",r.createElement(t.p,null,"When we observe data ",r.createElement(s.A,{text:"\\( (x_i, y_i) \\)"}),", we can update our beliefs about ",r.createElement(s.A,{text:"\\( f(x) \\)"})," at any point ",r.createElement(s.A,{text:"\\( x \\)"})," through Bayes' rule. This results in a posterior distribution over functions. In regression settings with Gaussian noise, this posterior is still a Gaussian process whose mean and covariance can be computed analytically, thus yielding predictive means and variances at any test input."),"\n",r.createElement(t.h3,{id:"role-of-uncertainty-epistemic-vs-aleatoric",style:{position:"relative"}},r.createElement(t.a,{href:"#role-of-uncertainty-epistemic-vs-aleatoric","aria-label":"role of uncertainty epistemic vs aleatoric permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"role of uncertainty: epistemic vs. aleatoric"),"\n",r.createElement(t.p,null,"Uncertainty in GPs typically has two main components:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(l.A,null,"Epistemic uncertainty"),", which is reducible as we observe more data. This arises from not knowing the true function. Far away from training data, the GP's epistemic uncertainty grows because there are many plausible function values that fit the data."),"\n",r.createElement(t.li,null,r.createElement(l.A,null,"Aleatoric uncertainty"),", which comes from inherent noise in the observations themselves (e.g., measurement noise). This uncertainty is often irreducible, even if we collect more data under the same noisy conditions."),"\n"),"\n",r.createElement(t.p,null,"By encoding both sources of uncertainty, Gaussian processes provide a coherent picture of where our model is confident or uncertain in its predictions."),"\n",r.createElement(n,{alt:"Illustration of data points with missing intervals",path:"",caption:"Illustration showing observed data and regions with missing inputs, highlighting growing GP uncertainty.",zoom:"false"}),"\n",r.createElement(t.h2,{id:"covariance-functions-and-kernels",style:{position:"relative"}},r.createElement(t.a,{href:"#covariance-functions-and-kernels","aria-label":"covariance functions and kernels permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"covariance functions and kernels"),"\n",r.createElement(t.p,null,"Covariance functions (often called kernels) are the core mechanism by which a Gaussian process encodes assumptions about our function's properties (e.g., smoothness, periodicity, amplitude, etc.). They govern how function values at different inputs are correlated."),"\n",r.createElement(t.h3,{id:"definition-of-covariancekernel-functions",style:{position:"relative"}},r.createElement(t.a,{href:"#definition-of-covariancekernel-functions","aria-label":"definition of covariancekernel functions permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"definition of covariance/kernel functions"),"\n",r.createElement(t.p,null,"A covariance function ",r.createElement(s.A,{text:"\\( k(x,x') \\)"})," must be symmetric and positive semidefinite. In other words, for any finite set of points ",r.createElement(s.A,{text:"\\( x_1,\\dots,x_n \\)"}),", the covariance matrix ",r.createElement(s.A,{text:"\\( K \\)"})," with entries ",r.createElement(s.A,{text:"\\( K_{ij} = k(x_i, x_j) \\)"})," must be symmetric (",r.createElement(s.A,{text:"\\( K_{ij} = K_{ji} \\)"}),") and positive semidefinite (all eigenvalues are nonnegative)."),"\n",r.createElement(t.p,null,"Commonly, we interpret ",r.createElement(s.A,{text:"\\( k(x,x') \\)"})," as defining some measure of similarity or correlation between ",r.createElement(s.A,{text:"\\( x \\)"})," and ",r.createElement(s.A,{text:"\\( x' \\)"}),'. If two inputs are "close," then ',r.createElement(s.A,{text:"\\( k(x,x') \\)"})," is large, implying their function values ",r.createElement(s.A,{text:"\\( f(x) \\)"})," and ",r.createElement(s.A,{text:"\\( f(x') \\)"})," should be highly correlated. Conversely, if they are far apart, ",r.createElement(s.A,{text:"\\( k(x,x') \\)"})," is small, allowing them to vary more independently."),"\n",r.createElement(t.h3,{id:"radial-basis-function-rbf-kernel",style:{position:"relative"}},r.createElement(t.a,{href:"#radial-basis-function-rbf-kernel","aria-label":"radial basis function rbf kernel permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"radial basis function (RBF) kernel"),"\n",r.createElement(t.p,null,"Perhaps the most widely used kernel is the ",r.createElement(l.A,null,"radial basis function (RBF) kernel")," (also known as the Gaussian kernel or the squared exponential kernel). It is often written as:"),"\n",r.createElement(s.A,{text:"\\[\nk_{\\mathrm{RBF}}(x,x') = a^2 \\exp\\!\\Bigl(-\\frac{\\|x - x'\\|^2}{2\\,\\ell^2}\\Bigr),\n\\]"}),"\n",r.createElement(t.p,null,"where:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(s.A,{text:"\\( a \\)"})," (sometimes called ",r.createElement(s.A,{text:"\\( \\sigma \\)"}),") is the amplitude parameter, scaling the overall variance of the function values."),"\n",r.createElement(t.li,null,r.createElement(s.A,{text:"\\( \\ell \\)"})," is the length-scale, controlling how quickly the correlation decreases with distance. A smaller ",r.createElement(s.A,{text:"\\( \\ell \\)"}),' implies "wigglier" functions that can change values rapidly over short distances, whereas a large ',r.createElement(s.A,{text:"\\( \\ell \\)"})," enforces smoother, slowly varying functions."),"\n"),"\n",r.createElement(t.p,null,"The RBF kernel is stationary (depends only on ",r.createElement(s.A,{text:"\\( x-x' \\)"}),") and is infinitely differentiable, encoding the assumption that the modeled function is very smooth. This property can be beneficial for data that is indeed quite smooth but might be too restrictive for some real-world settings (for instance, if the underlying function has sharp transitions or is not so smoothly varying)."),"\n",r.createElement(n,{alt:"Illustration of RBF kernel effect",path:"",caption:"Illustration showing how the RBF kernel yields smoother function samples with increasing length-scale.",zoom:"false"}),"\n",r.createElement(t.h3,{id:"other-common-kernels",style:{position:"relative"}},r.createElement(t.a,{href:"#other-common-kernels","aria-label":"other common kernels permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"other common kernels"),"\n",r.createElement(t.p,null,"Several alternative kernels exist to capture different properties:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,"\n",r.createElement(t.p,null,r.createElement(l.A,null,"Matern kernels"),": A family of kernels parameterized by a smoothness parameter ",r.createElement(s.A,{text:"\\( \\nu \\)"}),". For certain values of ",r.createElement(s.A,{text:"\\( \\nu \\)"}),", the function samples are only ",r.createElement(s.A,{text:"\\( \\nu \\)"}),"-times differentiable. This can model rougher or less smooth functions better than an RBF kernel. The Matern class is often recommended as a default choice if you suspect your data is not extremely smooth."),"\n"),"\n",r.createElement(t.li,null,"\n",r.createElement(t.p,null,r.createElement(l.A,null,"Ornstein-Uhlenbeck kernel"),": Often used for modeling time-series with certain Markov properties, especially in physics/finance contexts for processes with exponential decay correlations."),"\n"),"\n",r.createElement(t.li,null,"\n",r.createElement(t.p,null,r.createElement(l.A,null,"Neural network kernels"),": Stemming from Neal's (1996) work showing that infinitely wide neural networks converge to GPs with specific kernels. Modern variants such as the Neural Tangent Kernel or the ArcCosine kernel can reflect the inductive biases of deep networks."),"\n"),"\n",r.createElement(t.li,null,"\n",r.createElement(t.p,null,r.createElement(l.A,null,"Periodic kernels"),": Useful for data known to have strict periodicity (e.g., seasonal or cyclical data in time-series). One typical form is ",r.createElement(s.A,{text:"\\( k_{\\mathrm{per}}(x,x') = a^2 \\exp\\bigl(-\\frac{2\\sin^2\\bigl(\\pi|x-x'|/p\\bigr)}{\\ell^2}\\bigr) \\)"}),", capturing repeating structure with period ",r.createElement(s.A,{text:"\\( p \\)"}),"."),"\n"),"\n"),"\n",r.createElement(t.p,null,"In practice, kernels are also combined by summation or product, e.g., to encode that part of a function is periodic plus a slowly varying trend. Summation corresponds to modeling the function as ",r.createElement(s.A,{text:"\\( f_1(x)+f_2(x) \\)"}),", each drawn from separate GPs, while product can encode more intricate interactions."),"\n",r.createElement(t.h3,{id:"interpretable-hyperparameters-amplitude-length-scale-etc",style:{position:"relative"}},r.createElement(t.a,{href:"#interpretable-hyperparameters-amplitude-length-scale-etc","aria-label":"interpretable hyperparameters amplitude length scale etc permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"interpretable hyperparameters (amplitude, length-scale, etc.)"),"\n",r.createElement(t.p,null,"One of the major advantages of GPs over black-box parametric models is interpretability in terms of ",r.createElement(l.A,null,"hyperparameters")," (also called kernel parameters). For example:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"The amplitude ",r.createElement(s.A,{text:"\\( a \\)"})," (or ",r.createElement(s.A,{text:"\\( \\sigma \\)"}),") often controls the overall vertical variation of the function."),"\n",r.createElement(t.li,null,"The length-scale ",r.createElement(s.A,{text:"\\( \\ell \\)"})," influences how inputs close in ",r.createElement(s.A,{text:"\\( x \\)"}),"-space affect each other's function values."),"\n"),"\n",r.createElement(t.p,null,'By adjusting these hyperparameters, we control the "wiggliness" or smoothness and the general scale of the function variations. During model fitting, these parameters are typically learned via the marginal likelihood (in regression) or via approximate techniques in classification and other scenarios.'),"\n",r.createElement(t.h2,{id:"gaussian-process-regression",style:{position:"relative"}},r.createElement(t.a,{href:"#gaussian-process-regression","aria-label":"gaussian process regression permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"gaussian process regression"),"\n",r.createElement(t.p,null,"Regression is one of the most straightforward applications of Gaussian processes: we assume real-valued outputs that are corrupted by (often Gaussian) observation noise."),"\n",r.createElement(t.h3,{id:"noise-free-vs-noisy-observations",style:{position:"relative"}},r.createElement(t.a,{href:"#noise-free-vs-noisy-observations","aria-label":"noise free vs noisy observations permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"noise-free vs. noisy observations"),"\n",r.createElement(t.p,null,"Consider a latent function ",r.createElement(s.A,{text:"\\( f(\\cdot) \\)"})," drawn from a GP:"),"\n",r.createElement(s.A,{text:"\\[\nf(x) \\sim \\mathcal{GP}\\bigl(m(x), k(x,x')\\bigr).\n\\]"}),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(l.A,null,"Noise-free observations:")," If we measure ",r.createElement(s.A,{text:"\\( f(x) \\)"})," directly, so that ",r.createElement(s.A,{text:"\\( y_i = f(x_i) \\)"})," with no noise, then the GP posterior will exactly interpolate those data points."),"\n",r.createElement(t.li,null,r.createElement(l.A,null,"Noisy observations:")," More realistically, we only observe ",r.createElement(s.A,{text:"\\( y_i = f(x_i) + \\epsilon_i \\)"}),", where ",r.createElement(s.A,{text:"\\( \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2) \\)"}),". When ",r.createElement(s.A,{text:"\\( \\sigma^2 > 0 \\)"}),", the GP posterior does not necessarily interpolate exactly but finds a balance between fitting the data and smoothing based on the kernel structure."),"\n"),"\n",r.createElement(t.h3,{id:"prior-specification-mean-function-and-covariance-matrix",style:{position:"relative"}},r.createElement(t.a,{href:"#prior-specification-mean-function-and-covariance-matrix","aria-label":"prior specification mean function and covariance matrix permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"prior specification: mean function and covariance matrix"),"\n",r.createElement(t.p,null,"For a dataset ",r.createElement(s.A,{text:"\\( \\{(x_i, y_i)\\}_{i=1}^n \\)"}),", we gather the inputs in ",r.createElement(s.A,{text:"\\( X \\)"})," and outputs in ",r.createElement(s.A,{text:"\\( \\mathbf{y} \\in \\mathbb{R}^n \\)"}),". The latent function values at these inputs is ",r.createElement(s.A,{text:"\\( \\mathbf{f} = (f(x_1),\\ldots,f(x_n))^\\top \\)"}),". Under a GP prior,"),"\n",r.createElement(s.A,{text:"\\[\n\\mathbf{f} \\sim \\mathcal{N}\\bigl(\\boldsymbol{\\mu},\\, K(X,X)\\bigr),\n\\]"}),"\n",r.createElement(t.p,null,"where ",r.createElement(s.A,{text:"\\( \\boldsymbol{\\mu} \\)"})," is the mean vector (obtained by evaluating ",r.createElement(s.A,{text:"\\( m(\\cdot) \\)"})," at ",r.createElement(s.A,{text:"\\( x_i \\)"}),") and ",r.createElement(s.A,{text:"\\( K(X,X) \\)"})," is the covariance matrix built from the kernel ",r.createElement(s.A,{text:"\\( k(\\cdot,\\cdot) \\)"})," at all pairs of training points. When observational noise ",r.createElement(s.A,{text:"\\( \\epsilon \\)"})," is Gaussian with variance ",r.createElement(s.A,{text:"\\( \\sigma^2 \\)"}),", we have:"),"\n",r.createElement(s.A,{text:"\\[\n\\mathbf{y} = \\mathbf{f} + \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon}\\sim \\mathcal{N}\\bigl(\\mathbf{0},\\,\\sigma^2 I\\bigr).\n\\]"}),"\n",r.createElement(t.p,null,"Hence, ",r.createElement(s.A,{text:"\\( \\mathbf{y} \\)"})," itself is also Gaussian-distributed:"),"\n",r.createElement(s.A,{text:"\\[\n\\mathbf{y} \\sim \\mathcal{N}\\bigl(\\boldsymbol{\\mu},\\,K(X,X) + \\sigma^2 I\\bigr).\n\\]"}),"\n",r.createElement(t.h3,{id:"posterior-inference-predictive-mean-and-variance",style:{position:"relative"}},r.createElement(t.a,{href:"#posterior-inference-predictive-mean-and-variance","aria-label":"posterior inference predictive mean and variance permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"posterior inference: predictive mean and variance"),"\n",r.createElement(t.p,null,"Suppose we want predictions at new inputs ",r.createElement(s.A,{text:"\\( X_* = (x_{*1},\\ldots,x_{*m}) \\)"}),". Denote the latent function values at these new points by ",r.createElement(s.A,{text:"\\( \\mathbf{f}_* \\)"}),". Jointly, we have:"),"\n",r.createElement(s.A,{text:"\\[\n\\begin{bmatrix}\n\\mathbf{y}\\\\\n\\mathbf{f}_*\n\\end{bmatrix}\n\\sim \n\\mathcal{N}\\Bigl(\n\\mathbf{0},\n\\begin{bmatrix}\nK(X,X)+\\sigma^2 I & K(X,X_*)\\\\\nK(X_*,X) & K(X_*,X_*)\n\\end{bmatrix}\n\\Bigr),\n\\]"}),"\n",r.createElement(t.p,null,"assuming a zero mean function for simplicity (or we can incorporate ",r.createElement(s.A,{text:"\\( \\boldsymbol{\\mu} \\)"})," if non-zero). Using standard conditional Gaussian identities, the posterior distribution for ",r.createElement(s.A,{text:"\\( \\mathbf{f}_* \\)"})," given ",r.createElement(s.A,{text:"\\( \\mathbf{y} \\)"})," is also Gaussian:"),"\n",r.createElement(s.A,{text:"\\[\n\\mathbf{f}_* \\mid \\mathbf{y}, X, X_* \\sim \\mathcal{N}\\bigl(\\bar{\\mathbf{m}},\\, \\mathbf{S}\\bigr),\n\\]"}),"\n",r.createElement(t.p,null,"where"),"\n",r.createElement(s.A,{text:"\\[\n\\bar{\\mathbf{m}} = K(X_*,X)\\bigl[K(X,X)+\\sigma^2I\\bigr]^{-1}\\mathbf{y}, \n\\]"}),"\n",r.createElement(s.A,{text:"\\[\n\\mathbf{S} = K(X_*,X_*) \\;-\\; K(X_*,X)\\bigl[K(X,X)+\\sigma^2I\\bigr]^{-1} K(X,X_*).\n\\]"}),"\n",r.createElement(t.p,null,"The ",r.createElement(l.A,null,"predictive mean")," ",r.createElement(s.A,{text:"\\( \\bar{\\mathbf{m}} \\)"})," acts like a weighted combination of observed targets ",r.createElement(s.A,{text:"\\( \\mathbf{y} \\)"}),". The ",r.createElement(l.A,null,"predictive variance")," in ",r.createElement(s.A,{text:"\\( \\mathbf{S} \\)"})," reflects both epistemic uncertainty (which grows away from observed data) and, if we fold in the noise term ",r.createElement(s.A,{text:"\\( \\sigma^2 \\)"}),", observation noise as well."),"\n",r.createElement(t.h3,{id:"handling-observation-noise-and-adding-jitter",style:{position:"relative"}},r.createElement(t.a,{href:"#handling-observation-noise-and-adding-jitter","aria-label":"handling observation noise and adding jitter permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"handling observation noise and adding jitter"),"\n",r.createElement(t.p,null,'To ensure numerical stability, especially for kernels like the RBF that can yield near-singular covariance matrices, it is standard to add small "jitter" ',r.createElement(s.A,{text:"\\( \\delta \\approx 10^{-6} \\)"})," to the diagonal. Thus, if we are in a low-noise regime, we replace ",r.createElement(s.A,{text:"\\( K(X,X) + \\sigma^2 I \\)"})," by ",r.createElement(s.A,{text:"\\( K(X,X) + \\sigma^2 I + \\delta I \\)"}),". This ensures positive definiteness and avoids numerical issues in matrix factorizations like the Cholesky decomposition."),"\n",r.createElement(t.h2,{id:"hyperparameter-learning-with-the-marginal-likelihood",style:{position:"relative"}},r.createElement(t.a,{href:"#hyperparameter-learning-with-the-marginal-likelihood","aria-label":"hyperparameter learning with the marginal likelihood permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"hyperparameter learning with the marginal likelihood"),"\n",r.createElement(t.p,null,"A vital step in deploying Gaussian processes is choosing appropriate kernel hyperparameters (e.g., amplitude ",r.createElement(s.A,{text:"\\( a \\)"}),", length-scale ",r.createElement(s.A,{text:"\\( \\ell \\)"}),", noise variance ",r.createElement(s.A,{text:"\\( \\sigma^2 \\)"}),", etc.). We do so by maximizing the marginal likelihood (sometimes called the evidence) of the observed data."),"\n",r.createElement(t.h3,{id:"marginal-likelihood-derivation",style:{position:"relative"}},r.createElement(t.a,{href:"#marginal-likelihood-derivation","aria-label":"marginal likelihood derivation permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"marginal likelihood derivation"),"\n",r.createElement(t.p,null,"From the joint distribution ",r.createElement(s.A,{text:"\\( \\mathbf{y} \\sim \\mathcal{N}\\bigl(\\mathbf{0}, K_{\\theta}(X,X)+\\sigma^2I\\bigr) \\)"}),", the marginal log-likelihood is:"),"\n",r.createElement(s.A,{text:"\\[\n\\log p(\\mathbf{y}\\mid X,\\theta) \n= -\\tfrac{1}{2}\\,\\mathbf{y}^{\\!\\top}\\bigl[K_{\\theta}(X,X)+\\sigma^2I\\bigr]^{-1}\\mathbf{y} \\;-\\; \\tfrac{1}{2}\\,\\log\\!\\bigl\\lvert K_{\\theta}(X,X)+\\sigma^2I\\bigr\\rvert \\;-\\; \\tfrac{n}{2}\\,\\log(2\\pi),\n\\]"}),"\n",r.createElement(t.p,null,"where ",r.createElement(s.A,{text:"\\( \\theta \\)"})," collectively denotes the kernel hyperparameters. We then find:"),"\n",r.createElement(s.A,{text:"\\[\n\\hat{\\theta} = \\arg\\max_\\theta \\;\\log p(\\mathbf{y}\\mid X,\\theta).\n\\]"}),"\n",r.createElement(t.p,null,"This objective has three terms:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,"The data fit term ",r.createElement(s.A,{text:"\\( -\\tfrac{1}{2}\\,\\mathbf{y}^{\\!\\top}[K_{\\theta}+\\sigma^2I]^{-1}\\mathbf{y} \\)"}),"."),"\n",r.createElement(t.li,null,"A complexity penalty ",r.createElement(s.A,{text:"\\( -\\tfrac{1}{2}\\,\\log\\!\\lvert K_{\\theta}+\\sigma^2I\\rvert \\)"}),", which can be viewed as an ",r.createElement(l.A,null,"Occam's razor")," effect."),"\n",r.createElement(t.li,null,"A constant term ",r.createElement(s.A,{text:"\\( -\\tfrac{n}{2}\\,\\log(2\\pi) \\)"}),"."),"\n"),"\n",r.createElement(t.p,null,"Intuitively, the marginal likelihood tries to fit the data in as simple a manner as possible, which often prevents overfitting and yields a natural approach to model selection."),"\n",r.createElement(t.h3,{id:"balancing-model-fit-and-complexity-occams-razor",style:{position:"relative"}},r.createElement(t.a,{href:"#balancing-model-fit-and-complexity-occams-razor","aria-label":"balancing model fit and complexity occams razor permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"balancing model fit and complexity (Occam's razor)"),"\n",r.createElement(t.p,null,"The second term, ",r.createElement(s.A,{text:"\\( -\\frac{1}{2}\\,\\log\\!\\lvert K_{\\theta}+\\sigma^2I\\rvert \\)"}),', penalizes overly flexible kernels that produce large covariance structures "able to fit everything." As we tweak hyperparameters, the determinant of the covariance matrix changes, capturing how "broad" or "narrow" our prior over functions is. This interplay automatically encodes a preference for simpler explanations that still match the data.'),"\n",r.createElement(t.h3,{id:"common-pitfalls-and-initialization-strategies",style:{position:"relative"}},r.createElement(t.a,{href:"#common-pitfalls-and-initialization-strategies","aria-label":"common pitfalls and initialization strategies permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"common pitfalls and initialization strategies"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(l.A,null,"Local optima"),": The log marginal likelihood is not guaranteed to be convex in hyperparameters. Good initialization is key."),"\n",r.createElement(t.li,null,r.createElement(l.A,null,"Overly small length-scales"),": Might cause overfitting to noise or numerical instabilities."),"\n",r.createElement(t.li,null,r.createElement(l.A,null,"Overly large length-scales"),': Can lead to underfitting (the function is too "smooth").'),"\n",r.createElement(t.li,null,r.createElement(l.A,null,"Noise variance mismatch"),": If we start from a poor initial guess for ",r.createElement(s.A,{text:"\\( \\sigma^2 \\)"}),", the optimizer might fail to find the correct region in parameter space."),"\n"),"\n",r.createElement(t.p,null,"Reasonable heuristics include starting from length-scales on the order of the typical distance between points, starting from small to moderate noise variance, or from random draws within an acceptable range."),"\n",r.createElement(t.h2,{id:"implementation-from-scratch",style:{position:"relative"}},r.createElement(t.a,{href:"#implementation-from-scratch","aria-label":"implementation from scratch permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"implementation from scratch"),"\n",r.createElement(t.p,null,"In this section, we illustrate a GP regression workflow step by step: (1) constructing a kernel matrix, (2) adding noise terms, (3) solving linear systems for predictions, (4) computing log determinants for the marginal likelihood, and (5) visualizing posterior results."),"\n",r.createElement(t.h3,{id:"constructing-the-kernel-matrix-and-adding-noise-terms",style:{position:"relative"}},r.createElement(t.a,{href:"#constructing-the-kernel-matrix-and-adding-noise-terms","aria-label":"constructing the kernel matrix and adding noise terms permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"constructing the kernel matrix and adding noise terms"),"\n",r.createElement(t.p,null,"Below is a simple Python snippet to build an RBF kernel matrix:"),"\n",r.createElement(o.A,{text:'\nimport numpy as np\n\ndef rbf_kernel(X1, X2, lengthscale, amplitude=1.0):\n    """\n    Computes the RBF kernel matrix between two sets of inputs X1 and X2.\n    X1: shape (N, D)\n    X2: shape (M, D)\n    lengthscale: float\n    amplitude: float\n    Returns: kernel matrix of shape (N, M)\n    """\n    # Euclidean distance computations\n    dist_sq = np.sum(X1**2, axis=1).reshape(-1,1)               + np.sum(X2**2, axis=1)               - 2.0*np.dot(X1, X2.T)\n    K = amplitude**2 * np.exp(-0.5 * dist_sq / (lengthscale**2))\n    return K\n'}),"\n",r.createElement(t.p,null,"After constructing ",r.createElement(s.A,{text:"\\( K(X,X) \\)"}),", we incorporate the observation noise ",r.createElement(s.A,{text:"\\( \\sigma^2 I \\)"}),". We may also add a small jitter ",r.createElement(s.A,{text:"\\( \\delta I \\)"})," for stability:"),"\n",r.createElement(o.A,{text:"\ndef kernel_with_noise(X, lengthscale, amplitude, sigma_noise, jitter=1e-6):\n    K_xx = rbf_kernel(X, X, lengthscale, amplitude)\n    n = X.shape[0]\n    # Add noise variance + jitter\n    K_xx += (sigma_noise**2 + jitter) * np.eye(n)\n    return K_xx\n"}),"\n",r.createElement(t.h3,{id:"solving-linear-systems-and-computing-log-determinants",style:{position:"relative"}},r.createElement(t.a,{href:"#solving-linear-systems-and-computing-log-determinants","aria-label":"solving linear systems and computing log determinants permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"solving linear systems and computing log determinants"),"\n",r.createElement(t.p,null,"A typical approach to invert ",r.createElement(s.A,{text:"\\( K(X,X) + \\sigma^2 I \\)"})," is by using a Cholesky decomposition (which is stable for symmetric positive definite matrices). In NumPy:"),"\n",r.createElement(o.A,{text:'\nimport numpy as np\n\ndef log_marginal_likelihood(K, y):\n    """\n    Computes the log marginal likelihood term:\n    -0.5*y^T K^{-1} y - 0.5*log|K| - n/2*log(2*pi)\n    """\n    # Cholesky\n    L = np.linalg.cholesky(K)  # L is lower-triangular\n    alpha = np.linalg.solve(L.T, np.linalg.solve(L, y))\n    \n    # compute log determinant from L\n    log_det_K = 2.0 * np.sum(np.log(np.diag(L)))\n    n = y.shape[0]\n    \n    term1 = -0.5 * y.dot(alpha)\n    term2 = -0.5 * log_det_K\n    term3 = -0.5 * n * np.log(2.0 * np.pi)\n    return term1 + term2 + term3\n'}),"\n",r.createElement(t.h3,{id:"step-by-step-example-of-fitting-a-gp-to-data",style:{position:"relative"}},r.createElement(t.a,{href:"#step-by-step-example-of-fitting-a-gp-to-data","aria-label":"step by step example of fitting a gp to data permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"step-by-step example of fitting a GP to data"),"\n",r.createElement(t.p,null,"Let's walk through a synthetic example:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,"Generate data from a known function with noise."),"\n",r.createElement(t.li,null,"Construct the kernel matrix with initial hyperparameters."),"\n",r.createElement(t.li,null,"Optimize the marginal likelihood to learn hyperparameters."),"\n",r.createElement(t.li,null,"Obtain posterior predictive distributions at test points."),"\n"),"\n",r.createElement(o.A,{text:'\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# 1) Generate data\nnp.random.seed(42)\nX_train = np.linspace(0, 5, 30)[:, None]\nf_true = np.sin(X_train).ravel()\nnoise_std = 0.2\ny_train = f_true + noise_std * np.random.randn(len(f_true))\n\nX_test = np.linspace(-1, 6, 100)[:, None]\nf_test_true = np.sin(X_test).ravel()\n\n# 2) Define a function to compute negative log marginal likelihood\ndef neg_log_marg_lik(params):\n    lengthscale, amplitude, sigma_noise = params\n    K = kernel_with_noise(X_train, lengthscale, amplitude, sigma_noise)\n    return -log_marginal_likelihood(K, y_train)\n\n# 3) Optimize hyperparameters\ninit_params = np.array([1.0, 1.0, 0.5])\nbnds = ((1e-3, 10.0), (1e-3, 10.0), (1e-3, 10.0))\nres = minimize(neg_log_marg_lik, init_params, bounds=bnds)\n\nlengthscale_opt, amplitude_opt, sigma_noise_opt = res.x\nprint("Optimized lengthscale:", lengthscale_opt)\nprint("Optimized amplitude:", amplitude_opt)\nprint("Optimized noise std:", sigma_noise_opt)\n'}),"\n",r.createElement(t.p,null,"Once we have the optimized hyperparameters, we form the posterior predictive distribution at each test input ",r.createElement(s.A,{text:"\\( x_* \\)"}),":"),"\n",r.createElement(o.A,{text:"\ndef posterior_predictive(X_star, X, y, lengthscale, amplitude, sigma_noise):\n    # Construct training covariance\n    K_xx = kernel_with_noise(X, lengthscale, amplitude, sigma_noise)\n    # Cross-covariances\n    K_x_xstar = rbf_kernel(X, X_star, lengthscale, amplitude)\n    K_xstar_xstar = rbf_kernel(X_star, X_star, lengthscale, amplitude)\n    \n    # Cholesky\n    L = np.linalg.cholesky(K_xx)\n    alpha = np.linalg.solve(L.T, np.linalg.solve(L, y))\n    \n    # Posterior mean\n    f_star_mean = K_x_xstar.T.dot(alpha)\n    \n    # Compute v = L^{-1} * K_x_xstar\n    v = np.linalg.solve(L, K_x_xstar)\n    # Posterior covariance\n    f_star_cov = K_xstar_xstar - v.T.dot(v)\n    \n    return f_star_mean, f_star_cov\n"}),"\n",r.createElement(t.p,null,"Finally, we can visualize the mean and credible intervals:"),"\n",r.createElement(o.A,{text:"\nimport matplotlib.pyplot as plt\n\n# 4) Posterior predictions at test points\nf_mean, f_cov = posterior_predictive(X_test, X_train, y_train,\n                                     lengthscale_opt, amplitude_opt, sigma_noise_opt)\n\nstd_post = np.sqrt(np.diag(f_cov))\n\nplt.figure(figsize=(8,5))\nplt.scatter(X_train, y_train, color='black', label='Training data')\nplt.plot(X_test, f_test_true, 'g--', label='True function')\nplt.plot(X_test, f_mean, 'b-', label='GP mean')\nplt.fill_between(X_test.ravel(),\n                 f_mean - 2*std_post,\n                 f_mean + 2*std_post,\n                 alpha=0.2,\n                 color='blue',\n                 label='95% cred. interval')\nplt.legend()\nplt.show()\n"}),"\n",r.createElement(n,{alt:"Plot of from-scratch GP regression",path:"",caption:"A from-scratch GP regression fit, showing training data, true function, GP posterior mean, and confidence bands.",zoom:"false"}),"\n",r.createElement(t.h3,{id:"visualizing-posterior-means-credible-intervals-and-posterior-samples",style:{position:"relative"}},r.createElement(t.a,{href:"#visualizing-posterior-means-credible-intervals-and-posterior-samples","aria-label":"visualizing posterior means credible intervals and posterior samples permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"visualizing posterior means, credible intervals, and posterior samples"),"\n",r.createElement(t.p,null,"In practice, it is also illuminating to visualize posterior ",r.createElement(l.A,null,"samples")," — actual draws from the posterior distribution. These reveal the variety of plausible functions that remain consistent with the observed data. We can sample from ",r.createElement(s.A,{text:"\\( \\mathcal{N}(f_{\\!*},\\, \\mathbf{S}_*) \\)"})," using a standard multivariate normal routine."),"\n",r.createElement(t.p,null,'Such plots make it extremely clear how the GP encodes increasing uncertainty in regions where we do not have data, often "fanning out" to express that many solution curves could pass through those points.'),"\n",r.createElement(t.h2,{id:"implementation-with-gpytorch",style:{position:"relative"}},r.createElement(t.a,{href:"#implementation-with-gpytorch","aria-label":"implementation with gpytorch permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"implementation with gpytorch"),"\n",r.createElement(t.p,null,"Manually coding every step can be instructive but becomes cumbersome for advanced kernels, large-scale approximations, or non-Gaussian likelihoods. In these cases, ",r.createElement(l.A,null,"gpytorch")," is a modern Python library that simplifies working with Gaussian processes in PyTorch."),"\n",r.createElement(t.h3,{id:"overview-of-gpytorch-and-its-exact-gp-models",style:{position:"relative"}},r.createElement(t.a,{href:"#overview-of-gpytorch-and-its-exact-gp-models","aria-label":"overview of gpytorch and its exact gp models permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"overview of gpytorch and its exact GP models"),"\n",r.createElement(t.p,null,"gpytorch provides:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(l.A,null,"Exact GP")," models for Gaussian likelihood, letting you do closed-form inference."),"\n",r.createElement(t.li,null,r.createElement(l.A,null,"Variational GP")," classes for classification or large-scale data with approximate inference."),"\n",r.createElement(t.li,null,"Tools for advanced kernel constructions (e.g., Matern, periodic, spectral mixture)."),"\n",r.createElement(t.li,null,"GPU support and scalable approximations (e.g., inducing points, structured kernel interpolation)."),"\n"),"\n",r.createElement(t.p,null,"Below is a succinct overview of how you set up and train an ",r.createElement(l.A,null,"Exact GP")," with an RBF kernel:"),"\n",r.createElement(o.A,{text:"\nimport torch\nimport gpytorch\nimport numpy as np\n\n# Suppose you have training data X_train, y_train\nX_train_torch = torch.from_numpy(X_train).float()\ny_train_torch = torch.from_numpy(y_train).float()\n\nclass ExactGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super().__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.ZeroMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(\n            gpytorch.kernels.RBFKernel()\n        )\n        \n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\nlikelihood = gpytorch.likelihoods.GaussianLikelihood()\nmodel = ExactGPModel(X_train_torch, y_train_torch, likelihood)\n"}),"\n",r.createElement(t.h3,{id:"specifying-mean-functions-kernels-and-likelihoods",style:{position:"relative"}},r.createElement(t.a,{href:"#specifying-mean-functions-kernels-and-likelihoods","aria-label":"specifying mean functions kernels and likelihoods permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"specifying mean functions, kernels, and likelihoods"),"\n",r.createElement(t.p,null,"We can swap out the RBFKernel for a MaternKernel, PeriodicKernel, or combine multiple via additive kernels as needed. Also, if the data demands it, we can specify a different mean function (e.g., ConstantMean or LinearMean) to reflect prior beliefs about the baseline function shape. Meanwhile, ",r.createElement(l.A,null,"GaussianLikelihood")," is the standard choice for regression."),"\n",r.createElement(t.h3,{id:"training-hyperparameters-via-marginal-log-likelihood",style:{position:"relative"}},r.createElement(t.a,{href:"#training-hyperparameters-via-marginal-log-likelihood","aria-label":"training hyperparameters via marginal log likelihood permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"training hyperparameters via marginal log-likelihood"),"\n",r.createElement(t.p,null,"gpytorch uses standard PyTorch optimizers (like Adam) to optimize the negative marginal log-likelihood. The general procedure is:"),"\n",r.createElement(o.A,{text:"\nmodel.train()\nlikelihood.train()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\nmll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n\nnum_iter = 50\nfor i in range(num_iter):\n    optimizer.zero_grad()\n    output = model(X_train_torch)\n    loss = -mll(output, y_train_torch)\n    loss.backward()\n    optimizer.step()\n"}),"\n",r.createElement(t.h3,{id:"making-predictions-and-computing-confidence-regions",style:{position:"relative"}},r.createElement(t.a,{href:"#making-predictions-and-computing-confidence-regions","aria-label":"making predictions and computing confidence regions permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"making predictions and computing confidence regions"),"\n",r.createElement(t.p,null,"At inference time, switch the model and likelihood to evaluation mode:"),"\n",r.createElement(o.A,{text:"\nmodel.eval()\nlikelihood.eval()\n\n# Suppose we have test data X_test\nX_test_torch = torch.from_numpy(X_test).float()\n\nwith torch.no_grad():\n    preds = likelihood(model(X_test_torch))\n    mean = preds.mean\n    lower, upper = preds.confidence_region()\n"}),"\n",r.createElement(t.p,null,"The ",r.createElement(l.A,null,"confidence_region")," method typically provides upper and lower bounds corresponding to approximately two standard deviations in observation space. If you want the pure latent function uncertainty, you can retrieve ",r.createElement(s.A,{text:"\\( \\mathrm{cov}(f_*) \\)"})," from ",r.createElement(l.A,null,"model(X_test_torch)")," directly (which returns a ",r.createElement(l.A,null,"MultivariateNormal"),")."),"\n",r.createElement(t.h3,{id:"comparison-with-the-from-scratch-approach",style:{position:"relative"}},r.createElement(t.a,{href:"#comparison-with-the-from-scratch-approach","aria-label":"comparison with the from scratch approach permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"comparison with the from-scratch approach"),"\n",r.createElement(t.p,null,"The final predictive results match the from-scratch approach:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"Both yield the same posterior ",r.createElement(s.A,{text:"\\( \\bar{\\mathbf{m}} \\)"})," and ",r.createElement(s.A,{text:"\\( \\mathbf{S} \\)"}),"."),"\n",r.createElement(t.li,null,"gpytorch handles the internal operations (Cholesky factor, caching, etc.)."),"\n",r.createElement(t.li,null,"It also provides a unified framework to seamlessly switch kernels, incorporate GPU acceleration, or scale beyond ",r.createElement(s.A,{text:"\\( \\mathcal{O}(n^3) \\)"})," with approximate methods."),"\n"),"\n",r.createElement(n,{alt:"Screenshot of gpytorch regression fit",path:"",caption:"Using gpytorch to train a GP for regression and visualize the posterior.",zoom:"false"}),"\n",r.createElement(t.h2,{id:"conclusion",style:{position:"relative"}},r.createElement(t.a,{href:"#conclusion","aria-label":"conclusion permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"conclusion"),"\n",r.createElement(t.p,null,"Gaussian processes offer a remarkably elegant way to reason about unknown functions by specifying correlations via kernel functions. Their ability to provide uncertainty estimates (epistemic and aleatoric) and the interpretability of their hyperparameters (like length-scale and amplitude) have made them a staple in applications needing robust regression and well-calibrated confidence intervals."),"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"Key highlights from this discussion include:")),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Covariance kernels")," are central to GP assumptions, controlling function smoothness, periodicity, or other structure."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Gaussian process regression")," can be derived in closed form for noise-free or noisy observations under a Gaussian likelihood."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Hyperparameters")," are typically learned by ",r.createElement(t.strong,null,"maximizing the marginal likelihood"),", balancing goodness-of-fit with a penalty that promotes simpler explanations."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"From-scratch implementation")," clarifies the underlying math but can be tedious for large datasets or advanced kernels."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"gpytorch")," allows rapid experimentation with kernel choices, inference algorithms, and scaling strategies, all within a PyTorch environment."),"\n"),"\n",r.createElement(t.p,null,"In advanced settings — like classification, large data (where ",r.createElement(s.A,{text:"\\( \\mathcal{O}(n^3) \\)"})," complexity is prohibitive), or deep kernel learning — further approximations or specialized methods (e.g., variational inference, inducing points, kernel interpolation techniques) become crucial. Even so, the conceptual foundation remains the same: a prior over functions combined with data yields a posterior that captures our updated beliefs about function values at unobserved points."),"\n",r.createElement(t.p,null,"By understanding the core Gaussian process machinery, researchers and practitioners can apply GPs with confidence, interpret the results meaningfully, and adapt them to a broad range of modern machine learning and data science challenges."))}var m=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,i.RP)(),e.components);return t?r.createElement(t,e,r.createElement(c,e)):c(e)};var h=n(36710),d=n(58481),p=n.n(d),u=n(36310),f=n(87245),g=n(27042),v=n(59849),y=n(5591),b=n(61122),E=n(9219),x=n(33203),w=n(95751),_=n(94328),k=n(80791),A=n(78137);const S=e=>{let{toc:t}=e;if(!t||!t.items)return null;return r.createElement("nav",{className:k.R},r.createElement("ul",null,t.items.map(((e,t)=>r.createElement("li",{key:t},r.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const n=t.replace("#",""),a=document.getElementById(n);a&&a.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&r.createElement(S,{toc:{items:e.items}}))))))};function H(e){let{data:{mdx:t,allMdx:l,allPostImages:o},children:s}=e;const{frontmatter:c,body:m,tableOfContents:h}=t,d=c.index,v=c.slug.split("/")[1],k=l.nodes.filter((e=>e.frontmatter.slug.includes(`/${v}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),H=k.findIndex((e=>e.frontmatter.index===d)),z=k[H+1],T=k[H-1],C=c.slug.replace(/\/$/,""),M=/[^/]*$/.exec(C)[0],X=`posts/${v}/content/${M}/`,{0:I,1:G}=(0,r.useState)(c.flagWideLayoutByDefault),{0:P,1:B}=(0,r.useState)(!1);var N;(0,r.useEffect)((()=>{B(!0);const e=setTimeout((()=>B(!1)),340);return()=>clearTimeout(e)}),[I]),"adventures"===v?N=E.cb:"research"===v?N=E.Qh:"thoughts"===v&&(N=E.T6);const V=p()(m).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,L=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),n=e%60;return n<=30?`~${t}${n>0?".5":""} h`:`~${t+1} h`}(Math.ceil(V/N)+(c.extraReadTimeMin||0)),K=[{flag:c.flagDraft,component:()=>Promise.all([n.e(3231),n.e(8809)]).then(n.bind(n,28809))},{flag:c.flagMindfuckery,component:()=>Promise.all([n.e(3231),n.e(2471)]).then(n.bind(n,67709))},{flag:c.flagRewrite,component:()=>Promise.all([n.e(3231),n.e(6764)]).then(n.bind(n,62002))},{flag:c.flagOffensive,component:()=>Promise.all([n.e(3231),n.e(2443)]).then(n.bind(n,17681))},{flag:c.flagProfane,component:()=>Promise.all([n.e(3231),n.e(8048)]).then(n.bind(n,53286))},{flag:c.flagMultilingual,component:()=>Promise.all([n.e(3231),n.e(4069)]).then(n.bind(n,78831))},{flag:c.flagUnreliably,component:()=>Promise.all([n.e(3231),n.e(3417)]).then(n.bind(n,8179))},{flag:c.flagPolitical,component:()=>Promise.all([n.e(3231),n.e(5195)]).then(n.bind(n,30433))},{flag:c.flagCognitohazard,component:()=>Promise.all([n.e(3231),n.e(3175)]).then(n.bind(n,8413))},{flag:c.flagHidden,component:()=>Promise.all([n.e(3231),n.e(9556)]).then(n.bind(n,14794))}],{0:O,1:R}=(0,r.useState)([]);return(0,r.useEffect)((()=>{K.forEach((e=>{let{flag:t,component:n}=e;t&&n().then((e=>{R((t=>[].concat((0,a.A)(t),[e.default])))}))}))}),[]),r.createElement(g.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},r.createElement(y.A,{postNumber:c.index,date:c.date,updated:c.updated,readTime:L,difficulty:c.difficultyLevel,title:c.title,desc:c.desc,banner:c.banner,section:v,postKey:M,isMindfuckery:c.flagMindfuckery,mainTag:c.mainTag}),r.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},c.otherTags.map(((e,t)=>r.createElement("span",{key:t,className:`noselect ${A.MW}`,style:{margin:"0 5px 5px 0"}},e)))),r.createElement("div",{className:"postBody"},r.createElement(S,{toc:h})),r.createElement("br"),r.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},r.createElement(g.P.button,{className:`noselect ${_.pb}`,id:_.xG,onClick:()=>{G(!I)},whileTap:{scale:.93}},r.createElement(g.P.div,{className:w.DJ,key:I,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},I?"Switch to default layout":"Switch to wide layout"))),r.createElement("br"),r.createElement("div",{className:"postBody",style:{margin:I?"0 -14%":"",maxWidth:I?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},r.createElement("div",{className:`${_.P_} ${P?_.Xn:_.qG}`},O.map(((e,t)=>r.createElement(e,{key:t}))),c.indexCourse?r.createElement(x.A,{index:c.indexCourse,category:c.courseCategoryName}):"",r.createElement(u.Z.Provider,{value:{images:o.nodes,basePath:X.replace(/\/$/,"")+"/"}},r.createElement(i.xA,{components:{Image:f.A}},s)))),r.createElement(b.A,{nextPost:z,lastPost:T,keyCurrent:M,section:v}))}function z(e){return r.createElement(H,e,r.createElement(m,e))}function T(e){var t,n,a,i,l;let{data:o}=e;const{frontmatter:s}=o.mdx,c=s.titleSEO||s.title,m=s.titleOG||c,d=s.titleTwitter||c,p=s.descSEO||s.desc,u=s.descOG||p,f=s.descTwitter||p,g=s.schemaType||"BlogPosting",y=s.keywordsSEO,b=s.date,E=s.updated||b,x=s.imageOG||(null===(t=s.banner)||void 0===t||null===(n=t.childImageSharp)||void 0===n||null===(a=n.gatsbyImageData)||void 0===a||null===(i=a.images)||void 0===i||null===(l=i.fallback)||void 0===l?void 0:l.src),w=s.imageAltOG||u,_=s.imageTwitter||x,k=s.imageAltTwitter||f,A=s.canonicalURL,S=s.flagHidden||!1,H=s.mainTag||"Posts",z=s.slug.split("/")[1]||"posts",{siteUrl:T}=(0,h.Q)(),C={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:T},{"@type":"ListItem",position:2,name:H,item:`${T}/${s.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${T}${s.slug}`}]};return r.createElement(v.A,{title:c+" - avrtt.blog",titleOG:m,titleTwitter:d,description:p,descriptionOG:u,descriptionTwitter:f,schemaType:g,keywords:y,datePublished:b,dateModified:E,imageOG:x,imageAltOG:w,imageTwitter:_,imageAltTwitter:k,canonicalUrl:A,flagHidden:S,mainTag:H,section:z,type:"article"},r.createElement("script",{type:"application/ld+json"},JSON.stringify(C)))}},96098:function(e,t,n){var a=n(96540),i=n(7978);t.A=e=>{let{text:t}=e;return a.createElement(i.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-js-content-file-path-src-pages-posts-research-gaussian-processes-mdx-f9c5eb6bd14a8517578c.js.map