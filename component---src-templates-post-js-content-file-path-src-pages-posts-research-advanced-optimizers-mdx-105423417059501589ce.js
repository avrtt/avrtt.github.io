"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[6131],{85304:function(e,t,a){a.r(t),a.d(t,{Head:function(){return M},PostTemplate:function(){return A},default:function(){return k}});var n=a(54506),r=a(28453),i=a(96540),l=a(16886),s=a(46295),o=a(96098);function c(e){const t=Object.assign({p:"p",h2:"h2",a:"a",span:"span",ul:"ul",li:"li",strong:"strong",h3:"h3",h4:"h4",h5:"h5",ol:"ol",br:"br",em:"em",hr:"hr"},(0,r.RP)(),e.components),{Image:a}=t;return a||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),i.createElement(i.Fragment,null,"\n",i.createElement("br"),"\n","\n","\n",i.createElement(t.p,null,"In the area of machine learning, model performance and training efficiency often hinge critically on the choice of optimization techniques used to update model parameters. While classical methods like (batch) ",i.createElement(l.A,null,"gradient descent"),' remain foundational, more advanced strategies have become indispensable for tackling the demands of modern data science and deep learning, especially as model architectures grow in size, datasets grow in scale, and loss landscapes become more intricate. This article embarks on a comprehensive journey through a wide spectrum of "advanced optimizers," diving far beyond the plain-vanilla gradient descent. We will explore how and why these methods help in overcoming slow convergence, circumventing problematic saddle points, taming high-dimensional problems, and contending with large-scale datasets.'),"\n",i.createElement(t.p,null,"Our overall aim is not solely to provide a catalog of advanced optimizers. We also plan to discuss how these methods connect to fundamental mathematical concepts — from momentum-based acceleration that draws on physics analogies, to coordinate-based optimization that shares themes with modern distributed computing, and second-order methods reminiscent of classical Newton-like updates. Where relevant, we will highlight the interplay between these methods and the loss-landscape geometry that arises in deep learning."),"\n",i.createElement(t.p,null,"We assume readers are well-versed in basic linear algebra, calculus, probability, and have hands-on experience training machine learning models. Hence, we will jump quickly into advanced technical details, referencing both older classical approaches (e.g., quasi-Newton methods) and the newest research directions (e.g., more refined Adam variants, advanced scheduling, and gradient-free strategies for specialized tasks)."),"\n",i.createElement(t.h2,{id:"1-introduction",style:{position:"relative"}},i.createElement(t.a,{href:"#1-introduction","aria-label":"1 introduction permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"1. Introduction"),"\n",i.createElement(t.p,null,"Contemporary machine learning systems, especially deep neural networks, rely heavily on iterative algorithms to approximate the minimization of a chosen loss function. Traditional batch gradient descent, though conceptually straightforward, can become prohibitively expensive on large datasets. Moreover, naive updates can fail to adapt to the nuanced geometry of nonconvex, high-dimensional loss surfaces typical of modern architectures (e.g., Transformers with billions of parameters). These limitations catalyzed the development of more advanced techniques:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Momentum-based approaches")," accelerate convergence by incorporating velocity terms in updates, effectively smoothing out noisy gradient directions."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Adaptive methods")," adjust learning rates on a per-parameter basis, greatly improving stability and speed in practice."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Second-order methods")," consider curvature information, yielding more direct steps but often at a significant computational or memory cost."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Gradient-free optimization")," can be vital when gradients are unavailable, unstable, or extremely noisy, focusing instead on direct function evaluations."),"\n"),"\n",i.createElement(t.p,null,"All these techniques can be combined with sophisticated ",i.createElement(t.strong,null,"learning rate scheduling")," strategies, ensuring that the step sizes remain neither too large nor too small, but carefully modulated through training."),"\n",i.createElement(t.p,null,"Understanding these advanced methods is critical for practitioners who aim to train large-scale deep models effectively. This knowledge also provides a gateway to reading state-of-the-art research — many of the newest results in conferences like NeurIPS or ICML revolve around nuanced improvements to existing optimizers or specialized variants for specific tasks."),"\n",i.createElement(t.h2,{id:"2-motivations-for-advanced-optimization-approaches",style:{position:"relative"}},i.createElement(t.a,{href:"#2-motivations-for-advanced-optimization-approaches","aria-label":"2 motivations for advanced optimization approaches permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2. Motivations for advanced optimization approaches"),"\n",i.createElement(t.p,null,"While basic gradient descent remains a simple and instructive foundation, it rarely suffices in practice. Here are some key motivations for going beyond that:"),"\n",i.createElement(t.h3,{id:"21-dealing-with-complex-loss-landscapes",style:{position:"relative"}},i.createElement(t.a,{href:"#21-dealing-with-complex-loss-landscapes","aria-label":"21 dealing with complex loss landscapes permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.1 Dealing with complex loss landscapes"),"\n",i.createElement(t.p,null,"Deep neural networks often exhibit highly nonconvex landscapes, studded with:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Local minima")," and ",i.createElement(t.strong,null,"saddle points"),", though many local minima can be surprisingly acceptable for large models."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Flat regions")," or plateau-like structures that slow naive gradient updates to a crawl."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Sharp and narrow ravines")," that make naive gradient descent bounce across the ravine walls."),"\n"),"\n",i.createElement(t.p,null,"Advanced optimizers combat these structural challenges by smoothing the update trajectory (momentum) or re-scaling coordinate directions adaptively (Adagrad, RMSProp, Adam), thereby forging more stable and effective paths toward regions of low training error."),"\n",i.createElement(t.h3,{id:"22-overcoming-slow-convergence-and-saddle-points",style:{position:"relative"}},i.createElement(t.a,{href:"#22-overcoming-slow-convergence-and-saddle-points","aria-label":"22 overcoming slow convergence and saddle points permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.2 Overcoming slow convergence and saddle points"),"\n",i.createElement(t.p,null,'In classical gradient descent, updates can stall near saddle points (where partial derivatives vanish but the point is not a local minimum). Momentum-based methods, for instance, can generate sufficient "inertia" to escape these plateaus or saddles more reliably. Furthermore, advanced methods can mitigate the zigzagging phenomenon in narrow valleys, speeding up training.'),"\n",i.createElement(t.h3,{id:"23-handling-large-scale-datasets-and-high-dimensional-problems",style:{position:"relative"}},i.createElement(t.a,{href:"#23-handling-large-scale-datasets-and-high-dimensional-problems","aria-label":"23 handling large scale datasets and high dimensional problems permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.3 Handling large-scale datasets and high-dimensional problems"),"\n",i.createElement(t.p,null,"In high-dimensional parameter spaces, coordinate-wise scaling of updates often becomes critical. For instance, certain neurons, filters, or layers might receive systematically larger or smaller gradients. Adaptive gradient methods like RMSProp or Adam help by normalizing the effective step sizes across these distinct directions. This coordinate-wise adaptivity proves especially beneficial in very deep networks, where gradients in some layers can vanish or explode without careful control."),"\n",i.createElement(t.p,null,"Likewise, ",i.createElement(t.strong,null,"stochastic")," or ",i.createElement(t.strong,null,"mini-batch")," approaches reduce the computational burden per update from ",i.createElement(o.A,{text:" \\( \\mathcal{O}(n) \\)"})," (full dataset) to ",i.createElement(o.A,{text:" \\( \\mathcal{O}(1) \\)"})," or ",i.createElement(o.A,{text:" \\( \\mathcal{O}(b) \\)"}),". The noisy gradient estimates can also help escape certain local minima, though noise management (by controlling batch size or using momentum) is essential for stable training."),"\n",i.createElement(t.h2,{id:"3-gradient-descent-modifications",style:{position:"relative"}},i.createElement(t.a,{href:"#3-gradient-descent-modifications","aria-label":"3 gradient descent modifications permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3. Gradient descent modifications"),"\n",i.createElement(t.p,null,"Since deep learning's standard approach remains some variant of gradient-based optimization, researchers have expended much effort in refining gradient descent to make it more stable, faster, or more adaptive. Here, we discuss three main categories of modifications:"),"\n",i.createElement(t.h3,{id:"31-momentum-based-methods",style:{position:"relative"}},i.createElement(t.a,{href:"#31-momentum-based-methods","aria-label":"31 momentum based methods permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.1 Momentum-based methods"),"\n",i.createElement(t.p,null,'The story of momentum-based methods starts with the observation that naive stochastic gradient descent (SGD) can produce a noisy, slow trajectory in certain directions. Momentum methods incorporate a history of past gradients into the current update, akin to a "velocity" term from physics.'),"\n",i.createElement(t.h4,{id:"311-classical-momentum",style:{position:"relative"}},i.createElement(t.a,{href:"#311-classical-momentum","aria-label":"311 classical momentum permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.1.1 Classical momentum"),"\n",i.createElement(t.p,null,"The classical momentum approach modifies the gradient-based update:"),"\n",i.createElement(o.A,{text:"\\[\n\\mathbf{v}_t = \\beta \\mathbf{v}_{t-1} + \\nabla f(\\mathbf{x}_t),\n\\]"}),"\n",i.createElement(o.A,{text:"\\[\n\\mathbf{x}_{t+1} = \\mathbf{x}_t - \\eta \\mathbf{v}_t,\n\\]"}),"\n",i.createElement(t.p,null,"Here,"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(o.A,{text:"\\( \\mathbf{v}_t \\)"})," is the velocity (the exponentially averaged past gradients),"),"\n",i.createElement(t.li,null,i.createElement(o.A,{text:"\\( \\beta \\)"})," is the momentum parameter (often around 0.9),"),"\n",i.createElement(t.li,null,i.createElement(o.A,{text:"\\( \\eta \\)"})," is the base learning rate."),"\n"),"\n",i.createElement(t.p,null,"Over multiple steps, the updates accumulate in the velocity vector, enabling the optimizer to keep moving in consistent gradient directions but dampening oscillations in directions with conflicting gradient signs. This approach can dramatically reduce convergence time in ravine-like regions."),"\n",i.createElement(t.h4,{id:"312-nesterov-accelerated-gradient",style:{position:"relative"}},i.createElement(t.a,{href:"#312-nesterov-accelerated-gradient","aria-label":"312 nesterov accelerated gradient permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.1.2 Nesterov accelerated gradient"),"\n",i.createElement(t.p,null,'Originally proposed by Yurii Nesterov, this variant attempts to refine momentum by using a "lookahead" gradient:'),"\n",i.createElement(o.A,{text:"\\[\n\\mathbf{v}_{t+1} = \\beta \\mathbf{v}_t + \\nabla f(\\mathbf{x}_t - \\eta \\beta \\mathbf{v}_t),\n\\]"}),"\n",i.createElement(o.A,{text:"\\[\n\\mathbf{x}_{t+1} = \\mathbf{x}_t - \\eta \\mathbf{v}_{t+1}.\n\\]"}),"\n",i.createElement(t.p,null,"This means the momentum is applied to a point slightly ahead in the direction of velocity, which can yield more accurate gradient corrections, leading in theory to improved convergence rates. In practice, Nesterov momentum often outperforms classical momentum, though the differences can be subtle."),"\n",i.createElement(t.h5,{id:"code-snippet-for-momentum-based-methods",style:{position:"relative"}},i.createElement(t.a,{href:"#code-snippet-for-momentum-based-methods","aria-label":"code snippet for momentum based methods permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Code snippet for momentum-based methods"),"\n",i.createElement(t.p,null,"Below is a minimal example in Python illustrating classical momentum in a training loop context:"),"\n",i.createElement(s.A,{text:"\nimport numpy as np\n\ndef sgd_momentum(params, grads, velocity, lr=0.01, momentum=0.9):\n    # params, grads, velocity are lists of same length\n    # param[i], grad[i], velocity[i] are np arrays\n    for i in range(len(params)):\n        velocity[i] = momentum * velocity[i] + grads[i]\n        params[i] -= lr * velocity[i]\n\n# Example usage in a training step:\n# velocity = [np.zeros_like(p) for p in params]\n# sgd_momentum(params, grads, velocity, lr=0.01, momentum=0.9)\n"}),"\n",i.createElement(t.p,null,"Users can adapt it to frameworks like PyTorch, TensorFlow, or JAX by substituting the arrays with tensors. Most modern frameworks, though, already have momentum built into their SGD implementations."),"\n",i.createElement(t.h3,{id:"32-adaptive-gradient-methods",style:{position:"relative"}},i.createElement(t.a,{href:"#32-adaptive-gradient-methods","aria-label":"32 adaptive gradient methods permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.2 Adaptive gradient methods"),"\n",i.createElement(t.p,null,"Adaptive gradient algorithms aim to moderate or amplify learning rates on a ",i.createElement(t.strong,null,"per-parameter")," basis, guided by estimates of historical gradient magnitudes. This approach is indispensable for dealing with coordinates of widely varying scales or frequently occurring vs. infrequently occurring features."),"\n",i.createElement(t.h4,{id:"321-adagrad-and-its-drawbacks",style:{position:"relative"}},i.createElement(t.a,{href:"#321-adagrad-and-its-drawbacks","aria-label":"321 adagrad and its drawbacks permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.2.1 Adagrad and its drawbacks"),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Adagrad")," was among the earliest popular adaptively scaled methods. It accumulates the squares of partial derivatives in each dimension:"),"\n",i.createElement(o.A,{text:"\\[\n\\mathbf{s}_t \\leftarrow \\mathbf{s}_{t-1} + \\nabla f(\\mathbf{x}_t)^2,\n\\]"}),"\n",i.createElement(o.A,{text:"\\[\n\\mathbf{x}_{t+1} = \\mathbf{x}_t - \\eta \\frac{\\nabla f(\\mathbf{x}_t)}{\\sqrt{\\mathbf{s}_t} + \\epsilon},\n\\]"}),"\n",i.createElement(t.p,null,"with ",i.createElement(o.A,{text:"\\( \\epsilon \\)"})," a small constant to avoid division by zero. By dividing by ",i.createElement(o.A,{text:"\\( \\sqrt{\\mathbf{s}_t} \\)"}),", features or parameters that have historically had large gradients automatically get smaller steps, while coordinates that rarely get updates have larger relative steps."),"\n",i.createElement(t.p,null,"Drawbacks:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Monotonically decreasing")," effective learning rates can become extremely small, stalling further progress."),"\n",i.createElement(t.li,null,"For nonconvex, complicated tasks like deep learning, the quick decay in step sizes can be detrimental once near an optimum region."),"\n"),"\n",i.createElement(t.h4,{id:"322-rmsprop-for-decaying-averages",style:{position:"relative"}},i.createElement(t.a,{href:"#322-rmsprop-for-decaying-averages","aria-label":"322 rmsprop for decaying averages permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.2.2 RMSProp for decaying averages"),"\n",i.createElement(t.p,null,"To mitigate Adagrad's unbounded accumulation, ",i.createElement(t.strong,null,"RMSProp")," uses an exponential moving average of squared gradients:"),"\n",i.createElement(o.A,{text:"\\[\n\\mathbf{s}_t = \\gamma \\mathbf{s}_{t-1} + (1 - \\gamma) \\nabla f(\\mathbf{x}_t)^2,\n\\]"}),"\n",i.createElement(o.A,{text:"\\[\n\\mathbf{x}_{t+1} = \\mathbf{x}_t - \\eta \\frac{\\nabla f(\\mathbf{x}_t)}{\\sqrt{\\mathbf{s}_t} + \\epsilon}.\n\\]"}),"\n",i.createElement(t.p,null,"Here, ",i.createElement(o.A,{text:"\\( \\gamma \\)"})," (often around 0.9) dictates how quickly the historical gradients fade. RMSProp remains a go-to optimizer for many architectures, though it is often overshadowed by Adam in mainstream deep learning practice."),"\n",i.createElement(t.h4,{id:"323-adam-and-adamw-for-efficient-momentum--rmsprop",style:{position:"relative"}},i.createElement(t.a,{href:"#323-adam-and-adamw-for-efficient-momentum--rmsprop","aria-label":"323 adam and adamw for efficient momentum  rmsprop permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.2.3 Adam and AdamW for efficient momentum + RMSProp"),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Adam"),' ("Adaptive Moment Estimation") consolidates the ideas of RMSProp with momentum:'),"\n",i.createElement(o.A,{text:"\\[\n\\mathbf{v}_t = \\beta_1 \\mathbf{v}_{t-1} + (1 - \\beta_1) \\nabla f(\\mathbf{x}_t),\n\\]"}),"\n",i.createElement(o.A,{text:"\\[\n\\mathbf{s}_t = \\beta_2 \\mathbf{s}_{t-1} + (1 - \\beta_2) \\nabla f(\\mathbf{x}_t)^2,\n\\]"}),"\n",i.createElement(t.p,null,"with bias corrections:"),"\n",i.createElement(o.A,{text:"\\[\n\\hat{\\mathbf{v}}_t = \\frac{\\mathbf{v}_t}{1 - \\beta_1^t}, \\quad \\hat{\\mathbf{s}}_t = \\frac{\\mathbf{s}_t}{1 - \\beta_2^t},\n\\]"}),"\n",i.createElement(t.p,null,"and updates:"),"\n",i.createElement(o.A,{text:"\\[\n\\mathbf{x}_{t+1} = \\mathbf{x}_t - \\eta \\frac{\\hat{\\mathbf{v}}_t}{\\sqrt{\\hat{\\mathbf{s}}_t} + \\epsilon}.\n\\]"}),"\n",i.createElement(t.p,null,"Adam's popularity in deep learning stems from:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Low memory footprint"),": only requires storing the first and second moment estimates."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Good empirical default"),": typically ",i.createElement(o.A,{text:"\\( \\beta_1=0.9, \\beta_2=0.999, \\epsilon=10^{-8} \\)"})," provide robust results."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Momentum-based smoothing")," plus ",i.createElement(t.strong,null,"adaptive scaling")," unify the benefits of RMSProp and momentum."),"\n"),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"AdamW")," modifies Adam's approach to weight decay by decoupling the L2 penalty from the gradient-based scaling:"),"\n",i.createElement(o.A,{text:"\\[\n\\mathbf{x}_{t+1} = \\mathbf{x}_t - \\eta \\left( \\frac{\\hat{\\mathbf{v}}_t}{\\sqrt{\\hat{\\mathbf{s}}_t} + \\epsilon} + \\lambda \\mathbf{x}_t \\right),\n\\]"}),"\n",i.createElement(t.p,null,"which can slightly improve generalization. Many large-scale state-of-the-art models now adopt AdamW as the default."),"\n",i.createElement(t.h5,{id:"example-code-for-adam-in-pytorch",style:{position:"relative"}},i.createElement(t.a,{href:"#example-code-for-adam-in-pytorch","aria-label":"example code for adam in pytorch permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Example code for Adam in PyTorch"),"\n",i.createElement(s.A,{text:"\nimport torch\n\n# Suppose we have a model and a loss function\nmodel = ...\nloss_fn = ...\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-8)\n\nfor epoch in range(num_epochs):\n    for X, y in data_loader:  # data_loader yields mini-batches\n        optimizer.zero_grad()\n        predictions = model(X)\n        loss = loss_fn(predictions, y)\n        loss.backward()\n        optimizer.step()\n"}),"\n",i.createElement(t.p,null,"Because of Adam's reliability, it is frequently recommended as a starting point for training new models. The main hyperparameters (",i.createElement(o.A,{text:"\\( \\beta_1, \\beta_2 \\)"}),", and the learning rate) can be tuned, but often the defaults suffice as a strong baseline for many tasks."),"\n",i.createElement(t.h3,{id:"33-second-order-methods",style:{position:"relative"}},i.createElement(t.a,{href:"#33-second-order-methods","aria-label":"33 second order methods permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.3 Second-order methods"),"\n",i.createElement(t.p,null,'Unlike first-order methods that rely only on gradients, second-order approaches incorporate information from the Hessian (the matrix of second derivatives). This can yield more direct updates, effectively "preconditioning" the local geometry. However, for large problems (millions or billions of parameters), computing or storing the full Hessian is impractical. Hence, second-order methods in deep learning are less commonly used.'),"\n",i.createElement(t.h4,{id:"331-newtons-method",style:{position:"relative"}},i.createElement(t.a,{href:"#331-newtons-method","aria-label":"331 newtons method permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.3.1 Newton's method"),"\n",i.createElement(t.p,null,"Newton's method updates parameters with:"),"\n",i.createElement(o.A,{text:"\\[\n\\mathbf{x}_{t+1} = \\mathbf{x}_t - \\left(\\nabla^2 f(\\mathbf{x}_t)\\right)^{-1} \\nabla f(\\mathbf{x}_t).\n\\]"}),"\n",i.createElement(t.p,null,"While it can converge extremely quickly near local minima (exhibiting quadratic convergence in convex problems), it suffers from:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Prohibitively large Hessian"),": in dimension ",i.createElement(o.A,{text:"\\( d \\)"})," (where ",i.createElement(o.A,{text:"\\( d \\)"})," can be up to billions in deep nets), ",i.createElement(o.A,{text:"\\( \\nabla^2 f(\\mathbf{x}_t) \\)"})," is a ",i.createElement(o.A,{text:"\\( d \\times d \\)"})," matrix."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Costly matrix inversion"),": even storing and inverting a ",i.createElement(o.A,{text:"\\( 10^4 \\times 10^4 \\)"})," Hessian is enormous."),"\n"),"\n",i.createElement(t.p,null,"Hence, pure Newton's method is rarely feasible for deep learning."),"\n",i.createElement(t.h4,{id:"332-quasi-newton-techniques",style:{position:"relative"}},i.createElement(t.a,{href:"#332-quasi-newton-techniques","aria-label":"332 quasi newton techniques permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.3.2 Quasi-Newton techniques"),"\n",i.createElement(t.p,null,"Quasi-Newton methods approximate the Hessian or its inverse on the fly (e.g., BFGS or L-BFGS). They reduce memory usage significantly by storing only a limited set of vectors representing curvature information. L-BFGS (Limited-memory BFGS) can be used for moderate-scale tasks, but still rarely competes with simpler first-order methods in real-world deep learning, owing to complicated, nonconvex surfaces and large parameter counts. Nonetheless, in smaller specialized tasks (e.g., moderate-scale logistic regression, some reinforcement learning setups), L-BFGS can converge faster than naive first-order updates."),"\n",i.createElement(t.h2,{id:"4-different-classes-and-types-of-optimization-algorithms",style:{position:"relative"}},i.createElement(t.a,{href:"#4-different-classes-and-types-of-optimization-algorithms","aria-label":"4 different classes and types of optimization algorithms permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4. Different classes and types of optimization algorithms"),"\n",i.createElement(t.p,null,"Beyond mainstream gradient-based techniques, it pays to categorize optimizers more broadly. This helps us see how certain advanced methods can combine seemingly disparate elements (e.g., coordinate-based searching with momentum, or gradient-based searching with evolutionary heuristics)."),"\n",i.createElement(t.h3,{id:"41-stochastic-vs-batch-optimization",style:{position:"relative"}},i.createElement(t.a,{href:"#41-stochastic-vs-batch-optimization","aria-label":"41 stochastic vs batch optimization permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.1 Stochastic vs. batch optimization"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Batch")," (or full) gradient descent:","\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"Uses the entire dataset to compute the exact gradient at each step."),"\n",i.createElement(t.li,null,"The cost per update is ",i.createElement(o.A,{text:"\\( \\mathcal{O}(n) \\)"}),", where ",i.createElement(o.A,{text:"\\( n \\)"})," is dataset size."),"\n",i.createElement(t.li,null,"Converges smoothly in convex settings, but can be prohibitively expensive for large ",i.createElement(o.A,{text:"\\( n \\)"}),"."),"\n"),"\n"),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Stochastic")," gradient descent (SGD):","\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"Uses a single or a small batch ",i.createElement(o.A,{text:"\\( b \\)"})," of samples to compute a noisy gradient."),"\n",i.createElement(t.li,null,"The cost per update is ",i.createElement(o.A,{text:"\\( \\mathcal{O}(b) \\)"}),", with ",i.createElement(o.A,{text:"\\( b=1 \\)"})," in pure SGD."),"\n",i.createElement(t.li,null,"Typically ",i.createElement(o.A,{text:"\\( b \\)"})," is chosen between 32 and a few thousands in practice (mini-batch)."),"\n"),"\n"),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Trade-offs"),":","\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"Larger batches reduce gradient variance but increase computational cost per step."),"\n",i.createElement(t.li,null,'Smaller batches are more "noisy" but can handle huge data.'),"\n"),"\n"),"\n"),"\n",i.createElement(t.h3,{id:"42-gradient-based-vs-gradient-free-approaches",style:{position:"relative"}},i.createElement(t.a,{href:"#42-gradient-based-vs-gradient-free-approaches","aria-label":"42 gradient based vs gradient free approaches permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.2 Gradient-based vs. gradient-free approaches"),"\n",i.createElement(t.p,null,"Despite being the mainstream workhorse, gradient-based optimization can fail in certain specialized scenarios:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Model-based policy optimization")," in reinforcement learning: the environment might provide reward signals but no direct gradient."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Hyperparameter optimization")," or neural architecture search: discrete or structured hyperparameter spaces not amenable to direct gradients."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Black-box optimization"),": only function values (or approximate metrics) are available, not partial derivatives."),"\n"),"\n",i.createElement(t.p,null,"In such cases, ",i.createElement(t.strong,null,"gradient-free")," or ",i.createElement(t.strong,null,"derivative-free")," methods are used:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Evolutionary algorithms")," (genetic algorithms, CMA-ES)"),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Bayesian optimization")," (Gaussian process or TPE-based)"),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Finite-difference approximations")," to approximate gradients, but potentially expensive."),"\n"),"\n",i.createElement(t.p,null,"Although often outclassed by gradient-based methods in standard supervised learning, gradient-free methods remain relevant for meta-learning or architecture search, bridging the gap where derivatives are hard to define or too noisy to be reliable."),"\n",i.createElement(t.h3,{id:"43-proximal-and-coordinate-based-optimization",style:{position:"relative"}},i.createElement(t.a,{href:"#43-proximal-and-coordinate-based-optimization","aria-label":"43 proximal and coordinate based optimization permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.3 Proximal and coordinate-based optimization"),"\n",i.createElement(t.p,null,'Another interesting class of methods revolves around regularization or "priors" on parameters and efficient ways to handle them:'),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Proximal methods"),': "Proximal operators" generalize gradient steps to handle possibly non-differentiable terms like ',i.createElement(o.A,{text:"\\( \\ell_1 \\)"})," norms. The proximal step effectively solves a small subproblem involving a penalty or constraint."),"\n",i.createElement(o.A,{text:"\\[\n\\mathbf{x}_{t+1} = \\mathrm{argmin}_\\mathbf{z} \\; \\left\\{ \\langle \\nabla f(\\mathbf{x}_t), \\mathbf{z}-\\mathbf{x}_t \\rangle\n+ \\frac{1}{2\\eta} \\|\\mathbf{z} - \\mathbf{x}_t\\|^2 + \\lambda \\, g(\\mathbf{z}) \\right\\}.\n\\]"}),"\n",i.createElement(t.p,null,"For many penalties ",i.createElement(o.A,{text:"\\( g(\\mathbf{x}) \\)"})," (like ",i.createElement(o.A,{text:"\\( \\|\\mathbf{x}\\|_1 \\)"}),", group-lasso norms, etc.), the solution can be computed in closed form (soft-thresholding for ",i.createElement(o.A,{text:"\\( \\ell_1 \\)"}),")."),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Coordinate-based methods"),": Instead of updating all coordinates at once, these methods update subsets or individual coordinates in a round-robin or random fashion. They can exploit sparse or structured constraints effectively (e.g., large-scale linear models with ",i.createElement(o.A,{text:"\\( \\ell_1 \\)"})," penalty). Examples include coordinate descent or block coordinate descent."),"\n"),"\n"),"\n",i.createElement(t.p,null,"These methods can be combined with advanced momentum or scheduling. However, for most large-scale deep learning tasks, classical implementations of coordinate descent are overshadowed by better parallelizable mini-batch SGD. That said, proximal gradient and similar concepts still appear in research contexts, e.g., handling structured sparsity or specialized constraints."),"\n",i.createElement(t.h2,{id:"5-learning-rate-scheduler",style:{position:"relative"}},i.createElement(t.a,{href:"#5-learning-rate-scheduler","aria-label":"5 learning rate scheduler permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5. Learning rate scheduler"),"\n",i.createElement(t.p,null,"A crucial but sometimes overlooked ingredient for effective optimization is ",i.createElement(t.strong,null,"learning rate scheduling")," (a.k.a. step size scheduling). The learning rate controls how large of an update is made at each iteration. If it's too high, training can diverge or bounce chaotically. If it's too low, training can stagnate. Various scheduling strategies systematically reduce (or modulate) the learning rate over time."),"\n",i.createElement(t.h3,{id:"51-fixed-vs-adaptive-schedules",style:{position:"relative"}},i.createElement(t.a,{href:"#51-fixed-vs-adaptive-schedules","aria-label":"51 fixed vs adaptive schedules permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.1 Fixed vs. adaptive schedules"),"\n",i.createElement(t.p,null,"A ",i.createElement(t.strong,null,"fixed")," learning rate ",i.createElement(o.A,{text:"\\( \\eta \\)"})," is rarely used in large-scale training, except for early or experimental tries. Instead, we can define a function ",i.createElement(o.A,{text:"\\( \\eta(t) \\)"})," that reduces ",i.createElement(o.A,{text:"\\( \\eta \\)"})," after certain epochs or steps:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Step-based decay"),":","\n",i.createElement(o.A,{text:"\\( \\eta(t) = \\eta_0 \\times \\gamma^{\\lfloor t / \\tau \\rfloor} \\)"}),"\n","Every ",i.createElement(o.A,{text:"\\( \\tau \\)"})," steps, the rate decays by factor ",i.createElement(o.A,{text:"\\( \\gamma < 1 \\)"}),"."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Exponential decay"),":",i.createElement(t.br),"\n",i.createElement(o.A,{text:"\\( \\eta(t) = \\eta_0 \\exp(-\\lambda t) \\)"}),"."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Polynomial or inverse-square-root decay"),":",i.createElement(t.br),"\n",i.createElement(o.A,{text:"\\( \\eta(t) = \\frac{\\eta_0}{(1 + \\alpha t)^\\beta} \\)"}),". Often ",i.createElement(o.A,{text:"\\( \\beta = 0.5 \\)"})," works well in practice."),"\n"),"\n",i.createElement(t.p,null,"Each strategy can be coded easily, for instance in PyTorch or TensorFlow, with built-in schedulers:"),"\n",i.createElement(s.A,{text:"\nimport torch\n\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n\nfor epoch in range(num_epochs):\n    train_one_epoch(...)\n    scheduler.step()\n"}),"\n",i.createElement(t.h3,{id:"52-cyclical-and-warm-restarts",style:{position:"relative"}},i.createElement(t.a,{href:"#52-cyclical-and-warm-restarts","aria-label":"52 cyclical and warm restarts permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.2 Cyclical and warm restarts"),"\n",i.createElement(t.p,null,'Despite general practice that the learning rate should gently decrease, some advanced heuristics let it occasionally increase or "restart." This can help the model traverse local minima or saddle points more efficiently.'),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Cyclical learning rates"),": ",i.createElement(o.A,{text:"\\( \\eta(t) \\)"})," cycles up and down (usually within a range ",i.createElement(o.A,{text:" \\([ \\eta_{\\min}, \\eta_{\\max}]\\)"}),") in a periodic manner. Proposed by Leslie Smith, it can yield faster convergence or help escape local minima."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Stochastic gradient descent with restarts (SGDR)"),': at specified intervals, the learning rate "resets" to a higher value. The model effectively restarts training from a new vantage, often improving final accuracy or generalization.'),"\n"),"\n",i.createElement(t.h3,{id:"53-advanced-scheduling-techniques",style:{position:"relative"}},i.createElement(t.a,{href:"#53-advanced-scheduling-techniques","aria-label":"53 advanced scheduling techniques permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.3 Advanced scheduling techniques"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Cosine annealing"),": smoothly decays ",i.createElement(o.A,{text:"\\( \\eta \\)"})," following a half-cosine schedule, repeated in restarts. Gains popularity in many computer vision training pipelines."),"\n",i.createElement(o.A,{text:"\\[\n\\eta(t) = \\eta_{\\min} + \\frac{\\eta_{\\max} - \\eta_{\\min}}{2}\\left[1 + \\cos\\left(\\frac{\\pi t}{T}\\right)\\right].\n\\]"}),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Hyperparameter search for scheduling"),": automatically searching or learning the best schedule, e.g., using Bayesian optimization or random search. Some sophisticated approaches treat the schedule itself as a policy to be optimized."),"\n"),"\n"),"\n",i.createElement(t.p,null,"In practice, users often experiment with step-based or piecewise schedules, or adopt well-tested defaults (e.g., a piecewise schedule that decays by 10 every 30 epochs in classical ResNet training). Meanwhile, advanced cyclical or warm-restart methods can yield small gains or improved final accuracy, especially in large-scale tasks where precise fine-tuning of hyperparameters is feasible."),"\n",i.createElement(t.h2,{id:"6-implementations",style:{position:"relative"}},i.createElement(t.a,{href:"#6-implementations","aria-label":"6 implementations permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6. Implementations"),"\n",i.createElement(t.p,null,"Having explored theoretical and conceptual details of advanced optimizers, let's illustrate some straightforward code examples. We focus on a typical PyTorch-like pseudocode (though the logic is easily replicated in other frameworks). We assume a model, a data loader, and a standard training loop. The difference lies in how we instantiate and step through the optimizer."),"\n",i.createElement(t.h3,{id:"61-momentum-based-sgd",style:{position:"relative"}},i.createElement(t.a,{href:"#61-momentum-based-sgd","aria-label":"61 momentum based sgd permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.1 Momentum-based SGD"),"\n",i.createElement(s.A,{text:"\nimport torch\n\nmodel = ...\nloss_fn = ...\n\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\nfor epoch in range(num_epochs):\n    for X, y in data_loader:\n        optimizer.zero_grad()\n        y_hat = model(X)\n        loss = loss_fn(y_hat, y)\n        loss.backward()\n        optimizer.step()\n"}),"\n",i.createElement(t.p,null,'Here, the "momentum=0.9" argument implements classical momentum updates behind the scenes. If we replace it with "nesterov=True," it performs Nesterov accelerated gradient.'),"\n",i.createElement(t.h3,{id:"62-adam-with-weight-decay-adamw",style:{position:"relative"}},i.createElement(t.a,{href:"#62-adam-with-weight-decay-adamw","aria-label":"62 adam with weight decay adamw permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.2 Adam with weight decay (AdamW)"),"\n",i.createElement(s.A,{text:"\nimport torch\n\nmodel = ...\nloss_fn = ...\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n\nfor epoch in range(num_epochs):\n    for X, y in data_loader:\n        optimizer.zero_grad()\n        loss = loss_fn(model(X), y)\n        loss.backward()\n        optimizer.step()\n"}),"\n",i.createElement(t.p,null,"This snippet shows AdamW in action, with both an initial learning rate and a weight decay parameter. Weight decay is a commonly used technique to mitigate overfitting (especially beneficial for large neural nets)."),"\n",i.createElement(t.h3,{id:"63-learning-rate-scheduler-usage",style:{position:"relative"}},i.createElement(t.a,{href:"#63-learning-rate-scheduler-usage","aria-label":"63 learning rate scheduler usage permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.3 Learning rate scheduler usage"),"\n",i.createElement(t.p,null,"Here is a minimal snippet combining an SGD optimizer with a step-based scheduler in PyTorch:"),"\n",i.createElement(s.A,{text:"\nimport torch\n\nmodel = ...\nloss_fn = ...\n\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\nfor epoch in range(num_epochs):\n    for X, y in data_loader:\n        optimizer.zero_grad()\n        out = model(X)\n        loss = loss_fn(out, y)\n        loss.backward()\n        optimizer.step()\n    # After each epoch\n    scheduler.step()\n"}),"\n",i.createElement(t.p,null,"The learning rate will be decayed by a factor of 0.1 every 10 epochs. Alternatively, for cyclical schedules or cosine annealing, one can use other built-in classes in ",i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">torch.optim.lr_scheduler</code>'}}),"."),"\n",i.createElement(t.h3,{id:"64-gradient-free-example-simplified",style:{position:"relative"}},i.createElement(t.a,{href:"#64-gradient-free-example-simplified","aria-label":"64 gradient free example simplified permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.4 Gradient-free example (simplified)"),"\n",i.createElement(t.p,null,"Though not as common in daily deep learning workflows, the following minimal snippet exemplifies a simple ",i.createElement(t.em,null,"random search")," gradient-free approach for parameter tuning:"),"\n",i.createElement(s.A,{text:"\nimport numpy as np\n\ndef random_search(objective_fn, param_dim, num_iter=1000):\n    best_params = np.random.randn(param_dim)\n    best_score = objective_fn(best_params)\n    for i in range(num_iter):\n        candidate = np.random.randn(param_dim)\n        score = objective_fn(candidate)\n        if score < best_score:\n            best_params, best_score = candidate, score\n    return best_params, best_score\n"}),"\n",i.createElement(t.p,null,"Of course, advanced gradient-free optimizers (e.g., CMA-ES, Bayesian optimization) are more sophisticated, employing clever strategies to sample candidate solutions and model the search space."),"\n",i.createElement(t.hr),"\n",i.createElement(a,{alt:"Visualization of optimization in a 2D non-convex landscape",path:"",caption:"Conceptual representation of an advanced optimizer navigating a 2D non-convex surface.",zoom:"false"}),"\n",i.createElement(t.p,null,"In practice, as networks and tasks scale up, advanced optimizers with well-chosen schedules are crucial for stable, efficient training. A single procedure might combine mini-batch momentum-based updates, adaptive scaling, and cyclical LR schedules. The synergy among these methods consistently yields improved training times and final model performance."),"\n",i.createElement(t.h2,{id:"conclusion",style:{position:"relative"}},i.createElement(t.a,{href:"#conclusion","aria-label":"conclusion permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Conclusion"),"\n",i.createElement(t.p,null,"Advanced optimization methods are indispensable for training modern, large-scale machine learning and deep neural network models. Starting with early momentum-based modifications to gradient descent, the field has evolved to incorporate powerful adaptive algorithms like Adagrad, RMSProp, Adam, and AdamW. These methods effectively handle various challenges in high-dimensional, nonconvex landscapes, significantly reducing training times compared to naive implementations of (stochastic) gradient descent."),"\n",i.createElement(t.p,null,"Moreover, research in second-order or approximate second-order methods continues to refine possibilities for accelerating convergence, albeit at significant computational cost in typical deep learning contexts. In situations without reliable gradients, gradient-free methods remain essential, albeit often overshadowed by first-order methods in mainstream deep learning tasks."),"\n",i.createElement(t.p,null,"Crucially, all these optimizers benefit from suitable learning rate scheduling. Whether via step-based decays, cyclical schedules, or advanced annealing techniques, controlling the step size through training can mean the difference between failing to converge and efficiently reaching near-optimal solutions."),"\n",i.createElement(t.p,null,"In summary, advanced optimizers serve as the backbone of successful deep learning practice, bridging fundamental algorithmic ideas (momentum, second-order approximations, adaptivity) with the realities of massive data and high-dimensional parameter spaces. Armed with these tools, machine learning practitioners can better navigate the often rugged terrain of neural network loss surfaces, ultimately unlocking higher performance with less computational cost."))}var m=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,r.RP)(),e.components);return t?i.createElement(t,e,i.createElement(c,e)):c(e)};var d=a(36710),h=a(58481),p=a.n(h),u=a(36310),g=a(87245),f=a(27042),v=a(59849),b=a(5591),y=a(61122),E=a(9219),w=a(33203),x=a(95751),S=a(94328),_=a(80791),z=a(78137);const H=e=>{let{toc:t}=e;if(!t||!t.items)return null;return i.createElement("nav",{className:_.R},i.createElement("ul",null,t.items.map(((e,t)=>i.createElement("li",{key:t},i.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const a=t.replace("#",""),n=document.getElementById(a);n&&n.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&i.createElement(H,{toc:{items:e.items}}))))))};function A(e){let{data:{mdx:t,allMdx:l,allPostImages:s},children:o}=e;const{frontmatter:c,body:m,tableOfContents:d}=t,h=c.index,v=c.slug.split("/")[1],_=l.nodes.filter((e=>e.frontmatter.slug.includes(`/${v}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),A=_.findIndex((e=>e.frontmatter.index===h)),k=_[A+1],M=_[A-1],T=c.slug.replace(/\/$/,""),C=/[^/]*$/.exec(T)[0],V=`posts/${v}/content/${C}/`,{0:I,1:L}=(0,i.useState)(c.flagWideLayoutByDefault),{0:N,1:B}=(0,i.useState)(!1);var P;(0,i.useEffect)((()=>{B(!0);const e=setTimeout((()=>B(!1)),340);return()=>clearTimeout(e)}),[I]),"adventures"===v?P=E.cb:"research"===v?P=E.Qh:"thoughts"===v&&(P=E.T6);const q=p()(m).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,G=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),a=e%60;return a<=30?`~${t}${a>0?".5":""} h`:`~${t+1} h`}(Math.ceil(q/P)+(c.extraReadTimeMin||0)),O=[{flag:c.flagDraft,component:()=>Promise.all([a.e(3231),a.e(8809)]).then(a.bind(a,28809))},{flag:c.flagMindfuckery,component:()=>Promise.all([a.e(3231),a.e(2471)]).then(a.bind(a,67709))},{flag:c.flagRewrite,component:()=>Promise.all([a.e(3231),a.e(6764)]).then(a.bind(a,62002))},{flag:c.flagOffensive,component:()=>Promise.all([a.e(3231),a.e(2443)]).then(a.bind(a,17681))},{flag:c.flagProfane,component:()=>Promise.all([a.e(3231),a.e(8048)]).then(a.bind(a,53286))},{flag:c.flagMultilingual,component:()=>Promise.all([a.e(3231),a.e(4069)]).then(a.bind(a,78831))},{flag:c.flagUnreliably,component:()=>Promise.all([a.e(3231),a.e(3417)]).then(a.bind(a,8179))},{flag:c.flagPolitical,component:()=>Promise.all([a.e(3231),a.e(5195)]).then(a.bind(a,30433))},{flag:c.flagCognitohazard,component:()=>Promise.all([a.e(3231),a.e(3175)]).then(a.bind(a,8413))},{flag:c.flagHidden,component:()=>Promise.all([a.e(3231),a.e(9556)]).then(a.bind(a,14794))}],{0:D,1:W}=(0,i.useState)([]);return(0,i.useEffect)((()=>{O.forEach((e=>{let{flag:t,component:a}=e;t&&a().then((e=>{W((t=>[].concat((0,n.A)(t),[e.default])))}))}))}),[]),i.createElement(f.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},i.createElement(b.A,{postNumber:c.index,date:c.date,updated:c.updated,readTime:G,difficulty:c.difficultyLevel,title:c.title,desc:c.desc,banner:c.banner,section:v,postKey:C,isMindfuckery:c.flagMindfuckery,mainTag:c.mainTag}),i.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},c.otherTags.map(((e,t)=>i.createElement("span",{key:t,className:`noselect ${z.MW}`,style:{margin:"0 5px 5px 0"}},e)))),i.createElement("div",{class:"postBody"},i.createElement(H,{toc:d})),i.createElement("br"),i.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},i.createElement(f.P.button,{class:"noselect",className:S.pb,id:S.xG,onClick:()=>{L(!I)},whileTap:{scale:.93}},i.createElement(f.P.div,{className:x.DJ,key:I,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},I?"Switch to default layout":"Switch to wide layout"))),i.createElement("br"),i.createElement("div",{class:"postBody",style:{margin:I?"0 -14%":"",maxWidth:I?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},i.createElement("div",{className:`${S.P_} ${N?S.Xn:S.qG}`},D.map(((e,t)=>i.createElement(e,{key:t}))),c.indexCourse?i.createElement(w.A,{index:c.indexCourse,category:c.courseCategoryName}):"",i.createElement(u.Z.Provider,{value:{images:s.nodes,basePath:V.replace(/\/$/,"")+"/"}},i.createElement(r.xA,{components:{Image:g.A}},o)))),i.createElement(y.A,{nextPost:k,lastPost:M,keyCurrent:C,section:v}))}function k(e){return i.createElement(A,e,i.createElement(m,e))}function M(e){var t,a,n,r,l;let{data:s}=e;const{frontmatter:o}=s.mdx,c=o.titleSEO||o.title,m=o.titleOG||c,h=o.titleTwitter||c,p=o.descSEO||o.desc,u=o.descOG||p,g=o.descTwitter||p,f=o.schemaType||"BlogPosting",b=o.keywordsSEO,y=o.date,E=o.updated||y,w=o.imageOG||(null===(t=o.banner)||void 0===t||null===(a=t.childImageSharp)||void 0===a||null===(n=a.gatsbyImageData)||void 0===n||null===(r=n.images)||void 0===r||null===(l=r.fallback)||void 0===l?void 0:l.src),x=o.imageAltOG||u,S=o.imageTwitter||w,_=o.imageAltTwitter||g,z=o.canonicalURL,H=o.flagHidden||!1,A=o.mainTag||"Posts",k=o.slug.split("/")[1]||"posts",{siteUrl:M}=(0,d.Q)(),T={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:M},{"@type":"ListItem",position:2,name:A,item:`${M}/${o.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${M}${o.slug}`}]};return i.createElement(v.A,{title:c+" - avrtt.blog",titleOG:m,titleTwitter:h,description:p,descriptionOG:u,descriptionTwitter:g,schemaType:f,keywords:b,datePublished:y,dateModified:E,imageOG:w,imageAltOG:x,imageTwitter:S,imageAltTwitter:_,canonicalUrl:z,flagHidden:H,mainTag:A,section:k,type:"article"},i.createElement("script",{type:"application/ld+json"},JSON.stringify(T)))}},96098:function(e,t,a){var n=a(96540),r=a(7978);t.A=e=>{let{text:t}=e;return n.createElement(r.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-js-content-file-path-src-pages-posts-research-advanced-optimizers-mdx-105423417059501589ce.js.map