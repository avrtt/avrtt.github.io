"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[1083],{3962:function(e,a){a.A="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iaXNvLTg4NTktMSI/Pgo8IS0tIEdlbmVyYXRvcjogQWRvYmUgSWxsdXN0cmF0b3IgMTYuMC4wLCBTVkcgRXhwb3J0IFBsdWctSW4gLiBTVkcgVmVyc2lvbjogNi4wMCBCdWlsZCAwKSAgLS0+CjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+CjxzdmcgdmVyc2lvbj0iMS4xIiBpZD0iQ2FwYV8xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB4PSIwcHgiIHk9IjBweCIKCSB3aWR0aD0iNDE2Ljk3OXB4IiBoZWlnaHQ9IjQxNi45NzlweCIgdmlld0JveD0iMCAwIDQxNi45NzkgNDE2Ljk3OSIgc3R5bGU9ImVuYWJsZS1iYWNrZ3JvdW5kOm5ldyAwIDAgNDE2Ljk3OSA0MTYuOTc5OyIKCSB4bWw6c3BhY2U9InByZXNlcnZlIj4KPGc+Cgk8cGF0aCBkPSJNMzU2LjAwNCw2MS4xNTZjLTgxLjM3LTgxLjQ3LTIxMy4zNzctODEuNTUxLTI5NC44NDgtMC4xODJjLTgxLjQ3LDgxLjM3MS04MS41NTIsMjEzLjM3OS0wLjE4MSwyOTQuODUKCQljODEuMzY5LDgxLjQ3LDIxMy4zNzgsODEuNTUxLDI5NC44NDksMC4xODFDNDM3LjI5MywyNzQuNjM2LDQzNy4zNzUsMTQyLjYyNiwzNTYuMDA0LDYxLjE1NnogTTIzNy42LDM0MC43ODYKCQljMCwzLjIxNy0yLjYwNyw1LjgyMi01LjgyMiw1LjgyMmgtNDYuNTc2Yy0zLjIxNSwwLTUuODIyLTIuNjA1LTUuODIyLTUuODIyVjE2Ny44ODVjMC0zLjIxNywyLjYwNy01LjgyMiw1LjgyMi01LjgyMmg0Ni41NzYKCQljMy4yMTUsMCw1LjgyMiwyLjYwNCw1LjgyMiw1LjgyMlYzNDAuNzg2eiBNMjA4LjQ5LDEzNy45MDFjLTE4LjYxOCwwLTMzLjc2Ni0xNS4xNDYtMzMuNzY2LTMzLjc2NQoJCWMwLTE4LjYxNywxNS4xNDctMzMuNzY2LDMzLjc2Ni0zMy43NjZjMTguNjE5LDAsMzMuNzY2LDE1LjE0OCwzMy43NjYsMzMuNzY2QzI0Mi4yNTYsMTIyLjc1NSwyMjcuMTA3LDEzNy45MDEsMjA4LjQ5LDEzNy45MDF6Ii8+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPC9zdmc+Cg=="},47372:function(e,a,t){t.r(a),t.d(a,{Head:function(){return A},PostTemplate:function(){return H},default:function(){return C}});var n=t(54506),i=t(28453),r=t(96540),l=t(16886),s=t(46295),o=t(96098),c=t(66501);function m(e){const a=Object.assign({p:"p",h2:"h2",a:"a",span:"span",h3:"h3",ol:"ol",li:"li",strong:"strong",ul:"ul"},(0,i.RP)(),e.components);return r.createElement(r.Fragment,null,"\n",r.createElement("br"),"\n","\n","\n","\n",r.createElement(a.p,null,"Bayesian methods have become foundational in machine learning, data science, and statistical modeling because they offer a principled way to handle uncertainty, incorporate prior knowledge, and update beliefs in light of new evidence. This is often contrasted with frequentist approaches, in which parameters of a model are treated as fixed (though unknown) quantities. In the Bayesian worldview, parameters themselves are considered random variables, endowed with a prior distribution that expresses our initial assumptions. Then, upon seeing data, we leverage Bayes' rule to obtain a posterior distribution over these parameters. This shift — from thinking of parameters as unknown constants to viewing them as probability distributions — is at the heart of Bayesian reasoning and is the source of much of its conceptual power."),"\n",r.createElement(a.p,null,"While the frequentist approach often revolves around point estimates such as the maximum likelihood estimate (MLE) or the maximum a posteriori (MAP) estimate, Bayesian methods provide not just a single estimate but an entire distribution over possible parameter values. This posterior distribution can then be used for inference, prediction, decision making, or further modeling. In practical machine learning work, Bayesian models are appealing because they can naturally model parameter uncertainty, help with regularization by way of informative priors, and allow for intuitive interpretations of predictions (e.g., predictive distributions rather than single predictions)."),"\n",r.createElement(a.p,null,"It can be illuminating to view classical machine learning models (like linear regression, logistic regression, or even neural networks) from a Bayesian perspective. Bayesian linear regression, for instance, modifies ordinary linear regression by placing priors on the regression coefficients. Bayesian neural networks do similarly for network weights, though often with approximate inference techniques to handle the computational complexity."),"\n",r.createElement(a.p,null,"There is a spectrum of complexity when building Bayesian models. At one end, one might rely on closed-form formulas for posterior distributions (using conjugate priors). At the other end, advanced Monte Carlo and variational inference methods can handle cases where those closed forms do not exist. The elegance and flexibility of these approaches, however, must be balanced with the computational overhead that typically arises in Bayesian computations."),"\n",r.createElement(a.p,null,"In this article, I aim to demonstrate how Bayesian modeling ideas permeate various facets of machine learning, from classical classifiers such as ",r.createElement(l.A,null,"Naive Bayes")," to more sophisticated constructs like ",r.createElement(l.A,null,"Bayesian Belief Networks")," and ",r.createElement(l.A,null,"Bayesian regression"),". I begin with the foundations of Bayesian statistics, exploring key concepts like prior, posterior, and likelihood. I then discuss classification with Bayes' theorem, detailing how the ",r.createElement(c.A,{text:"Naive Bayes is so named because of the (usually unrealistic) assumption that all features are conditionally independent given the class"}),"Naive Bayes family of algorithms emerges from those principles. Further on, I explain advanced Bayesian approaches such as Bayesian networks (BBNs) and Bayesian regression. Throughout, I also provide step-by-step implementations in Python, present best practices (e.g., the role of priors in controlling overfitting), and mention alternative or extended techniques like hierarchical Bayesian models and advanced inference methods."),"\n",r.createElement(a.p,null,"By the end, you should see how Bayesian thinking helps unify seemingly disparate tasks: classification, regression, inference, and decision-making, all revolve around the central idea of using probabilities to represent our uncertainty about unknown quantities. Let's begin with the theoretical underpinnings."),"\n",r.createElement("br"),"\n",r.createElement(a.h2,{id:"2-foundations-of-bayesian-statistics",style:{position:"relative"}},r.createElement(a.a,{href:"#2-foundations-of-bayesian-statistics","aria-label":"2 foundations of bayesian statistics permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2. Foundations of bayesian statistics"),"\n",r.createElement(a.h3,{id:"21-prior-likelihood-and-posterior",style:{position:"relative"}},r.createElement(a.a,{href:"#21-prior-likelihood-and-posterior","aria-label":"21 prior likelihood and posterior permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.1 Prior, likelihood, and posterior"),"\n",r.createElement(a.p,null,"Bayesian reasoning uses three core components to frame a statistical model: the prior distribution, the likelihood of observed data, and the posterior distribution."),"\n",r.createElement(a.ol,null,"\n",r.createElement(a.li,null,"\n",r.createElement(a.p,null,r.createElement(a.strong,null,"Prior distribution"),": Denoted as ",r.createElement(o.A,{text:"\\(p(\\theta)\\)"}),", it encapsulates our beliefs (or assumptions) about the parameters ",r.createElement(o.A,{text:"\\(\\theta\\)"})," before seeing any data. This can be highly informative or weakly informative (even uniform). For instance, in a simple coin-flip scenario, we might choose a ",r.createElement(l.A,null,"Beta(",r.createElement(o.A,{text:"\\(\\alpha, \\beta\\)"}),")")," prior to describe our initial assumptions about the bias of the coin."),"\n"),"\n",r.createElement(a.li,null,"\n",r.createElement(a.p,null,r.createElement(a.strong,null,"Likelihood"),": Denoted as ",r.createElement(o.A,{text:"\\(p(D \\mid \\theta)\\)"}),", it expresses how probable the observed data ",r.createElement(o.A,{text:"\\(D\\)"})," are, conditional on a particular parameter setting ",r.createElement(o.A,{text:"\\(\\theta\\)"}),". For example, in the coin-flip problem, the likelihood might be a binomial distribution specifying the probability of observing a certain number of heads in a series of flips, given a particular bias ",r.createElement(o.A,{text:"\\(\\theta\\)"}),"."),"\n"),"\n",r.createElement(a.li,null,"\n",r.createElement(a.p,null,r.createElement(a.strong,null,"Posterior distribution"),": Denoted as ",r.createElement(o.A,{text:"\\(p(\\theta \\mid D)\\)"}),", it combines the prior and the likelihood according to Bayes' rule. Formally:"),"\n",r.createElement(o.A,{text:"\\[\np(\\theta \\mid D) = \\frac{p(D \\mid \\theta) \\, p(\\theta)}{p(D)}.\n\\]"}),"\n",r.createElement(a.p,null,"Here, ",r.createElement(o.A,{text:"\\(p(D)\\)"})," is the evidence (or marginal likelihood), which is often expressed as ",r.createElement(o.A,{text:"\\(p(D) = \\int p(D \\mid \\theta)\\,p(\\theta)\\,d\\theta\\)"}),". For many models, this integral can be difficult to compute analytically."),"\n"),"\n"),"\n",r.createElement(a.p,null,"Intuitively, Bayesian inference is the process of starting from a prior belief, then observing data and updating that prior to obtain a posterior belief. This iterative refinement of beliefs as new data arrive is a very natural way to incorporate domain knowledge, constraints, or assumptions into an ML pipeline."),"\n",r.createElement(a.h3,{id:"22-probability-distributions-in-the-bayesian-framework",style:{position:"relative"}},r.createElement(a.a,{href:"#22-probability-distributions-in-the-bayesian-framework","aria-label":"22 probability distributions in the bayesian framework permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.2 Probability distributions in the bayesian framework"),"\n",r.createElement(a.p,null,"In Bayesian statistics, parameters are random variables. This means that the full distribution of parameters is central. When we attempt to do predictions or classification, we can integrate over all possible parameter values, weighted by their posterior probability. We thereby obtain the ",r.createElement(l.A,null,"posterior predictive distribution"),". For a new data point ",r.createElement(o.A,{text:"\\(x_{\\text{new}}\\)"})," and target variable ",r.createElement(o.A,{text:"\\(y_{\\text{new}}\\)"}),", the posterior predictive is:"),"\n",r.createElement(o.A,{text:"\\[\np\\bigl(y_{\\text{new}} \\mid x_{\\text{new}}, D\\bigr) = \\int p\\bigl(y_{\\text{new}} \\mid x_{\\text{new}}, \\theta\\bigr)\\,p\\bigl(\\theta \\mid D\\bigr)\\,d\\theta.\n\\]"}),"\n",r.createElement(a.p,null,"In practice, performing that integral exactly is challenging except for certain special cases (e.g., conjugate priors). That is why the computational tools to approximate or sample from the posterior, such as Markov Chain Monte Carlo (MCMC) or variational inference, are so important."),"\n",r.createElement(a.h3,{id:"23-conjugate-priors-and-predictive-distributions",style:{position:"relative"}},r.createElement(a.a,{href:"#23-conjugate-priors-and-predictive-distributions","aria-label":"23 conjugate priors and predictive distributions permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.3 Conjugate priors and predictive distributions"),"\n",r.createElement(a.p,null,"A prior ",r.createElement(o.A,{text:"\\(p(\\theta)\\)"})," is said to be ",r.createElement(a.strong,null,"conjugate")," to a likelihood ",r.createElement(o.A,{text:"\\(p(D\\mid \\theta)\\)"})," if the posterior ",r.createElement(o.A,{text:"\\(p(\\theta\\mid D)\\)"})," is in the same functional family as the prior. For example, the Beta distribution is conjugate to the binomial likelihood, and the Normal distribution is conjugate to itself under a Normal likelihood (with known variance). Conjugate priors simplify computation drastically because:"),"\n",r.createElement(a.ul,null,"\n",r.createElement(a.li,null,"The posterior has the same form as the prior, making analytic updates straightforward."),"\n",r.createElement(a.li,null,"Posterior predictive distributions often come in closed form."),"\n"),"\n",r.createElement(a.p,null,"An example is the ",r.createElement(a.strong,null,"Beta-Binomial")," pairing:"),"\n",r.createElement(a.ul,null,"\n",r.createElement(a.li,null,"If ",r.createElement(o.A,{text:"\\(\\theta\\)"})," is the probability of success in a Bernoulli/Binomial process,"),"\n",r.createElement(a.li,null,"A Beta(",r.createElement(o.A,{text:"\\(\\alpha, \\beta\\)"}),") prior on ",r.createElement(o.A,{text:"\\(\\theta\\)"})," yields a posterior that is Beta(",r.createElement(o.A,{text:"\\(\\alpha + k, \\beta + n - k\\)"}),"), where ",r.createElement(o.A,{text:"\\(k\\)"})," is the number of observed successes out of ",r.createElement(o.A,{text:"\\(n\\)"})," trials. This is a direct application of:","\n",r.createElement(o.A,{text:"\\[\np(\\theta \\mid D) \\;\\propto\\; p(D \\mid \\theta)\\,p(\\theta).\n\\]"}),"\n"),"\n"),"\n",r.createElement(a.p,null,"Common conjugate pairs in ML include:"),"\n",r.createElement(a.ul,null,"\n",r.createElement(a.li,null,"Beta-Binomial"),"\n",r.createElement(a.li,null,"Dirichlet-Multinomial"),"\n",r.createElement(a.li,null,"Normal-Normal (e.g., for Bayesian linear regression with known variance)"),"\n",r.createElement(a.li,null,"Gamma-Poisson"),"\n"),"\n",r.createElement(a.p,null,"When models can be expressed in terms of such conjugacies, Bayesian updates (and posterior predictive calculations) become almost formulaic. This synergy is part of the reason for the popularity of ",r.createElement(l.A,null,"Naive Bayes classifiers"),", in which each feature-likelihood distribution can be chosen to have a conjugate prior."),"\n",r.createElement(a.h3,{id:"24-bayesian-inference-in-practice",style:{position:"relative"}},r.createElement(a.a,{href:"#24-bayesian-inference-in-practice","aria-label":"24 bayesian inference in practice permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.4 Bayesian inference in practice"),"\n",r.createElement(a.p,null,"In many real-world tasks, we don't have neat conjugate forms or we have large, complex models (e.g., hierarchical Bayesian models, deep Bayesian networks). We then need approximate inference. Two large families of techniques exist:"),"\n",r.createElement(a.ol,null,"\n",r.createElement(a.li,null,"\n",r.createElement(a.p,null,r.createElement(a.strong,null,"Markov Chain Monte Carlo (MCMC)"),": This involves constructing a Markov chain over the parameter space whose stationary distribution is the posterior. Common methods include:"),"\n",r.createElement(a.ul,null,"\n",r.createElement(a.li,null,"\n",r.createElement(l.A,null,"Metropolis-Hastings"),"\n"),"\n",r.createElement(a.li,null,"\n",r.createElement(l.A,null,"Gibbs sampling"),"\n"),"\n",r.createElement(a.li,null,r.createElement(l.A,null,"Hamiltonian Monte Carlo (HMC)"),", including the No-U-Turn Sampler (NUTS)"),"\n"),"\n",r.createElement(a.p,null,"MCMC can produce samples from arbitrarily complex posteriors, though it may be computationally expensive and often requires careful tuning for convergence."),"\n"),"\n",r.createElement(a.li,null,"\n",r.createElement(a.p,null,r.createElement(a.strong,null,"Variational inference"),": Instead of sampling, we posit a parametric family of approximations ",r.createElement(o.A,{text:"\\(q(\\theta)\\)"})," to the true posterior ",r.createElement(o.A,{text:"\\(p(\\theta \\mid D)\\)"})," and attempt to find the best fit in that family via optimization. The method typically involves minimizing the Kullback-Leibler divergence ",r.createElement(o.A,{text:"\\(\\mathrm{KL}(q \\parallel p)\\)"})," or maximizing the evidence lower bound (ELBO). Variational inference is often much faster for high-dimensional models, but the approximation might be biased by the chosen family ",r.createElement(o.A,{text:"\\(q(\\cdot)\\)"}),"."),"\n"),"\n"),"\n",r.createElement(a.p,null,"Bayesian practitioners in advanced settings may mix both (e.g., using variational inference as an initialization before finishing with MCMC) or use specialized algorithms like ",r.createElement(l.A,null,"Sequential Monte Carlo"),", bridging the gap between purely sampling-based approaches and purely optimization-based approaches."),"\n",r.createElement(a.h3,{id:"25-examples-of-prior-knowledge-and-how-it-shapes-our-posterior",style:{position:"relative"}},r.createElement(a.a,{href:"#25-examples-of-prior-knowledge-and-how-it-shapes-our-posterior","aria-label":"25 examples of prior knowledge and how it shapes our posterior permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.5 Examples of prior knowledge and how it shapes our posterior"),"\n",r.createElement(a.p,null,"One of the biggest advantages of Bayesian methods is the ability to incorporate real, domain-specific beliefs. For example:"),"\n",r.createElement(a.ul,null,"\n",r.createElement(a.li,null,"If you expect a parameter in your regression to be very small, you might place a strongly peaked prior around zero. This acts similarly to an ",r.createElement(o.A,{text:"\\(\\ell_2\\)"})," regularization in frequentist terms, but is more interpretable in the Bayesian sense."),"\n",r.createElement(a.li,null,"If you believe most data points come from a distribution with small variance, you might use an ",r.createElement(l.A,null,"Inverse-Gamma")," prior over the variance parameter. This would bias the posterior to favor smaller variances unless the data strongly suggests otherwise."),"\n",r.createElement(a.li,null,"In hierarchical Bayesian modeling, a hyperprior can encode how parameters differ across subgroups but still share commonalities at a higher level."),"\n"),"\n",r.createElement(a.h3,{id:"26-posterior-predictive-distribution",style:{position:"relative"}},r.createElement(a.a,{href:"#26-posterior-predictive-distribution","aria-label":"26 posterior predictive distribution permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.6 Posterior predictive distribution"),"\n",r.createElement(a.p,null,"Once you have a posterior ",r.createElement(o.A,{text:"\\(p(\\theta \\mid D)\\)"}),", you can form predictions about new data ",r.createElement(o.A,{text:"\\(x_{\\text{new}}\\)"})," (and possibly the associated label or target ",r.createElement(o.A,{text:"\\(y_{\\text{new}}\\)"}),"). The Bayesian prescription is:"),"\n",r.createElement(o.A,{text:"\\[\np(y_{\\text{new}} \\mid x_{\\text{new}}, D) \\;=\\; \\int p(y_{\\text{new}} \\mid x_{\\text{new}}, \\theta)\\,p(\\theta \\mid D)\\, d\\theta.\n\\]"}),"\n",r.createElement(a.p,null,"This integral can be intractable for complicated models, but approximate methods or closed-form solutions (in conjugate scenarios) can yield a distribution rather than just a point estimate. The shape of this predictive distribution reveals how uncertain the model is about the outcome."),"\n",r.createElement(a.h3,{id:"27-overview-of-mcmc-variational-inference-and-other-methods",style:{position:"relative"}},r.createElement(a.a,{href:"#27-overview-of-mcmc-variational-inference-and-other-methods","aria-label":"27 overview of mcmc variational inference and other methods permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.7 Overview of MCMC, variational inference, and other methods"),"\n",r.createElement(a.p,null,"Although the remainder of this article focuses primarily on simpler Bayesian classifiers (Naive Bayes) and some direct Bayesian regression methods, it's crucial to remember that large-scale Bayesian modeling is possible when combined with MCMC or variational approaches:"),"\n",r.createElement(a.ul,null,"\n",r.createElement(a.li,null,"MCMC is often used in fields like Bayesian hierarchical modeling, Bayesian neural networks, and complex graphical models."),"\n",r.createElement(a.li,null,"Variational methods are popular in high-dimensional scenarios where MCMC might be prohibitively slow."),"\n",r.createElement(a.li,null,"There are also specialized inference approaches like the ",r.createElement(l.A,null,"Expectation-Maximization (EM)")," algorithm for latent variable models such as Gaussian Mixture Models, although strictly speaking EM can be interpreted in both Bayesian and frequentist settings."),"\n"),"\n",r.createElement(a.p,null,"In the next chapters, we apply this foundation to classical supervised tasks like classification and regression. By focusing on simpler, more direct Bayesian classifiers (and the linear regression example), we can concretely see how Bayesian updating is performed, how posterior distributions yield predictions, and how strong or weak priors affect results."),"\n",r.createElement("br"),"\n",r.createElement(a.h2,{id:"3-bayes-classification",style:{position:"relative"}},r.createElement(a.a,{href:"#3-bayes-classification","aria-label":"3 bayes classification permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3. Bayes classification"),"\n",r.createElement(a.p,null,"Bayes classification is a general framework for decision making under uncertainty. The ideal classifier, often called the ",r.createElement(l.A,null,"Bayes Optimal Classifier"),", assigns a label ",r.createElement(o.A,{text:"\\(y\\)"})," to an input ",r.createElement(o.A,{text:"\\(\\mathbf{x}\\)"})," by maximizing:"),"\n",r.createElement(o.A,{text:"\\[\nh(\\mathbf{x}) = \\arg\\max_y \\; p(y \\mid \\mathbf{x}).\n\\]"}),"\n",r.createElement(a.p,null,"This formula can be expanded using Bayes' theorem as:"),"\n",r.createElement(o.A,{text:"\\[\nh(\\mathbf{x}) = \\arg\\max_y \\; \\frac{p(\\mathbf{x} \\mid y)\\,p(y)}{p(\\mathbf{x})}.\n\\]"}),"\n",r.createElement(a.p,null,"Because ",r.createElement(o.A,{text:"\\(p(\\mathbf{x})\\)"})," is constant for all candidate labels, we only compare:"),"\n",r.createElement(o.A,{text:"\\[\nh(\\mathbf{x}) = \\arg\\max_y \\;\\;p(\\mathbf{x} \\mid y) \\;p(y).\n\\]"}),"\n",r.createElement(a.p,null,"In practice, we don't know ",r.createElement(o.A,{text:"\\(p(\\mathbf{x}\\mid y)\\)"})," or ",r.createElement(o.A,{text:"\\(p(y)\\)"})," exactly. Instead, we estimate them from data. For classification tasks, we typically assume a parametric form for ",r.createElement(o.A,{text:"\\(p(\\mathbf{x}\\mid y)\\)"})," and specify or estimate ",r.createElement(o.A,{text:"\\(p(y)\\)"})," based on class frequencies or domain knowledge."),"\n",r.createElement(a.h3,{id:"31-overview-of-bayes-classifier",style:{position:"relative"}},r.createElement(a.a,{href:"#31-overview-of-bayes-classifier","aria-label":"31 overview of bayes classifier permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.1 Overview of bayes classifier"),"\n",r.createElement(a.p,null,"The Bayes classifier is an ideal baseline: if we truly knew the data-generating process, it would be the best possible classifier for that process (minimizing the expected error rate). In practice, we approximate it. The simplest approach uses the training set to estimate ",r.createElement(o.A,{text:"\\(p(y)\\)"})," (the prior class distribution) and ",r.createElement(o.A,{text:"\\(p(\\mathbf{x}\\mid y)\\)"}),". Then predictions are made by applying the Bayes rule for any new ",r.createElement(o.A,{text:"\\(\\mathbf{x}\\)"}),"."),"\n",r.createElement(a.h3,{id:"32-relationship-to-map-decision-rule",style:{position:"relative"}},r.createElement(a.a,{href:"#32-relationship-to-map-decision-rule","aria-label":"32 relationship to map decision rule permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.2 Relationship to MAP decision rule"),"\n",r.createElement(a.p,null,"If we consider that the parameters themselves are unknown and we have a prior distribution over them, then in principle we might want to average over all parameter possibilities. In simpler treatments, we might just fix a point estimate for the parameters, known as a MAP estimate. Even though this is no longer purely Bayesian (strictly speaking, a fully Bayesian approach would marginalize over the parameter posterior), using MAP or MLE parameter estimates still yields a classifier that we can conceptually treat as an approximation of the full Bayes classifier."),"\n",r.createElement(a.h3,{id:"33-decision-boundaries-and-posterior-probabilities",style:{position:"relative"}},r.createElement(a.a,{href:"#33-decision-boundaries-and-posterior-probabilities","aria-label":"33 decision boundaries and posterior probabilities permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.3 Decision boundaries and posterior probabilities"),"\n",r.createElement(a.p,null,"A fascinating outcome of the Bayes rule is that the decision boundary is typically formed by comparing:"),"\n",r.createElement(o.A,{text:"\\[\np(\\mathbf{x} \\mid y_1)\\,p(y_1) \\quad\\text{vs.}\\quad p(\\mathbf{x} \\mid y_2)\\,p(y_2),\n\\]"}),"\n",r.createElement(a.p,null,"for multiple classes ",r.createElement(o.A,{text:"\\(y_1, y_2, \\dots\\)"}),". Often these distributions become something simple (e.g., Gaussians), in which case the decision boundary might be linear or quadratic. For instance, Gaussian Naive Bayes in a two-class scenario with the same variance across classes can yield linear boundaries."),"\n",r.createElement(a.h3,{id:"34-conditional-independence-assumption",style:{position:"relative"}},r.createElement(a.a,{href:"#34-conditional-independence-assumption","aria-label":"34 conditional independence assumption permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.4 Conditional independence assumption"),"\n",r.createElement(a.p,null,"When modeling ",r.createElement(o.A,{text:"\\(p(\\mathbf{x}\\mid y)\\)"}),", we might face the curse of dimensionality if ",r.createElement(o.A,{text:"\\(\\mathbf{x}\\)"})," is high-dimensional. A widely used simplification is the ",r.createElement(l.A,null,"Naive Bayes assumption")," — that features ",r.createElement(o.A,{text:"\\(x_1, x_2, \\dots, x_d\\)"})," are conditionally independent given ",r.createElement(o.A,{text:"\\(y\\)"}),". In that case:"),"\n",r.createElement(o.A,{text:"\\[\np(\\mathbf{x} \\mid y) = \\prod_{\\alpha=1}^d p\\bigl(x_\\alpha \\mid y\\bigr).\n\\]"}),"\n",r.createElement(a.p,null,"Although rarely true in practice, it often works well in classification tasks, especially text classification or spam detection, where feature dependencies might be complicated but not strong enough to overshadow the benefits of the assumption."),"\n",r.createElement(a.h3,{id:"35-basic-naive-bayes-classifier",style:{position:"relative"}},r.createElement(a.a,{href:"#35-basic-naive-bayes-classifier","aria-label":"35 basic naive bayes classifier permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.5 Basic naive bayes classifier"),"\n",r.createElement(a.p,null,"Thus, the naive Bayes classifier is:"),"\n",r.createElement(o.A,{text:"\\[\n\\hat{y} = \\arg\\max_y \\; p(y)\\;\\prod_{\\alpha=1}^d p(x_\\alpha \\mid y).\n\\]"}),"\n",r.createElement(a.p,null,"Depending on the nature of the features (continuous, categorical, or count-based), we obtain different variants like Gaussian Naive Bayes, Multinomial Naive Bayes, or Bernoulli/Categorical Naive Bayes."),"\n",r.createElement("br"),"\n",r.createElement(a.h2,{id:"4-multinomial-naive-bayes",style:{position:"relative"}},r.createElement(a.a,{href:"#4-multinomial-naive-bayes","aria-label":"4 multinomial naive bayes permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4. Multinomial naive bayes"),"\n",r.createElement(a.h3,{id:"41-application-to-text-classification",style:{position:"relative"}},r.createElement(a.a,{href:"#41-application-to-text-classification","aria-label":"41 application to text classification permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.1 Application to text classification"),"\n",r.createElement(a.p,null,"Multinomial Naive Bayes is a popular choice when dealing with features representing discrete counts — for instance, word frequencies in a text document. Let ",r.createElement(o.A,{text:"\\(x_\\alpha\\)"})," be the count of word ",r.createElement(o.A,{text:"\\(\\alpha\\)"})," in a document, and let ",r.createElement(o.A,{text:"\\(d\\)"})," be the total vocabulary size. Then ",r.createElement(o.A,{text:"\\(\\mathbf{x}\\)"})," is a ",r.createElement(c.A,{text:"Vector with nonnegative integer entries that sum to the total word count in the document."}),"count vector of dimension ",r.createElement(o.A,{text:"\\(d\\)"}),". The model posits:"),"\n",r.createElement(o.A,{text:"\\[\nP(\\mathbf{x} \\mid y=c) \\;=\\; \\frac{(\\sum_{\\alpha=1}^d x_\\alpha)!}{\\prod_{\\alpha=1}^d (x_\\alpha!)}\\;\\prod_{\\alpha=1}^d (\\theta_{\\alpha c})^{\\,x_\\alpha},\n\\]"}),"\n",r.createElement(a.p,null,"where ",r.createElement(o.A,{text:"\\(\\theta_{\\alpha c}\\)"})," is the probability of word ",r.createElement(o.A,{text:"\\(\\alpha\\)"})," in class ",r.createElement(o.A,{text:"\\(c\\)"}),". In practice, we do not handle factorials of huge numbers explicitly, because classification only requires comparing log probabilities, which simplifies the formula to a sum of ",r.createElement(o.A,{text:"\\(x_\\alpha \\log(\\theta_{\\alpha c})\\)"})," terms."),"\n",r.createElement(a.h3,{id:"42-handling-word-counts-and-discrete-features",style:{position:"relative"}},r.createElement(a.a,{href:"#42-handling-word-counts-and-discrete-features","aria-label":"42 handling word counts and discrete features permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.2 Handling word counts and discrete features"),"\n",r.createElement(a.p,null,"You can see how well suited the multinomial distribution is for text classification: each document is conceptually the result of sampling a certain number of words from a distribution over words associated with the class. By calibrating ",r.createElement(o.A,{text:"\\(\\theta_{\\alpha c}\\)"})," using training data from each class, we effectively learn which words are most indicative of each class label."),"\n",r.createElement(a.p,null,"Practically, for each class ",r.createElement(o.A,{text:"\\(c\\)"}),", we collect all documents in that class, sum up the total occurrences of each word across those documents, and then normalize to obtain ",r.createElement(o.A,{text:"\\(\\theta_{\\alpha c}\\)"}),". Smoothing (e.g., Laplace or additive smoothing) is typically used to avoid zero probabilities for words not observed in the training set."),"\n",r.createElement("br"),"\n",r.createElement(a.h2,{id:"5-gaussian-naive-bayes",style:{position:"relative"}},r.createElement(a.a,{href:"#5-gaussian-naive-bayes","aria-label":"5 gaussian naive bayes permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5. Gaussian naive bayes"),"\n",r.createElement(a.h3,{id:"51-assumption-of-normally-distributed-features",style:{position:"relative"}},r.createElement(a.a,{href:"#51-assumption-of-normally-distributed-features","aria-label":"51 assumption of normally distributed features permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.1 Assumption of normally distributed features"),"\n",r.createElement(a.p,null,"Gaussian Naive Bayes is used for continuous features where we assume each feature ",r.createElement(o.A,{text:"\\(x_\\alpha\\)"})," is (conditionally) normally distributed around a mean ",r.createElement(o.A,{text:"\\(\\mu_{\\alpha c}\\)"})," with variance ",r.createElement(o.A,{text:"\\(\\sigma_{\\alpha c}^2\\)"})," for each class ",r.createElement(o.A,{text:"\\(c\\)"}),". Formally:"),"\n",r.createElement(o.A,{text:"\\[\np(x_\\alpha \\mid y=c) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma_{\\alpha c}}\\exp\\!\\Bigl(-\\frac{(x_\\alpha - \\mu_{\\alpha c})^2}{2\\,\\sigma_{\\alpha c}^2}\\Bigr).\n\\]"}),"\n",r.createElement(a.p,null,"By the naive Bayes assumption,"),"\n",r.createElement(o.A,{text:"\\[\np(\\mathbf{x}\\mid y=c) = \\prod_{\\alpha=1}^d p(x_\\alpha \\mid y=c).\n\\]"}),"\n",r.createElement(a.p,null,"This approach often works well in settings where continuous data is at least somewhat unimodal around class-specific means, though real data might deviate from the normal shape."),"\n",r.createElement(a.h3,{id:"52-use-cases-in-continuous-feature-spaces",style:{position:"relative"}},r.createElement(a.a,{href:"#52-use-cases-in-continuous-feature-spaces","aria-label":"52 use cases in continuous feature spaces permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.2 Use cases in continuous feature spaces"),"\n",r.createElement(a.p,null,"Gaussian Naive Bayes finds applications in tasks like:"),"\n",r.createElement(a.ul,null,"\n",r.createElement(a.li,null,"Real-valued sensor data classification, where each sensor dimension is treated as a Gaussian."),"\n",r.createElement(a.li,null,"Simple image recognition tasks where pixel intensities can be approximated as Gaussians for each class (though more advanced methods are usually preferred)."),"\n",r.createElement(a.li,null,"Preliminary experiments in new domains with real-valued features, just to get a baseline classification performance."),"\n"),"\n",r.createElement("br"),"\n",r.createElement(a.h2,{id:"6-aode-averaged-one-dependence-estimators",style:{position:"relative"}},r.createElement(a.a,{href:"#6-aode-averaged-one-dependence-estimators","aria-label":"6 aode averaged one dependence estimators permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6. AODE (averaged one-dependence estimators)"),"\n",r.createElement(a.h3,{id:"61-relaxing-some-assumptions-of-naive-bayes",style:{position:"relative"}},r.createElement(a.a,{href:"#61-relaxing-some-assumptions-of-naive-bayes","aria-label":"61 relaxing some assumptions of naive bayes permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.1 Relaxing some assumptions of naive bayes"),"\n",r.createElement(a.p,null,"Naive Bayes drastically assumes independence among features given the class. AODE (",r.createElement(l.A,null,"Averaged One-Dependence Estimators"),") tries to improve upon this by allowing each feature to depend on the class and one other feature, but not more. In other words, it introduces a single additional edge in the Bayesian network for each feature. This is sometimes called a ",r.createElement(l.A,null,"one-dependence")," classifier."),"\n",r.createElement(a.h3,{id:"62-combining-multiple-simple-bayesian-models-for-robust-results",style:{position:"relative"}},r.createElement(a.a,{href:"#62-combining-multiple-simple-bayesian-models-for-robust-results","aria-label":"62 combining multiple simple bayesian models for robust results permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.2 Combining multiple simple bayesian models for robust results"),"\n",r.createElement(a.p,null,"AODE effectively averages over multiple “weakly dependent” naive Bayes models, each capturing an extra conditional dependence. This often yields better accuracy than naive Bayes, at the cost of more computational overhead. The name “averaged” arises because it constructs many such one-dependence models and averages their predictions, reminiscent of ensemble methods."),"\n",r.createElement(a.p,null,"In practice, AODE can be seen as a stepping stone between naive Bayes and more complex Bayesian networks that capture arbitrary conditional dependencies."),"\n",r.createElement("br"),"\n",r.createElement(a.h2,{id:"7-bbn-bayesian-belief-networks",style:{position:"relative"}},r.createElement(a.a,{href:"#7-bbn-bayesian-belief-networks","aria-label":"7 bbn bayesian belief networks permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7. BBN (bayesian belief networks)"),"\n",r.createElement(a.h3,{id:"71-representation-of-conditional-independencies-via-directed-acyclic-graphs",style:{position:"relative"}},r.createElement(a.a,{href:"#71-representation-of-conditional-independencies-via-directed-acyclic-graphs","aria-label":"71 representation of conditional independencies via directed acyclic graphs permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.1 Representation of conditional independencies via directed acyclic graphs"),"\n",r.createElement(a.p,null,"A ",r.createElement(l.A,null,"Bayesian Belief Network")," (BBN) — or more succinctly, a Bayesian Network (BN) — is a directed acyclic graph (DAG) where nodes represent random variables and edges encode direct dependencies. The joint distribution factorizes according to:"),"\n",r.createElement(o.A,{text:"\\[\np(X_1, X_2, \\dots, X_n) \\;=\\; \\prod_{i=1}^n p\\bigl(X_i \\mid \\text{Parents}(X_i)\\bigr).\n\\]"}),"\n",r.createElement(a.p,null,"This offers an expressive but compact way to represent complex distributions. Naive Bayes is actually a very simple Bayesian network where the class node has arrows pointing to each feature node, with no edges among features themselves. Real-world BNs can be far more intricate, capturing context-specific dependencies and conditional independencies."),"\n",r.createElement(a.h3,{id:"72-exact-vs-approximate-inference-in-bbn",style:{position:"relative"}},r.createElement(a.a,{href:"#72-exact-vs-approximate-inference-in-bbn","aria-label":"72 exact vs approximate inference in bbn permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.2 Exact vs. approximate inference in BBN"),"\n",r.createElement(a.p,null,"In a general Bayesian Network with many interconnected variables, computing the exact posterior of a node can be ",r.createElement(c.A,{text:"NP-hard in the worst case"}),"computationally challenging. Therefore, we might rely on:"),"\n",r.createElement(a.ul,null,"\n",r.createElement(a.li,null,r.createElement(a.strong,null,"Exact inference")," methods like variable elimination, junction trees, and belief propagation in smaller networks or networks with special structures."),"\n",r.createElement(a.li,null,r.createElement(a.strong,null,"Approximate inference")," methods such as MCMC or variational algorithms for large networks."),"\n"),"\n",r.createElement(a.p,null,"Bayesian Belief Networks find broad usage in domains like medical diagnosis, sensor fusion, risk assessment, and anywhere else we want an interpretable model of uncertain relationships."),"\n",r.createElement("br"),"\n",r.createElement(a.h2,{id:"8-bn-bayesian-networks-in-classification-tasks",style:{position:"relative"}},r.createElement(a.a,{href:"#8-bn-bayesian-networks-in-classification-tasks","aria-label":"8 bn bayesian networks in classification tasks permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8. BN (bayesian networks) in classification tasks"),"\n",r.createElement(a.h3,{id:"81-building-and-interpreting-bayesian-networks",style:{position:"relative"}},r.createElement(a.a,{href:"#81-building-and-interpreting-bayesian-networks","aria-label":"81 building and interpreting bayesian networks permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.1 Building and interpreting bayesian networks"),"\n",r.createElement(a.p,null,"When focusing on classification, we typically designate a node ",r.createElement(o.A,{text:"\\(Y\\)"})," for the class label, and other nodes for features ",r.createElement(o.A,{text:"\\(\\mathbf{X}\\)"}),". The edges define how features depend on each other and/or on the class. If we keep it fully naive, each feature depends only on ",r.createElement(o.A,{text:"\\(Y\\)"}),", leading to a star-like structure from ",r.createElement(o.A,{text:"\\(Y\\)"})," to each ",r.createElement(o.A,{text:"\\(X_\\alpha\\)"}),". Alternatively, we can incorporate additional edges among features if we have domain knowledge."),"\n",r.createElement(a.p,null,"Bayesian networks for classification can be seen as a generalization of naive Bayes. The advantage is better modeling of correlations among features. The disadvantage is that the network structure must be learned or specified, and inference can become more complex."),"\n",r.createElement(a.h3,{id:"82-example-extended-naive-bayes-with-dependencies",style:{position:"relative"}},r.createElement(a.a,{href:"#82-example-extended-naive-bayes-with-dependencies","aria-label":"82 example extended naive bayes with dependencies permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.2 Example: extended naive bayes with dependencies"),"\n",r.createElement(a.p,null,"Consider a spam detection scenario. Suppose you know that the presence of specific words is strongly correlated (e.g., synonyms or certain phrases). You might create edges among those words in the BN, indicating that their distributions are not independent once you know the class. Learning or hand-crafting such networks can yield better classification accuracy than naive Bayes if done well."),"\n",r.createElement("br"),"\n",r.createElement(a.h2,{id:"9-bayesian-regression",style:{position:"relative"}},r.createElement(a.a,{href:"#9-bayesian-regression","aria-label":"9 bayesian regression permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9. Bayesian regression"),"\n",r.createElement(a.h3,{id:"91-introduction-to-bayes-regression",style:{position:"relative"}},r.createElement(a.a,{href:"#91-introduction-to-bayes-regression","aria-label":"91 introduction to bayes regression permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9.1 Introduction to bayes regression"),"\n",r.createElement(a.p,null,"In Bayesian regression, we place priors on the parameters of a regression model, such as linear regression. For a linear model:"),"\n",r.createElement(o.A,{text:"\\[\ny = \\mathbf{w}^\\top \\mathbf{x} + \\epsilon,\n\\]"}),"\n",r.createElement(a.p,null,"we might treat ",r.createElement(o.A,{text:"\\(\\mathbf{w}\\)"})," as a random vector, typically with a prior like ",r.createElement(o.A,{text:"\\(\\mathbf{w}\\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2_w \\mathbf{I})\\)"}),". Observations come with noise ",r.createElement(o.A,{text:"\\(\\epsilon\\sim \\mathcal{N}(0, \\sigma^2)\\)"}),". Then, after seeing data ",r.createElement(o.A,{text:"\\(D = \\{(\\mathbf{x}_i, y_i)\\}\\)"}),", the posterior distribution of ",r.createElement(o.A,{text:"\\(\\mathbf{w}\\)"})," is also Gaussian in the ",r.createElement(l.A,null,"conjugate")," case (assuming the noise variance is known)."),"\n",r.createElement(a.h3,{id:"92-from-linear-regression-to-a-bayesian-perspective",style:{position:"relative"}},r.createElement(a.a,{href:"#92-from-linear-regression-to-a-bayesian-perspective","aria-label":"92 from linear regression to a bayesian perspective permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9.2 From linear regression to a bayesian perspective"),"\n",r.createElement(a.p,null,"Classical (frequentist) linear regression solves for the least-squares estimate. Bayesian linear regression solves for the posterior ",r.createElement(o.A,{text:"\\(p(\\mathbf{w}\\mid D)\\)"}),". This posterior is typically:"),"\n",r.createElement(o.A,{text:"\\[\n\\mathbf{w} \\mid D \\sim \\mathcal{N}(\\mathbf{\\mu}_w, \\Sigma_w),\n\\]"}),"\n",r.createElement(a.p,null,"where ",r.createElement(o.A,{text:"\\(\\mathbf{\\mu}_w\\)"})," and ",r.createElement(o.A,{text:"\\(\\Sigma_w\\)"})," can be derived analytically when the prior is Gaussian and the likelihood is Gaussian. For instance, with prior ",r.createElement(o.A,{text:"\\(\\mathbf{w}\\sim\\mathcal{N}(\\mathbf{0},\\alpha^{-1} \\mathbf{I})\\)"}),", the posterior mean is:"),"\n",r.createElement(o.A,{text:"\\[\n\\mathbf{\\mu}_w = \\beta \\Sigma_w \\sum_{i=1}^n y_i \\mathbf{x}_i,\n\\]"}),"\n",r.createElement(a.p,null,"and the posterior covariance ",r.createElement(o.A,{text:"\\(\\Sigma_w\\)"})," is:"),"\n",r.createElement(o.A,{text:"\\[\n\\Sigma_w = \\bigl(\\alpha \\mathbf{I} + \\beta \\sum_{i=1}^n \\mathbf{x}_i \\mathbf{x}_i^\\top \\bigr)^{-1}.\n\\]"}),"\n",r.createElement(a.p,null,"Here, ",r.createElement(o.A,{text:"\\(\\beta\\)"})," is the inverse of the noise variance ",r.createElement(o.A,{text:"\\(\\sigma^2\\)"}),"."),"\n",r.createElement(a.h3,{id:"93-model-complexity-and-regularization",style:{position:"relative"}},r.createElement(a.a,{href:"#93-model-complexity-and-regularization","aria-label":"93 model complexity and regularization permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9.3 Model complexity and regularization"),"\n",r.createElement(a.p,null,"The Bayesian perspective automatically injects a form of regularization through the prior distribution. A Gaussian prior with small variance around zero on ",r.createElement(o.A,{text:"\\(\\mathbf{w}\\)"})," shrinks the parameter estimates, preventing overfitting. In frequentist terms, this is akin to ridge regression with a penalty. But the Bayesian viewpoint also provides an entire distribution that quantifies uncertainty over ",r.createElement(o.A,{text:"\\(\\mathbf{w}\\)"}),"."),"\n",r.createElement(a.h3,{id:"94-capturing-parameter-uncertainty",style:{position:"relative"}},r.createElement(a.a,{href:"#94-capturing-parameter-uncertainty","aria-label":"94 capturing parameter uncertainty permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9.4 Capturing parameter uncertainty"),"\n",r.createElement(a.p,null,"Rather than a single “best fit” vector ",r.createElement(o.A,{text:"\\(\\mathbf{w}^*\\)"}),", the posterior distribution expresses how uncertain we are about each parameter, given the data. When making predictions for a new input ",r.createElement(o.A,{text:"\\(\\mathbf{x}_{\\text{new}}\\)"}),", the predictive distribution is:"),"\n",r.createElement(o.A,{text:"\\[\np(y_{\\text{new}}\\mid \\mathbf{x}_{\\text{new}}, D) = \\int p(y_{\\text{new}}\\mid \\mathbf{x}_{\\text{new}}, \\mathbf{w}) \\; p(\\mathbf{w}\\mid D)\\, d\\mathbf{w}.\n\\]"}),"\n",r.createElement(a.p,null,"If everything is Gaussian, this integral has a closed form:"),"\n",r.createElement(o.A,{text:"\\[\ny_{\\text{new}}\\mid \\mathbf{x}_{\\text{new}}, D \\sim \\mathcal{N}\\!\\bigl(\\mathbf{\\mu}_w^\\top \\mathbf{x}_{\\text{new}}, \\;\\mathbf{x}_{\\text{new}}^\\top \\Sigma_w\\,\\mathbf{x}_{\\text{new}} + \\sigma^2 \\bigr).\n\\]"}),"\n",r.createElement(a.p,null,"This distribution conveys not only the expected outcome (the mean) but also how uncertain we are (the variance)."),"\n",r.createElement(a.h3,{id:"95-common-prior-choices",style:{position:"relative"}},r.createElement(a.a,{href:"#95-common-prior-choices","aria-label":"95 common prior choices permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9.5 Common prior choices"),"\n",r.createElement(a.ul,null,"\n",r.createElement(a.li,null,r.createElement(a.strong,null,"Gaussian prior on weights"),": The standard approach for Bayesian linear regression."),"\n",r.createElement(a.li,null,r.createElement(a.strong,null,"Sparsity-inducing priors"),": E.g., Laplace priors to mimic ",r.createElement(o.A,{text:"\\(\\ell_1\\)"}),"-type regularization or a horseshoe prior for improved sparsity in large parametric spaces."),"\n",r.createElement(a.li,null,r.createElement(a.strong,null,"Hierarchical priors"),": If we have groups of features, we might want a hierarchical structure to share statistical strength across them (akin to partial pooling in hierarchical linear models)."),"\n"),"\n",r.createElement("br"),"\n",r.createElement(a.h2,{id:"10-step-by-step-implementations-in-python",style:{position:"relative"}},r.createElement(a.a,{href:"#10-step-by-step-implementations-in-python","aria-label":"10 step by step implementations in python permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"10. Step-by-step implementations in python"),"\n",r.createElement(a.p,null,"Below are minimal, educational Python examples showcasing how to implement various Bayesian models from scratch (for demonstration) and using libraries (like ",r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">scikit-learn</code>'}})," or ",r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">pymc</code>'}}),"/",r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">pymc3</code>'}}),"/",r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">pymc4</code>'}}),"). Real-world usage typically relies on well-tested libraries, but implementing toy versions helps clarify the underlying concepts."),"\n",r.createElement(a.h3,{id:"101-implementing-basic-naive-bayes",style:{position:"relative"}},r.createElement(a.a,{href:"#101-implementing-basic-naive-bayes","aria-label":"101 implementing basic naive bayes permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"10.1 Implementing basic naive bayes"),"\n",r.createElement(a.p,null,"Let's illustrate a simple Bernoulli Naive Bayes from scratch. Assume each feature ",r.createElement(o.A,{text:"\\(x_\\alpha\\)"})," is 0 or 1. We want to model ",r.createElement(o.A,{text:"\\(p(y=c)\\)"})," and ",r.createElement(o.A,{text:"\\(p(x_\\alpha=1\\mid y=c)\\)"}),":"),"\n",r.createElement(s.A,{text:'\nimport numpy as np\n\nclass BernoulliNaiveBayes:\n    def __init__(self, alpha=1.0):\n        self.alpha = alpha  # Laplace smoothing parameter\n        \n    def fit(self, X, y):\n        # X is shape (n_samples, n_features), each feature is 0 or 1\n        # y is shape (n_samples,), representing class labels\n        self.classes_ = np.unique(y)\n        n_samples, n_features = X.shape\n        self.class_counts_ = {}\n        self.class_log_prior_ = {}\n        self.feature_probs_ = {}\n        \n        for cls in self.classes_:\n            X_c = X[y == cls]\n            # P(y=cls)\n            self.class_counts_[cls] = X_c.shape[0]\n            self.class_log_prior_[cls] = np.log((X_c.shape[0] + self.alpha) \n                                                / (n_samples + len(self.classes_) * self.alpha))\n            # P(x_alpha=1|y=cls)\n            # Using Laplace smoothing for each feature\n            feature_sum = X_c.sum(axis=0)\n            self.feature_probs_[cls] = (feature_sum + self.alpha) / (X_c.shape[0] + 2 * self.alpha)\n        \n    def predict(self, X):\n        # Compute log posterior = log p(y) + sum over features of log p(x_alpha|y) or log(1 - p(x_alpha|y))\n        predictions = []\n        for x in X:\n            class_scores = {}\n            for cls in self.classes_:\n                log_prob = self.class_log_prior_[cls]\n                # sum log probabilities across features\n                for alpha_i, x_val in enumerate(x):\n                    p_alpha_1 = self.feature_probs_[cls][alpha_i]\n                    if x_val == 1:\n                        log_prob += np.log(p_alpha_1)\n                    else:\n                        log_prob += np.log(1.0 - p_alpha_1)\n                class_scores[cls] = log_prob\n            predictions.append(max(class_scores, key=class_scores.get))\n        return np.array(predictions)\n\n# Example usage:\nX = np.array([[0,1,1],[1,1,0],[0,0,1],[1,1,1],[1,0,1],[0,0,0]])\ny = np.array([0,0,1,1,1,0])  # 2 classes: 0 and 1\nmodel = BernoulliNaiveBayes(alpha=1.0)\nmodel.fit(X, y)\npreds = model.predict(X)\nprint("Predictions:", preds)\n'}),"\n",r.createElement(a.p,null,"This demonstrates the naive Bayes structure: we estimate the class prior log probability and the probability of each feature being 1 given the class. At prediction time, we compute the log posterior for each class and pick the maximum."),"\n",r.createElement(a.h3,{id:"102-implementing-multinomial-naive-bayes",style:{position:"relative"}},r.createElement(a.a,{href:"#102-implementing-multinomial-naive-bayes","aria-label":"102 implementing multinomial naive bayes permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"10.2 Implementing multinomial naive bayes"),"\n",r.createElement(a.p,null,"For text classification with bag-of-words:"),"\n",r.createElement(s.A,{text:'\nimport numpy as np\n\nclass MultinomialNaiveBayes:\n    def __init__(self, alpha=1.0):\n        self.alpha = alpha\n        \n    def fit(self, X, y):\n        # X is (n_samples, n_features) of nonnegative integer counts\n        self.classes_ = np.unique(y)\n        n_samples, n_features = X.shape\n        \n        # Count total words per class\n        self.class_word_count_ = {}\n        self.class_counts_ = {}\n        self.feature_log_probs_ = {}\n        self.class_log_prior_ = {}\n        \n        for cls in self.classes_:\n            X_c = X[y == cls]\n            class_count = X_c.shape[0]\n            self.class_counts_[cls] = class_count\n            # Prior\n            self.class_log_prior_[cls] = np.log((class_count + self.alpha)\n                                                / (n_samples + len(self.classes_)*self.alpha))\n            # Sum of word counts in each dimension\n            word_sum = X_c.sum(axis=0)\n            total_count_in_class = word_sum.sum()\n            # Probability of each word in the vocabulary\n            self.feature_log_probs_[cls] = np.log((word_sum + self.alpha)\n                                                  / (total_count_in_class + n_features*self.alpha))\n            self.class_word_count_[cls] = total_count_in_class\n    \n    def predict(self, X):\n        # For each sample, compute log p(y) + sum_{features} [ x_alpha * log p(word_alpha|y) ]\n        predictions = []\n        for x in X:\n            class_scores = {}\n            for cls in self.classes_:\n                log_prob = self.class_log_prior_[cls]\n                log_prob += (x * self.feature_log_probs_[cls]).sum()\n                class_scores[cls] = log_prob\n            predictions.append(max(class_scores, key=class_scores.get))\n        return np.array(predictions)\n\n# Example usage:\nX = np.array([[2,1,0,0],[0,2,0,1],[1,0,1,0],[0,0,0,3]])  # Word counts\ny = np.array([0,0,1,1])\nmodel = MultinomialNaiveBayes(alpha=1.0)\nmodel.fit(X, y)\npreds = model.predict(X)\nprint("Predictions:", preds)\n'}),"\n",r.createElement(a.h3,{id:"103-implementing-gaussian-naive-bayes",style:{position:"relative"}},r.createElement(a.a,{href:"#103-implementing-gaussian-naive-bayes","aria-label":"103 implementing gaussian naive bayes permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"10.3 Implementing gaussian naive bayes"),"\n",r.createElement(a.p,null,"A simple version with continuous features:"),"\n",r.createElement(s.A,{text:'\nimport numpy as np\n\nclass GaussianNaiveBayes:\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y):\n        self.classes_ = np.unique(y)\n        n_samples, n_features = X.shape\n        self.class_stats_ = {}\n        self.class_prior_ = {}\n        \n        for cls in self.classes_:\n            X_c = X[y==cls]\n            self.class_prior_[cls] = X_c.shape[0] / n_samples\n            means = X_c.mean(axis=0)\n            vars_ = X_c.var(axis=0)\n            self.class_stats_[cls] = (means, vars_)\n            \n    def predict(self, X):\n        predictions = []\n        for x in X:\n            class_scores = {}\n            for cls in self.classes_:\n                prior = self.class_prior_[cls]\n                means, vars_ = self.class_stats_[cls]\n                # Gaussian PDF in each feature\n                log_likelihood = 0.0\n                for alpha_i, val in enumerate(x):\n                    mu = means[alpha_i]\n                    sigma2 = vars_[alpha_i] if vars_[alpha_i] > 1e-9 else 1e-9\n                    log_coeff = -0.5*np.log(2.0*np.pi*sigma2)\n                    log_exp = - ((val - mu)**2)/(2*sigma2)\n                    log_likelihood += log_coeff + log_exp\n                class_scores[cls] = np.log(prior) + log_likelihood\n            predictions.append(max(class_scores, key=class_scores.get))\n        return np.array(predictions)\n\n# Example usage:\nX = np.array([[1.5,2.3],[2.1,2.2],[10.0,8.0],[9.8,8.2],[2.2,2.1],[9.0,7.9]])\ny = np.array([0,0,1,1,0,1])\nmodel = GaussianNaiveBayes()\nmodel.fit(X, y)\npreds = model.predict(X)\nprint("Predictions:", preds)\n'}),"\n",r.createElement(a.h3,{id:"104-implementing-aode",style:{position:"relative"}},r.createElement(a.a,{href:"#104-implementing-aode","aria-label":"104 implementing aode permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"10.4 Implementing AODE"),"\n",r.createElement(a.p,null,"AODE is more complex than naive Bayes because we must consider one-dependence for each feature. For brevity, I'll outline the conceptual steps rather than produce a fully-fledged code:"),"\n",r.createElement(a.ol,null,"\n",r.createElement(a.li,null,"For each feature ",r.createElement(o.A,{text:"\\(X_j\\)"}),", treat it as a “superparent,” building a network where all other features ",r.createElement(o.A,{text:"\\(X_i\\)"})," depend on ",r.createElement(o.A,{text:"\\(X_j\\)"})," and the class ",r.createElement(o.A,{text:"\\(Y\\)"}),"."),"\n",r.createElement(a.li,null,"Estimate the conditional distributions ",r.createElement(o.A,{text:"\\(p(X_i \\mid Y, X_j)\\)"}),"."),"\n",r.createElement(a.li,null,"Combine or average the predictions from all such networks."),"\n"),"\n",r.createElement(a.p,null,"The averaging step helps mitigate the strong independence assumptions. Implementations can be found in various machine learning libraries or specialized code for AODE."),"\n",r.createElement(a.h3,{id:"105-implementing-bbn--bn",style:{position:"relative"}},r.createElement(a.a,{href:"#105-implementing-bbn--bn","aria-label":"105 implementing bbn  bn permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"10.5 Implementing BBN & BN"),"\n",r.createElement(a.p,null,"Implementing a full Bayesian Belief Network from scratch can be quite involved, especially if we allow arbitrary DAG structures. We can use libraries like ",r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">pgmpy</code>'}})," or ",r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">bnlearn</code>'}})," in Python. For instance, using ",r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">pgmpy</code>'}}),":"),"\n",r.createElement(s.A,{text:"\n# This is just a schematic usage example:\nfrom pgmpy.models import BayesianNetwork\nfrom pgmpy.factors.discrete import TabularCPD\n\n# Define the structure\nmodel = BayesianNetwork([('Y', 'X1'), ('Y', 'X2'), ('X1','X3')])\n\n# Define the CPDs\ncpd_Y = TabularCPD(variable='Y', variable_card=2,\n                   values=[[0.6], [0.4]])  # Prior for Y\ncpd_X1 = TabularCPD(variable='X1', variable_card=2,\n                    values=[[0.2, 0.7],[0.8, 0.3]],\n                    evidence=['Y'], evidence_card=[2])\n\ncpd_X2 = TabularCPD(variable='X2', variable_card=2,\n                    values=[[0.3, 0.4],[0.7, 0.6]],\n                    evidence=['Y'], evidence_card=[2])\n\ncpd_X3 = TabularCPD(variable='X3', variable_card=2,\n                    values=[[0.9,0.5,0.8,0.2],[0.1,0.5,0.2,0.8]],\n                    evidence=['X1','Y'], evidence_card=[2,2])\n\nmodel.add_cpds(cpd_Y, cpd_X1, cpd_X2, cpd_X3)\nmodel.check_model()\n\n# Then we can do inference:\nfrom pgmpy.inference import VariableElimination\ninfer = VariableElimination(model)\nposterior = infer.query(['Y'], evidence={'X1':1,'X2':0})\nprint(posterior)\n"}),"\n",r.createElement(a.p,null,"This exemplifies how to define a small BN, specify conditional probability tables, and run queries to obtain posterior probabilities for a node of interest."),"\n",r.createElement(a.h3,{id:"106-implementing-bayesian-regression",style:{position:"relative"}},r.createElement(a.a,{href:"#106-implementing-bayesian-regression","aria-label":"106 implementing bayesian regression permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"10.6 Implementing bayesian regression"),"\n",r.createElement(a.p,null,"A straightforward approach uses PyMC (now ",r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">pymc</code>'}})," library) or PyStan, etc. Here's a small PyMC example for Bayesian linear regression:"),"\n",r.createElement(s.A,{text:"\n!pip install pymc  # If not installed\n\nimport pymc as pm\nimport numpy as np\n\n# Generate some synthetic data\nnp.random.seed(42)\nN = 100\nX = np.linspace(0,1,N)\ntrue_w0 = 1.0\ntrue_w1 = 2.5\ntrue_sigma = 0.2\ny = true_w0 + true_w1*X + np.random.normal(0,true_sigma,N)\n\nwith pm.Model() as model:\n    # Priors\n    w0 = pm.Normal('w0', mu=0, sigma=10)\n    w1 = pm.Normal('w1', mu=0, sigma=10)\n    sigma = pm.HalfCauchy('sigma', beta=1)\n    \n    # Likelihood\n    mu = w0 + w1*X\n    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=y)\n    \n    # Sampling from the posterior\n    trace = pm.sample(1000, tune=1000, cores=1)\n    \npm.summary(trace)\n"}),"\n",r.createElement(a.p,null,"We specify priors for ",r.createElement(o.A,{text:"\\(w_0\\)"}),", ",r.createElement(o.A,{text:"\\(w_1\\)"}),", and ",r.createElement(o.A,{text:"\\(\\sigma\\)"}),". We define the likelihood for the observed data ",r.createElement(o.A,{text:"\\(y\\)"}),". PyMC then uses MCMC (by default, the No-U-Turn Sampler) to sample from the joint posterior of these parameters."),"\n",r.createElement("br"),"\n",r.createElement(a.h2,{id:"11-misc-notes",style:{position:"relative"}},r.createElement(a.a,{href:"#11-misc-notes","aria-label":"11 misc notes permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"11. Misc notes"),"\n",r.createElement(a.h3,{id:"111-the-role-of-priors-in-controlling-overfitting",style:{position:"relative"}},r.createElement(a.a,{href:"#111-the-role-of-priors-in-controlling-overfitting","aria-label":"111 the role of priors in controlling overfitting permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"11.1 The role of priors in controlling overfitting"),"\n",r.createElement(a.p,null,"Placing a prior on model parameters can be viewed as imposing a regularization penalty in a frequentist sense. If you place a small-variance Gaussian prior on a weight ",r.createElement(o.A,{text:"\\(w\\)"}),", you strongly believe that ",r.createElement(o.A,{text:"\\(w\\)"})," is near zero unless data strongly indicates otherwise. This effectively shrinks the parameter estimates, preventing them from exploding in magnitude and leading to overfitting."),"\n",r.createElement(a.p,null,"In classification tasks, specifying a prior over class probabilities or feature-likelihood parameters can also help when the training set is small or if certain classes are more (or less) common than the data alone might indicate. For instance, in a spam detection system, your prior might be that 80% of email is non-spam; even if your training set is somewhat skewed, that prior will keep the model from drifting too far if the sample misrepresents reality."),"\n",r.createElement(a.h3,{id:"112-hierarchical-bayesian-models",style:{position:"relative"}},r.createElement(a.a,{href:"#112-hierarchical-bayesian-models","aria-label":"112 hierarchical bayesian models permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"11.2 Hierarchical bayesian models"),"\n",r.createElement(a.p,null,"Hierarchical (or multilevel) Bayesian models add another layer of complexity (and interpretability). Parameters that describe different subsets of the data share hyperparameters, capturing partial pooling. This can be especially powerful in scenarios like:"),"\n",r.createElement(a.ul,null,"\n",r.createElement(a.li,null,"Repeated measurements from multiple subjects (e.g., biomedical or psychological studies)."),"\n",r.createElement(a.li,null,"Group-level structures (e.g., schools, states, counties)."),"\n",r.createElement(a.li,null,"Time-series with state-space models."),"\n"),"\n",r.createElement(a.p,null,"Hierarchical models can reduce overfitting by borrowing statistical strength across groups. However, they do require advanced inference methods or large datasets if the model is complex."),"\n",r.createElement(a.h3,{id:"113-training-and-regularization",style:{position:"relative"}},r.createElement(a.a,{href:"#113-training-and-regularization","aria-label":"113 training and regularization permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"11.3 Training and regularization"),"\n",r.createElement(a.p,null,"In a Bayesian viewpoint, “training” can be thought of as computing or approximating the posterior distribution. Meanwhile, “regularization” naturally arises from priors. Tuning hyperparameters of the prior (e.g., the variance of a Gaussian prior) serves a similar role to tuning regularization strength in a frequentist model."),"\n",r.createElement(a.h3,{id:"114-improving-tuning-bayesian-models",style:{position:"relative"}},r.createElement(a.a,{href:"#114-improving-tuning-bayesian-models","aria-label":"114 improving tuning bayesian models permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"11.4 Improving (tuning) bayesian models"),"\n",r.createElement(a.p,null,"Practical aspects that can drastically improve performance:"),"\n",r.createElement(a.ul,null,"\n",r.createElement(a.li,null,r.createElement(a.strong,null,"Choice of prior"),": If domain knowledge is available, using an informed prior can be very helpful."),"\n",r.createElement(a.li,null,r.createElement(a.strong,null,"Feature engineering"),": As with any ML approach, the quality of features matters."),"\n",r.createElement(a.li,null,r.createElement(a.strong,null,"Inference method"),": Using more robust sampling or optimization-based approximations can help, especially if the posterior has multiple modes or strong correlations among parameters."),"\n",r.createElement(a.li,null,r.createElement(a.strong,null,"Model selection"),": Tools like ",r.createElement(l.A,null,"Bayes factors")," or ",r.createElement(l.A,null,"Deviance Information Criterion (DIC)")," can help compare different models or priors in a Bayesian context, although they can be expensive to compute."),"\n"),"\n",r.createElement(a.h3,{id:"115-use-cases",style:{position:"relative"}},r.createElement(a.a,{href:"#115-use-cases","aria-label":"115 use cases permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"11.5 Use cases"),"\n",r.createElement(a.p,null,"Bayesian models are used extensively in:"),"\n",r.createElement(a.ul,null,"\n",r.createElement(a.li,null,r.createElement(a.strong,null,"Medical domain"),": BNs for diagnosis, hierarchical modeling of treatment effects, etc."),"\n",r.createElement(a.li,null,r.createElement(a.strong,null,"Finance"),": Bayesian forecasting, volatility modeling, risk assessment with prior knowledge from historical events."),"\n",r.createElement(a.li,null,r.createElement(a.strong,null,"Natural Language Processing"),": Spam detection, text classification with naive Bayes, topic modeling with Dirichlet priors."),"\n",r.createElement(a.li,null,r.createElement(a.strong,null,"Scientific research"),": Where interpretability and quantification of uncertainty are paramount, e.g., astrophysics or ecology."),"\n"),"\n",r.createElement(a.h3,{id:"116-general-recommendations",style:{position:"relative"}},r.createElement(a.a,{href:"#116-general-recommendations","aria-label":"116 general recommendations permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"11.6 General recommendations"),"\n",r.createElement(a.p,null,"When deciding whether to adopt a Bayesian approach, weigh the interpretability and uncertainty quantification benefits against the computational cost. For many problems, a well-structured Bayesian model can yield more robust predictions and richer insights into uncertainty. That said, approximate inference is typically required outside the realm of conjugate priors, so plan your computational resources accordingly."),"\n",r.createElement("br"),"\n",r.createElement(a.h2,{id:"12-summary",style:{position:"relative"}},r.createElement(a.a,{href:"#12-summary","aria-label":"12 summary permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"12. Summary"),"\n",r.createElement(a.p,null,"Bayesian models ground machine learning in the language of probability theory, allowing you to encode prior knowledge, handle uncertainty in parameters, and systematically update beliefs based on observed data. Beginning with the fundamental notions of priors, likelihoods, and posteriors, we've explored how these ideas manifest in:"),"\n",r.createElement(a.ul,null,"\n",r.createElement(a.li,null,r.createElement(a.strong,null,"Naive Bayes classifiers"),", which, despite making strong conditional independence assumptions, can work remarkably well in practice, especially for text classification (multinomial NB) or continuous data (Gaussian NB)."),"\n",r.createElement(a.li,null,r.createElement(a.strong,null,"AODE"),", which relaxes naive Bayes by allowing one-dependence among features."),"\n",r.createElement(a.li,null,r.createElement(a.strong,null,"Bayesian Belief Networks"),", offering a more general DAG-based approach to capture complex dependencies."),"\n",r.createElement(a.li,null,r.createElement(a.strong,null,"Bayesian regression"),", particularly in linear models, where priors serve as regularizers, and posterior distributions quantify the uncertainty in the regression coefficients."),"\n"),"\n",r.createElement(a.p,null,"Along the way, we encountered the significance of conjugate priors, the complexities of inference (MCMC, variational methods), and the importance of carefully specifying priors to reflect domain knowledge or desired model complexity."),"\n",r.createElement(a.p,null,"While naive Bayes variants can be trained with closed-form or simple counting approaches, general Bayesian models often rely on advanced computational machinery. Nevertheless, the conceptual clarity of the Bayesian framework — representing knowledge as a distribution that evolves with data — remains a powerful tool for interpretability and robust predictions."),"\n",r.createElement(a.p,null,"Bayesian approaches will continue to play an important role in machine learning, whether in purely generative scenarios, structured graphical models, or combined with neural architectures (e.g., Bayesian deep learning). With the knowledge gained here, you can confidently explore the rich universe of Bayesian methods and decide when and how to use them in your own data science and machine learning pipelines."))}var h=function(e){void 0===e&&(e={});const{wrapper:a}=Object.assign({},(0,i.RP)(),e.components);return a?r.createElement(a,e,r.createElement(m,e)):m(e)},d=t(36710),p=t(58481),u=t.n(p),f=t(36310),g=t(87245),y=t(27042),v=t(59849),b=t(5591),E=t(61122),w=t(9219),x=t(33203),_=t(95751),S=t(94328),B=t(80791),M=t(78137);const k=e=>{let{toc:a}=e;if(!a||!a.items)return null;return r.createElement("nav",{className:B.R},r.createElement("ul",null,a.items.map(((e,a)=>r.createElement("li",{key:a},r.createElement("a",{href:e.url,onClick:a=>((e,a)=>{e.preventDefault();const t=a.replace("#",""),n=document.getElementById(t);n&&n.scrollIntoView({behavior:"smooth",block:"start"})})(a,e.url)},e.title),e.items&&r.createElement(k,{toc:{items:e.items}}))))))};function H(e){let{data:{mdx:a,allMdx:l,allPostImages:s},children:o}=e;const{frontmatter:c,body:m,tableOfContents:h}=a,d=c.index,p=c.slug.split("/")[1],v=l.nodes.filter((e=>e.frontmatter.slug.includes(`/${p}/`))).sort(((e,a)=>e.frontmatter.index-a.frontmatter.index)),B=v.findIndex((e=>e.frontmatter.index===d)),H=v[B+1],C=v[B-1],A=c.slug.replace(/\/$/,""),z=/[^/]*$/.exec(A)[0],N=`posts/${p}/content/${z}/`,{0:T,1:I}=(0,r.useState)(c.flagWideLayoutByDefault),{0:L,1:V}=(0,r.useState)(!1);var j;(0,r.useEffect)((()=>{V(!0);const e=setTimeout((()=>V(!1)),340);return()=>clearTimeout(e)}),[T]),"adventures"===p?j=w.cb:"research"===p?j=w.Qh:"thoughts"===p&&(j=w.T6);const D=u()(m).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,P=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const a=Math.floor(e/60),t=e%60;return t<=30?`~${a}${t>0?".5":""} h`:`~${a+1} h`}(Math.ceil(D/j)+(c.extraReadTimeMin||0)),X=[{flag:c.flagDraft,component:()=>Promise.all([t.e(3231),t.e(8809)]).then(t.bind(t,28809))},{flag:c.flagMindfuckery,component:()=>Promise.all([t.e(3231),t.e(2471)]).then(t.bind(t,67709))},{flag:c.flagRewrite,component:()=>Promise.all([t.e(3231),t.e(6764)]).then(t.bind(t,62002))},{flag:c.flagOffensive,component:()=>Promise.all([t.e(3231),t.e(2443)]).then(t.bind(t,17681))},{flag:c.flagProfane,component:()=>Promise.all([t.e(3231),t.e(8048)]).then(t.bind(t,53286))},{flag:c.flagMultilingual,component:()=>Promise.all([t.e(3231),t.e(4069)]).then(t.bind(t,78831))},{flag:c.flagUnreliably,component:()=>Promise.all([t.e(3231),t.e(3417)]).then(t.bind(t,8179))},{flag:c.flagPolitical,component:()=>Promise.all([t.e(3231),t.e(5195)]).then(t.bind(t,30433))},{flag:c.flagCognitohazard,component:()=>Promise.all([t.e(3231),t.e(3175)]).then(t.bind(t,8413))},{flag:c.flagHidden,component:()=>Promise.all([t.e(3231),t.e(9556)]).then(t.bind(t,14794))}],{0:G,1:O}=(0,r.useState)([]);return(0,r.useEffect)((()=>{X.forEach((e=>{let{flag:a,component:t}=e;a&&t().then((e=>{O((a=>[].concat((0,n.A)(a),[e.default])))}))}))}),[]),r.createElement(y.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},r.createElement(b.A,{postNumber:c.index,date:c.date,updated:c.updated,readTime:P,difficulty:c.difficultyLevel,title:c.title,desc:c.desc,banner:c.banner,section:p,postKey:z,isMindfuckery:c.flagMindfuckery,mainTag:c.mainTag}),r.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},c.otherTags.map(((e,a)=>r.createElement("span",{key:a,className:`noselect ${M.MW}`,style:{margin:"0 5px 5px 0"}},e)))),r.createElement("div",{class:"postBody"},r.createElement(k,{toc:h})),r.createElement("br"),r.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},r.createElement(y.P.button,{class:"noselect",className:S.pb,id:S.xG,onClick:()=>{I(!T)},whileTap:{scale:.93}},r.createElement(y.P.div,{className:_.DJ,key:T,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},T?"Switch to default layout":"Switch to wide layout"))),r.createElement("br"),r.createElement("div",{class:"postBody",style:{margin:T?"0 -14%":"",maxWidth:T?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},r.createElement("div",{className:`${S.P_} ${L?S.Xn:S.qG}`},G.map(((e,a)=>r.createElement(e,{key:a}))),c.indexCourse?r.createElement(x.A,{index:c.indexCourse,category:c.courseCategoryName}):"",r.createElement(f.Z.Provider,{value:{images:s.nodes,basePath:N.replace(/\/$/,"")+"/"}},r.createElement(i.xA,{components:{Image:g.A}},o)))),r.createElement(E.A,{nextPost:H,lastPost:C,keyCurrent:z,section:p}))}function C(e){return r.createElement(H,e,r.createElement(h,e))}function A(e){var a,t,n,i,l;let{data:s}=e;const{frontmatter:o}=s.mdx,c=o.titleSEO||o.title,m=o.titleOG||c,h=o.titleTwitter||c,p=o.descSEO||o.desc,u=o.descOG||p,f=o.descTwitter||p,g=o.schemaType||"BlogPosting",y=o.keywordsSEO,b=o.date,E=o.updated||b,w=o.imageOG||(null===(a=o.banner)||void 0===a||null===(t=a.childImageSharp)||void 0===t||null===(n=t.gatsbyImageData)||void 0===n||null===(i=n.images)||void 0===i||null===(l=i.fallback)||void 0===l?void 0:l.src),x=o.imageAltOG||u,_=o.imageTwitter||w,S=o.imageAltTwitter||f,B=o.canonicalURL,M=o.flagHidden||!1,k=o.mainTag||"Posts",H=o.slug.split("/")[1]||"posts",{siteUrl:C}=(0,d.Q)(),A={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:C},{"@type":"ListItem",position:2,name:k,item:`${C}/${o.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${C}${o.slug}`}]};return r.createElement(v.A,{title:c+" - avrtt.blog",titleOG:m,titleTwitter:h,description:p,descriptionOG:u,descriptionTwitter:f,schemaType:g,keywords:y,datePublished:b,dateModified:E,imageOG:w,imageAltOG:x,imageTwitter:_,imageAltTwitter:S,canonicalUrl:B,flagHidden:M,mainTag:k,section:H,type:"article"},r.createElement("script",{type:"application/ld+json"},JSON.stringify(A)))}},66501:function(e,a,t){t.d(a,{A:function(){return l}});var n=t(96540),i=t(3962),r="styles-module--tooltiptext--a263b";var l=e=>{let{text:a,isBadge:t=!1}=e;const{0:l,1:s}=(0,n.useState)(!1),o=(0,n.useRef)(null);return(0,n.useEffect)((()=>{function e(e){o.current&&!o.current.contains(e.target)&&s(!1)}return document.addEventListener("click",e),()=>{document.removeEventListener("click",e)}}),[]),n.createElement("span",{className:"styles-module--tooltipWrapper--75ebf",ref:o},n.createElement("img",{id:t?"styles-module--infoBadge--e3d66":"styles-module--info--26c1f",src:i.A,alt:"info",onClick:e=>{e.stopPropagation(),s((e=>!e))}}),n.createElement("span",{className:l?`${r} styles-module--visible--c063c`:r},a))}},96098:function(e,a,t){var n=t(96540),i=t(7978);a.A=e=>{let{text:a}=e;return n.createElement(i.A,null,a)}}}]);
//# sourceMappingURL=component---src-templates-post-js-content-file-path-src-pages-posts-research-bayesian-models-mdx-57c2a2814de8ffeab788.js.map