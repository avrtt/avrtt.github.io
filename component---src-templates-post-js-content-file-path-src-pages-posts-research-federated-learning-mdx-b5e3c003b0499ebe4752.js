"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[863],{51905:function(e,t,a){a.r(t),a.d(t,{Head:function(){return C},PostTemplate:function(){return x},default:function(){return _}});var n=a(54506),i=a(28453),r=a(96540),l=a(16886),o=a(46295),s=a(96098);function c(e){const t=Object.assign({p:"p",h3:"h3",a:"a",span:"span",h2:"h2",h4:"h4",ol:"ol",li:"li",ul:"ul",strong:"strong",hr:"hr"},(0,i.RP)(),e.components),{Image:a}=t;return a||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),r.createElement(r.Fragment,null,"\n",r.createElement("br"),"\n","\n",r.createElement(t.p,null,"Federated learning, often abbreviated as FL, has emerged in recent years as one of the most compelling paradigms for training machine learning models on distributed data without the need to centralize it. The idea that data can stay on the devices where it was produced — such as smartphones, hospital data centers, or other corporate data silos — offers significant advantages in terms of privacy and efficiency. I find it particularly fascinating because, in the modern digital landscape, organizations and individuals alike produce vast amounts of data that often cannot be shared freely due to governance constraints, confidentiality concerns, or compliance with regulations like HIPAA, GDPR, CCPA, and so on."),"\n",r.createElement(t.h3,{id:"11-why-federated-learning",style:{position:"relative"}},r.createElement(t.a,{href:"#11-why-federated-learning","aria-label":"11 why federated learning permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"1.1. Why federated learning?"),"\n",r.createElement(t.p,null,"The traditional paradigm in machine learning has been: gather data from multiple sources, pool it into a single data store (e.g., a data warehouse or a cloud environment), and then train a model. This approach, though widely adopted, suffers from multiple drawbacks. First and foremost, privacy is endangered because data from individuals might be shared with third parties. Second, for large organizations, centralizing data can become incredibly expensive in terms of network bandwidth and storage. Finally, there are regulatory frameworks and legal guidelines that explicitly forbid transferring certain categories of sensitive data across national or organizational boundaries."),"\n",r.createElement(t.p,null,"Federated learning addresses these issues by moving the computation (i.e., model training) to the devices themselves and allowing them to train locally. Only the model updates — such as gradients or weight deltas — are sent back to an aggregating server that consolidates these updates into a global model. This approach ensures the raw data never leaves the device or local data center, significantly reducing the possibility of privacy breaches."),"\n",r.createElement(t.h3,{id:"12-motivation-and-business-impacts",style:{position:"relative"}},r.createElement(t.a,{href:"#12-motivation-and-business-impacts","aria-label":"12 motivation and business impacts permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"1.2. Motivation and business impacts"),"\n",r.createElement(t.p,null,'From a business perspective, federated learning paves the way for data monetization and machine learning improvements without the usual privacy pitfalls. Organizations that might have once been adversaries or competitors in the data space now have more opportunities for collaboration. For instance, two rival telecom companies could potentially perform a federated learning project on their user data without either side "seeing" the other\'s sensitive user information. This fosters new types of partnerships and data-driven products.'),"\n",r.createElement(t.p,null,"Moreover, the capacity to train models without incurring substantial data transfer costs can be very appealing. Edge devices, such as smartphones, IoT sensors, or wearables, generate large volumes of data daily. By harnessing this data locally, service providers can create more personalized, real-time, and adaptive models. For example, think of a voice recognition system on mobile phones — training continuously on your device based on your personal usage patterns — while also contributing to an overarching global model that benefits from aggregated experiences across millions of devices."),"\n",r.createElement(t.h3,{id:"13-industry-adoption-and-case-studies",style:{position:"relative"}},r.createElement(t.a,{href:"#13-industry-adoption-and-case-studies","aria-label":"13 industry adoption and case studies permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"1.3. Industry adoption and case studies"),"\n",r.createElement(t.p,null,'Several well-known companies have embraced federated learning. One of the earliest large-scale adopters was Google, which used it for improving the predictive keyboard on Android devices (the "Gboard" application). Instead of collecting typed text from user devices — a privacy nightmare — Google tested and updated language models locally and then aggregated only the learned updates. Another example is Apple\'s use of on-device machine learning for Siri, enabling partial personalization of speech recognition models on iPhones.'),"\n",r.createElement(t.p,null,"In healthcare, federated learning has gained traction as a solution for collaborative medical research. Hospitals can train a shared model (for example, for cancer detection in medical images) without having to pool sensitive patient data in a single repository. Recent work, such as Sheller and gang in the Medical Image Computing and Computer-Assisted Intervention conference, demonstrated that federated learning can allow multiple hospitals to jointly build stronger radiological diagnostic models than any single hospital could on its own data. This collaborative approach is crucial for expanding data diversity and improving model robustness."),"\n",r.createElement(t.p,null,"In finance, banks and financial institutions are experimenting with federated approaches to detect fraud, analyze credit risk, or build recommendation models for personalized financial products. Each bank retains its user data but contributes to a global model that benefits from knowledge across multiple institutions."),"\n",r.createElement(t.p,null,"These examples underscore the multifaceted impact of federated learning, from reducing privacy concerns, cutting down data migration costs, to enabling consortia that co-develop advanced AI systems."),"\n",r.createElement(t.h2,{id:"2-core-concepts-of-federated-learning",style:{position:"relative"}},r.createElement(t.a,{href:"#2-core-concepts-of-federated-learning","aria-label":"2 core concepts of federated learning permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2. Core concepts of federated learning"),"\n",r.createElement(t.p,null,"At its heart, federated learning is about training a single global model across multiple devices or data centers that each hold their own private datasets. The objective is typically to minimize a global loss function:"),"\n",r.createElement(s.A,{text:"\\[\n\\min_{w} \\frac{1}{K} \\sum_{k=1}^{K} L_k(w),\n\\]"}),"\n",r.createElement(t.p,null,"where ",r.createElement(s.A,{text:"\\(K\\)"})," is the number of clients (devices or data silos), ",r.createElement(s.A,{text:"\\(w\\)"})," is the model parameter vector, and ",r.createElement(s.A,{text:"\\(L_k(w)\\)"})," is the local loss on the ",r.createElement(s.A,{text:"k^{th}"})," device. The subtlety is that you want to achieve this minimization without shipping all local data to a central server."),"\n",r.createElement(t.h3,{id:"21-the-architecture-of-federated-learning-systems",style:{position:"relative"}},r.createElement(t.a,{href:"#21-the-architecture-of-federated-learning-systems","aria-label":"21 the architecture of federated learning systems permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.1. The architecture of federated learning systems"),"\n",r.createElement(t.p,null,"A typical federated learning system follows a client-server architecture. The central server coordinates the learning process, selecting which clients participate in each training round, sending them the current global model, receiving updates, and aggregating these updates into a new global model. Clients are devices or data centers that each store some portion of the data. That data never leaves the client. Usually, the server orchestrates periodic global updates (e.g., once per round), while clients do local computations on their data."),"\n",r.createElement(t.h4,{id:"traditional-pipeline",style:{position:"relative"}},r.createElement(t.a,{href:"#traditional-pipeline","aria-label":"traditional pipeline permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Traditional pipeline"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,"The server initializes a global model ",r.createElement(s.A,{text:"\\(w_0\\)"}),"."),"\n",r.createElement(t.li,null,"The server selects a subset of clients (randomly or based on availability)."),"\n",r.createElement(t.li,null,"Each selected client trains the current model on its local dataset, obtaining updates (e.g., gradient steps)."),"\n",r.createElement(t.li,null,"These local updates are transmitted to the server."),"\n",r.createElement(t.li,null,"The server aggregates these updates (e.g., using Federated Averaging, or some more sophisticated method)."),"\n",r.createElement(t.li,null,"The server obtains a new global model ",r.createElement(s.A,{text:"\\(w_1\\)"}),"."),"\n",r.createElement(t.li,null,"The process repeats until some convergence criteria is met."),"\n"),"\n",r.createElement(t.h3,{id:"22-data-decentralization-and-local-training",style:{position:"relative"}},r.createElement(t.a,{href:"#22-data-decentralization-and-local-training","aria-label":"22 data decentralization and local training permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.2. Data decentralization and local training"),"\n",r.createElement(t.p,null,"One of the hallmark aspects of federated learning is data decentralization. Instead of a single data warehouse, each participating node (client) has partial data. This is more than just a design choice — it is the fundamental premise. Training occurs where the data resides. This approach is particularly powerful in scenarios where data is not only large but also highly sensitive. Hospitals, for instance, cannot share raw medical records due to regulations, but they can train local models on their premises and only share model updates."),"\n",r.createElement(t.h3,{id:"23-aggregation-techniques",style:{position:"relative"}},r.createElement(t.a,{href:"#23-aggregation-techniques","aria-label":"23 aggregation techniques permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.3. Aggregation techniques"),"\n",r.createElement(t.p,null,"The fundamental operation that transforms local updates into a global model is called aggregation. The simplest and still one of the most widely-used aggregation methods is ",r.createElement(l.A,null,"Federated Averaging")," (FedAvg), introduced by McMahan and gang (2017). In this procedure, each client computes an updated set of weights ",r.createElement(s.A,{text:"\\(w_k\\)"})," after local training. Then the central server aggregates these by taking a weighted average of the client updates, typically weighted by the size of each client's dataset:"),"\n",r.createElement(s.A,{text:"\\[\nw_{t+1} = \\sum_{k=1}^K \\frac{n_k}{n_{\\text{total}}} w_k^t,\n\\]"}),"\n",r.createElement(t.p,null,"where ",r.createElement(s.A,{text:"\\(n_k\\)"})," is the number of training samples on client ",r.createElement(s.A,{text:"k\\)"})," and ",r.createElement(s.A,{text:"\\(n_{\\text{total}} = \\sum_{k=1}^K n_k\\)"})," is the total number of training samples across all selected clients. This ensures that clients with more data have a greater say in the global model."),"\n",r.createElement(t.p,null,"Advanced aggregation methods might involve robust aggregation strategies (e.g., ignoring outliers or malicious updates), secure aggregation that uses cryptographic techniques to hide individual updates, or gradient-based merging that better aligns with the distribution of the data."),"\n",r.createElement(t.h2,{id:"3-types-of-federated-learning",style:{position:"relative"}},r.createElement(t.a,{href:"#3-types-of-federated-learning","aria-label":"3 types of federated learning permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3. Types of federated learning"),"\n",r.createElement(t.p,null,"Federated learning can be categorized based on how the data across clients is distributed — horizontally, vertically, or in some hybrid scenario. Another perspective is the difference in feature spaces across clients versus the difference in user sets."),"\n",r.createElement(t.h3,{id:"31-horizontal-federated-learning",style:{position:"relative"}},r.createElement(t.a,{href:"#31-horizontal-federated-learning","aria-label":"31 horizontal federated learning permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.1. Horizontal federated learning"),"\n",r.createElement(t.p,null,"Horizontal federated learning (HFL) is also sometimes referred to as sample-partitioned federated learning. In HFL, all participants (clients) share the same feature space but have different sets of data samples. For instance, each smartphone user has data from the same set of features (e.g., text typed in the keyboard, usage logs, etc.) but each user has unique training examples. This is the most common form of federated learning, and the typical FedAvg approach is often explained in terms of horizontal FL."),"\n",r.createElement(t.h3,{id:"32-vertical-federated-learning",style:{position:"relative"}},r.createElement(t.a,{href:"#32-vertical-federated-learning","aria-label":"32 vertical federated learning permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.2. Vertical federated learning"),"\n",r.createElement(t.p,null,"Vertical federated learning (VFL) is feature-partitioned. This implies that two or more organizations have overlapping user bases (same set of data instances or subjects), but they store different features for these individuals. For example, a bank and an e-commerce platform might share many of the same customers but maintain different attributes for each. The bank has financial transaction data, while the e-commerce site has purchase behaviors. When these two organizations wish to build a predictive model together (e.g., for credit scoring or personalized recommendations), they face a challenge: they need a complete feature vector for each user, but that vector is split across two or more organizations."),"\n",r.createElement(t.p,null,"Vertical FL solutions typically involve secure entity alignment protocols and cryptographic mechanisms to join partial features without revealing sensitive data. One line of research uses partial homomorphic encryption to ensure that model updates can be securely aggregated across the different feature owners."),"\n",r.createElement(t.h3,{id:"33-federated-transfer-learning",style:{position:"relative"}},r.createElement(t.a,{href:"#33-federated-transfer-learning","aria-label":"33 federated transfer learning permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.3. Federated transfer learning"),"\n",r.createElement(t.p,null,"Federated transfer learning (FTL) addresses scenarios where the data distribution differs across the participating parties, and they might only partially overlap in terms of users. In some cases, the aim is to apply knowledge learned from a large dataset in one domain to a different but related domain. This situation might arise when an institution has a small dataset for a specific set of features, while a partner institution has a large dataset but not exactly the same features or the same distribution."),"\n",r.createElement(t.p,null,"For instance, a health insurance firm might want to team up with a hospital chain. The hospital has clinical records (medical images, lab tests), while the insurer has claims data. The distribution of data is different, but certain shared elements — such as patient IDs or partial demographic information — can align them. Federated transfer learning then becomes a powerful approach to incorporate knowledge from each domain while respecting all privacy constraints."),"\n",r.createElement(t.h3,{id:"34-comparison-of-different-types",style:{position:"relative"}},r.createElement(t.a,{href:"#34-comparison-of-different-types","aria-label":"34 comparison of different types permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.4. Comparison of different types"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Horizontal FL")," is simpler in concept, focusing on how to aggregate model updates from many clients that share the same features but have different data samples."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Vertical FL")," requires more sophisticated privacy-preserving techniques to combine complementary features. This is typically more complex due to the need for secure joint modeling over partially overlapping user sets."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Federated transfer learning")," supports scenarios where both data distribution and feature spaces may differ significantly across participants, leveraging domain adaptation or transfer learning concepts."),"\n"),"\n",r.createElement(t.h2,{id:"4-key-components-of-federated-learning-systems",style:{position:"relative"}},r.createElement(t.a,{href:"#4-key-components-of-federated-learning-systems","aria-label":"4 key components of federated learning systems permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4. Key components of federated learning systems"),"\n",r.createElement(t.p,null,"No matter which type of FL we discuss, some building blocks recur: a client-server architecture, secure aggregation protocols, effective communication channels, optimization strategies that handle model updates, and the enabling hardware (often edge devices)."),"\n",r.createElement(t.h3,{id:"41-client-server-architecture",style:{position:"relative"}},r.createElement(t.a,{href:"#41-client-server-architecture","aria-label":"41 client server architecture permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.1. Client-server architecture"),"\n",r.createElement(t.p,null,"Most real-world federated learning deployments rely on a central coordinating server. That server:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"Maintains the global model."),"\n",r.createElement(t.li,null,"Chooses a set of clients to participate in each round."),"\n",r.createElement(t.li,null,"Sends them the model parameters."),"\n",r.createElement(t.li,null,"Collects the updates."),"\n",r.createElement(t.li,null,"Aggregates them."),"\n",r.createElement(t.li,null,"Distributes the updated global model back to the participating clients."),"\n"),"\n",r.createElement(t.p,null,"The clients may be mobile devices (phones, wearables, etc.), small data centers in edge environments, or siloed enterprise data warehouses. The main objective is to minimize communication overhead while maximizing the efficiency and reliability of local computations."),"\n",r.createElement(t.h3,{id:"42-secure-aggregation-protocols",style:{position:"relative"}},r.createElement(t.a,{href:"#42-secure-aggregation-protocols","aria-label":"42 secure aggregation protocols permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.2. Secure aggregation protocols"),"\n",r.createElement(t.p,null,'One of the biggest draws of federated learning is privacy enhancement. However, naive approaches to distributing updates can still leak private information. For instance, if model gradients are shared in the clear, an attacker who gains access to those gradients might reconstruct some aspects of local data (this is sometimes referred to as an "inversion attack"). Hence, specialized cryptographic protocols or advanced privacy-preserving methods (e.g., differential privacy or homomorphic encryption) can be introduced.'),"\n",r.createElement(t.p,null,"A well-cited solution is the \"Secure Aggregation\" protocol proposed by Bonawitz and gang (2017). In that framework, the server only sees the sum of the client updates, not the individual updates themselves. Each client's update is masked with random vectors that cancel out when aggregated, thereby hiding each client's contribution. This approach can significantly reduce the risk of data leakage through model updates."),"\n",r.createElement(t.h3,{id:"43-communication-methods-and-optimization",style:{position:"relative"}},r.createElement(t.a,{href:"#43-communication-methods-and-optimization","aria-label":"43 communication methods and optimization permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.3. Communication methods and optimization"),"\n",r.createElement(t.p,null,"Communication is often the bottleneck in federated learning. The success of a global model update depends on how effectively information from each client is captured in as few bits as possible. Techniques like gradient compression, quantization, and sparsification aim to reduce message size. Recently, there has been research into using advanced compression mechanisms, e.g., top-<k> gradient selection, or error-compensated quantization."),"\n",r.createElement(t.p,null,"Beyond the mechanics of communication, protocol optimization is also crucial. Federated learning environments often deal with partial participation, because not all clients are available or online at once, and some clients have limited bandwidth or battery. Hence, scheduling which clients should participate in each round is a non-trivial problem."),"\n",r.createElement(t.h3,{id:"44-role-of-edge-devices",style:{position:"relative"}},r.createElement(t.a,{href:"#44-role-of-edge-devices","aria-label":"44 role of edge devices permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.4. Role of edge devices"),"\n",r.createElement(t.p,null,"As the name suggests, a significant portion of federated learning research and industry applications revolve around edge devices such as smartphones, tablets, or IoT devices. These devices have constraints: limited compute power, limited storage, and sporadic connectivity. Therefore, FL algorithms must be resource-efficient, making local updates feasible within a small time window (for example, while the device is charging and connected to Wi-Fi). This impetus toward on-device learning has stimulated broad interest in model distillation and smaller neural architectures."),"\n",r.createElement(t.h3,{id:"45-additional-components",style:{position:"relative"}},r.createElement(t.a,{href:"#45-additional-components","aria-label":"45 additional components permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.5. Additional components"),"\n",r.createElement(t.p,null,"While not always highlighted, there are further vital components:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Reconnection policies")," for devices that drop out mid-training."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Client sampling strategies")," for fairness or resource balancing."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Logging and monitoring")," to understand how the global model is improving (or not) across rounds."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Fallback or fallback models")," in case aggregated updates are insufficient or corrupted."),"\n"),"\n",r.createElement(t.h2,{id:"5-algorithms-used-in-federated-learning",style:{position:"relative"}},r.createElement(t.a,{href:"#5-algorithms-used-in-federated-learning","aria-label":"5 algorithms used in federated learning permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5. Algorithms used in federated learning"),"\n",r.createElement(t.p,null,"Federated learning has spurred numerous specialized algorithms. The general idea is that each client performs local updates (e.g., gradient descent) for one or several epochs, and then the server aggregates these updates into a new global state."),"\n",r.createElement(t.h3,{id:"51-federated-averaging-fedavg",style:{position:"relative"}},r.createElement(t.a,{href:"#51-federated-averaging-fedavg","aria-label":"51 federated averaging fedavg permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.1. Federated averaging (FedAvg)"),"\n",r.createElement(t.p,null,"The classical FedAvg algorithm proposed by McMahan and gang (2017) is the foundation of most FL implementations. The server begins with an initial model ",r.createElement(s.A,{text:"\\(w_0\\)"}),". In each global round ",r.createElement(s.A,{text:"\\(t\\)"}),":"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,"The server randomly chooses a subset of clients ",r.createElement(s.A,{text:"(\\mathcal{S}_t\\)"}),"."),"\n",r.createElement(t.li,null,"It sends ",r.createElement(s.A,{text:"\\(w_t\\)"})," to each client in ",r.createElement(s.A,{text:"(\\mathcal{S}_t\\)"}),"."),"\n",r.createElement(t.li,null,"Each selected client performs a local update of ",r.createElement(s.A,{text:"\\(w_t\\)"})," by running multiple epochs of stochastic gradient descent (SGD) on its local data, yielding ",r.createElement(s.A,{text:"\\(w_k^t\\)"}),"."),"\n",r.createElement(t.li,null,"The server collects these local updates and computes"),"\n"),"\n",r.createElement(s.A,{text:"\\[\nw_{t+1} = \\sum_{k \\in \\mathcal{S}_t} \\frac{n_k}{\\sum_{j \\in \\mathcal{S}_t} n_j} w_k^t.\n\\]"}),"\n",r.createElement(t.ol,{start:"5"},"\n",r.createElement(t.li,null,"This becomes the new global model."),"\n"),"\n",r.createElement(t.p,null,"FedAvg is straightforward and surprisingly effective in many practical settings, although it struggles with certain issues such as very heterogeneous data distributions, or pathological non-IID data splits."),"\n",r.createElement(t.h3,{id:"52-stochastic-gradient-descent-sgd-adaptations",style:{position:"relative"}},r.createElement(t.a,{href:"#52-stochastic-gradient-descent-sgd-adaptations","aria-label":"52 stochastic gradient descent sgd adaptations permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.2. Stochastic gradient descent (SGD) adaptations"),"\n",r.createElement(t.p,null,"Since local updates in federated learning are effectively a form of distributed SGD, many well-known SGD variants can be adapted. For instance, one could use local momentum, adaptive learning rates (e.g., Adam), or coordinate descent approaches. However, due to the typically limited computational resources on edge devices, simpler optimizers with fewer hyperparameters (like vanilla SGD or momentum) are more common."),"\n",r.createElement(t.p,null,"In addition, some approaches reduce the local computation steps to a single epoch or a single batch update (one step), especially when training on large datasets. This approach is known as Federated Stochastic Gradient Descent (FedSGD), where each client just computes a single gradient step per round, sending back the gradient rather than a fully updated set of weights."),"\n",r.createElement(t.h3,{id:"53-optimization-techniques-for-federated-settings",style:{position:"relative"}},r.createElement(t.a,{href:"#53-optimization-techniques-for-federated-settings","aria-label":"53 optimization techniques for federated settings permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.3. Optimization techniques for federated settings"),"\n",r.createElement(t.p,null,'Federated learning typically faces the "Non-IID + Unbalanced" data challenge: each client may have a different data distribution, and the total number of data points can vary widely. There is a rich research line exploring specialized optimization strategies, for instance:'),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"FedProx"),": An algorithm that adds a proximal term to the local objective. This term penalizes local updates from drifting too far from the global model, controlling the effect of heterogeneous local data. (Li and gang, 2020)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"SCAFFOLD"),': Stands for "Stochastic Controlled Averaging Federated Learning", designed to correct for the client drift problem by introducing control variates that reduce gradient variance across heterogeneous clients (Karimireddy and gang, ICML 2020).'),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"FedNova"),": A normalized averaging approach that tackles objective inconsistency issues when clients perform different numbers of local updates (Wang and gang, NeurIPS 2020)."),"\n"),"\n",r.createElement(t.p,null,"These algorithms target faster convergence and better stability in non-IID data settings, which is crucial for realistic federated deployments."),"\n",r.createElement(t.h3,{id:"54-advanced-algorithms-for-non-iid-data",style:{position:"relative"}},r.createElement(t.a,{href:"#54-advanced-algorithms-for-non-iid-data","aria-label":"54 advanced algorithms for non iid data permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.4. Advanced algorithms for non-iid data"),"\n",r.createElement(t.p,null,"Non-IID data (where each client sees a different distribution of classes or features) is perhaps the single most defining challenge in federated learning. Techniques to mitigate it include:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Personalized FL"),": Instead of training one global model for all clients, each client ends up with a local variant that is specialized to its data distribution. This can be achieved with meta-learning approaches or multi-task learning frameworks."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Clustered FL"),": The global population of clients is partitioned into clusters of similar distributions, so the algorithm can learn different global models for each cluster."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Data sharing approaches"),": A small fraction of globally shared data or synthetic data might be used to reduce the mismatch between clients."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Regularization"),": Encouraging local models to remain close to a shared representation layer while still allowing local fine-tuning."),"\n"),"\n",r.createElement(t.h2,{id:"6-challenges",style:{position:"relative"}},r.createElement(t.a,{href:"#6-challenges","aria-label":"6 challenges permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6. Challenges"),"\n",r.createElement(t.p,null,"While federated learning is undoubtedly promising, it faces numerous technical and operational challenges that shape ongoing research and real-world adoption."),"\n",r.createElement(t.h3,{id:"61-non-iid-data-distribution-challenges",style:{position:"relative"}},r.createElement(t.a,{href:"#61-non-iid-data-distribution-challenges","aria-label":"61 non iid data distribution challenges permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.1. Non-iid data distribution challenges"),"\n",r.createElement(t.p,null,"Models in federated learning typically see highly heterogeneous data. A single phone might have predominantly text in French, while another phone might be used by an English speaker. Hospitals in different geographical regions might have vastly different patient demographics or diseases prevalence. This variability slows training convergence, increases the risk of local overfitting, and can degrade the final global model's accuracy."),"\n",r.createElement(t.p,null,"Solutions are multifaceted. They can involve advanced optimization algorithms (FedProx, SCAFFOLD), data augmentation strategies, or personalization layers that adapt the final model to each client's local distribution."),"\n",r.createElement(t.h3,{id:"62-communication-bottlenecks-and-latency-issues",style:{position:"relative"}},r.createElement(t.a,{href:"#62-communication-bottlenecks-and-latency-issues","aria-label":"62 communication bottlenecks and latency issues permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.2. Communication bottlenecks and latency issues"),"\n",r.createElement(t.p,null,"Communication in federated learning can be expensive. If there are millions (or even billions) of devices, one cannot possibly communicate with all of them every single training round, as that would overload the network and cause unacceptable latency. Partial participation and client sampling are necessary, but then fewer clients means less global information per round, potentially slowing model convergence."),"\n",r.createElement(t.p,null,"Additionally, some clients might have slow or intermittent connections. This can cause straggler problems, where the server must wait a long time to receive updates from certain devices. Many architectures are forced to drop or skip updates from slow clients. Handling these network constraints remains a key engineering hurdle."),"\n",r.createElement(t.h3,{id:"63-client-availability-and-stragglers",style:{position:"relative"}},r.createElement(t.a,{href:"#63-client-availability-and-stragglers","aria-label":"63 client availability and stragglers permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.3. Client availability and stragglers"),"\n",r.createElement(t.p,null,"In typical cross-device federated learning scenarios (like mobile phones), clients come online and go offline unpredictably. If you are training a keyboard prediction model, you can only train at certain times (e.g., while the phone is charging and on Wi-Fi). This ephemeral availability complicates scheduling and can introduce biases in the training dataset if certain subsets of users are more frequently available."),"\n",r.createElement(t.p,null,"Straggler mitigation techniques include:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"Limiting the time window for each training round (ignoring late updates)."),"\n",r.createElement(t.li,null,"Prioritizing clients that are historically more reliable or relevant."),"\n",r.createElement(t.li,null,"Using asynchronous federated learning, in which the server does not wait for all updates but aggregates whenever an update arrives."),"\n"),"\n",r.createElement(t.h3,{id:"64-scalability-of-federated-systems",style:{position:"relative"}},r.createElement(t.a,{href:"#64-scalability-of-federated-systems","aria-label":"64 scalability of federated systems permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.4. Scalability of federated systems"),"\n",r.createElement(t.p,null,"Taking a proof-of-concept prototype to production in a system with potentially millions of devices is a non-trivial undertaking. You need an infrastructure capable of selecting subsets of clients, distributing models or updates, collecting them back, ensuring reliability, preventing malicious or erroneous updates, and so on. This requires distributed systems engineering expertise. Adding advanced cryptography or differential privacy further increases the computational overhead."),"\n",r.createElement(t.p,null,"Moreover, different hardware capabilities among clients can cause the system to behave unpredictably. The presence of older devices with limited compute might drag the entire system behind if they are always included. There are open research questions on how best to scale FL to truly global populations."),"\n",r.createElement(t.h2,{id:"7-applications",style:{position:"relative"}},r.createElement(t.a,{href:"#7-applications","aria-label":"7 applications permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7. Applications"),"\n",r.createElement(t.p,null,"Federated learning has found an expanding range of applications. The most notable domain is probably mobile phone personalization. Yet, beyond that, we see:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Healthcare"),": Collaborative models for disease detection, medical image segmentation, and diagnostic classification that preserve patient confidentiality."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Finance"),": Fraud detection, credit risk scoring, and anti-money laundering checks that combine data across multiple banks without exposing sensitive account-level details."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Manufacturing and IoT"),": Predictive maintenance models across devices deployed in factories, wind turbines, or other equipment, enabling data-driven insights without centralizing data from multiple production lines or facilities."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Recommendation systems"),": Personalized recommendation algorithms that learn from user interactions on devices."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Smart vehicles"),": Autonomous driving features or in-car personalization that rely on local sensor data from each vehicle, aggregated to improve a global driving policy or detection system."),"\n"),"\n",r.createElement(t.p,null,"Given that privacy, compliance, or bandwidth constraints are common in these domains, federated learning is often the perfect fit."),"\n",r.createElement(t.h2,{id:"8-tools",style:{position:"relative"}},r.createElement(t.a,{href:"#8-tools","aria-label":"8 tools permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8. Tools"),"\n",r.createElement(t.p,null,"Several open-source frameworks have emerged to help developers and researchers experiment with federated learning. These frameworks typically provide libraries for simulating federated environments, implementing secure aggregation, and orchestrating distributed training."),"\n",r.createElement(t.h3,{id:"81-tensorflow-federated",style:{position:"relative"}},r.createElement(t.a,{href:"#81-tensorflow-federated","aria-label":"81 tensorflow federated permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.1. TensorFlow Federated"),"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"TensorFlow Federated")," (TFF) is a framework developed by Google that extends the TensorFlow ecosystem to support federated learning. TFF provides a high-level API for describing computations that occur on local devices, as well as how to aggregate them on a central server. One can define a training loop that leverages federated averaging or other algorithms seamlessly."),"\n",r.createElement(t.p,null,"A very simplified example of TFF might look like this:"),"\n",r.createElement(o.A,{text:"\nimport tensorflow as tf\nimport tensorflow_federated as tff\n\n# Define a simple model in Keras\ndef create_keras_model():\n    return tf.keras.Sequential([\n        tf.keras.layers.Dense(10, activation='relu', input_shape=(784,)),\n        tf.keras.layers.Dense(10, activation='softmax')\n    ])\n\n# Convert Keras model to TFF model\ndef model_fn():\n    keras_model = create_keras_model()\n    return tff.learning.from_keras_model(\n        keras_model,\n        input_spec=(tf.TensorSpec(shape=[None, 784], dtype=tf.float32),\n                    tf.TensorSpec(shape=[None], dtype=tf.int64)),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n    )\n\n# Example of an iterative process using federated averaging\niterative_process = tff.learning.algorithms.build_weighted_fed_avg(\n    model_fn=model_fn\n)\n\n# Suppose we have a function get_federated_data() returning local datasets\nfederated_data = get_federated_data()\n\nstate = iterative_process.initialize()\n\nfor round_num in range(1, 11):\n    state, metrics = iterative_process.next(state, federated_data)\n    print(f\"Round {round_num}, metrics={metrics}\")\n"}),"\n",r.createElement(t.p,null,"Though this snippet is simplified for illustration, TFF can be used to run simulations of federated learning using local datasets, and in principle, can connect to real distributed data in more sophisticated deployments."),"\n",r.createElement(t.h3,{id:"82-pysyft-and-openmined",style:{position:"relative"}},r.createElement(t.a,{href:"#82-pysyft-and-openmined","aria-label":"82 pysyft and openmined permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.2. PySyft and OpenMined"),"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"PySyft"),", developed by the OpenMined community, is a Python library that integrates secure multi-party computation (MPC) and differential privacy to facilitate privacy-preserving data science and machine learning. PySyft aims to allow training on decentralized data while ensuring no raw data is ever exposed."),"\n",r.createElement(t.p,null,'A typical PySyft workflow might allow you to create "virtual workers" that represent different data holders, then you can write your normal PyTorch code while the data remains on these separate workers. The library also provides hooks to seamlessly integrate with PyTorch, enabling operations on remote data as if it were local, but under the hood, it ensures cryptographic or protocol-based safeguards are applied.'),"\n",r.createElement(t.h3,{id:"83-federated-learning-libraries-in-pytorch",style:{position:"relative"}},r.createElement(t.a,{href:"#83-federated-learning-libraries-in-pytorch","aria-label":"83 federated learning libraries in pytorch permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.3. Federated learning libraries in PyTorch"),"\n",r.createElement(t.p,null,"While PySyft is one of the earliest entrants, there are other libraries and toolkits that provide FL capabilities on top of PyTorch. Some are official, some are community-driven. Often, they rely on the data parallelism abstractions that PyTorch offers, but with modifications to accommodate partial participation or secure aggregation steps. Some frameworks aim to give out-of-the-box support for FedAvg, FedProx, and other standard algorithms, while also simplifying the process of adding custom local training loops or aggregator logic."),"\n",r.createElement(t.h3,{id:"84-comparison-of-available-tools",style:{position:"relative"}},r.createElement(t.a,{href:"#84-comparison-of-available-tools","aria-label":"84 comparison of available tools permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.4. Comparison of available tools"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"TensorFlow Federated"),": Strong integration with TensorFlow's ecosystem, straightforward simulation environment, a well-structured approach to defining federated computations. Might have a steeper learning curve if you are used to standard TensorFlow/Eager mode."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"PySyft"),": Emphasizes secure data science, bridging differential privacy, secure computation, and federated learning. Integrates well with PyTorch, but can sometimes be less stable due to frequent library updates."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Other libraries"),": There are many new solutions cropping up. For instance, Flower, FATE (Federated AI Technology Enabler by WeBank), or IBM's Federated Learning framework. Each might have unique features (e.g., specialized HPC integration, cryptographic primitives, or domain-specific tooling)."),"\n"),"\n",r.createElement(t.p,null,"In general, the choice depends on your existing infrastructure, programming language preferences, and the scope of your project. For cutting-edge academic research, TFF and PySyft remain popular; for enterprise solutions, you might find specialized commercial products that integrate with private data centers, cryptography modules, or existing HPC clusters."),"\n",r.createElement(t.h2,{id:"9-future-of-federated-learning",style:{position:"relative"}},r.createElement(t.a,{href:"#9-future-of-federated-learning","aria-label":"9 future of federated learning permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9. Future of federated learning"),"\n",r.createElement(t.p,null,"Federated learning is still rapidly evolving. Many open questions remain, and new frontiers are being explored. I expect that in the coming years, we will see:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Increased standardization"),": Tools, protocols, and best practices for building and deploying federated learning systems across multiple industries."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Better personalization"),": Federated learning models that adapt to each client's unique data distribution, bridging global knowledge with local specializations."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Advanced privacy guarantees"),": Incorporating more robust cryptographic techniques (e.g., fully homomorphic encryption, secure multi-party computation) and advanced differential privacy mechanisms that reduce the risk of model inversion attacks."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Decentralized orchestration"),": Instead of the conventional client-server approach, some research is exploring decentralized or peer-to-peer topologies. This eliminates the single point of failure or the trust assumption in a central server."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Federated analytics"),": Going beyond just training a single global model to more general data analytics tasks that can be run across distributed data. This might include federated clustering, federated dimension reduction, or other forms of unsupervised/supervised analytics that do not rely on centralizing the data."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Hardware improvements"),": The development of specialized chips and hardware accelerators for edge devices that can handle on-device training more efficiently, drastically reducing energy consumption and computation time."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Federated learning in 5G/6G networks"),": With the rise of ultra-fast and low-latency networks, federated learning can expand to even more distributed contexts, from real-time sensor arrays to large-scale IoT ecosystems."),"\n"),"\n",r.createElement(t.p,null,"The synergy between federated learning, privacy-preserving technologies, and the unstoppable momentum of data growth suggests that FL will become an integral part of the machine learning landscape. As privacy regulations become more stringent, and as the demand for real-time, personalized AI solutions increases, federated learning can fill the gap. This approach can harness massive amounts of distributed data that was previously inaccessible, forging new horizons in medicine, finance, autonomous systems, and beyond."),"\n",r.createElement(t.p,null,"Ultimately, federated learning is not a panacea. It's one compelling solution among many. However, its potential to respect user privacy, comply with data regulations, reduce bandwidth usage, and open up new collaborative AI scenarios is undeniable. Whether you're a data scientist, researcher, or business strategist, understanding the principles, algorithms, and challenges of federated learning is fast becoming essential knowledge."),"\n",r.createElement(t.hr),"\n",r.createElement(t.p,null,"I hope that this detailed exploration has shed some light on the conceptual underpinnings, technical aspects, and practical implications of federated learning. The field evolves rapidly, so keeping abreast of the latest research conferences, open-source tools, and industrial case studies is key to staying on top of best practices. It's an exciting time to be working on — or experimenting with — federated learning, given that it touches on everything from advanced optimization algorithms and cryptographic methods to real-world business considerations and device-level constraints."),"\n",r.createElement(t.p,null,"For any budding or experienced practitioner, I believe that building a foundation in federated averaging, secure aggregation, and dealing with non-IID distributions is paramount. Then, exploring advanced techniques such as personalized FL, vertical federated approaches, and cutting-edge communication optimizations can help push your skill set even further. Above all, it's crucial to appreciate that federated learning is more than a single algorithm or protocol; it's a paradigm that weaves together machine learning, systems engineering, privacy, and distributed computing, offering a genuinely novel way to harness the vast, heterogeneous data that populates today's digital ecosystem."),"\n",r.createElement(t.h2,{id:"additional-expansions-and-deep-dives",style:{position:"relative"}},r.createElement(t.a,{href:"#additional-expansions-and-deep-dives","aria-label":"additional expansions and deep dives permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Additional expansions and deep dives"),"\n",r.createElement(t.p,null,"Given the depth and breadth of federated learning, I would like to extend a series of discussions on advanced topics that might be of particular interest to those seeking medium-to-advanced theoretical grounding in the domain:"),"\n",r.createElement(t.h3,{id:"91-differential-privacy-in-federated-learning",style:{position:"relative"}},r.createElement(t.a,{href:"#91-differential-privacy-in-federated-learning","aria-label":"91 differential privacy in federated learning permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9.1. Differential privacy in federated learning"),"\n",r.createElement(t.p,null,"While secure aggregation protocols protect individual client updates from direct inspection, they do not fully address the possibility that aggregated updates may still leak sensitive information. ",r.createElement(l.A,null,"Differential privacy")," (DP) provides a formal framework for ensuring that the outputs of a computation (like a federated model update) do not reveal too much about any single data point within a client. By adding carefully calibrated noise to gradients or model parameters, we can bound the degree to which any individual's data influences the final model."),"\n",r.createElement(t.p,null,"The key measure in DP is the ",r.createElement(s.A,{text:"\\(\\epsilon\\)"})," (epsilon) parameter, which roughly measures the privacy loss. A lower ",r.createElement(s.A,{text:"\\(\\epsilon\\)"})," means higher privacy but can lead to decreased model accuracy. In a federated context, one might incorporate an algorithm called ",r.createElement(l.A,null,"DP-FedAvg"),", which modifies the local gradient steps to add noise before sending them to the server. Another approach is for the server to aggregate updates first and then apply noise to the aggregated result."),"\n",r.createElement(t.h3,{id:"92-homomorphic-encryption-and-secure-multiparty-computation",style:{position:"relative"}},r.createElement(t.a,{href:"#92-homomorphic-encryption-and-secure-multiparty-computation","aria-label":"92 homomorphic encryption and secure multiparty computation permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9.2. Homomorphic encryption and secure multiparty computation"),"\n",r.createElement(t.p,null,"For organizations that require very strong confidentiality, solutions that rely on ",r.createElement(l.A,null,"homomorphic encryption")," (HE) or ",r.createElement(l.A,null,"secure multiparty computation")," (SMPC) can be integrated into the federated pipeline. Homomorphic encryption allows arithmetic operations to be performed on encrypted data without decrypting it. In a federated learning context, clients can encrypt their local gradients before sending them to the server, which can then combine these encrypted values and produce an encrypted aggregate. The server might never need to decrypt, or if it does, it does so under carefully managed protocols."),"\n",r.createElement(t.p,null,"SMPC protocols, on the other hand, distribute the secret (in this case, the local updates or the model parameters) among multiple participants such that no individual party ever sees the entire secret. By carefully orchestrating computations among these participants, one can ensure that the final aggregated output emerges without revealing intermediate values. This approach can be combined with decentralized topologies where there is no single server."),"\n",r.createElement(t.p,null,"These methods often come with computational overheads. Homomorphic encryption can be expensive in terms of CPU usage and memory, making it less feasible for edge devices in some scenarios. However, for certain vertical FL setups between major institutions, these overheads might be acceptable to comply with regulatory demands."),"\n",r.createElement(t.h3,{id:"93-robustness-against-adversarial-or-malicious-clients",style:{position:"relative"}},r.createElement(t.a,{href:"#93-robustness-against-adversarial-or-malicious-clients","aria-label":"93 robustness against adversarial or malicious clients permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9.3. Robustness against adversarial or malicious clients"),"\n",r.createElement(t.p,null,"An often-overlooked aspect of federated learning is that some clients might not be trustworthy. In open cross-device scenarios, it's conceivable that an attacker could poison updates, either to degrade the global model's performance or to embed hidden backdoors. For instance, a malicious client might repeatedly send updates that cause the model to misclassify a certain trigger pattern as a benign class (backdoor attack)."),"\n",r.createElement(t.p,null,"To mitigate such threats, robust aggregation rules have been proposed. Examples include:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Coordinate-wise median")," or ",r.createElement(t.strong,null,"coordinate-wise trimmed mean")," to remove outlier gradients."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Krum"),' (Blanchard and gang, 2017), which selects the gradient that is most "in agreement" with the majority of other gradients.'),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Bulyan")," and other advanced versions that aim to detect and eliminate malicious updates."),"\n"),"\n",r.createElement(t.p,null,"These defenses can help in ensuring that a few malicious clients do not subvert the global model. However, they can also reduce efficiency or hamper accuracy if large subsets of the data appear outlier-like, e.g., in the presence of legitimate but highly heterogeneous local data distributions."),"\n",r.createElement(t.h3,{id:"94-fairness-and-incentives",style:{position:"relative"}},r.createElement(t.a,{href:"#94-fairness-and-incentives","aria-label":"94 fairness and incentives permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9.4. Fairness and incentives"),"\n",r.createElement(t.p,null,"Another subtlety is ",r.createElement(l.A,null,"fairness"),". In federated learning, not all participants have the same quantity of data or the same data distribution. Some participants might be underrepresented, leading to a global model biased toward the data from major participants. Fairness approaches attempt to ensure that the global model performs adequately across all relevant sub-populations of clients."),"\n",r.createElement(t.p,null,"Incentive mechanisms also come into play. Why would a client want to participate in federated learning? Is there an incentive system (e.g., monetary compensation, improved local performance, free product upgrades) encouraging them to keep contributing updates? This fosters the idea of ",r.createElement(l.A,null,"federated marketplaces"),", where data owners can trade model updates for some kind of benefit, while preserving confidentiality."),"\n",r.createElement(t.h3,{id:"95-communication-efficient-fl",style:{position:"relative"}},r.createElement(t.a,{href:"#95-communication-efficient-fl","aria-label":"95 communication efficient fl permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9.5. Communication-efficient FL"),"\n",r.createElement(t.p,null,"Since communication is a major bottleneck, a considerable body of research focuses on ",r.createElement(l.A,null,"communication-efficient FL"),". Basic techniques include compressing gradients through quantization (e.g., 8-bit or 1-bit compression), top-<k> selection of gradient components, or using iterative refinements. Some frameworks incorporate error feedback mechanisms so that the error introduced by compression is periodically corrected in subsequent updates. The overarching goal is to drastically reduce the number of bits that need to be transferred per client per round, especially for large neural network models."),"\n",r.createElement(t.h3,{id:"96-asynchronous-and-decentralized-fl",style:{position:"relative"}},r.createElement(t.a,{href:"#96-asynchronous-and-decentralized-fl","aria-label":"96 asynchronous and decentralized fl permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9.6. Asynchronous and decentralized FL"),"\n",r.createElement(t.p,null,"The classical federated learning paradigm is synchronous, with a central server waiting for updates from clients chosen to participate in a round. But real-world constraints push us toward asynchronous or decentralized solutions. In asynchronous federated learning, any client can send updates whenever it completes local training. The server updates the global model on a rolling basis. This can be more flexible but requires carefully weighting or scaling updates from stale models."),"\n",r.createElement(t.p,null,r.createElement(l.A,null,"Decentralized FL")," or fully peer-to-peer solutions eliminate the central server. Instead, clients communicate in a graph topology, exchanging updates with neighbors to arrive at a consensus model. This approach can be robust to server failures but complicates the design of secure aggregation and demands efficient consensus protocols."),"\n",r.createElement(t.h3,{id:"97-personalized-federated-learning",style:{position:"relative"}},r.createElement(t.a,{href:"#97-personalized-federated-learning","aria-label":"97 personalized federated learning permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9.7. Personalized federated learning"),"\n",r.createElement(t.p,null,"Realizing that a single global model may not be optimal for every client's data distribution, ",r.createElement(l.A,null,"personalized federated learning")," is a growing area of interest. The idea is that each client ends up with a personalized model adapted to its local domain, yet still benefiting from the shared knowledge of other participants. Approaches to personalization include:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Fine-tuning"),": Each client uses the global model as initialization and does local gradient descent on its data to specialize the model."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Multi-task learning"),": The learning process is framed as a multi-task problem in which each client's objective is somewhat distinct but related to the overall tasks."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Meta-learning"),": Techniques like MAML (Model-Agnostic Meta-Learning) can be adapted to federated contexts, teaching the global model to be quickly adaptable to each client's local distribution."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Layer partitioning"),": Some layers (like the feature extractor) are shared globally, while the final layers are trained locally, capturing local patterns or preferences."),"\n"),"\n",r.createElement(t.p,null,"Personalization can yield significantly improved local performance while sacrificing minimal amounts of global consistency."),"\n",r.createElement(t.h3,{id:"98-practical-deployment-considerations",style:{position:"relative"}},r.createElement(t.a,{href:"#98-practical-deployment-considerations","aria-label":"98 practical deployment considerations permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9.8. Practical deployment considerations"),"\n",r.createElement(t.p,null,"Before concluding our enormous deep dive, it's worth emphasizing the practical considerations that must be tackled to deploy a federated learning solution at scale:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Infrastructure"),": Managing orchestration of client selection, update distribution, and result collection, typically in a cloud-based environment that interacts with a network of devices."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Security"),": Ensuring that the communication channels are secure and that malicious updates or eavesdroppers do not compromise the system."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Compliance"),": Verifying that the solution meets all relevant legal and regulatory guidelines (HIPAA, GDPR, CPRA, etc.)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Logging and Auditing"),": Maintaining records of which clients contributed updates, how the model was aggregated, and how it performed to ensure transparency and traceability."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Model Debugging"),": Diagnosing model failures is more complex in a federated setting because you cannot simply look at the raw data. Tools to trace the source of anomalies or distribution shifts are required."),"\n"),"\n",r.createElement(t.h3,{id:"99-an-extended-example-federated-recommendation-system",style:{position:"relative"}},r.createElement(t.a,{href:"#99-an-extended-example-federated-recommendation-system","aria-label":"99 an extended example federated recommendation system permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9.9. An extended example: Federated recommendation system"),"\n",r.createElement(t.p,null,"To illustrate how federated learning can be practically applied, consider a recommendation system in a cross-company collaborative scenario. Suppose you have multiple music streaming services that each want to improve their recommendation algorithms by leveraging user behaviors across platforms, but none of them are willing or legally allowed to pool their user data in a shared location. With federated learning:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,"Each streaming service hosts a local model that trains on usage logs (songs played, likes, skips)."),"\n",r.createElement(t.li,null,"Periodically, these local models produce gradient updates or parameter deltas that are encrypted and shared with a central aggregator or a shared peer-to-peer network."),"\n",r.createElement(t.li,null,"The aggregator merges these updates into a global model that captures general user preferences."),"\n",r.createElement(t.li,null,"The aggregator (or decentralized consensus) sends the updated global model parameters back to each streaming service."),"\n",r.createElement(t.li,null,"Each service fine-tunes the model further for its own user base, achieving personalization while benefiting from the knowledge gained by the entire consortium of streaming platforms."),"\n"),"\n",r.createElement(t.p,null,"This approach can allow smaller or niche streaming services to significantly enhance their recommendation quality without sacrificing user privacy or independence."),"\n",r.createElement(t.h2,{id:"a-massive-overarching-view",style:{position:"relative"}},r.createElement(t.a,{href:"#a-massive-overarching-view","aria-label":"a massive overarching view permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"A massive overarching view"),"\n",r.createElement(t.p,null,"The field of federated learning stands at the intersection of multiple disciplines — distributed systems, cryptography, machine learning, data privacy, and network protocols. Hence, a real mastery requires familiarity with each of these areas. Ongoing research is replete with novel variations, from bridging active learning and federated learning to combining reinforcement learning with FL for distributed robotics or embedded systems."),"\n",r.createElement(t.p,null,"Researchers continue to propose new ways of tackling the non-IID challenge, improving computational efficiency, or guaranteeing privacy. The swift uptake of FL in industry also means that enterprise-grade solutions are becoming more common. As data grows, and as the call for privacy and compliance intensifies, federated learning is poised to remain a fundamental approach for collaborative AI."),"\n",r.createElement(l.A,null,"In essence, if you're a data scientist, developer, or AI enthusiast, there has never been a better time to invest in learning about federated frameworks, investigating advanced optimization techniques, and staying abreast of evolving security and privacy standards in distributed settings."),"\n",r.createElement(t.h2,{id:"potential-pitfalls-and-misconceptions",style:{position:"relative"}},r.createElement(t.a,{href:"#potential-pitfalls-and-misconceptions","aria-label":"potential pitfalls and misconceptions permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Potential pitfalls and misconceptions"),"\n",r.createElement(t.p,null,"I would like to also highlight a few pitfalls that frequently arise when someone first approaches federated learning:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,'"Federated learning guarantees privacy automatically."')," While FL mitigates certain privacy threats by avoiding centralization of raw data, there remain attack vectors such as gradient inversion. Additional measures (differential privacy, secure aggregation) are typically needed for rigorous privacy guarantees."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,'"Federated learning is easy to scale once a basic prototype is built."')," In reality, deploying FL at scale demands specialized system engineering to handle billions of devices, ephemeral connectivity, and limited resources."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,'"Non-IID data can be handled in the same way as in typical distributed learning."')," Distributed learning often assumes data is IID across workers, which is not typically the case in federated scenarios, necessitating specialized algorithms."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,'"All federated learning is about cross-device scenarios."')," In fact, cross-silo federated learning (like hospital networks or banks) might be more relevant for many enterprise settings, and the constraints and solutions differ from cross-device FL."),"\n"),"\n",r.createElement(t.h2,{id:"wrapping-up",style:{position:"relative"}},r.createElement(t.a,{href:"#wrapping-up","aria-label":"wrapping up permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Wrapping up"),"\n",r.createElement(t.p,null,"Federated learning is a formidable innovation in how we think about distributed AI. It offers both philosophical and practical breakthroughs, changing the AI community's assumptions about centralization. While there is hype around privacy and personalization, the real breakthroughs lie in a combination of robust cryptographic protocols, advanced distributed optimization, creative approaches to model personalization, and the synergy with network engineering."),"\n",r.createElement(t.p,null,"Going forward, I encourage you to:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Experiment with frameworks")," like TensorFlow Federated or PySyft to get hands-on experience."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Study advanced algorithms")," like FedProx, SCAFFOLD, or personalized FL approaches if you're dealing with heterogeneous data distributions."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Incorporate privacy")," from day one in your design, especially if you operate in regulated industries."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Stay current")," by following leading conferences (e.g., NeurIPS, ICML, ICLR), where cutting-edge federated learning research continues to be a focal point."),"\n"),"\n",r.createElement(t.p,null,"Federated learning is more than just a technical curiosity; it's an evolving paradigm that has already influenced the AI strategies of tech giants, healthcare consortiums, and financial institutions. As models become larger and data becomes more siloed, the impetus for distributed, privacy-conscious collaboration will only intensify, and federated learning may well be at the heart of that transformation."),"\n",r.createElement(a,{alt:"Federated Learning Illustration",path:"",caption:"A conceptual diagram showing a central server coordinating with multiple edge devices, each holding its own local dataset, to train a shared global model without data centralization.",zoom:"false"}))}var d=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,i.RP)(),e.components);return t?r.createElement(t,e,r.createElement(c,e)):c(e)};var h=a(36710),m=a(58481),p=a.n(m),u=a(36310),g=a(87245),f=a(27042),v=a(59849),y=a(5591),b=a(61122),E=a(9219),w=a(33203),S=a(95751),k=a(94328),H=a(80791),z=a(78137);const T=e=>{let{toc:t}=e;if(!t||!t.items)return null;return r.createElement("nav",{className:H.R},r.createElement("ul",null,t.items.map(((e,t)=>r.createElement("li",{key:t},r.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const a=t.replace("#",""),n=document.getElementById(a);n&&n.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&r.createElement(T,{toc:{items:e.items}}))))))};function x(e){let{data:{mdx:t,allMdx:l,allPostImages:o},children:s}=e;const{frontmatter:c,body:d,tableOfContents:h}=t,m=c.index,v=c.slug.split("/")[1],H=l.nodes.filter((e=>e.frontmatter.slug.includes(`/${v}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),x=H.findIndex((e=>e.frontmatter.index===m)),_=H[x+1],C=H[x-1],I=c.slug.replace(/\/$/,""),M=/[^/]*$/.exec(I)[0],F=`posts/${v}/content/${M}/`,{0:A,1:L}=(0,r.useState)(c.flagWideLayoutByDefault),{0:V,1:P}=(0,r.useState)(!1);var B;(0,r.useEffect)((()=>{P(!0);const e=setTimeout((()=>P(!1)),340);return()=>clearTimeout(e)}),[A]),"adventures"===v?B=E.cb:"research"===v?B=E.Qh:"thoughts"===v&&(B=E.T6);const N=p()(d).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,q=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),a=e%60;return a<=30?`~${t}${a>0?".5":""} h`:`~${t+1} h`}(Math.ceil(N/B)+(c.extraReadTimeMin||0)),D=[{flag:c.flagDraft,component:()=>Promise.all([a.e(3231),a.e(8809)]).then(a.bind(a,28809))},{flag:c.flagMindfuckery,component:()=>Promise.all([a.e(3231),a.e(2471)]).then(a.bind(a,67709))},{flag:c.flagRewrite,component:()=>Promise.all([a.e(3231),a.e(6764)]).then(a.bind(a,62002))},{flag:c.flagOffensive,component:()=>Promise.all([a.e(3231),a.e(2443)]).then(a.bind(a,17681))},{flag:c.flagProfane,component:()=>Promise.all([a.e(3231),a.e(8048)]).then(a.bind(a,53286))},{flag:c.flagMultilingual,component:()=>Promise.all([a.e(3231),a.e(4069)]).then(a.bind(a,78831))},{flag:c.flagUnreliably,component:()=>Promise.all([a.e(3231),a.e(3417)]).then(a.bind(a,8179))},{flag:c.flagPolitical,component:()=>Promise.all([a.e(3231),a.e(5195)]).then(a.bind(a,30433))},{flag:c.flagCognitohazard,component:()=>Promise.all([a.e(3231),a.e(3175)]).then(a.bind(a,8413))},{flag:c.flagHidden,component:()=>Promise.all([a.e(3231),a.e(9556)]).then(a.bind(a,14794))}],{0:O,1:G}=(0,r.useState)([]);return(0,r.useEffect)((()=>{D.forEach((e=>{let{flag:t,component:a}=e;t&&a().then((e=>{G((t=>[].concat((0,n.A)(t),[e.default])))}))}))}),[]),r.createElement(f.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},r.createElement(y.A,{postNumber:c.index,date:c.date,updated:c.updated,readTime:q,difficulty:c.difficultyLevel,title:c.title,desc:c.desc,banner:c.banner,section:v,postKey:M,isMindfuckery:c.flagMindfuckery,mainTag:c.mainTag}),r.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},c.otherTags.map(((e,t)=>r.createElement("span",{key:t,className:`noselect ${z.MW}`,style:{margin:"0 5px 5px 0"}},e)))),r.createElement("div",{className:"postBody"},r.createElement(T,{toc:h})),r.createElement("br"),r.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},r.createElement(f.P.button,{className:`noselect ${k.pb}`,id:k.xG,onClick:()=>{L(!A)},whileTap:{scale:.93}},r.createElement(f.P.div,{className:S.DJ,key:A,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},A?"Switch to default layout":"Switch to wide layout"))),r.createElement("br"),r.createElement("div",{className:"postBody",style:{margin:A?"0 -14%":"",maxWidth:A?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},r.createElement("div",{className:`${k.P_} ${V?k.Xn:k.qG}`},O.map(((e,t)=>r.createElement(e,{key:t}))),c.indexCourse?r.createElement(w.A,{index:c.indexCourse,category:c.courseCategoryName}):"",r.createElement(u.Z.Provider,{value:{images:o.nodes,basePath:F.replace(/\/$/,"")+"/"}},r.createElement(i.xA,{components:{Image:g.A}},s)))),r.createElement(b.A,{nextPost:_,lastPost:C,keyCurrent:M,section:v}))}function _(e){return r.createElement(x,e,r.createElement(d,e))}function C(e){var t,a,n,i,l;let{data:o}=e;const{frontmatter:s}=o.mdx,c=s.titleSEO||s.title,d=s.titleOG||c,m=s.titleTwitter||c,p=s.descSEO||s.desc,u=s.descOG||p,g=s.descTwitter||p,f=s.schemaType||"BlogPosting",y=s.keywordsSEO,b=s.date,E=s.updated||b,w=s.imageOG||(null===(t=s.banner)||void 0===t||null===(a=t.childImageSharp)||void 0===a||null===(n=a.gatsbyImageData)||void 0===n||null===(i=n.images)||void 0===i||null===(l=i.fallback)||void 0===l?void 0:l.src),S=s.imageAltOG||u,k=s.imageTwitter||w,H=s.imageAltTwitter||g,z=s.canonicalURL,T=s.flagHidden||!1,x=s.mainTag||"Posts",_=s.slug.split("/")[1]||"posts",{siteUrl:C}=(0,h.Q)(),I={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:C},{"@type":"ListItem",position:2,name:x,item:`${C}/${s.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${C}${s.slug}`}]};return r.createElement(v.A,{title:c+" - avrtt.blog",titleOG:d,titleTwitter:m,description:p,descriptionOG:u,descriptionTwitter:g,schemaType:f,keywords:y,datePublished:b,dateModified:E,imageOG:w,imageAltOG:S,imageTwitter:k,imageAltTwitter:H,canonicalUrl:z,flagHidden:T,mainTag:x,section:_,type:"article"},r.createElement("script",{type:"application/ld+json"},JSON.stringify(I)))}},96098:function(e,t,a){var n=a(96540),i=a(7978);t.A=e=>{let{text:t}=e;return n.createElement(i.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-js-content-file-path-src-pages-posts-research-federated-learning-mdx-b5e3c003b0499ebe4752.js.map