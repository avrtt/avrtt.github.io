"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[9002],{3962:function(e,t){t.A="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iaXNvLTg4NTktMSI/Pgo8IS0tIEdlbmVyYXRvcjogQWRvYmUgSWxsdXN0cmF0b3IgMTYuMC4wLCBTVkcgRXhwb3J0IFBsdWctSW4gLiBTVkcgVmVyc2lvbjogNi4wMCBCdWlsZCAwKSAgLS0+CjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+CjxzdmcgdmVyc2lvbj0iMS4xIiBpZD0iQ2FwYV8xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB4PSIwcHgiIHk9IjBweCIKCSB3aWR0aD0iNDE2Ljk3OXB4IiBoZWlnaHQ9IjQxNi45NzlweCIgdmlld0JveD0iMCAwIDQxNi45NzkgNDE2Ljk3OSIgc3R5bGU9ImVuYWJsZS1iYWNrZ3JvdW5kOm5ldyAwIDAgNDE2Ljk3OSA0MTYuOTc5OyIKCSB4bWw6c3BhY2U9InByZXNlcnZlIj4KPGc+Cgk8cGF0aCBkPSJNMzU2LjAwNCw2MS4xNTZjLTgxLjM3LTgxLjQ3LTIxMy4zNzctODEuNTUxLTI5NC44NDgtMC4xODJjLTgxLjQ3LDgxLjM3MS04MS41NTIsMjEzLjM3OS0wLjE4MSwyOTQuODUKCQljODEuMzY5LDgxLjQ3LDIxMy4zNzgsODEuNTUxLDI5NC44NDksMC4xODFDNDM3LjI5MywyNzQuNjM2LDQzNy4zNzUsMTQyLjYyNiwzNTYuMDA0LDYxLjE1NnogTTIzNy42LDM0MC43ODYKCQljMCwzLjIxNy0yLjYwNyw1LjgyMi01LjgyMiw1LjgyMmgtNDYuNTc2Yy0zLjIxNSwwLTUuODIyLTIuNjA1LTUuODIyLTUuODIyVjE2Ny44ODVjMC0zLjIxNywyLjYwNy01LjgyMiw1LjgyMi01LjgyMmg0Ni41NzYKCQljMy4yMTUsMCw1LjgyMiwyLjYwNCw1LjgyMiw1LjgyMlYzNDAuNzg2eiBNMjA4LjQ5LDEzNy45MDFjLTE4LjYxOCwwLTMzLjc2Ni0xNS4xNDYtMzMuNzY2LTMzLjc2NQoJCWMwLTE4LjYxNywxNS4xNDctMzMuNzY2LDMzLjc2Ni0zMy43NjZjMTguNjE5LDAsMzMuNzY2LDE1LjE0OCwzMy43NjYsMzMuNzY2QzI0Mi4yNTYsMTIyLjc1NSwyMjcuMTA3LDEzNy45MDEsMjA4LjQ5LDEzNy45MDF6Ii8+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPC9zdmc+Cg=="},9360:function(e,t,a){a.d(t,{A:function(){return i}});var n=a(96540),l=a(3962),r="styles-module--tooltiptext--a263b";var i=e=>{let{text:t,isBadge:a=!1}=e;const{0:i,1:s}=(0,n.useState)(!1),o=(0,n.useRef)(null);return(0,n.useEffect)((()=>{function e(e){o.current&&e.target instanceof Node&&!o.current.contains(e.target)&&s(!1)}return document.addEventListener("click",e),()=>{document.removeEventListener("click",e)}}),[]),n.createElement("span",{className:"styles-module--tooltipWrapper--75ebf",ref:o},n.createElement("img",{id:a?"styles-module--infoBadge--e3d66":"styles-module--info--26c1f",src:l.A,alt:"info",onClick:e=>{e.stopPropagation(),s((e=>!e))}}),n.createElement("span",{className:i?`${r} styles-module--visible--c063c`:r},t))}},12485:function(e,t,a){a.r(t),a.d(t,{Head:function(){return T},PostTemplate:function(){return H},default:function(){return C}});var n=a(28453),l=a(96540),r=a(9360),i=a(61992),s=(a(62087),a(90548));function o(e){const t=Object.assign({p:"p",hr:"hr",h2:"h2",a:"a",span:"span",ul:"ul",li:"li",strong:"strong",h3:"h3",ol:"ol",h4:"h4",em:"em"},(0,n.RP)(),e.components),{Image:a}=t;return a||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),l.createElement(l.Fragment,null,"\n",l.createElement("br"),"\n","\n","\n",l.createElement(t.p,null,"Time series analysis is one of the cornerstones of data science and machine learning, particularly useful in fields such as finance (stock price forecasting), supply chain management (inventory prediction), economics (macroeconomic indicators), climate science, and various engineering domains. A ",l.createElement(i.A,null,"time series")," can be defined as a sequence of data points measured at consistent time intervals. The objective of time series analysis is often to understand patterns such as trends or seasonality and, ultimately, to generate forecasts into the future."),"\n",l.createElement(t.p,null,"In modern data science, time series problems have grown in significance due to the abundance of chronological data from sensors, transactions, logs, and so on. The discipline involves a broad range of methods â€” from classical statistical techniques such as ",l.createElement(i.A,null,"ARIMA"),", ",l.createElement(i.A,null,"SARIMA"),", or ",l.createElement(i.A,null,"Exponential Smoothing")," to advanced approaches, including ",l.createElement(r.A,{text:"Recurrent Neural Networks (RNN), LSTM, Transformers, etc."}),". Recent cutting-edge research has also explored combining time series with MLOps, deep learning, or advanced probabilistic modeling (Gaussian processes, deep state-space models, advanced generative models, etc.)."),"\n",l.createElement(t.p,null,"This article will guide you through core concepts, advanced techniques, pitfalls, and best practices in time series forecasting. We assume you have a substantial background in ML, so we will dive deep into theoretical underpinnings, references to relevant research, and code snippets illustrating the concepts in Python. Where possible, we use ",l.createElement(i.A,null,"LaTeX")," notation for clarity of formulas and math."),"\n",l.createElement(t.hr),"\n",l.createElement(t.h2,{id:"2-key-concepts-and-components",style:{position:"relative"}},l.createElement(t.a,{href:"#2-key-concepts-and-components","aria-label":"2 key concepts and components permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2. Key concepts and components"),"\n",l.createElement(t.p,null,"When working with time series, it is helpful to decompose or conceptualize the data in terms of underlying structures. The main elements typically considered are:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Stationarity"),": A process is (weakly or covariance) stationary if its mean and autocovariance remain the same over time. Nonstationarity is common in real-life data due to trends, seasonal fluctuations, and other effects."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Trend"),": A relatively long-term increase or decrease in the data. A trend may be linear or nonlinear."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Seasonality"),": Regular and periodic fluctuations that occur at a known frequency (e.g., monthly, weekly, daily). Seasonality is especially prominent in data related to climate, retail sales cycles, or daily website traffic."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Cyclical patterns"),": Recurrent behavior, but not strictly tied to a fixed calendar-based frequency (unlike true seasonality). An economic cycle (e.g., expansions and recessions) might be cyclical."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Noise"),": The random component, or remainder term, capturing fluctuations not explained by the main structure (trend, seasonality, cycles) or by known exogenous variables."),"\n"),"\n",l.createElement(t.h3,{id:"21-formalizing-a-time-series",style:{position:"relative"}},l.createElement(t.a,{href:"#21-formalizing-a-time-series","aria-label":"21 formalizing a time series permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.1. Formalizing a time series"),"\n",l.createElement(t.p,null,"Mathematically, we can treat a univariate time series as an ordered set of data ",l.createElement(s.A,{text:"\\(\\{y_t\\}_{t=1}^T\\)"}),", where ",l.createElement(s.A,{text:"\\(y_t\\)"})," is the observation at time ",l.createElement(s.A,{text:"\\(t\\)"}),". Often, we incorporate exogenous variables, forming a multivariate time series. Let ",l.createElement(s.A,{text:"\\( \\mathbf{x}_t \\)"})," be the vector of exogenous covariates at time ",l.createElement(s.A,{text:"\\( t \\)"}),". Then a typical time series model might be:"),"\n",l.createElement(s.A,{text:"\\[\ny_t = f(\\mathbf{x}_t, \\theta) + \\varepsilon_t\n\\]"}),"\n",l.createElement(t.p,null,"Here, ",l.createElement(s.A,{text:"\\(f\\)"})," is some function (linear or nonlinear) with parameters ",l.createElement(s.A,{text:"\\(\\theta\\)"}),", and ",l.createElement(s.A,{text:"\\(\\varepsilon_t\\)"})," is a noise or error term (often assumed to be white noise or having certain autocorrelation properties)."),"\n",l.createElement(t.h3,{id:"22-notions-of-stationarity",style:{position:"relative"}},l.createElement(t.a,{href:"#22-notions-of-stationarity","aria-label":"22 notions of stationarity permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.2. Notions of stationarity"),"\n",l.createElement(t.p,null,"A process ",l.createElement(s.A,{text:"\\(\\{y_t\\}\\)"})," is ",l.createElement(t.strong,null,"strictly stationary")," if the joint distribution of ",l.createElement(s.A,{text:"\\((y_{t_1}, y_{t_2}, \\ldots, y_{t_k})\\)"})," does not depend on time ",l.createElement(s.A,{text:"\\(t\\)"}),". In practice, we often adopt ",l.createElement(t.strong,null,"weak stationarity")," (covariance stationarity) which requires:"),"\n",l.createElement(t.ol,null,"\n",l.createElement(t.li,null,l.createElement(s.A,{text:"\\( \\mathbb{E}[y_t] = \\mu \\)"})," (constant mean),"),"\n",l.createElement(t.li,null,l.createElement(s.A,{text:"\\( \\mathrm{Var}(y_t) = \\sigma^2 \\)"})," (constant variance),"),"\n",l.createElement(t.li,null,l.createElement(s.A,{text:"\\( \\mathrm{Cov}(y_t, y_{t+h})\\)"})," depends only on ",l.createElement(s.A,{text:"\\(h\\)"}),", not on ",l.createElement(s.A,{text:"\\(t\\)"}),"."),"\n"),"\n",l.createElement(t.p,null,"Classical time series approaches (like ARIMA) often rely on stationarity assumptions. Common transformations to achieve stationarity include differencing or applying a Box-Cox transformation (e.g., log transform)."),"\n",l.createElement(t.h3,{id:"23-role-of-trend-seasonality-and-cycles",style:{position:"relative"}},l.createElement(t.a,{href:"#23-role-of-trend-seasonality-and-cycles","aria-label":"23 role of trend seasonality and cycles permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.3. Role of trend, seasonality, and cycles"),"\n",l.createElement(t.p,null,"A common decomposition of time series is:"),"\n",l.createElement(t.h4,{id:"additive-decomposition",style:{position:"relative"}},l.createElement(t.a,{href:"#additive-decomposition","aria-label":"additive decomposition permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Additive decomposition"),"\n",l.createElement(s.A,{text:"\\( y_t = T_t + S_t + R_t \\)"}),"\n",l.createElement(t.p,null,"where ",l.createElement(s.A,{text:"\\(T_t\\)"})," denotes trend, ",l.createElement(s.A,{text:"\\(S_t\\)"})," denotes seasonality, and ",l.createElement(s.A,{text:"\\(R_t\\)"})," is the remainder component."),"\n",l.createElement(t.h4,{id:"multiplicative-decomposition",style:{position:"relative"}},l.createElement(t.a,{href:"#multiplicative-decomposition","aria-label":"multiplicative decomposition permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Multiplicative decomposition"),"\n",l.createElement(s.A,{text:"\\( y_t = T_t \\times S_t \\times R_t \\)"}),"\n",l.createElement(t.p,null,"used when the amplitude of seasonal fluctuations grows with the level of the series. One might also do partial transformations (e.g., log) if the multiplicative form is more appropriate."),"\n",l.createElement(t.h3,{id:"24-example-stock-price-time-series",style:{position:"relative"}},l.createElement(t.a,{href:"#24-example-stock-price-time-series","aria-label":"24 example stock price time series permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.4. Example: Stock price time series"),"\n",l.createElement(t.p,null,"Consider daily stock price data ",l.createElement(s.A,{text:"\\(\\{p_t\\}\\)"})," for a particular company. We might observe cyclical patterns associated with economic expansions, short-term trends triggered by earnings announcements, weekly seasonality due to Monday vs. Friday trading volumes, or daily changes from news events (noise). Stationarity rarely holds in raw price data. Often, we take daily returns:"),"\n",l.createElement(s.A,{text:"\\( r_t = p_t - p_{t-1} \\)"}),"\n",l.createElement(t.p,null,"(or ",l.createElement(s.A,{text:"\\(\\log(p_t/p_{t-1})\\)"}),") to attempt stationarity."),"\n",l.createElement(t.hr),"\n",l.createElement(t.h2,{id:"3-exploratory-data-analysis",style:{position:"relative"}},l.createElement(t.a,{href:"#3-exploratory-data-analysis","aria-label":"3 exploratory data analysis permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3. Exploratory data analysis"),"\n",l.createElement(t.p,null,"Exploratory analysis of a time series is crucial. We want to examine patterns, outliers, missing data, and other features that might influence model selection or preprocessing steps."),"\n",l.createElement(t.h3,{id:"31-visualizing-time-series",style:{position:"relative"}},l.createElement(t.a,{href:"#31-visualizing-time-series","aria-label":"31 visualizing time series permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.1. Visualizing time series"),"\n",l.createElement(t.p,null,"A well-chosen set of plots can reveal important structure. Some popular approaches:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Line plot")," over time (the standard approach)."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Seasonal plot"),': For instance, group data by month to see seasonal fluctuations or produce a "seasonal subseries plot."'),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Autocorrelation plot (ACF)"),": Correlation of a series with its own past values. Helps detect seasonality or correlation structure."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Partial autocorrelation plot (PACF)"),": Helps identify autoregressive terms."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Lag scatter plots"),": Plot ",l.createElement(s.A,{text:"\\(y_t\\)"})," vs. ",l.createElement(s.A,{text:"\\(y_{t-1}\\)"}),", ",l.createElement(s.A,{text:"\\(y_t\\)"})," vs. ",l.createElement(s.A,{text:"\\(y_{t-2}\\)"}),", etc."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Box plot by season"),": e.g., separate box plots for each month to show distribution and outliers."),"\n"),"\n",l.createElement(a,{alt:"Example of a time series line plot",path:"",caption:"Line plot of a monthly data series highlighting seasonal peaks",zoom:"false"}),"\n",l.createElement(t.p,null,"Below is an example code snippet in Python (using pandas and matplotlib) to visualize a univariate time series and its autocorrelation:"),"\n",l.createElement(t.span,{dangerouslySetInnerHTML:{__html:"<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">&lt;Code text={`\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import lag_plot\nfrom statsmodels.graphics.tsaplots import plot_acf\n\n# Suppose we have a CSV file with a Date column and a Value column\ndf = pd.read_csv('time_series_data.csv', parse_dates=['Date'], index_col='Date')\nseries = df['Value']\n\n# 1) Basic line plot\nplt.figure(figsize=(10,4))\nplt.plot(series, label='Time series')\nplt.title('Line plot of time series')\nplt.legend()\nplt.show()\n\n# 2) Lag scatter plot\nplt.figure()\nlag_plot(series)\nplt.title('Lag plot at lag=1')\nplt.show()\n\n# 3) Autocorrelation plot\nplt.figure()\nplot_acf(series, lags=30)\nplt.show()\n`} /></code></pre></div>"}}),"\n",l.createElement(t.h3,{id:"32-statistical-metrics-mean-variance-autocorrelation-partial-autocorrelation",style:{position:"relative"}},l.createElement(t.a,{href:"#32-statistical-metrics-mean-variance-autocorrelation-partial-autocorrelation","aria-label":"32 statistical metrics mean variance autocorrelation partial autocorrelation permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.2. Statistical metrics (mean, variance, autocorrelation, partial autocorrelation)"),"\n",l.createElement(t.p,null,"Basic descriptive statistics such as mean, median, variance, minimum, and maximum across the time dimension can yield valuable insight, especially to check if the data is stationary or if it has noticeable trends. The ",l.createElement(t.strong,null,"autocorrelation function (ACF)")," is a key concept. For a lag ",l.createElement(s.A,{text:"\\(k\\)"}),", the sample autocorrelation is approximately:"),"\n",l.createElement(t.p,null,l.createElement(s.A,{text:"\\( \\rho(k) = \\frac{\\sum_{t=k+1}^{T} (y_t - \\bar{y})(y_{t-k} - \\bar{y})}{\\sum_{t=1}^{T} (y_t - \\bar{y})^2} \\)"}),'" />'),"\n",l.createElement(t.p,null,"The ",l.createElement(t.strong,null,"partial autocorrelation function (PACF)")," measures correlation between ",l.createElement(s.A,{text:"\\(y_t\\)"})," and ",l.createElement(s.A,{text:"\\(y_{t-k}\\)"})," once we remove the influence of other lags in-between."),"\n",l.createElement(t.h3,{id:"33-handling-missing-data-and-outliers",style:{position:"relative"}},l.createElement(t.a,{href:"#33-handling-missing-data-and-outliers","aria-label":"33 handling missing data and outliers permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.3. Handling missing data and outliers"),"\n",l.createElement(t.p,null,"Time series often have missing timestamps. Strategies include:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"Imputation with forward fill or backward fill."),"\n",l.createElement(t.li,null,"Interpolation (linear, spline) if that suits the domain."),"\n",l.createElement(t.li,null,"More sophisticated approaches using domain knowledge."),"\n"),"\n",l.createElement(t.p,null,"Outliers may result from sensor failures, data entry errors, or actual anomalies. Some practitioners prefer robust models or robust decomposition (like STL with a robust setting) to mitigate outlier influence. Alternatively, certain advanced methods (e.g., LOF-based anomaly detection, or isolation forests) can be used to systematically identify and handle outliers."),"\n",l.createElement(t.hr),"\n",l.createElement(t.h2,{id:"4-classical-forecasting-methods",style:{position:"relative"}},l.createElement(t.a,{href:"#4-classical-forecasting-methods","aria-label":"4 classical forecasting methods permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4. Classical forecasting methods"),"\n",l.createElement(t.p,null,"Classical forecasting methods remain widely used, especially where interpretability and transparency are paramount. They often serve as strong baselines that are easily explainable to stakeholders."),"\n",l.createElement(t.p,null,'Common "classical" approaches:'),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Moving average and exponential smoothing"),": Techniques that produce smoothed versions of past data and extrapolate."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Simple regression-based methods"),": Linear or polynomial regression on time index or other factors."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Decomposition-based forecasting"),": Separate out trend and seasonality from data, then forecast the remainder."),"\n"),"\n",l.createElement(t.h3,{id:"41-moving-average-ma-and-exponential-smoothing-es",style:{position:"relative"}},l.createElement(t.a,{href:"#41-moving-average-ma-and-exponential-smoothing-es","aria-label":"41 moving average ma and exponential smoothing es permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.1. Moving average (MA) and exponential smoothing (ES)"),"\n",l.createElement(t.h4,{id:"411-moving-average",style:{position:"relative"}},l.createElement(t.a,{href:"#411-moving-average","aria-label":"411 moving average permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.1.1. Moving average"),"\n",l.createElement(t.p,null,"A simple moving average of order ",l.createElement(s.A,{text:"\\(m\\)"})," for time step ",l.createElement(s.A,{text:"\\(t\\)"})," is:"),"\n",l.createElement(s.A,{text:"\\[\n\\hat{y}_t = \\frac{1}{m}\\sum_{i=t-m+1}^{t} y_i\n\\]"}),"\n",l.createElement(t.p,null,"The forecast for time ",l.createElement(s.A,{text:"\\(t+1\\)"})," is simply ",l.createElement(s.A,{text:"\\(\\hat{y}_{t+1} = \\hat{y}_t\\)"}),". This is a naive smoothing approach, ignoring seasonality or other complexities."),"\n",l.createElement(t.h4,{id:"412-exponential-smoothing",style:{position:"relative"}},l.createElement(t.a,{href:"#412-exponential-smoothing","aria-label":"412 exponential smoothing permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.1.2. Exponential smoothing"),"\n",l.createElement(t.p,null,"Exponential smoothing assigns exponentially decreasing weights over time. The simplest form, ",l.createElement(t.em,null,"simple exponential smoothing"),", is:"),"\n",l.createElement(s.A,{text:"\\[\n\\hat{y}_{t+1} = \\alpha y_t + (1 - \\alpha)\\hat{y}_t\n\\]"}),"\n",l.createElement(t.p,null,"where ",l.createElement(s.A,{text:"\\(0 < \\alpha < 1\\)"}),". More advanced versions (Holt's linear trend method, Holt-Winters method) incorporate trend and seasonality. For example, the Holt-Winters additive model can be written as:"),"\n",l.createElement(s.A,{text:"\\[\n\\begin{aligned}\n& l_{t} = \\alpha (y_{t} - s_{t-m}) + (1-\\alpha)(l_{t-1}+b_{t-1}), \\\\\n& b_{t} = \\beta (l_{t} - l_{t-1}) + (1-\\beta) b_{t-1}, \\\\\n& s_{t} = \\gamma (y_{t} - l_{t}) + (1-\\gamma)s_{t-m}.\n\\end{aligned}\n\\]"}),"\n",l.createElement(t.p,null,"Here, ",l.createElement(s.A,{text:"\\(l_t\\)"})," is the level at time ",l.createElement(s.A,{text:"\\(t\\)"}),", ",l.createElement(s.A,{text:"\\(b_t\\)"})," is the slope or trend, ",l.createElement(s.A,{text:"\\(s_t\\)"})," is the seasonal component, and ",l.createElement(s.A,{text:"\\(m\\)"})," is the seasonal period. The parameters ",l.createElement(s.A,{text:"\\(\\alpha, \\beta, \\gamma\\)"})," are smoothing coefficients."),"\n",l.createElement(t.h3,{id:"42-simple-regression-based-approaches",style:{position:"relative"}},l.createElement(t.a,{href:"#42-simple-regression-based-approaches","aria-label":"42 simple regression based approaches permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.2. Simple regression-based approaches"),"\n",l.createElement(t.p,null,"One might regress ",l.createElement(s.A,{text:"\\(y_t\\)"})," on:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"The time index ",l.createElement(s.A,{text:"\\(t\\)"})," (for a global trend)."),"\n",l.createElement(t.li,null,"Categorical dummy variables for each season (e.g., months)."),"\n",l.createElement(t.li,null,"Additional exogenous variables (like ",l.createElement(s.A,{text:"\\(\\mathbf{x}_t\\)"}),")."),"\n"),"\n",l.createElement(t.p,null,"For example, a simple additive model:"),"\n",l.createElement(s.A,{text:"\\[\ny_t = \\beta_0 + \\beta_1 t + \\sum_{j=1}^{m-1} \\beta_{2+j} D_{j,t} + \\varepsilon_t\n\\]"}),"\n",l.createElement(t.p,null,"where ",l.createElement(s.A,{text:"\\(D_{j,t}\\)"})," are seasonal dummy variables (1 for season j at time t, 0 otherwise). This might be extended with polynomial terms or splines for the trend."),"\n",l.createElement(t.h3,{id:"43-decomposition-based-forecasting",style:{position:"relative"}},l.createElement(t.a,{href:"#43-decomposition-based-forecasting","aria-label":"43 decomposition based forecasting permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.3. Decomposition-based forecasting"),"\n",l.createElement(t.p,null,"If we have an additive decomposition ",l.createElement(s.A,{text:"\\( y_t = T_t + S_t + R_t \\)"}),", a naive approach is:"),"\n",l.createElement(t.ol,null,"\n",l.createElement(t.li,null,"Estimate ",l.createElement(s.A,{text:"\\(T_t\\)"})," and ",l.createElement(s.A,{text:"\\(S_t\\)"})," from historical data (e.g., by a decomposition such as STL)."),"\n",l.createElement(t.li,null,"Forecast the trend part with a method for non-seasonal data (like an AR(1) or simple exponential smoothing)."),"\n",l.createElement(t.li,null,"Forecast the seasonal part by reusing the last observed season or by some smoothing across years."),"\n",l.createElement(t.li,null,"Combine them to get the final forecast: ",l.createElement(s.A,{text:"\\(\\hat{y}_{t+h} = \\hat{T}_{t+h} + \\hat{S}_{t+h}\\)"}),"."),"\n"),"\n",l.createElement(t.p,null,"This approach can perform well in many practical settings but might underfit abrupt changes or more complex relationships."),"\n",l.createElement(t.hr),"\n",l.createElement(t.h2,{id:"5-arima-models",style:{position:"relative"}},l.createElement(t.a,{href:"#5-arima-models","aria-label":"5 arima models permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5. ARIMA models"),"\n",l.createElement(t.h3,{id:"51-autoregressive-ar-process",style:{position:"relative"}},l.createElement(t.a,{href:"#51-autoregressive-ar-process","aria-label":"51 autoregressive ar process permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.1. Autoregressive (AR) process"),"\n",l.createElement(t.p,null,"An ",l.createElement(t.strong,null,"AR(p)")," model uses ",l.createElement(s.A,{text:"\\(p\\)"})," lags of the time series as predictors:"),"\n",l.createElement(s.A,{text:"\\[\ny_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_p y_{t-p} + \\varepsilon_t\n\\]"}),"\n",l.createElement(t.p,null,"Here, ",l.createElement(s.A,{text:"\\(\\phi_1, \\phi_2, \\dots, \\phi_p\\)"})," are parameters, ",l.createElement(s.A,{text:"\\(c\\)"})," is a constant, and ",l.createElement(s.A,{text:"\\(\\varepsilon_t \\sim \\mathrm{N}(0, \\sigma^2)\\)"}),"."),"\n",l.createElement(t.h3,{id:"52-moving-average-ma-process",style:{position:"relative"}},l.createElement(t.a,{href:"#52-moving-average-ma-process","aria-label":"52 moving average ma process permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.2. Moving average (MA) process"),"\n",l.createElement(t.p,null,"An ",l.createElement(t.strong,null,"MA(q)")," model uses ",l.createElement(s.A,{text:"\\(q\\)"})," lags of the noise or error:"),"\n",l.createElement(s.A,{text:"\\[\ny_t = c + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\dots + \\theta_q \\varepsilon_{t-q}.\n\\]"}),"\n",l.createElement(t.p,null,"In essence, the new observation is a linear combination of white noise terms."),"\n",l.createElement(t.h3,{id:"53-combined-arma-and-arima",style:{position:"relative"}},l.createElement(t.a,{href:"#53-combined-arma-and-arima","aria-label":"53 combined arma and arima permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.3. Combined ARMA and ARIMA"),"\n",l.createElement(t.p,null,"By combining AR(p) and MA(q), we get ARMA(p, q):"),"\n",l.createElement(s.A,{text:"\\[\ny_t = c + \\phi_1 y_{t-1} + \\dots + \\phi_p y_{t-p} + \\theta_1 \\varepsilon_{t-1} + \\dots + \\theta_q \\varepsilon_{t-q} + \\varepsilon_t.\n\\]"}),"\n",l.createElement(t.p,null,"However, real data often exhibit nonstationary behaviors (trend or random walk). By differencing the series ",l.createElement(s.A,{text:"\\( d \\)"})," times, we can (hopefully) achieve stationarity. This leads to ",l.createElement(t.strong,null,"ARIMA(p, d, q)"),", meaning an ARMA model on the differenced data:"),"\n",l.createElement(t.ol,null,"\n",l.createElement(t.li,null,"Let ",l.createElement(s.A,{text:"\\(\\nabla^d y_t\\)"})," denote differencing ",l.createElement(s.A,{text:"\\(d\\)"})," times (with ",l.createElement(s.A,{text:"\\(\\nabla y_t = y_t - y_{t-1}\\)"}),")."),"\n",l.createElement(t.li,null,"Then an ARIMA(p, d, q) model satisfies:\n",l.createElement(s.A,{text:"\\( \\nabla^d y_t = \\text{ARMA}(p,q) \\)"}),'."/>'),"\n"),"\n",l.createElement(t.h3,{id:"54-parameter-selection-and-model-diagnostics",style:{position:"relative"}},l.createElement(t.a,{href:"#54-parameter-selection-and-model-diagnostics","aria-label":"54 parameter selection and model diagnostics permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.4. Parameter selection and model diagnostics"),"\n",l.createElement(t.p,null,"Selecting ",l.createElement(s.A,{text:"\\((p, d, q)\\)"})," can be done via:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"Looking at ACF/PACF plots."),"\n",l.createElement(t.li,null,"Minimizing information criteria (AIC, BIC)."),"\n",l.createElement(t.li,null,"Using auto.arima-type procedures in software packages (e.g., ",l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">pmdarima</code>'}})," in Python or ",l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">forecast</code>'}})," in R)."),"\n"),"\n",l.createElement(t.p,null,"After fitting, one checks diagnostics:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"Residuals should be white noise (check ACF of residuals)."),"\n",l.createElement(t.li,null,"Model should not be overfit or underfit (use AIC/BIC or cross-validation for out-of-sample performance)."),"\n"),"\n",l.createElement(t.hr),"\n",l.createElement(t.h2,{id:"6-sarima-and-seasonal-extensions",style:{position:"relative"}},l.createElement(t.a,{href:"#6-sarima-and-seasonal-extensions","aria-label":"6 sarima and seasonal extensions permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6. SARIMA and seasonal extensions"),"\n",l.createElement(t.p,null,"Seasonality means a repeated pattern every ",l.createElement(s.A,{text:"\\(m\\)"})," observations. For monthly data, ",l.createElement(s.A,{text:"\\(m=12\\)"}),"; for quarterly data, ",l.createElement(s.A,{text:"\\(m=4\\)"}),"; for daily data with weekly patterns, ",l.createElement(s.A,{text:"\\(m=7\\)"}),"."),"\n",l.createElement(t.h3,{id:"61-sarima-structure",style:{position:"relative"}},l.createElement(t.a,{href:"#61-sarima-structure","aria-label":"61 sarima structure permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.1. SARIMA structure"),"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Seasonal ARIMA")," is typically written as: ",l.createElement(s.A,{text:"\\(\\mathrm{ARIMA}(p,d,q)\\times(P,D,Q)_m\\)"}),". For seasonal data, we do both regular differencing ",l.createElement(s.A,{text:"\\(d\\)"})," and seasonal differencing ",l.createElement(s.A,{text:"\\(D\\)"}),'. The model integrates the idea of ARIMA at two levels: the standard "non-seasonal" part plus a "seasonal" part.'),"\n",l.createElement(t.p,null,"The general form is:"),"\n",l.createElement(s.A,{text:"\\[\n\\Phi_P(L^m) \\phi_p(L) (1 - L)^d (1 - L^m)^D y_t = \\Theta_Q(L^m) \\theta_q(L) \\varepsilon_t\n\\]"}),"\n",l.createElement(t.p,null,"where ",l.createElement(s.A,{text:"\\(L\\)"})," is the lag operator (",l.createElement(s.A,{text:"\\(Ly_t = y_{t-1}\\)"}),"), ",l.createElement(s.A,{text:"\\(\\phi_p(L)\\)"})," is the non-seasonal AR polynomial, ",l.createElement(s.A,{text:"\\(\\Phi_P(L^m)\\)"})," is the seasonal AR polynomial, ",l.createElement(s.A,{text:"\\(\\theta_q(L)\\)"})," and ",l.createElement(s.A,{text:"\\(\\Theta_Q(L^m)\\)"})," are the non-seasonal and seasonal MA polynomials, respectively."),"\n",l.createElement(t.h3,{id:"62-identifying-seasonality-parameters-",style:{position:"relative"}},l.createElement(t.a,{href:"#62-identifying-seasonality-parameters-","aria-label":"62 identifying seasonality parameters  permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.2. Identifying seasonality parameters ",l.createElement(s.A,{text:"\\((P, D, Q)\\)"})),"\n",l.createElement(t.p,null,"Similar to identifying ",l.createElement(s.A,{text:"\\((p, d, q)\\)"}),", we use:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"Seasonal differencing if there is an apparent seasonal pattern that is not stable in level."),"\n",l.createElement(t.li,null,"Seasonal ACF/PACF for initial guesses of ",l.createElement(s.A,{text:"\\(P\\)"})," and ",l.createElement(s.A,{text:"\\(Q\\)"}),"."),"\n",l.createElement(t.li,null,"Automated selection by AIC/BIC or heuristics in packages."),"\n"),"\n",l.createElement(t.h3,{id:"63-model-selection-criteria-aic-bic",style:{position:"relative"}},l.createElement(t.a,{href:"#63-model-selection-criteria-aic-bic","aria-label":"63 model selection criteria aic bic permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.3. Model selection criteria (AIC, BIC)"),"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"AIC"),": Akaike Information Criterion,"),"\n",l.createElement(s.A,{text:"\\( \\mathrm{AIC} = 2k - 2\\ln(\\hat{L}) \\)"}),"\n",l.createElement(t.p,null,"where ",l.createElement(s.A,{text:"\\(k\\)"})," is the number of estimated parameters and ",l.createElement(s.A,{text:"\\(\\hat{L}\\)"})," is the maximum value of the likelihood function."),"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"BIC"),": Bayesian Information Criterion,"),"\n",l.createElement(s.A,{text:"\\( \\mathrm{BIC} = k \\ln(n) - 2\\ln(\\hat{L}) \\)"}),"\n",l.createElement(t.p,null,"A lower AIC/BIC indicates a better trade-off between model fit and complexity."),"\n",l.createElement(t.h3,{id:"64-residual-analysis",style:{position:"relative"}},l.createElement(t.a,{href:"#64-residual-analysis","aria-label":"64 residual analysis permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.4. Residual analysis"),"\n",l.createElement(t.p,null,"Once the model is fit, we check the ",l.createElement(t.strong,null,"residuals"),":"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"Ideally, residuals are uncorrelated and exhibit constant variance."),"\n",l.createElement(t.li,null,"Plot ACF of residuals. No significant spikes at any lag implies good fit."),"\n",l.createElement(t.li,null,"Perform Ljung-Box test to see if significant autocorrelations remain."),"\n"),"\n",l.createElement(t.hr),"\n",l.createElement(t.h2,{id:"7-advanced-time-series-approaches",style:{position:"relative"}},l.createElement(t.a,{href:"#7-advanced-time-series-approaches","aria-label":"7 advanced time series approaches permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7. Advanced time series approaches"),"\n",l.createElement(t.p,null,"While ARIMA and SARIMA remain mainstays, more advanced or specialized approaches can outperform them in certain contexts."),"\n",l.createElement(t.h3,{id:"71-vector-autoregression-var",style:{position:"relative"}},l.createElement(t.a,{href:"#71-vector-autoregression-var","aria-label":"71 vector autoregression var permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.1. Vector autoregression (VAR)"),"\n",l.createElement(t.p,null,"When dealing with ",l.createElement(t.strong,null,"multivariate")," time series with potential interdependencies, ",l.createElement(t.strong,null,"Vector Autoregression")," extends AR by modeling multiple time series simultaneously:"),"\n",l.createElement(s.A,{text:"\\[\n\\mathbf{y}_t = \\mathbf{c} + \\Phi_1 \\mathbf{y}_{t-1} + \\dots + \\Phi_p \\mathbf{y}_{t-p} + \\boldsymbol\\varepsilon_t\n\\]"}),"\n",l.createElement(t.p,null,"Here, ",l.createElement(s.A,{text:"\\(\\mathbf{y}_t\\)"})," is a vector of multiple variables, and ",l.createElement(s.A,{text:"\\(\\Phi_i\\)"})," are coefficient matrices."),"\n",l.createElement(t.h3,{id:"72-state-space-models-and-kalman-filters",style:{position:"relative"}},l.createElement(t.a,{href:"#72-state-space-models-and-kalman-filters","aria-label":"72 state space models and kalman filters permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.2. State space models and Kalman filters"),"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"State space")," formulations represent time series via latent (unobserved) states that evolve over time, plus an observation model:"),"\n",l.createElement(s.A,{text:"\\[\n\\begin{aligned}\n\\mathbf{x}_{t+1} &= F_t \\mathbf{x}_t + \\mathbf{w}_t, \\quad \\mathbf{w}_t \\sim \\mathcal{N}(0, Q_t), \\\\\n\\mathbf{y}_t &= H_t \\mathbf{x}_t + \\mathbf{v}_t, \\quad \\mathbf{v}_t \\sim \\mathcal{N}(0, R_t),\n\\end{aligned}\n\\]"}),"\n",l.createElement(t.p,null,"where ",l.createElement(s.A,{text:"\\(\\mathbf{x}_t\\)"})," is the hidden state. The ",l.createElement(t.strong,null,"Kalman filter")," is a recursive procedure for linear Gaussian state space models, extended to nonlinear cases as the Extended or Unscented Kalman Filter. State space models can incorporate time-varying parameters, handle missing data elegantly, and capture complex behaviors."),"\n",l.createElement(t.h3,{id:"73-long-short-term-memory-networks-lstm-motivation",style:{position:"relative"}},l.createElement(t.a,{href:"#73-long-short-term-memory-networks-lstm-motivation","aria-label":"73 long short term memory networks lstm motivation permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.3. Long Short-Term Memory networks (LSTM): motivation"),"\n",l.createElement(t.p,null,"In deep learning, ",l.createElement(t.strong,null,"LSTM")," networks (Hochreiter & Schmidhuber, 1997) overcame the vanishing gradient problem of standard RNNs, making it possible to capture long-range dependencies in sequences. Although we have a dedicated article on LSTMs, the fundamental motivation is that many time series tasks require memory of events far in the past (like multi-step seasonal or cyclical phenomena). LSTMs handle that by gating mechanisms."),"\n",l.createElement(t.p,null,"In practice, LSTMs (and GRUs) are widely used for forecasting tasks, especially for complex real-world data with multiple related series. Extensions, such as ",l.createElement(t.strong,null,"Seq2Seq")," or ",l.createElement(t.strong,null,"Encoder-Decoder")," architectures, further empower multi-step or variable-length forecasting."),"\n",l.createElement(t.h3,{id:"74-prophet-and-other-modern-libraries",style:{position:"relative"}},l.createElement(t.a,{href:"#74-prophet-and-other-modern-libraries","aria-label":"74 prophet and other modern libraries permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.4. Prophet and other modern libraries"),"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Prophet"),", developed by Meta (Facebook) researchers, is a library focusing on a parametric approach:"),"\n",l.createElement(s.A,{text:"\\[\ny(t) = g(t) + s(t) + h(t) + \\epsilon_t\n\\]"}),"\n",l.createElement(t.p,null,"where ",l.createElement(s.A,{text:"\\(g(t)\\)"})," is a piecewise linear or logistic growth curve, ",l.createElement(s.A,{text:"\\(s(t)\\)"})," is a sum of Fourier series modeling seasonality, and ",l.createElement(s.A,{text:"\\(h(t)\\)"})," is special holiday effects. Prophet automates hyperparameter selection, handles missing data, and can be quite robust to outliers or sudden changes."),"\n",l.createElement(t.p,null,"Meanwhile, modern open-source frameworks like ",l.createElement(t.strong,null,"GluonTS")," (by Amazon) or ",l.createElement(t.strong,null,"Orbit")," (by Uber) also incorporate advanced Bayesian or deep learning methods."),"\n",l.createElement(t.hr),"\n",l.createElement(t.h2,{id:"8-time-series-clustering",style:{position:"relative"}},l.createElement(t.a,{href:"#8-time-series-clustering","aria-label":"8 time series clustering permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8. Time series clustering"),"\n",l.createElement(t.p,null,'Time series clustering is an unsupervised task to group similar time series. The challenge is the definition of "similarity." Approaches include:'),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"Using ",l.createElement(i.A,null,"Dynamic Time Warping (DTW)")," as a distance metric, allowing for warping along the time dimension. Then apply standard clustering (k-means with a custom distance, hierarchical clustering, DBSCAN, etc.)."),"\n",l.createElement(t.li,null,"Feature-based methods: engineer features (mean, standard deviation, number of peaks, etc.) to embed each series in a fixed dimension, then apply standard clustering. Or use shapelets or wavelet-based features."),"\n"),"\n",l.createElement(t.p,null,"For instance, ",l.createElement(r.A,{text:"Parkinson's disease data or speech data clustering are popular use-cases for DTW-based clustering."}),". Also, advanced methods use neural embeddings (like an autoencoder) to represent each series in a latent space, then cluster in that space."),"\n",l.createElement(t.h3,{id:"81-similarity-measures-dtw-euclidean-distance",style:{position:"relative"}},l.createElement(t.a,{href:"#81-similarity-measures-dtw-euclidean-distance","aria-label":"81 similarity measures dtw euclidean distance permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.1. Similarity measures (DTW, Euclidean distance)"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Euclidean"),": Quick, but sensitive to phase shifts or varying lengths."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"DTW"),": Allows time distortions. Very popular, albeit more computationally expensive."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Correlation-based"),": If the shape is relevant more than amplitude, correlation-based distance can be used."),"\n"),"\n",l.createElement(t.h3,{id:"82-clustering-algorithms-k-means-hierarchical-spectral-applied-to-time-series",style:{position:"relative"}},l.createElement(t.a,{href:"#82-clustering-algorithms-k-means-hierarchical-spectral-applied-to-time-series","aria-label":"82 clustering algorithms k means hierarchical spectral applied to time series permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.2. Clustering algorithms (k-means, hierarchical, spectral) applied to time series"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"k-means"),": Minimizes within-cluster variance, but must handle custom distance carefully (like DTW barycenter averaging (DBA))."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Hierarchical"),": Builds a dendrogram, merges or splits clusters. Flexible but can be slow for large datasets."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Spectral"),": Uses graph-based representation from pairwise similarity. Potentially captures complex cluster shapes."),"\n"),"\n",l.createElement(t.h3,{id:"83-applications-and-interpretation",style:{position:"relative"}},l.createElement(t.a,{href:"#83-applications-and-interpretation","aria-label":"83 applications and interpretation permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.3. Applications and interpretation"),"\n",l.createElement(t.p,null,"Time series clustering is used in marketing (grouping customer usage patterns), anomaly detection (clustering normal vs. abnormal time series), medical signals, IoT sensor groupings, etc."),"\n",l.createElement(t.hr),"\n",l.createElement(t.h2,{id:"9-time-series-cross-validation",style:{position:"relative"}},l.createElement(t.a,{href:"#9-time-series-cross-validation","aria-label":"9 time series cross validation permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9. Time series cross-validation"),"\n",l.createElement(t.p,null,"Time series cross-validation differs from standard cross-validation because data is time-ordered. We generally avoid random splits that mix future data into the training set."),"\n",l.createElement(t.h3,{id:"91-rolling-window-sliding-validation",style:{position:"relative"}},l.createElement(t.a,{href:"#91-rolling-window-sliding-validation","aria-label":"91 rolling window sliding validation permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9.1. Rolling window (sliding) validation"),"\n",l.createElement(t.p,null,"Split the time series into training and validation sets in a rolling fashion:"),"\n",l.createElement(t.ol,null,"\n",l.createElement(t.li,null,"Fit on ",l.createElement(s.A,{text:"\\(\\{1, \\dots, t\\}\\)"}),", validate on ",l.createElement(s.A,{text:"\\(t+1\\)"}),"."),"\n",l.createElement(t.li,null,"Move forward, fit on ",l.createElement(s.A,{text:"\\(\\{1, \\dots, t+1\\}\\)"}),", validate on ",l.createElement(s.A,{text:"\\(t+2\\)"}),"."),"\n",l.createElement(t.li,null,"Repeat until we run out of data."),"\n"),"\n",l.createElement(t.p,null,"This allows each test set to be strictly after the training set in time."),"\n",l.createElement(t.h3,{id:"92-expanding-window-validation",style:{position:"relative"}},l.createElement(t.a,{href:"#92-expanding-window-validation","aria-label":"92 expanding window validation permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9.2. Expanding window validation"),"\n",l.createElement(t.p,null,"Similar to rolling, but each step expands the training window forward (never dropping early data). For example:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"Fit on ",l.createElement(s.A,{text:"\\(\\{1, \\dots, t\\}\\)"}),", predict ",l.createElement(s.A,{text:"\\(t+1\\)"}),"."),"\n",l.createElement(t.li,null,"Fit on ",l.createElement(s.A,{text:"\\(\\{1, \\dots, t+1\\}\\)"}),", predict ",l.createElement(s.A,{text:"\\(t+2\\)"}),"."),"\n",l.createElement(t.li,null,"etc."),"\n"),"\n",l.createElement(t.h3,{id:"93-timeseriessplit-in-practice",style:{position:"relative"}},l.createElement(t.a,{href:"#93-timeseriessplit-in-practice","aria-label":"93 timeseriessplit in practice permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9.3. TimeSeriesSplit in practice"),"\n",l.createElement(t.p,null,"In scikit-learn, ",l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">TimeSeriesSplit</code>'}})," splits the data according to time order. Alternatively, specialized frameworks like ",l.createElement(i.A,null,"statsmodels")," or ",l.createElement(i.A,null,"pmdarima")," have built-in time-series cross-validation tools."),"\n",l.createElement(t.h3,{id:"94-evaluating-model-performance-over-time",style:{position:"relative"}},l.createElement(t.a,{href:"#94-evaluating-model-performance-over-time","aria-label":"94 evaluating model performance over time permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9.4. Evaluating model performance over time"),"\n",l.createElement(t.p,null,"Often we compute metrics like RMSE or MAPE across multiple forecast horizons. We might also examine how errors change over time, or for different sub-seasons."),"\n",l.createElement(t.p,null,"One must also note the difference between 1-step vs. multi-step ahead forecasting. For multi-step, repeated rolling or expanding windows are typical to measure performance thoroughly."),"\n",l.createElement(t.hr),"\n",l.createElement(t.h2,{id:"10-practical-considerations-and-best-practices",style:{position:"relative"}},l.createElement(t.a,{href:"#10-practical-considerations-and-best-practices","aria-label":"10 practical considerations and best practices permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"10. Practical considerations and best practices"),"\n",l.createElement(t.h3,{id:"101-data-preprocessing-and-feature-engineering",style:{position:"relative"}},l.createElement(t.a,{href:"#101-data-preprocessing-and-feature-engineering","aria-label":"101 data preprocessing and feature engineering permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"10.1. Data preprocessing and feature engineering"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Normalization")," or scaling can help models converge, especially neural networks."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Fourier or wavelet transforms")," to capture cyclical or seasonal patterns, if relevant."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Date/time features"),": For instance, day of week, month of year, holiday indicators. These features can be very informative."),"\n"),"\n",l.createElement(t.h3,{id:"102-dealing-with-non-stationarity-and-transformations",style:{position:"relative"}},l.createElement(t.a,{href:"#102-dealing-with-non-stationarity-and-transformations","aria-label":"102 dealing with non stationarity and transformations permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"10.2. Dealing with non-stationarity and transformations"),"\n",l.createElement(t.p,null,"Many forecasting models (ARIMA, etc.) require or prefer stationarity. Techniques:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Differencing"),": Subtract or integrate."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Detrending"),": Fit a trend, subtract it out."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Box-Cox or log transform"),": Stabilizes variance or turns multiplicative relationships into additive."),"\n"),"\n",l.createElement(t.h3,{id:"103-handling-large-scale-datasets",style:{position:"relative"}},l.createElement(t.a,{href:"#103-handling-large-scale-datasets","aria-label":"103 handling large scale datasets permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"10.3. Handling large-scale datasets"),"\n",l.createElement(t.p,null,"For big time series or many parallel series, we might need distributed computing (Spark, Dask) or online algorithms that update incrementally. Modern cloud-based pipelines can handle partial fitting or streaming data in real time."),"\n",l.createElement(t.h3,{id:"104-model-deployment-and-maintenance",style:{position:"relative"}},l.createElement(t.a,{href:"#104-model-deployment-and-maintenance","aria-label":"104 model deployment and maintenance permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"10.4. Model deployment and maintenance"),"\n",l.createElement(t.p,null,"Time series models degrade over time due to concept drift. Best practices:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"Implement an automated monitoring system to track forecast errors."),"\n",l.createElement(t.li,null,"Retrain periodically with recent data or use online learning approaches."),"\n",l.createElement(t.li,null,"Archive old models for reproducibility and potential ensemble methods."),"\n"),"\n",l.createElement(t.hr),"\n",l.createElement(t.h2,{id:"extra-chapters-and-deeper-expansions",style:{position:"relative"}},l.createElement(t.a,{href:"#extra-chapters-and-deeper-expansions","aria-label":"extra chapters and deeper expansions permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Extra chapters and deeper expansions"),"\n",l.createElement(t.p,null,"Although not explicitly required, let's add more content to fill out theoretical and state-of-the-art aspects beyond the main outline. These expansions will help achieve a deeper, advanced-level discussion and ensure coherence with the rest of the course."),"\n",l.createElement(t.h3,{id:"11-hierarchical-time-series-forecasting",style:{position:"relative"}},l.createElement(t.a,{href:"#11-hierarchical-time-series-forecasting","aria-label":"11 hierarchical time series forecasting permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"11. Hierarchical time series forecasting"),"\n",l.createElement(t.p,null,"In business contexts, we often have data aggregated at multiple levels (e.g., total sales vs. product-level sales). Hierarchical forecasting reconciles predictions across these groupings. A widely cited approach is the hierarchical reconciliation method by Hyndman and gang (2008, JASA), where bottom-up or top-down or middle-out techniques are used, or the well-known MinT approach."),"\n",l.createElement(t.h3,{id:"12-transfer-learning-for-time-series",style:{position:"relative"}},l.createElement(t.a,{href:"#12-transfer-learning-for-time-series","aria-label":"12 transfer learning for time series permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"12. Transfer learning for time series"),"\n",l.createElement(t.p,null,"A cutting-edge area: large, pre-trained neural networks for time series. Research at NeurIPS and ICML has explored how to train a big model on diverse time series from multiple domains, then fine-tune on a new dataset. This is reminiscent of large language models in NLP, though the domain constraints differ."),"\n",l.createElement(t.h3,{id:"13-interpretable-time-series-modeling",style:{position:"relative"}},l.createElement(t.a,{href:"#13-interpretable-time-series-modeling","aria-label":"13 interpretable time series modeling permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"13. Interpretable time series modeling"),"\n",l.createElement(t.p,null,"Various methods strive to preserve interpretability:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Additive models")," (like Prophet) are relatively interpretable because each component is isolated."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Shapelet-based")," methods in classification tasks."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Attention-based")," neural networks (like Transformers) can highlight parts of the input that contributed strongly to the forecast, though interpretability is partial."),"\n"),"\n",l.createElement(t.h3,{id:"14-causality-vs-correlation-in-time-series",style:{position:"relative"}},l.createElement(t.a,{href:"#14-causality-vs-correlation-in-time-series","aria-label":"14 causality vs correlation in time series permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"14. Causality vs. correlation in time series"),"\n",l.createElement(t.p,null,"Forecasting does not necessarily require causal relationships. However, advanced approaches like ",l.createElement(t.strong,null,"Granger causality")," testing or ",l.createElement(t.strong,null,"structural time series")," can glean causal insights. Bayesian networks or do-calculus methods are sometimes used for causal modeling in time series, though it remains a complex topic."),"\n",l.createElement(t.h3,{id:"15-esg-based-forecasting-or-sustainability-contexts",style:{position:"relative"}},l.createElement(t.a,{href:"#15-esg-based-forecasting-or-sustainability-contexts","aria-label":"15 esg based forecasting or sustainability contexts permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"15. ESG-based forecasting or sustainability contexts"),"\n",l.createElement(t.p,null,"Another emergent domain is environment, social, governance (ESG) analytics. For example, forecasting carbon emissions or measuring ESG metrics for companies often requires advanced time series and exogenous regressors. Some specialized transformations or domain-specific constraints might be relevant."),"\n",l.createElement(t.hr),"\n",l.createElement(t.h2,{id:"illustrative-python-code-examples",style:{position:"relative"}},l.createElement(t.a,{href:"#illustrative-python-code-examples","aria-label":"illustrative python code examples permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Illustrative Python code examples"),"\n",l.createElement(t.p,null,"To clarify various steps, here is an integrated example that uses ",l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">statsmodels</code>'}})," for ARIMA and ",l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">pmdarima</code>'}})," for auto-ARIMA, plus an example of an LSTM-based model using Keras, all applied to a sample dataset. These code snippets are simplified; in practice, you might structure your code differently or incorporate more robust data preparation."),"\n",l.createElement(t.h3,{id:"151-arima-with-cross-validation",style:{position:"relative"}},l.createElement(t.a,{href:"#151-arima-with-cross-validation","aria-label":"151 arima with cross validation permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"15.1. ARIMA with cross-validation"),"\n",l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">&lt;Code text={`\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom sklearn.metrics import mean_squared_error\nimport math\n\n# Example: monthly data\ndf = pd.read_csv(\'monthly_data.csv\', parse_dates=[\'Month\'], index_col=\'Month\')\nseries = df[\'value\']\n\n# Train/test split\ntrain_size = int(len(series) * 0.8)\ntrain, test = series.iloc[:train_size], series.iloc[train_size:]\n\nhistory = train.tolist()\npredictions = []\n\nfor t in range(len(test)):\n    model = ARIMA(history, order=(2,1,2))\n    model_fit = model.fit()\n    yhat = model_fit.forecast(steps=1)\n    predictions.append(yhat[0])\n    obs = test.iloc[t]\n    history.append(obs)\n\nrmse = math.sqrt(mean_squared_error(test, predictions))\nprint("Test RMSE:", rmse)\n`} /></code></pre></div>'}}),"\n",l.createElement(t.h3,{id:"152-automated-sarima",style:{position:"relative"}},l.createElement(t.a,{href:"#152-automated-sarima","aria-label":"152 automated sarima permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"15.2. Automated SARIMA"),"\n",l.createElement(t.span,{dangerouslySetInnerHTML:{__html:"<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">&lt;Code text={`\nimport pmdarima as pm\nimport pandas as pd\n\ndf = pd.read_csv('monthly_data.csv', parse_dates=['Month'], index_col='Month')\nseries = df['value']\n\n# auto_arima can figure out p, d, q, P, D, Q, m, etc. based on AIC\nmodel = pm.auto_arima(series, seasonal=True, m=12,\n                      start_p=1, start_q=1, max_p=5, max_q=5,\n                      start_P=0, start_Q=0, max_P=5, max_Q=5,\n                      d=None, D=None,\n                      trace=True, \n                      error_action='ignore', \n                      suppress_warnings=True,\n                      stepwise=True)\n\nprint(model.summary())\n\n# Forecast the next 12 months\nforecast, conf_int = model.predict(n_periods=12, return_conf_int=True)\n`} /></code></pre></div>"}}),"\n",l.createElement(t.h3,{id:"153-lstm-example-for-univariate-series",style:{position:"relative"}},l.createElement(t.a,{href:"#153-lstm-example-for-univariate-series","aria-label":"153 lstm example for univariate series permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"15.3. LSTM example for univariate series"),"\n",l.createElement(t.span,{dangerouslySetInnerHTML:{__html:"<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">&lt;Code text={`\nimport numpy as np\nimport pandas as pd\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense\nfrom sklearn.preprocessing import MinMaxScaler\n\ndf = pd.read_csv('daily_data.csv', parse_dates=['Date'], index_col='Date')\nvalues = df['value'].values.reshape(-1,1)\n\n# Scale data\nscaler = MinMaxScaler(feature_range=(0,1))\nscaled = scaler.fit_transform(values)\n\n# Transform to supervised problem\ndef series_to_supervised(data, n_in=1, n_out=1):\n    X, y = [], []\n    for i in range(len(data) - n_in - n_out + 1):\n        X.append(data[i:(i+n_in), 0])\n        y.append(data[(i+n_in):(i+n_in+n_out), 0])\n    return np.array(X), np.array(y)\n\nn_lag = 3\nn_out = 1\nX, y = series_to_supervised(scaled, n_lag, n_out)\ntrain_size = int(len(X)*0.8)\nX_train, X_test = X[:train_size], X[train_size:]\ny_train, y_test = y[:train_size], y[train_size:]\n\n# Reshape for LSTM [samples, timesteps, features]\nX_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\nX_test  = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n\nmodel = Sequential()\nmodel.add(LSTM(50, input_shape=(n_lag, 1), activation='relu'))\nmodel.add(Dense(n_out))\nmodel.compile(optimizer='adam', loss='mse')\n\nmodel.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1)\n\n# Make predictions\npreds = model.predict(X_test)\npreds_rescaled = scaler.inverse_transform(preds)\n\n# Evaluate\ny_test_rescaled = scaler.inverse_transform(y_test)\n`} /></code></pre></div>"}}),"\n",l.createElement(t.p,null,'One can expand upon this to do multi-step forecasts, incorporate more features, or build more sophisticated neural network architectures like CNN-LSTMs, Transformers, or advanced "N-BEATS" approaches (Oreshkin and gang, 2020, ICLR) for time series forecasting.'),"\n",l.createElement(t.hr),"\n",l.createElement(t.h2,{id:"references",style:{position:"relative"}},l.createElement(t.a,{href:"#references","aria-label":"references permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"References"),"\n",l.createElement(t.p,null,"Below is a partial list of references and further reading:"),"\n",l.createElement(t.ol,null,"\n",l.createElement(t.li,null,'Cleveland, R. B., Cleveland, W. S., McRae, J. E., & Terpenning, I. (1990). "STL: A Seasonal-Trend Decomposition Procedure Based on Loess." ',l.createElement(t.em,null,"Journal of Official Statistics"),", 6, 3â€“73."),"\n",l.createElement(t.li,null,"Hyndman, R. J., & Athanasopoulos, G. (2021). ",l.createElement(t.em,null,"Forecasting: principles and practice")," (3rd ed). OTexts."),"\n",l.createElement(t.li,null,"Box, G. E. P., Jenkins, G. M., & Reinsel, G. C. (2015). ",l.createElement(t.em,null,"Time Series Analysis: Forecasting and Control"),". John Wiley & Sons."),"\n",l.createElement(t.li,null,'Hochreiter, S., & Schmidhuber, J. (1997). "Long Short-Term Memory." ',l.createElement(t.em,null,"Neural Computation"),", 9(8), 1735â€“1780."),"\n",l.createElement(t.li,null,'Oreshkin, B. N., Carpov, D., Chapados, N., & Bengio, Y. (2020). "N-BEATS: Neural basis expansion analysis for interpretable time series forecasting." ',l.createElement(t.em,null,"International Conference on Learning Representations")," (ICLR)."),"\n",l.createElement(t.li,null,'Taylor, S. J., & Letham, B. (2018). "Forecasting at Scale." ',l.createElement(t.em,null,"The American Statistician"),", 72(1), 37â€“45."),"\n",l.createElement(t.li,null,"Durbin, J., & Koopman, S. J. (2012). ",l.createElement(t.em,null,"Time Series Analysis by State Space Methods"),". Oxford University Press."),"\n",l.createElement(t.li,null,'Athanasopoulos, G., Hyndman, R. J., Song, H., & Wu, D. C. (2011). "The tourism forecasting competition data." ',l.createElement(t.em,null,"International Journal of Forecasting"),", 27(3), 822â€“832."),"\n",l.createElement(t.li,null,'Seeger, M., Salinas, D., & Flunkert, V. (2017). "Bayesian Intermittent Demand Forecasting for Large Inventories." ',l.createElement(t.em,null,"NeurIPS"),"."),"\n",l.createElement(t.li,null,'Salinas, D., Flunkert, V., Gasthaus, J., & Januschowski, T. (2020). "DeepAR: Probabilistic forecasting with autoregressive recurrent networks." ',l.createElement(t.em,null,"International Journal of Forecasting"),", 36(3), 1181â€“1191."),"\n"),"\n",l.createElement(t.hr),"\n",l.createElement(t.h2,{id:"conclusion",style:{position:"relative"}},l.createElement(t.a,{href:"#conclusion","aria-label":"conclusion permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Conclusion"),"\n",l.createElement(t.p,null,"Time series analysis and forecasting is a rich field with broad applications and a large toolbox of methods. Classical approaches like ARIMA or exponential smoothing remain widely used and are important references for interpretability and reliability. However, data scientists now also have access to advanced approaches (state space models, vector autoregression for multivariate, deep learning architectures such as LSTM, CNN-LSTM, Transformers, etc.)."),"\n",l.createElement(t.p,null,"No single method universally outperforms all others; success depends heavily on domain knowledge, data characteristics (trend, seasonality, outliers, missing values), the forecast horizon, and evaluation criteria. Cross-validation designed specifically for time series is crucial for robust performance estimation. Always start with an understanding of your data and domain context, build baseline models for reference (like naive or basic exponential smoothing), then iterate with more sophisticated approaches, carefully validating and monitoring your forecasts over time."),"\n",l.createElement(t.p,null,"In professional settings, consider the full lifecycle of time series solutions: data ingestion, cleaning, model building, validation, deployment, monitoring, and retraining. With thorough exploratory data analysis, well-chosen methods, and a systematic approach to validation, you can produce accurate and trustworthy time series forecasts that drive valuable decisions and insights."),"\n",l.createElement(t.hr))}var c=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,n.RP)(),e.components);return t?l.createElement(t,e,l.createElement(o,e)):o(e)};var m=a(54506),d=a(88864),h=a(58481),p=a.n(h),u=a(5984),g=a(43672),v=a(27042),f=a(72031),E=a(81817),y=a(27105),b=a(17265),S=a(2043),x=a(95751),_=a(94328),w=a(80791),M=a(78137);const A=e=>{let{toc:t}=e;if(!t||!t.items)return null;return l.createElement("nav",{className:w.R},l.createElement("ul",null,t.items.map(((e,t)=>l.createElement("li",{key:t},l.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const a=t.replace("#",""),n=document.getElementById(a);n&&n.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&l.createElement(A,{toc:{items:e.items}}))))))};function H(e){let{data:{mdx:t,allMdx:r,allPostImages:i},children:s}=e;const{frontmatter:o,body:c,tableOfContents:d}=t,h=o.index,f=o.slug.split("/")[1],w=r.nodes.filter((e=>e.frontmatter.slug.includes(`/${f}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),H=w.findIndex((e=>e.frontmatter.index===h)),C=w[H+1],T=w[H-1],L=o.slug.replace(/\/$/,""),z=/[^/]*$/.exec(L)[0],k=`posts/${f}/content/${z}/`,{0:I,1:N}=(0,l.useState)(o.flagWideLayoutByDefault),{0:V,1:B}=(0,l.useState)(!1);var j;(0,l.useEffect)((()=>{B(!0);const e=setTimeout((()=>B(!1)),340);return()=>clearTimeout(e)}),[I]),"adventures"===f?j=b.cb:"research"===f?j=b.Qh:"thoughts"===f&&(j=b.T6);const D=p()(c).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,P=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),a=e%60;return a<=30?`~${t}${a>0?".5":""} h`:`~${t+1} h`}(Math.ceil(D/j)+(o.extraReadTimeMin||0)),R=[{flag:o.flagDraft,component:()=>Promise.all([a.e(5850),a.e(9833)]).then(a.bind(a,49833))},{flag:o.flagMindfuckery,component:()=>Promise.all([a.e(5850),a.e(7805)]).then(a.bind(a,27805))},{flag:o.flagRewrite,component:()=>Promise.all([a.e(5850),a.e(8916)]).then(a.bind(a,78916))},{flag:o.flagOffensive,component:()=>Promise.all([a.e(5850),a.e(6731)]).then(a.bind(a,49112))},{flag:o.flagProfane,component:()=>Promise.all([a.e(5850),a.e(3336)]).then(a.bind(a,83336))},{flag:o.flagMultilingual,component:()=>Promise.all([a.e(5850),a.e(2343)]).then(a.bind(a,62343))},{flag:o.flagUnreliably,component:()=>Promise.all([a.e(5850),a.e(6865)]).then(a.bind(a,11627))},{flag:o.flagPolitical,component:()=>Promise.all([a.e(5850),a.e(4417)]).then(a.bind(a,24417))},{flag:o.flagCognitohazard,component:()=>Promise.all([a.e(5850),a.e(8669)]).then(a.bind(a,18669))},{flag:o.flagHidden,component:()=>Promise.all([a.e(5850),a.e(8124)]).then(a.bind(a,48124))}],{0:F,1:O}=(0,l.useState)([]);return(0,l.useEffect)((()=>{R.forEach((e=>{let{flag:t,component:a}=e;t&&a().then((e=>{O((t=>[].concat((0,m.A)(t),[e.default])))}))}))}),[]),l.createElement(v.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},l.createElement(E.A,{postNumber:o.index,date:o.date,updated:o.updated,readTime:P,difficulty:o.difficultyLevel,title:o.title,desc:o.desc,banner:o.banner,section:f,postKey:z,isMindfuckery:o.flagMindfuckery,mainTag:o.mainTag}),l.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},o.otherTags.map(((e,t)=>l.createElement("span",{key:t,className:`noselect ${M.MW}`,style:{margin:"0 5px 5px 0"}},e)))),l.createElement("div",{className:"postBody"},l.createElement(A,{toc:d})),l.createElement("br",null),l.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},l.createElement(v.P.button,{className:`noselect ${_.pb}`,id:_.xG,onClick:()=>{N(!I)},whileTap:{scale:.93}},l.createElement(v.P.div,{className:x.DJ,key:I,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},I?"Switch to default layout":"Switch to wide layout"))),l.createElement("br",null),l.createElement("div",{className:"postBody",style:{margin:I?"0 -14%":"",maxWidth:I?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},l.createElement("div",{className:`${_.P_} ${V?_.Xn:_.qG}`},F.map(((e,t)=>l.createElement(e,{key:t}))),o.indexCourse?l.createElement(S.A,{index:o.indexCourse,category:o.courseCategoryName}):"",l.createElement(u.Z.Provider,{value:{images:i.nodes,basePath:k.replace(/\/$/,"")+"/"}},l.createElement(n.xA,{components:{Image:g.A}},s)))),l.createElement(y.A,{nextPost:C,lastPost:T,keyCurrent:z,section:f}))}function C(e){return l.createElement(H,e,l.createElement(c,e))}function T(e){var t,a,n,r,i;let{data:s}=e;const{frontmatter:o}=s.mdx,c=o.titleSEO||o.title,m=o.titleOG||c,h=o.titleTwitter||c,p=o.descSEO||o.desc,u=o.descOG||p,g=o.descTwitter||p,v=o.schemaType||"BlogPosting",E=o.keywordsSEO,y=o.date,b=o.updated||y,S=o.imageOG||(null===(t=o.banner)||void 0===t||null===(a=t.childImageSharp)||void 0===a||null===(n=a.gatsbyImageData)||void 0===n||null===(r=n.images)||void 0===r||null===(i=r.fallback)||void 0===i?void 0:i.src),x=o.imageAltOG||u,_=o.imageTwitter||S,w=o.imageAltTwitter||g,M=o.canonicalURL,A=o.flagHidden||!1,H=o.mainTag||"Posts",C=o.slug.split("/")[1]||"posts",{siteUrl:T}=(0,d.Q)(),L={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:T},{"@type":"ListItem",position:2,name:H,item:`${T}/${o.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${T}${o.slug}`}]};return l.createElement(f.A,{title:c+" - avrtt.blog",titleOG:m,titleTwitter:h,description:p,descriptionOG:u,descriptionTwitter:g,schemaType:v,keywords:E,datePublished:y,dateModified:b,imageOG:S,imageAltOG:x,imageTwitter:_,imageAltTwitter:w,canonicalUrl:M,flagHidden:A,mainTag:H,section:C,type:"article"},l.createElement("script",{type:"application/ld+json"},JSON.stringify(L)))}},90548:function(e,t,a){var n=a(96540),l=a(7978);t.A=e=>{let{text:t}=e;return n.createElement(l.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-tsx-content-file-path-src-pages-posts-research-time-series-mdx-5e1cdad82f6d7e423925.js.map