"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[4519],{65862:function(e,t,n){n.r(t),n.d(t,{Head:function(){return T},PostTemplate:function(){return H},default:function(){return z}});var a=n(54506),i=n(28453),o=n(96540),r=n(16886),l=n(46295),s=n(96098);function c(e){const t=Object.assign({p:"p",ul:"ul",li:"li",strong:"strong",h2:"h2",a:"a",span:"span",h3:"h3",ol:"ol",h4:"h4",br:"br",hr:"hr"},(0,i.RP)(),e.components),{Image:n}=t;return n||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),o.createElement(o.Fragment,null,"\n",o.createElement("br"),"\n","\n",o.createElement(t.p,null,'Topic modeling remains one of the most prominent unsupervised learning techniques used to automatically infer the hidden thematic structure from large collections of documents. While originally devised for textual data, it has also been extended to a variety of domains such as bioinformatics, image processing (where "topics" can represent clusters of visual features), or event detection in social networks. The overall motivation stems from the desire to discover latent factors or hidden topics that shape the content of a corpus. These latent factors in text usually manifest themselves as sets of terms that frequently co-occur together — revealing emergent themes that an analyst might not have previously anticipated.'),"\n",o.createElement(t.p,null,"Topic modeling enables researchers, data scientists, and domain experts to drastically reduce the complexity of large textual archives. Instead of manually reading thousands (or millions) of documents, one can employ topic modeling to group documents by high-level themes. This has found wide application in:"),"\n",o.createElement(t.ul,null,"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Content recommendation systems"),": A news website or a blog aggregator might use topic modeling to categorize articles, surface them to relevant audiences, and suggest thematically related stories."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Academic research"),": Digital humanities scholars utilize topic modeling to explore large bodies of literature or historical archives, gleaning patterns across centuries of texts, and discovering how themes have evolved."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Market research and brand monitoring"),": Social media content, customer feedback, or product reviews can be studied to find recurring topics or sentiments. Topic modeling helps to cluster these texts into marketing-relevant themes (for instance, design features, shipping problems, pricing concerns)."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Search and indexing"),": Topic models can be used to index documents more efficiently, so that searching by topic yields more relevant results than naive keyword-based retrieval."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Trend analysis in social networks"),": Online communities often generate textual content at astonishing velocity. Topic modeling can be applied to identify trending topics, reveal emergent phenomena, or detect changes in discourse over time."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Fraud detection and security"),": In certain contexts, textual logs (like emails, chat transcripts, or security logs with textual descriptions) might contain hidden patterns. Topic models may reveal suspicious themes or patterns that signal fraud or illicit behavior."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Medical and biomedical research"),": Clinical notes, scientific publications in medicine, and patient feedback can be massive in scale. Topic modeling helps cluster key areas of concern and could even highlight previously overlooked subtopics."),"\n"),"\n",o.createElement(t.p,null,"In sum, topic modeling is a powerful lens through which textual data can be reinterpreted. The automatically extracted topics serve as an abstraction or compressed representation of the text, helping us to handle, navigate, and conceptualize immense corpora more effectively."),"\n",o.createElement(t.h2,{id:"chapter-2-foundational-concepts",style:{position:"relative"}},o.createElement(t.a,{href:"#chapter-2-foundational-concepts","aria-label":"chapter 2 foundational concepts permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Chapter 2. Foundational concepts"),"\n",o.createElement(t.h3,{id:"latent-variables-in-text-modeling",style:{position:"relative"}},o.createElement(t.a,{href:"#latent-variables-in-text-modeling","aria-label":"latent variables in text modeling permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Latent variables in text modeling"),"\n",o.createElement(t.p,null,'In most topic modeling approaches, the notion of "latent variables" is central. A latent variable is a hidden or unobserved variable that influences the observed data. In text modeling, each document is assumed to be generated by a mixture of underlying topics — topics themselves are distributions over words or terms. The intensities or probabilities with which topics appear in a document are these latent variables that we attempt to infer.'),"\n",o.createElement(t.p,null,"For instance, suppose you have a large collection of news articles about world events, sports, politics, technology, and the arts. Each article is formed by certain combinations of these topic distributions (latent variables). If you find a document with 50% focus on technology, 20% on business, 20% on politics, and 10% on sports, that is effectively a set of inferred latent variables describing the composition of that document. The presence of these hidden factors is not directly observable but can be inferred through statistical means."),"\n",o.createElement(t.h3,{id:"probability-distributions-and-their-role-in-topic-modeling",style:{position:"relative"}},o.createElement(t.a,{href:"#probability-distributions-and-their-role-in-topic-modeling","aria-label":"probability distributions and their role in topic modeling permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Probability distributions and their role in topic modeling"),"\n",o.createElement(t.p,null,"Topic modeling frameworks usually adopt a probabilistic generative perspective. In the classic generative story of Latent Dirichlet Allocation (LDA) — which we will discuss more thoroughly soon — each topic ",o.createElement(s.A,{text:"\\( z \\)"})," is represented by a probability distribution over words from the vocabulary. Likewise, each document ",o.createElement(s.A,{text:"\\( d \\)"})," is represented by a distribution over topics. Mathematically, you might see the topic distribution for document ",o.createElement(s.A,{text:"\\( d \\)"})," denoted as ",o.createElement(s.A,{text:"\\( \\theta_d \\)"})," (a vector of probabilities summing to 1), and the word distribution for topic ",o.createElement(s.A,{text:"\\( k \\)"})," denoted as ",o.createElement(s.A,{text:"\\( \\phi_k \\)"}),"."),"\n",o.createElement(t.p,null,"Then, if we pick a particular word from document ",o.createElement(s.A,{text:"\\( d \\)"}),", we do so by first choosing a topic ",o.createElement(s.A,{text:"\\( z \\)"})," with probability ",o.createElement(s.A,{text:"\\( \\theta_d[z] \\)"})," and then choosing a particular word ",o.createElement(s.A,{text:"\\( w \\)"})," from the distribution over words for that topic ",o.createElement(s.A,{text:"\\( \\phi_z[w] \\)"}),". Formally, you might see something along the lines of:"),"\n",o.createElement(s.A,{text:"\\[\nP(w \\mid d) = \\sum_{k=1}^K P(w \\mid z=k) P(z=k \\mid d)\n\\]"}),"\n",o.createElement(t.p,null,"Where:"),"\n",o.createElement(t.ul,null,"\n",o.createElement(t.li,null,o.createElement(s.A,{text:"\\( K \\)"})," is the total number of topics."),"\n",o.createElement(t.li,null,o.createElement(s.A,{text:"\\( P(z=k \\mid d) \\equiv \\theta_{d}[k] \\)"})," is the probability that topic ",o.createElement(s.A,{text:"\\(k\\)"})," is chosen when generating a word from document ",o.createElement(s.A,{text:"\\(d\\)"}),"."),"\n",o.createElement(t.li,null,o.createElement(s.A,{text:"\\( P(w \\mid z=k) \\equiv \\phi_k[w] \\)"})," is the probability of word ",o.createElement(s.A,{text:"\\(w\\)"})," under topic ",o.createElement(s.A,{text:"\\(k\\)"}),"."),"\n"),"\n",o.createElement(t.p,null,"Hence, each topic is defined as a distribution over words, and each document is defined as a distribution over topics. This perspective allows the data scientist to exploit the entire probabilistic machinery (e.g., Bayesian inference, maximum likelihood approaches, variational inference, or Gibbs sampling) to estimate these latent variables."),"\n",o.createElement(t.h3,{id:"dimensionality-reduction-and-its-connection-to-topic-modeling",style:{position:"relative"}},o.createElement(t.a,{href:"#dimensionality-reduction-and-its-connection-to-topic-modeling","aria-label":"dimensionality reduction and its connection to topic modeling permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Dimensionality reduction and its connection to topic modeling"),"\n",o.createElement(t.p,null,"Topic modeling can be viewed as a form of ",o.createElement(t.strong,null,"dimensionality reduction"),". A given corpus of documents can be extremely high-dimensional if we consider each unique word as a dimension. Suppose your vocabulary has ",o.createElement(s.A,{text:"\\( V \\)"})," words, then each document is nominally represented as a point in a ",o.createElement(s.A,{text:"\\( V \\)"}),"-dimensional space (e.g., as a bag-of-words vector). However, in a topic model with ",o.createElement(s.A,{text:"\\( K \\)"})," latent topics, each document is effectively captured by a ",o.createElement(s.A,{text:"\\( K \\)"}),"-dimensional representation: its topic mixture distribution. Thus, the raw dimensionality ",o.createElement(s.A,{text:"\\( V \\)"})," is reduced to ",o.createElement(s.A,{text:"\\( K \\)"}),", while hopefully retaining most of the semantic content."),"\n",o.createElement(t.p,null,"In that sense, topic modeling is loosely analogous to other matrix factorization or decomposition techniques such as Principal Component Analysis (PCA) or Non-negative Matrix Factorization (NMF). However, the difference in most topic modeling approaches is the explicit probabilistic interpretation: each dimension in the reduced space (i.e., each topic) is a probability distribution over words rather than a purely algebraic factor."),"\n",o.createElement(t.h3,{id:"terminology-of-topic-modeling",style:{position:"relative"}},o.createElement(t.a,{href:"#terminology-of-topic-modeling","aria-label":"terminology of topic modeling permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Terminology of topic modeling"),"\n",o.createElement(t.p,null,"Some commonly encountered terms in topic modeling:"),"\n",o.createElement(t.ul,null,"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Topic"),": A probabilistic distribution over the vocabulary that tends to reflect a coherent theme (e.g., sports, politics, or technology)."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Topic mixture"),": The distribution over the set of topics for a specific document. Usually denoted as ",o.createElement(s.A,{text:"\\( \\theta \\)"})," in many topic modeling frameworks."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Word distribution"),": For each topic, we have a distribution over words, typically denoted ",o.createElement(s.A,{text:"\\( \\phi \\)"})," or sometimes ",o.createElement(s.A,{text:"\\( \\beta \\)"}),"."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Hyperparameters"),": These are parameters controlling the shape of the distributions over topics and words (e.g., alpha and beta in LDA)."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Variational inference / Gibbs sampling"),": Algorithms used in some topic models to perform inference and find the posterior distribution of latent variables."),"\n"),"\n",o.createElement(t.p,null,"Topic modeling frameworks vary in how they formulate the relationships among these components, but the above terms tend to remain consistent across many methods."),"\n",o.createElement(t.h2,{id:"chapter-3-popular-topic-modeling-frameworks",style:{position:"relative"}},o.createElement(t.a,{href:"#chapter-3-popular-topic-modeling-frameworks","aria-label":"chapter 3 popular topic modeling frameworks permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Chapter 3. Popular topic modeling frameworks"),"\n",o.createElement(t.p,null,"Topic modeling has evolved to encompass an array of frameworks and paradigms, each with unique strengths, assumptions, and computational complexities. Below are some of the most commonly used:"),"\n",o.createElement(t.h3,{id:"latent-dirichlet-allocation-lda",style:{position:"relative"}},o.createElement(t.a,{href:"#latent-dirichlet-allocation-lda","aria-label":"latent dirichlet allocation lda permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Latent Dirichlet Allocation (LDA)"),"\n",o.createElement(t.p,null,o.createElement(t.strong,null,"Latent Dirichlet Allocation"),' (Blei, Ng, & Jordan, JMLR 2003) is probably the most well-known method of topic modeling. LDA introduced a fully generative Bayesian model for documents. The name "Dirichlet" arises from the prior placed over the per-document topic distribution and the per-topic word distribution.'),"\n",o.createElement(t.ul,null,"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Generative process"),":","\n",o.createElement(t.ol,null,"\n",o.createElement(t.li,null,"For each document ",o.createElement(s.A,{text:"\\( d \\)"}),", draw a distribution over topics ",o.createElement(s.A,{text:"\\( \\theta_d \\)"})," from a Dirichlet prior with parameter ",o.createElement(s.A,{text:"\\( \\alpha \\)"}),"."),"\n",o.createElement(t.li,null,"For each topic ",o.createElement(s.A,{text:"\\( k \\)"}),", draw a distribution over words ",o.createElement(s.A,{text:"\\( \\phi_k \\)"})," from a Dirichlet prior with parameter ",o.createElement(s.A,{text:"\\( \\beta \\)"}),"."),"\n",o.createElement(t.li,null,"For each word in a document, choose a topic assignment ",o.createElement(s.A,{text:"\\( z \\)"})," according to ",o.createElement(s.A,{text:"\\( \\theta_d \\)"}),", then choose a word ",o.createElement(s.A,{text:"\\( w \\)"})," according to ",o.createElement(s.A,{text:"\\( \\phi_z \\)"}),"."),"\n"),"\n"),"\n"),"\n",o.createElement(t.p,null,"A commonly used formula for the joint distribution of the latent variables and observed words in LDA is:"),"\n",o.createElement(s.A,{text:"\\[\nP(\\theta, z, w \\mid \\alpha, \\beta) = \\prod_{d=1}^{D} P(\\theta_d \\mid \\alpha)\n\\prod_{k=1}^{K} P(\\phi_k \\mid \\beta)\n\\prod_{d=1}^{D} \\prod_{n=1}^{N_d} P(z_{d,n} \\mid \\theta_d) P(w_{d,n} \\mid \\phi_{z_{d,n}})\n\\]"}),"\n",o.createElement(t.p,null,"Where:"),"\n",o.createElement(t.ul,null,"\n",o.createElement(t.li,null,o.createElement(s.A,{text:"\\( D \\)"})," is the number of documents."),"\n",o.createElement(t.li,null,o.createElement(s.A,{text:"\\( N_d \\)"})," is the number of words in document ",o.createElement(s.A,{text:"\\( d \\)"}),"."),"\n",o.createElement(t.li,null,o.createElement(s.A,{text:"\\( K \\)"})," is the number of topics."),"\n",o.createElement(t.li,null,o.createElement(s.A,{text:"\\( \\alpha \\)"})," and ",o.createElement(s.A,{text:"\\( \\beta \\)"})," are hyperparameters for the Dirichlet priors."),"\n",o.createElement(t.li,null,o.createElement(s.A,{text:"\\( \\theta_d \\)"})," is the distribution of topics in document ",o.createElement(s.A,{text:"\\( d \\)"}),"."),"\n",o.createElement(t.li,null,o.createElement(s.A,{text:"\\( \\phi_k \\)"})," is the distribution over words for topic ",o.createElement(s.A,{text:"\\( k \\)"}),"."),"\n",o.createElement(t.li,null,o.createElement(s.A,{text:"\\( z_{d,n} \\)"})," is the topic assignment for the ",o.createElement(s.A,{text:"\\( n \\)"}),"-th word in document ",o.createElement(s.A,{text:"\\( d \\)"}),"."),"\n",o.createElement(t.li,null,o.createElement(s.A,{text:"\\( w_{d,n} \\)"})," is the actual observed word."),"\n"),"\n",o.createElement(t.p,null,o.createElement(t.strong,null,"Inference")," for LDA can be done with Gibbs sampling, variational inference, or other approximate methods. Although LDA can be computationally expensive, it remains a standard reference due to its interpretability and strong theoretical grounding."),"\n",o.createElement(t.h3,{id:"probabilistic-latent-semantic-analysis-plsa",style:{position:"relative"}},o.createElement(t.a,{href:"#probabilistic-latent-semantic-analysis-plsa","aria-label":"probabilistic latent semantic analysis plsa permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Probabilistic latent semantic analysis (PLSA)"),"\n",o.createElement(t.p,null,o.createElement(t.strong,null,"Probabilistic Latent Semantic Analysis")," (Hofmann, 1999) can be viewed as a precursor to LDA. It also models documents in terms of latent topics, but it lacks a prior over topic distributions and is thus considered a non-Bayesian approach. Instead, it relies on a maximum likelihood estimation with an Expectation-Maximization (EM) algorithm to find the parameters."),"\n",o.createElement(t.p,null,"A key difference from LDA is that PLSA can overfit, especially because it introduces a large number of parameters without having a proper prior. LDA, by contrast, introduces Dirichlet priors that control the distributions and reduce overfitting. Despite this, PLSA is still used in certain contexts and provides a simpler conceptual introduction to how text can be factorized into latent topics."),"\n",o.createElement(t.h3,{id:"non-negative-matrix-factorization-nmf",style:{position:"relative"}},o.createElement(t.a,{href:"#non-negative-matrix-factorization-nmf","aria-label":"non negative matrix factorization nmf permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Non-negative matrix factorization (NMF)"),"\n",o.createElement(t.p,null,o.createElement(t.strong,null,"Non-negative matrix factorization")," is a purely algebraic approach to factor a term-document matrix into two lower-rank matrices, both of which have only non-negative entries. If ",o.createElement(s.A,{text:"\\( X \\)"})," is a ",o.createElement(s.A,{text:"\\( V \\times D \\)"})," matrix representing word counts (or TF-IDF scores) of ",o.createElement(s.A,{text:"\\( V \\)"})," words across ",o.createElement(s.A,{text:"\\( D \\)"})," documents, NMF attempts to find two matrices ",o.createElement(s.A,{text:"\\( W \\)"})," (size ",o.createElement(s.A,{text:"\\( V \\times K \\)"}),") and ",o.createElement(s.A,{text:"\\( H \\)"})," (size ",o.createElement(s.A,{text:"\\( K \\times D \\)"}),") such that:"),"\n",o.createElement(s.A,{text:"\\[\nX \\approx W \\times H\n\\]"}),"\n",o.createElement(t.p,null,"Here, ",o.createElement(s.A,{text:"\\( K \\)"})," is the reduced rank — often analogous to the number of topics. If ",o.createElement(s.A,{text:"\\( W \\)"})," is interpreted as containing basis vectors (topics) and ",o.createElement(s.A,{text:"\\( H \\)"})," as document loadings, each column of ",o.createElement(s.A,{text:"\\( H \\)"})," tells us how strongly a topic appears in a document. The non-negativity constraint makes the factorization more interpretable compared to, say, SVD-based methods like Latent Semantic Analysis (LSA)."),"\n",o.createElement(t.p,null,"NMF is frequently used as a faster approach compared to LDA, though it does not have a fully probabilistic interpretation. However, in practice, it can yield coherent topics that are relatively easy to interpret, especially because it keeps the weighting of words and topics strictly positive."),"\n",o.createElement(t.h3,{id:"hierarchical-dirichlet-processes-hdp",style:{position:"relative"}},o.createElement(t.a,{href:"#hierarchical-dirichlet-processes-hdp","aria-label":"hierarchical dirichlet processes hdp permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Hierarchical Dirichlet processes (HDP)"),"\n",o.createElement(t.p,null,o.createElement(t.strong,null,"Hierarchical Dirichlet Processes")," (Teh and gang, 2006) generalize the LDA concept to a scenario where we do not fix the number of topics ",o.createElement(s.A,{text:"\\( K \\)"})," in advance. Instead, the model can discover an appropriate number of topics automatically, guided by the data. This is done through a hierarchical Bayesian construction known as the Dirichlet process. Essentially, the model treats ",o.createElement(s.A,{text:"\\( K \\)"})," as infinite in principle, but in practice the posterior distribution typically places most probability mass on a finite set of topics."),"\n",o.createElement(t.p,null,"HDP is very appealing for large corpora where you might be uncertain about the number of distinct themes. It lets the data guide how many topics are discovered. The trade-off is that inference becomes more sophisticated, often involving specialized MCMC approaches or variational methods. Also, HDP can sometimes discover more topics than are practically interpretable, so it's not always a silver bullet."),"\n",o.createElement(t.h3,{id:"dynamic-topic-models",style:{position:"relative"}},o.createElement(t.a,{href:"#dynamic-topic-models","aria-label":"dynamic topic models permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Dynamic topic models"),"\n",o.createElement(t.p,null,o.createElement(t.strong,null,"Dynamic topic models"),' (Blei & Lafferty, ICML 2006) address the evolution of topics over time. Traditional LDA presumes that each document is exchangeable (i.e., no temporal ordering). But in many domains, documents come with timestamps, and the themes may shift over months or years. For example, the vocabulary around "technology" changes significantly over decades, as the popular jargon shifts from mainframe computing to cloud computing to machine learning.'),"\n",o.createElement(t.p,null,"Dynamic topic models incorporate temporal dependencies by indexing the latent distributions on discrete time slices. A typical approach is to model the topics at time slice ",o.createElement(s.A,{text:"\\( t \\)"})," as evolved from the topics at time slice ",o.createElement(s.A,{text:"\\( t-1 \\)"}),". This is often achieved via state space models or Brownian motion in the parameter space of the distributions. This allows the model to capture how word probabilities shift over time for each topic, providing a chronological storyline of how certain themes appear, evolve, and potentially vanish."),"\n",o.createElement(t.h2,{id:"chapter-4-steps-in-building-a-topic-model",style:{position:"relative"}},o.createElement(t.a,{href:"#chapter-4-steps-in-building-a-topic-model","aria-label":"chapter 4 steps in building a topic model permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Chapter 4. Steps in building a topic model"),"\n",o.createElement(t.h3,{id:"data-collection-and-corpus-preparation",style:{position:"relative"}},o.createElement(t.a,{href:"#data-collection-and-corpus-preparation","aria-label":"data collection and corpus preparation permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Data collection and corpus preparation"),"\n",o.createElement(t.p,null,"The first step in building any topic model is collecting the relevant text data. The data could come from web pages, academic journals, social media posts, news articles, or specialized corporate documents. Frequently, large-scale corpora are stored as sets of text files or in a database. The typical tasks in this phase include:"),"\n",o.createElement(t.ol,null,"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Gather documents"),": Make sure you have well-labeled or at least organized data sources."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Ensure coverage"),": If the corpus is supposed to capture a wide range of topics, you need a sufficiently broad variety of texts."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Remove duplicates"),": Check for repeated documents or near-duplicates that might bias the model."),"\n"),"\n",o.createElement(t.p,null,"Also, keep in mind the final application: if you are building a specialized model for, say, social media data, you want to ensure your corpus is representative of relevant user posts, not just random data that might not reflect your end goals."),"\n",o.createElement(t.h3,{id:"data-preprocessing-text-cleaning-tokenization-stopword-removal-and-filtering-stemming-vs-lemmatization",style:{position:"relative"}},o.createElement(t.a,{href:"#data-preprocessing-text-cleaning-tokenization-stopword-removal-and-filtering-stemming-vs-lemmatization","aria-label":"data preprocessing text cleaning tokenization stopword removal and filtering stemming vs lemmatization permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Data preprocessing: text cleaning, tokenization, stopword removal and filtering, stemming vs. lemmatization"),"\n",o.createElement(t.p,null,"Textual data is often noisy or inconsistent, especially if it is scraped from the web or user-generated. Proper preprocessing typically involves:"),"\n",o.createElement(t.ol,null,"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Text cleaning"),": Convert text to a consistent casing (often lowercase), remove non-alphanumeric characters (or decide how to handle them), handle punctuation, convert numbers or keep them if relevant."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Tokenization"),": Split the text into tokens, typically words or subwords. Tokenizers in languages like Python (NLTK, spaCy) provide robust ways to handle this."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Stopword removal"),': Many words (like "the", "is", "and", etc.) appear frequently but do not hold strong semantic content. Common practice is to remove them if your topic modeling approach focuses on content words.'),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Stemming or lemmatization"),": Reduce words to their root form. ",o.createElement(t.strong,null,"Stemming"),' is a rule-based approach that chops off word endings ("organization" → "organiz") whereas ',o.createElement(t.strong,null,"lemmatization"),' uses morphological analysis to map words to their actual lemma forms ("feet" → "foot", "organized" → "organize"). Lemmatization typically yields more coherent topics but is computationally heavier.'),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Filtering by token length or frequency"),": It may be beneficial to remove extremely rare words that appear in only a handful of documents, as well as overly frequent words that appear in almost every document."),"\n"),"\n",o.createElement(t.p,null,"These steps can dramatically affect the quality of your topic model. Preprocessing transformations might remove noise and help the model focus on semantically meaningful content."),"\n",o.createElement(t.h3,{id:"model-training",style:{position:"relative"}},o.createElement(t.a,{href:"#model-training","aria-label":"model training permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Model training"),"\n",o.createElement(t.p,null,"Once you have a clean corpus and you have decided on a specific approach (e.g., LDA, NMF, HDP, etc.), the next step is to train the model. For LDA, you might use:"),"\n",o.createElement(t.ul,null,"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Gensim")," in Python, which offers an efficient implementation of online LDA."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"MALLET")," (a Java-based library) for topic modeling, which uses an optimized Gibbs sampler."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Scikit-learn")," for simpler, smaller-scale LDA or NMF."),"\n"),"\n",o.createElement(t.p,null,"In a typical workflow, you do something like:"),"\n",o.createElement(l.A,{text:"\nfrom gensim import corpora, models\nfrom gensim.utils import simple_preprocess\n\n# Suppose you already have tokenized_texts: a list of lists of tokens\ndictionary = corpora.Dictionary(tokenized_texts)\ndictionary.filter_extremes(no_below=5, no_above=0.5)\nbow_corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\n\nlda_model = models.LdaModel(\n    bow_corpus, \n    num_topics=20, \n    id2word=dictionary, \n    passes=10, \n    alpha='auto', \n    random_state=42\n)\n\n# Inspect the topics\ntopics = lda_model.print_topics(num_words=5)\nfor topic_id, topic_words in topics:\n    print(f\"Topic {topic_id}: {topic_words}\")\n"}),"\n",o.createElement(t.p,null,"This snippet demonstrates a typical approach to training an LDA topic model with Gensim. The ",o.createElement(r.A,null,"LdaModel")," constructor requires a bag-of-words representation ",o.createElement(s.A,{text:"\\( bow\\_corpus \\)"}),", a chosen number of topics, the dictionary mapping, and other hyperparameters. The specific arguments can vary based on dataset size, interpretability requirements, and computational resources."),"\n",o.createElement(t.h3,{id:"selecting-hyperparameters",style:{position:"relative"}},o.createElement(t.a,{href:"#selecting-hyperparameters","aria-label":"selecting hyperparameters permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Selecting hyperparameters"),"\n",o.createElement(t.p,null,"In LDA, there are several hyperparameters that can significantly influence the outcome:"),"\n",o.createElement(t.ul,null,"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Number of topics (",o.createElement(s.A,{text:"\\( K \\)"}),")"),": Possibly the most important parameter. In practice, domain knowledge or interpretability concerns guide the choice of ",o.createElement(s.A,{text:"\\( K \\)"}),"."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Alpha (",o.createElement(s.A,{text:"\\( \\alpha \\)"}),")"),": The Dirichlet parameter that controls document–topic sparsity. A lower alpha typically yields more sparse distributions (each document is dominated by fewer topics)."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Beta (",o.createElement(s.A,{text:"\\( \\beta \\)"})," or sometimes referred to as ",o.createElement(s.A,{text:"\\( \\eta \\)"}),")"),": The Dirichlet parameter that controls topic–word sparsity. A lower beta means topics focus on fewer words."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Number of passes / iterations"),": Controls how many times the training algorithm iterates over the corpus. More passes can improve convergence but cost additional runtime."),"\n"),"\n",o.createElement(t.p,null,"In frameworks like HDP, there may be extra parameters controlling the base distribution of topics and the concentration parameters that govern how many new topics are introduced. NMF likewise has parameters for the rank ",o.createElement(s.A,{text:"\\( K \\)"})," and different update rules (e.g., multiplicative updates, coordinate descent)."),"\n",o.createElement(t.h3,{id:"ensuring-model-convergence",style:{position:"relative"}},o.createElement(t.a,{href:"#ensuring-model-convergence","aria-label":"ensuring model convergence permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Ensuring model convergence"),"\n",o.createElement(t.p,null,"Different inference algorithms have different stopping criteria and convergence diagnostics. For example:"),"\n",o.createElement(t.ul,null,"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Gibbs sampling")," might run for a specified number of iterations, after which the model's state is hopefully close to stationary. One can monitor log-likelihood or use heuristics to decide how many iterations are enough."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Variational inference")," can track the Evidence Lower BOund (ELBO), stopping when improvement becomes negligible."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"EM algorithm (for PLSA)")," might track the change in log-likelihood from one iteration to the next."),"\n"),"\n",o.createElement(t.p,null,"In practical scenarios, you also look at the stability of the resulting topics. If the top words in each topic keep shifting drastically between iterations, the model might not have converged yet."),"\n",o.createElement(t.h3,{id:"evaluating-training-progress",style:{position:"relative"}},o.createElement(t.a,{href:"#evaluating-training-progress","aria-label":"evaluating training progress permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Evaluating training progress"),"\n",o.createElement(t.p,null,"During training, it's common to observe metrics like the held-out perplexity or the likelihood of a validation set, or simply to look at the topic-word distributions qualitatively. Tools like ",o.createElement(t.strong,null,"pyLDAvis")," are often used to visually inspect how topics are laid out in a two-dimensional representation (via multidimensional scaling). If training is going well, you tend to see well-separated topics in the visualization, each with a fairly coherent cluster of words."),"\n",o.createElement(t.h3,{id:"model-evaluation",style:{position:"relative"}},o.createElement(t.a,{href:"#model-evaluation","aria-label":"model evaluation permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Model evaluation"),"\n",o.createElement(t.p,null,'Evaluating topic models is notoriously tricky because there is no ground-truth label for what a "good" topic is in many real-world data sets. Nonetheless, several strategies exist:'),"\n",o.createElement(t.h4,{id:"topic-coherence-metrics",style:{position:"relative"}},o.createElement(t.a,{href:"#topic-coherence-metrics","aria-label":"topic coherence metrics permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Topic coherence metrics"),"\n",o.createElement(t.p,null,"Topic coherence metrics (Mimno and gang, 2011) aim to quantify how semantically coherent the top words of a topic are. Common metrics include:"),"\n",o.createElement(t.ul,null,"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"C_v"),": A popular approach that uses word co-occurrence counts and a sliding window to measure coherence."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"C_uci, C_umass"),": Based on pointwise mutual information (PMI)."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"NPMI"),": Normalized pointwise mutual information-based metric."),"\n"),"\n",o.createElement(t.p,null,"Coherence scores are used heavily in practice to select the optimal number of topics ",o.createElement(s.A,{text:"\\( K \\)"}),", or to compare different variants of a model."),"\n",o.createElement(t.h4,{id:"perplexity-and-likelihood-based-metrics",style:{position:"relative"}},o.createElement(t.a,{href:"#perplexity-and-likelihood-based-metrics","aria-label":"perplexity and likelihood based metrics permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Perplexity and likelihood-based metrics"),"\n",o.createElement(t.p,null,"In the language modeling tradition, ",o.createElement(t.strong,null,"perplexity")," is sometimes used to measure how well a probabilistic model predicts a held-out set of documents. A lower perplexity indicates a better predictive model of text. However, perplexity can be somewhat misleading because an overly complex model can overfit, driving perplexity down but not necessarily providing more interpretable topics."),"\n",o.createElement(t.h4,{id:"human-evaluations-and-qualitative-checks",style:{position:"relative"}},o.createElement(t.a,{href:"#human-evaluations-and-qualitative-checks","aria-label":"human evaluations and qualitative checks permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Human evaluations and qualitative checks"),"\n",o.createElement(t.p,null,"In many practical scenarios, the final judge of a topic model is a domain expert. Does the topic's top words make sense as a coherent theme? Does the model separate distinct topics into separate clusters, or are multiple themes merged or split incorrectly?",o.createElement(t.br),"\n","Human-based evaluation is time-consuming, but it provides the ultimate test of interpretability, which is often the main goal of topic modeling."),"\n",o.createElement(t.h3,{id:"model-interpretation-and-visualization",style:{position:"relative"}},o.createElement(t.a,{href:"#model-interpretation-and-visualization","aria-label":"model interpretation and visualization permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Model interpretation and visualization"),"\n",o.createElement(t.p,null,"The real power of topic modeling comes in interpreting the discovered topics and applying them to the original corpus. Key tasks include:"),"\n",o.createElement(t.ul,null,"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Examining topic–word distributions"),": Inspect the top words and their probabilities for each topic. Sometimes you might label the topic based on the top words."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Topic labeling techniques"),": Automated techniques can be used to assign descriptive labels to each topic. For example, you can see which words appear frequently in that topic, or examine the most representative documents. Alternatively, some approaches (Mei and gang, 2007) propose using bigrams or salient phrases to label topics more precisely."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Visualizing topics"),": Tools like ",o.createElement(t.strong,null,"pyLDAvis")," or custom embeddings-based plots (e.g., projecting topics into a 2D space) help you see how topics relate to each other and to the documents."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Document–topic distributions"),": You can examine which topics are prevalent in each document, or how topics are distributed across different subsets of the corpus."),"\n"),"\n",o.createElement(t.h3,{id:"visualization-tools-and-libraries",style:{position:"relative"}},o.createElement(t.a,{href:"#visualization-tools-and-libraries","aria-label":"visualization tools and libraries permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Visualization tools and libraries"),"\n",o.createElement(t.ul,null,"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"pyLDAvis"),": A popular Python library for interactive topic model visualization."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Gensim"),"'s built-in visualization: Some basic plot functions exist, though typically one uses pyLDAvis."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Word cloud"),": Some practitioners generate word clouds from the top words for each topic."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Gephi or other graph-based tools"),": Sometimes used if you represent topics as networks of words."),"\n"),"\n",o.createElement(t.p,null,"Interpreting the output in these ways can provide valuable insights, especially for business intelligence or academic research where clarity is paramount."),"\n",o.createElement(t.h2,{id:"chapter-5-advanced-techniques-and-variations",style:{position:"relative"}},o.createElement(t.a,{href:"#chapter-5-advanced-techniques-and-variations","aria-label":"chapter 5 advanced techniques and variations permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Chapter 5. Advanced techniques and variations"),"\n",o.createElement(t.h3,{id:"neural-topic-modeling",style:{position:"relative"}},o.createElement(t.a,{href:"#neural-topic-modeling","aria-label":"neural topic modeling permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Neural topic modeling"),"\n",o.createElement(t.p,null,"Recently, ",o.createElement(t.strong,null,"neural topic modeling")," has gained momentum, aiming to leverage deep neural networks (often autoencoders or variational autoencoders, VAEs) to discover topics in text. Methods like ",o.createElement(t.strong,null,"ProdLDA")," (Srivastava & Sutton, 2017) re-parameterize the distribution over topics using neural networks, attempting to address some limitations of traditional LDA. The deep generative perspective can:"),"\n",o.createElement(t.ul,null,"\n",o.createElement(t.li,null,"Better capture non-linear text representations."),"\n",o.createElement(t.li,null,"Integrate with word embeddings or other neural features."),"\n",o.createElement(t.li,null,"Potentially scale to massive datasets using GPU acceleration."),"\n"),"\n",o.createElement(t.p,null,"Despite these benefits, neural topic models may introduce new complexities in training (e.g., optimizing VAE objectives, balancing reconstruction vs. KL divergence) and might require more hyperparameter tuning."),"\n",o.createElement(t.h3,{id:"combining-topic-modeling-with-word-embeddings",style:{position:"relative"}},o.createElement(t.a,{href:"#combining-topic-modeling-with-word-embeddings","aria-label":"combining topic modeling with word embeddings permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Combining topic modeling with word embeddings"),"\n",o.createElement(t.p,null,"Classic topic models treat words largely as discrete tokens, ignoring possible semantic similarities between them (for instance, synonyms). By combining topic modeling with ",o.createElement(t.strong,null,"word embeddings")," (e.g., Word2Vec, GloVe, or fastText), it becomes possible to produce topics that incorporate distributional semantics. Some approaches:"),"\n",o.createElement(t.ul,null,"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Top2Vec")," (Angelov, 2020): Leverages document embeddings and then clusters them, effectively discovering topic embeddings."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Embedding-based coherence"),": You can use word embedding similarities to measure how coherent a topic's words are, providing a more nuanced evaluation."),"\n"),"\n",o.createElement(t.p,null,"This synergy can produce topics that are more robust to synonyms or polysemy. On the downside, it adds another layer of complexity to the pipeline and can require a large, high-quality embedding model."),"\n",o.createElement(t.h3,{id:"deep-learning-approaches-for-topic-modeling",style:{position:"relative"}},o.createElement(t.a,{href:"#deep-learning-approaches-for-topic-modeling","aria-label":"deep learning approaches for topic modeling permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Deep learning approaches for topic modeling"),"\n",o.createElement(t.p,null,"Beyond straightforward neural topic modeling, there is a whole ecosystem of deep learning solutions that incorporate attention mechanisms, Transformers, or graph-based neural networks to glean more contextual or structural information from text:"),"\n",o.createElement(t.ul,null,"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Transformer-based encoders"),": They can be used to generate rich contextual embeddings for tokens or entire sentences, potentially leading to refined topic representations."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Graph neural networks"),": In specialized domains where the text might also be linked or references are crucial (e.g., scientific citation networks), GNN-based models can incorporate topological information."),"\n"),"\n",o.createElement(t.p,null,"Although these approaches can be powerful, they also come at a higher computational cost and often require carefully engineered architectures."),"\n",o.createElement(t.h3,{id:"cross-lingual-topic-modeling",style:{position:"relative"}},o.createElement(t.a,{href:"#cross-lingual-topic-modeling","aria-label":"cross lingual topic modeling permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Cross-lingual topic modeling"),"\n",o.createElement(t.p,null,o.createElement(t.strong,null,"Cross-lingual topic modeling")," addresses the scenario where you have documents in multiple languages and you want to discover overarching topics that span those languages. This can be approached by:"),"\n",o.createElement(t.ul,null,"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Sharing a common latent space")," for topics, with separate word distributions for each language."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Using bilingual word embeddings")," or multi-lingual representations so that synonyms across languages can align into a single topic dimension."),"\n"),"\n",o.createElement(t.p,null,"Such models are highly relevant in globally oriented text analytics, especially for multinational companies, cross-lingual search engines, or multilingual social media analysis."),"\n",o.createElement(t.h3,{id:"ensemble-methods-for-topic-discovery",style:{position:"relative"}},o.createElement(t.a,{href:"#ensemble-methods-for-topic-discovery","aria-label":"ensemble methods for topic discovery permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Ensemble methods for topic discovery"),"\n",o.createElement(t.p,null,"Ensemble approaches can also be employed for topic modeling:"),"\n",o.createElement(t.ul,null,"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Multiple random initializations"),": Running LDA multiple times with different seeds and combining or merging similar topics to yield more stable solutions."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Combining multiple methods"),": For example, using NMF to get an initial sense of topics, then refining those topics via an LDA-based approach. Or you can run LDA with different hyperparameters and ensemble the discovered topics if you find them complementary."),"\n"),"\n",o.createElement(t.p,null,"Ensemble methods may enhance robustness and produce well-rounded sets of topics that capture different aspects of the corpus."),"\n",o.createElement(t.h2,{id:"chapter-6-lets-code",style:{position:"relative"}},o.createElement(t.a,{href:"#chapter-6-lets-code","aria-label":"chapter 6 lets code permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Chapter 6. Let's code"),"\n",o.createElement(t.h3,{id:"building-complex-topic-model-from-scratch",style:{position:"relative"}},o.createElement(t.a,{href:"#building-complex-topic-model-from-scratch","aria-label":"building complex topic model from scratch permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Building complex topic model from scratch"),"\n",o.createElement(t.p,null,"In this final section, I will illustrate how you can build a more involved topic modeling pipeline in Python. Rather than just showing the typical Gensim snippet, we will aim to demonstrate the entire pipeline, complete with preprocessing, training, and evaluation of a topic model (such as LDA or NMF), along with a quick demonstration of how to visualize topics with pyLDAvis."),"\n",o.createElement(t.h4,{id:"step-1-data-ingestion-and-cleanup",style:{position:"relative"}},o.createElement(t.a,{href:"#step-1-data-ingestion-and-cleanup","aria-label":"step 1 data ingestion and cleanup permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Step 1: Data ingestion and cleanup"),"\n",o.createElement(t.p,null,"Let's suppose we have a dataset of documents stored in a file or database. We'll do a simplified illustration:"),"\n",o.createElement(l.A,{text:"\nimport pandas as pd\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom gensim.utils import simple_preprocess\n\nstop_words = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()\n\ndef preprocess_text(text):\n    # Lowercase\n    text = text.lower()\n    # Remove some non-alphanumeric characters (for demonstration)\n    text = re.sub(r'[^a-z0-9s]', '', text)\n    # Tokenize\n    tokens = simple_preprocess(text, deacc=True)  # deacc=True removes punctuations\n    # Remove stopwords and lemmatize\n    filtered_tokens = []\n    for tok in tokens:\n        if tok not in stop_words:\n            lemma = lemmatizer.lemmatize(tok)\n            filtered_tokens.append(lemma)\n    return filtered_tokens\n\n# Suppose we load a CSV with a column 'text'\ndf = pd.read_csv('documents.csv')\ndf['processed'] = df['text'].apply(preprocess_text)\n"}),"\n",o.createElement(t.p,null,"Here, we do some minimal cleaning, tokenization, stopword removal, and lemmatization. In a real scenario, you might do more sophisticated text normalization."),"\n",o.createElement(t.h4,{id:"step-2-building-a-dictionary-and-corpus",style:{position:"relative"}},o.createElement(t.a,{href:"#step-2-building-a-dictionary-and-corpus","aria-label":"step 2 building a dictionary and corpus permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Step 2: Building a dictionary and corpus"),"\n",o.createElement(t.p,null,"Next, we convert the processed tokens into a bag-of-words representation:"),"\n",o.createElement(l.A,{text:"\nfrom gensim import corpora\n\ndictionary = corpora.Dictionary(df['processed'])\ndictionary.filter_extremes(no_below=5, no_above=0.4)  # example thresholds\nbow_corpus = [dictionary.doc2bow(doc) for doc in df['processed']]\n"}),"\n",o.createElement(t.p,null,"We have also used ",o.createElement(r.A,null,"filter_extremes")," to remove very rare words (appear in fewer than 5 documents) and extremely common words (appear in more than 40% of documents). Adjust these thresholds depending on your data size and domain knowledge."),"\n",o.createElement(t.h4,{id:"step-3-training-an-lda-model",style:{position:"relative"}},o.createElement(t.a,{href:"#step-3-training-an-lda-model","aria-label":"step 3 training an lda model permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Step 3: Training an LDA model"),"\n",o.createElement(t.p,null,"Now, we can train an LDA model using Gensim's ",o.createElement(r.A,null,"LdaModel")," or ",o.createElement(r.A,null,"LdaMulticore")," (for parallelization):"),"\n",o.createElement(l.A,{text:"\nfrom gensim.models.ldamodel import LdaModel\n\nnum_topics = 10  # Choose your number of topics\nlda_model = LdaModel(\n    corpus=bow_corpus,\n    id2word=dictionary,\n    num_topics=num_topics,\n    random_state=42,\n    passes=10,\n    alpha='auto'\n)\n\n# Print out the topics\nfor idx, topic in lda_model.print_topics(num_words=5):\n    print(f\"Topic {idx}: {topic}\")\n"}),"\n",o.createElement(t.p,null,"Here, we have set the number of topics to 10, but you would typically try a range (like 5, 10, 15, 20, etc.) and pick the best number based on your chosen evaluation method (coherence, perplexity, or domain feedback)."),"\n",o.createElement(t.h4,{id:"step-4-evaluating-with-topic-coherence",style:{position:"relative"}},o.createElement(t.a,{href:"#step-4-evaluating-with-topic-coherence","aria-label":"step 4 evaluating with topic coherence permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Step 4: Evaluating with topic coherence"),"\n",o.createElement(t.p,null,"Gensim has built-in coherence measures:"),"\n",o.createElement(l.A,{text:"\nfrom gensim.models import CoherenceModel\n\ncoherence_model_lda = CoherenceModel(\n    model=lda_model, \n    texts=df['processed'], \n    dictionary=dictionary, \n    coherence='c_v'\n)\ncoherence_lda = coherence_model_lda.get_coherence()\nprint(f'Coherence Score (c_v): {coherence_lda}')\n"}),"\n",o.createElement(t.p,null,"You can also choose ",o.createElement(r.A,null,"'u_mass'")," or ",o.createElement(r.A,null,"'c_uci'")," if you have the necessary reference corpus or prefer those metrics. A higher coherence usually indicates more interpretable topics, although interpretation is still somewhat subjective."),"\n",o.createElement(t.h4,{id:"step-5-visualizing-with-pyldavis",style:{position:"relative"}},o.createElement(t.a,{href:"#step-5-visualizing-with-pyldavis","aria-label":"step 5 visualizing with pyldavis permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Step 5: Visualizing with pyLDAvis"),"\n",o.createElement(t.p,null,"An optional but highly recommended step is to visualize your topics:"),"\n",o.createElement(l.A,{text:"\nimport pyLDAvis\nimport pyLDAvis.gensim_models as gensimvis\n\npyLDAvis.enable_notebook()  # if you're in a notebook\nvis_data = gensimvis.prepare(lda_model, bow_corpus, dictionary)\npyLDAvis.save_html(vis_data, 'lda_visualization.html')\n"}),"\n",o.createElement(t.p,null,'This produces an interactive visualization showing the relationships between topics and the top terms in each topic. You can open the resulting "lda_visualization.html" to hover over different circles (representing topics) and see the word distributions.'),"\n",o.createElement(t.h4,{id:"step-6-trying-nmf-as-an-alternative",style:{position:"relative"}},o.createElement(t.a,{href:"#step-6-trying-nmf-as-an-alternative","aria-label":"step 6 trying nmf as an alternative permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Step 6: Trying NMF as an alternative"),"\n",o.createElement(t.p,null,"Sometimes, you might find that NMF is more straightforward or runs faster:"),"\n",o.createElement(l.A,{text:"\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import NMF\n\n# Convert documents to TF-IDF matrix\ntfidf_vectorizer = TfidfVectorizer(\n    min_df=5, \n    max_df=0.4, \n    stop_words='english'\n)\ntfidf_matrix = tfidf_vectorizer.fit_transform(\n    [' '.join(doc) for doc in df['processed']]\n)\n\n# Train NMF\nnum_topics = 10\nnmf_model = NMF(n_components=num_topics, random_state=42)\nW = nmf_model.fit_transform(tfidf_matrix)\nH = nmf_model.components_\n\n# Display top words for each topic\nterms = tfidf_vectorizer.get_feature_names_out()\nfor topic_idx, topic_vec in enumerate(H):\n    top_indices = topic_vec.argsort()[:-6:-1]  # top 5\n    top_terms = [terms[i] for i in top_indices]\n    print(f\"Topic {topic_idx}: {', '.join(top_terms)}\")\n"}),"\n",o.createElement(t.p,null,"While not probabilistic, NMF can be a very fast and intuitive approach, especially for exploring your data quickly."),"\n",o.createElement(t.hr),"\n",o.createElement(t.p,null,"Throughout these steps, you may incorporate advanced variations such as ",o.createElement(t.strong,null,"hierarchical")," approaches or ",o.createElement(t.strong,null,"dynamic")," topic modeling if your data is time-stamped. For more advanced neural methods, frameworks like PyTorch or TensorFlow are used to build the neural architecture for a topic model, often requiring additional custom code."),"\n",o.createElement(t.p,null,"Remember that real-world text analytics workflows often involve:"),"\n",o.createElement(t.ul,null,"\n",o.createElement(t.li,null,"Larger corpora that require distributed solutions (like Spark or HPC clusters)."),"\n",o.createElement(t.li,null,"More nuanced text cleaning, domain-specific tokenization or phrase detection."),"\n",o.createElement(t.li,null,"Ongoing iteration of topic number selection and hyperparameter tuning."),"\n",o.createElement(t.li,null,"Detailed interpretability checks with domain experts."),"\n"),"\n",o.createElement(t.p,null,"Finally, it's beneficial to combine computational metrics (like coherence or perplexity) with human feedback. Topic modeling is inherently interpretive: you want topics that not only fit the data statistically but also make sense to people who will use those insights for further decision-making."),"\n",o.createElement(n,{alt:"Topic model diagram",path:"",caption:"A conceptual diagram of a topic model illustrating a set of documents being mapped onto latent topics. Each topic is a distribution over terms, and each document is a distribution over topics.",zoom:"false"}),"\n",o.createElement(n,{alt:"Dynamic topic model illustration",path:"",caption:"Visualization of how topics change over time in a dynamic topic model. Each color-coded topic evolves to reflect new words or shifting probabilities.",zoom:"false"}),"\n",o.createElement(t.hr),"\n",o.createElement(t.p,null,"This concludes the detailed exploration of topic modeling. By engaging with fundamental ideas (latent variables, probability distributions, dimension reduction) and advanced concepts (hierarchical Dirichlet processes, neural topic modeling, cross-lingual topic discovery), topic modeling can be adapted to a wide range of practical text analytics scenarios. The final code examples illustrate a typical pipeline — starting with data ingestion, preprocessing, model training, evaluation, and visualization — which can be extended or combined with new innovations in the field."))}var m=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,i.RP)(),e.components);return t?o.createElement(t,e,o.createElement(c,e)):c(e)};var d=n(36710),h=n(58481),p=n.n(h),u=n(36310),g=n(87245),f=n(27042),v=n(59849),b=n(5591),y=n(61122),E=n(9219),w=n(33203),x=n(95751),S=n(94328),_=n(80791),k=n(78137);const A=e=>{let{toc:t}=e;if(!t||!t.items)return null;return o.createElement("nav",{className:_.R},o.createElement("ul",null,t.items.map(((e,t)=>o.createElement("li",{key:t},o.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const n=t.replace("#",""),a=document.getElementById(n);a&&a.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&o.createElement(A,{toc:{items:e.items}}))))))};function H(e){let{data:{mdx:t,allMdx:r,allPostImages:l},children:s}=e;const{frontmatter:c,body:m,tableOfContents:d}=t,h=c.index,v=c.slug.split("/")[1],_=r.nodes.filter((e=>e.frontmatter.slug.includes(`/${v}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),H=_.findIndex((e=>e.frontmatter.index===h)),z=_[H+1],T=_[H-1],M=c.slug.replace(/\/$/,""),C=/[^/]*$/.exec(M)[0],L=`posts/${v}/content/${C}/`,{0:V,1:D}=(0,o.useState)(c.flagWideLayoutByDefault),{0:I,1:N}=(0,o.useState)(!1);var P;(0,o.useEffect)((()=>{N(!0);const e=setTimeout((()=>N(!1)),340);return()=>clearTimeout(e)}),[V]),"adventures"===v?P=E.cb:"research"===v?P=E.Qh:"thoughts"===v&&(P=E.T6);const B=p()(m).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,F=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),n=e%60;return n<=30?`~${t}${n>0?".5":""} h`:`~${t+1} h`}(Math.ceil(B/P)+(c.extraReadTimeMin||0)),q=[{flag:c.flagDraft,component:()=>Promise.all([n.e(3231),n.e(8809)]).then(n.bind(n,28809))},{flag:c.flagMindfuckery,component:()=>Promise.all([n.e(3231),n.e(2471)]).then(n.bind(n,67709))},{flag:c.flagRewrite,component:()=>Promise.all([n.e(3231),n.e(6764)]).then(n.bind(n,62002))},{flag:c.flagOffensive,component:()=>Promise.all([n.e(3231),n.e(2443)]).then(n.bind(n,17681))},{flag:c.flagProfane,component:()=>Promise.all([n.e(3231),n.e(8048)]).then(n.bind(n,53286))},{flag:c.flagMultilingual,component:()=>Promise.all([n.e(3231),n.e(4069)]).then(n.bind(n,78831))},{flag:c.flagUnreliably,component:()=>Promise.all([n.e(3231),n.e(3417)]).then(n.bind(n,8179))},{flag:c.flagPolitical,component:()=>Promise.all([n.e(3231),n.e(5195)]).then(n.bind(n,30433))},{flag:c.flagCognitohazard,component:()=>Promise.all([n.e(3231),n.e(3175)]).then(n.bind(n,8413))},{flag:c.flagHidden,component:()=>Promise.all([n.e(3231),n.e(9556)]).then(n.bind(n,14794))}],{0:G,1:O}=(0,o.useState)([]);return(0,o.useEffect)((()=>{q.forEach((e=>{let{flag:t,component:n}=e;t&&n().then((e=>{O((t=>[].concat((0,a.A)(t),[e.default])))}))}))}),[]),o.createElement(f.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},o.createElement(b.A,{postNumber:c.index,date:c.date,updated:c.updated,readTime:F,difficulty:c.difficultyLevel,title:c.title,desc:c.desc,banner:c.banner,section:v,postKey:C,isMindfuckery:c.flagMindfuckery,mainTag:c.mainTag}),o.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},c.otherTags.map(((e,t)=>o.createElement("span",{key:t,className:`noselect ${k.MW}`,style:{margin:"0 5px 5px 0"}},e)))),o.createElement("div",{class:"postBody"},o.createElement(A,{toc:d})),o.createElement("br"),o.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},o.createElement(f.P.button,{class:"noselect",className:S.pb,id:S.xG,onClick:()=>{D(!V)},whileTap:{scale:.93}},o.createElement(f.P.div,{className:x.DJ,key:V,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},V?"Switch to default layout":"Switch to wide layout"))),o.createElement("br"),o.createElement("div",{class:"postBody",style:{margin:V?"0 -14%":"",maxWidth:V?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},o.createElement("div",{className:`${S.P_} ${I?S.Xn:S.qG}`},G.map(((e,t)=>o.createElement(e,{key:t}))),c.indexCourse?o.createElement(w.A,{index:c.indexCourse,category:c.courseCategoryName}):"",o.createElement(u.Z.Provider,{value:{images:l.nodes,basePath:L.replace(/\/$/,"")+"/"}},o.createElement(i.xA,{components:{Image:g.A}},s)))),o.createElement(y.A,{nextPost:z,lastPost:T,keyCurrent:C,section:v}))}function z(e){return o.createElement(H,e,o.createElement(m,e))}function T(e){var t,n,a,i,r;let{data:l}=e;const{frontmatter:s}=l.mdx,c=s.titleSEO||s.title,m=s.titleOG||c,h=s.titleTwitter||c,p=s.descSEO||s.desc,u=s.descOG||p,g=s.descTwitter||p,f=s.schemaType||"BlogPosting",b=s.keywordsSEO,y=s.date,E=s.updated||y,w=s.imageOG||(null===(t=s.banner)||void 0===t||null===(n=t.childImageSharp)||void 0===n||null===(a=n.gatsbyImageData)||void 0===a||null===(i=a.images)||void 0===i||null===(r=i.fallback)||void 0===r?void 0:r.src),x=s.imageAltOG||u,S=s.imageTwitter||w,_=s.imageAltTwitter||g,k=s.canonicalURL,A=s.flagHidden||!1,H=s.mainTag||"Posts",z=s.slug.split("/")[1]||"posts",{siteUrl:T}=(0,d.Q)(),M={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:T},{"@type":"ListItem",position:2,name:H,item:`${T}/${s.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${T}${s.slug}`}]};return o.createElement(v.A,{title:c+" - avrtt.blog",titleOG:m,titleTwitter:h,description:p,descriptionOG:u,descriptionTwitter:g,schemaType:f,keywords:b,datePublished:y,dateModified:E,imageOG:w,imageAltOG:x,imageTwitter:S,imageAltTwitter:_,canonicalUrl:k,flagHidden:A,mainTag:H,section:z,type:"article"},o.createElement("script",{type:"application/ld+json"},JSON.stringify(M)))}},96098:function(e,t,n){var a=n(96540),i=n(7978);t.A=e=>{let{text:t}=e;return a.createElement(i.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-js-content-file-path-src-pages-posts-research-topic-modeling-mdx-b146df4a1b5aaca04a95.js.map