"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[8572],{3962:function(e,t){t.A="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iaXNvLTg4NTktMSI/Pgo8IS0tIEdlbmVyYXRvcjogQWRvYmUgSWxsdXN0cmF0b3IgMTYuMC4wLCBTVkcgRXhwb3J0IFBsdWctSW4gLiBTVkcgVmVyc2lvbjogNi4wMCBCdWlsZCAwKSAgLS0+CjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+CjxzdmcgdmVyc2lvbj0iMS4xIiBpZD0iQ2FwYV8xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB4PSIwcHgiIHk9IjBweCIKCSB3aWR0aD0iNDE2Ljk3OXB4IiBoZWlnaHQ9IjQxNi45NzlweCIgdmlld0JveD0iMCAwIDQxNi45NzkgNDE2Ljk3OSIgc3R5bGU9ImVuYWJsZS1iYWNrZ3JvdW5kOm5ldyAwIDAgNDE2Ljk3OSA0MTYuOTc5OyIKCSB4bWw6c3BhY2U9InByZXNlcnZlIj4KPGc+Cgk8cGF0aCBkPSJNMzU2LjAwNCw2MS4xNTZjLTgxLjM3LTgxLjQ3LTIxMy4zNzctODEuNTUxLTI5NC44NDgtMC4xODJjLTgxLjQ3LDgxLjM3MS04MS41NTIsMjEzLjM3OS0wLjE4MSwyOTQuODUKCQljODEuMzY5LDgxLjQ3LDIxMy4zNzgsODEuNTUxLDI5NC44NDksMC4xODFDNDM3LjI5MywyNzQuNjM2LDQzNy4zNzUsMTQyLjYyNiwzNTYuMDA0LDYxLjE1NnogTTIzNy42LDM0MC43ODYKCQljMCwzLjIxNy0yLjYwNyw1LjgyMi01LjgyMiw1LjgyMmgtNDYuNTc2Yy0zLjIxNSwwLTUuODIyLTIuNjA1LTUuODIyLTUuODIyVjE2Ny44ODVjMC0zLjIxNywyLjYwNy01LjgyMiw1LjgyMi01LjgyMmg0Ni41NzYKCQljMy4yMTUsMCw1LjgyMiwyLjYwNCw1LjgyMiw1LjgyMlYzNDAuNzg2eiBNMjA4LjQ5LDEzNy45MDFjLTE4LjYxOCwwLTMzLjc2Ni0xNS4xNDYtMzMuNzY2LTMzLjc2NQoJCWMwLTE4LjYxNywxNS4xNDctMzMuNzY2LDMzLjc2Ni0zMy43NjZjMTguNjE5LDAsMzMuNzY2LDE1LjE0OCwzMy43NjYsMzMuNzY2QzI0Mi4yNTYsMTIyLjc1NSwyMjcuMTA3LDEzNy45MDEsMjA4LjQ5LDEzNy45MDF6Ii8+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPC9zdmc+Cg=="},66501:function(e,t,a){a.d(t,{A:function(){return r}});var n=a(96540),l=a(3962),i="styles-module--tooltiptext--a263b";var r=e=>{let{text:t,isBadge:a=!1}=e;const{0:r,1:s}=(0,n.useState)(!1),o=(0,n.useRef)(null);return(0,n.useEffect)((()=>{function e(e){o.current&&!o.current.contains(e.target)&&s(!1)}return document.addEventListener("click",e),()=>{document.removeEventListener("click",e)}}),[]),n.createElement("span",{className:"styles-module--tooltipWrapper--75ebf",ref:o},n.createElement("img",{id:a?"styles-module--infoBadge--e3d66":"styles-module--info--26c1f",src:l.A,alt:"info",onClick:e=>{e.stopPropagation(),s((e=>!e))}}),n.createElement("span",{className:r?`${i} styles-module--visible--c063c`:i},t))}},89835:function(e,t,a){a.r(t),a.d(t,{Head:function(){return C},PostTemplate:function(){return V},default:function(){return z}});var n=a(54506),l=a(28453),i=a(96540),r=a(66501),s=a(16886),o=(a(46295),a(96098));function c(e){const t=Object.assign({p:"p",h2:"h2",a:"a",span:"span",h3:"h3",ol:"ol",li:"li",strong:"strong",ul:"ul",hr:"hr"},(0,l.RP)(),e.components),{Image:a}=t;return a||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),i.createElement(i.Fragment,null,"\n",i.createElement("br"),"\n","\n","\n",i.createElement(t.p,null,"Support vector machines, often abbreviated as ",i.createElement(s.A,null,"SVMs"),", stand at the forefront of the family of classical machine learning algorithms used for classification, regression, and related tasks. Originally introduced in the 1990s by Vladimir Vapnik and his collaborators (Cortes and Vapnik, 1995), SVMs quickly became a central method for a wide range of applications such as text categorization, bioinformatics, and image recognition. Today, despite the rapid growth of deep learning architectures, SVMs remain both relevant and widely deployed, especially in scenarios where data is not excessively large or where interpretability and theoretical guarantees are particularly desirable."),"\n",i.createElement(t.p,null,'An SVM\'s core motivation lies in defining an optimal boundary — or hyperplane — between different classes of data in such a way that the margin (i.e., the gap) between the classes and the boundary is maximized. In essence, one can view the SVM algorithm as trying to find a decision boundary that is as far as possible from any training data point. This "maximum margin" principle helps SVMs achieve robust generalization on a variety of problems.'),"\n",i.createElement(t.p,null,'Unlike purely heuristic classifiers, SVMs rest on a foundation of convex optimization principles and geometric reasoning. Furthermore, their ability to handle nonlinear decision boundaries is powered by the concept of kernel functions, which allow SVMs to operate in (potentially) very high-dimensional feature spaces without explicitly computing every feature dimension. Thanks to this so-called "kernel trick," SVMs can learn extremely complex patterns while retaining mathematically elegant formulations and well-understood optimization properties.'),"\n",i.createElement(t.p,null,'Where do SVMs fit in the modern data scientist\'s toolbox? Because they can often achieve high accuracy on moderately sized datasets, are relatively robust to overfitting (once carefully regularized), and have a transparent mathematical foundation, they are frequently considered a go-to method for classical ML tasks such as text classification, image categorization, certain biomedical analyses, and more. Their interpretability can also be more direct compared to many deep learning approaches — one can often identify a subset of "support vectors" that are critical for defining the boundary. In short, SVMs fill a niche wherein strong mathematical guarantees, high accuracy, and moderate data scale meet.'),"\n",i.createElement(t.p,null,'In this article, we will dive into the underlying mathematics of support vector machines, explore both the "hard margin" and "soft margin" formulations, discuss the role of kernel methods, examine hyperparameter tuning considerations, reflect on SVM applications in high-dimensional data settings, highlight real-world use cases, and consider advanced extensions such as one-class SVMs and ensemble strategies. This piece aims to give readers a deep, well-rounded, and theoretically grounded perspective on what SVMs do, how they work, and when (and how) to use them.'),"\n",i.createElement(t.p,null,"To aid your understanding, we will include formulas (in a standardized LaTeX format), conceptual diagrams (placeholder image tags), and some code snippets in Python to illustrate the essential ideas."),"\n",i.createElement(t.h2,{id:"2-mathematical-foundations",style:{position:"relative"}},i.createElement(t.a,{href:"#2-mathematical-foundations","aria-label":"2 mathematical foundations permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2. Mathematical foundations"),"\n",i.createElement(t.p,null,"Support vector machines are, at their core, grounded in geometry, linear algebra, optimization, and statistical learning theory. Before we see how the final classification or regression algorithm emerges, it is important to walk through a few basic ideas: linear separability, hyperplanes, the nature of optimization in SVMs, the Lagrangian formulation, Karush-Kuhn-Tucker (KKT) conditions, and the geometric interpretation of margins."),"\n",i.createElement(t.h3,{id:"21-linear-separability-and-hyperplanes",style:{position:"relative"}},i.createElement(t.a,{href:"#21-linear-separability-and-hyperplanes","aria-label":"21 linear separability and hyperplanes permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.1. Linear separability and hyperplanes"),"\n",i.createElement(t.p,null,"A fundamental concept in SVM theory is that of a ",i.createElement(s.A,null,"hyperplane"),". In an ",i.createElement(o.A,{text:"\\(n\\)"}),"-dimensional feature space, a hyperplane is an ",i.createElement(o.A,{text:"\\((n-1)\\)"}),"-dimensional affine subspace that splits the space into two half-spaces. Concretely, we say a hyperplane ",i.createElement(o.A,{text:"\\(H\\)"})," can be represented in the form"),"\n",i.createElement(o.A,{text:"\\( \\langle \\mathbf{w}, \\mathbf{x} \\rangle - b = 0 \\)"}),"\n",i.createElement(t.p,null,"where ",i.createElement(o.A,{text:"\\(\\mathbf{w} \\in \\mathbb{R}^n\\)"})," is a weight (or normal) vector, and ",i.createElement(o.A,{text:"\\(b \\in \\mathbb{R}\\)"})," is a bias (or intercept) term. The vector ",i.createElement(o.A,{text:"\\(\\mathbf{w}\\)"})," is orthogonal (normal) to the hyperplane, while ",i.createElement(o.A,{text:"\\(\\frac{b}{\\|\\mathbf{w}\\|}\\)"})," can be viewed as a scalar shift relative to the origin."),"\n",i.createElement(t.p,null,"If we have a dataset of points ",i.createElement(o.A,{text:"\\(\\{\\mathbf{x}_i\\}\\)"})," in ",i.createElement(o.A,{text:"\\(\\mathbb{R}^n\\)"}),", each with a class label ",i.createElement(o.A,{text:"\\(y_i \\in \\{-1, +1\\}\\)"}),", we say the data is ",i.createElement(s.A,null,"linearly separable")," if there exists at least one hyperplane such that all points of class ",i.createElement(o.A,{text:"\\(+1\\)"})," lie in one half-space, and all points of class ",i.createElement(o.A,{text:"\\(-1\\)"})," lie in the other. For such a scenario, we need:"),"\n",i.createElement(o.A,{text:"\\( \ny_i (\\langle \\mathbf{w}, \\mathbf{x}_i \\rangle - b) > 0, \\quad \\forall i \n\\)"}),"\n",i.createElement(t.p,null,'Linear separability assumes no overlapping or ambiguous class assignments. While this assumption is not always realistic in real-world data, understanding it paves the way for the "hard margin" SVM. Later, we will see how the "soft margin" variant relaxes this assumption.'),"\n",i.createElement(t.h3,{id:"22-optimization-principles",style:{position:"relative"}},i.createElement(t.a,{href:"#22-optimization-principles","aria-label":"22 optimization principles permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.2. Optimization principles"),"\n",i.createElement(t.p,null,"The SVM approach is fundamentally an optimization problem. We do not simply want any hyperplane that separates the classes; we want the one that maximizes the ",i.createElement(s.A,null,"margin"),", the distance from the decision boundary to the nearest points of any class. In the linear separable case, the margin ",i.createElement(o.A,{text:"\\(\\gamma\\)"})," can be viewed as:"),"\n",i.createElement(o.A,{text:"\\(\n\\gamma = \\min_{i} \\left\\{ \\frac{| \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle - b |}{\\|\\mathbf{w}\\|} \\right\\}.\n\\)"}),"\n",i.createElement(t.p,null,"An alternative but common approach is to reformulate the problem so that the hyperplane with a specific normalization, typically ",i.createElement(o.A,{text:"\\(\\min_i y_i(\\langle \\mathbf{w}, \\mathbf{x}_i \\rangle - b) = 1\\)"}),", is found. Then, maximizing the margin corresponds to minimizing ",i.createElement(o.A,{text:"\\(\\|\\mathbf{w}\\|^2\\)"})," subject to the constraints ",i.createElement(o.A,{text:"\\(y_i(\\langle \\mathbf{w}, \\mathbf{x}_i \\rangle - b) \\geq 1\\)"}),". The margin in that formulation ends up being ",i.createElement(o.A,{text:"\\(\\frac{2}{\\|\\mathbf{w}\\|}\\)"}),", so maximizing it is equivalent to minimizing ",i.createElement(o.A,{text:"\\(\\frac{1}{2}\\|\\mathbf{w}\\|^2\\)"}),"."),"\n",i.createElement(t.p,null,"Hence, from an optimization standpoint, we can specify the ",i.createElement(s.A,null,"primal problem")," for the hard margin scenario:"),"\n",i.createElement(o.A,{text:"\\[\n\\begin{cases}\n\\frac{1}{2}\\| \\mathbf{w} \\|^2 \\rightarrow \\min_{\\mathbf{w}, b} \\\\\n\\text{subject to: } y_i(\\langle \\mathbf{w}, \\mathbf{x}_i \\rangle - b) \\geq 1, \\; i=1,\\ldots,\\ell.\n\\end{cases}\n\\]"}),"\n",i.createElement(t.h3,{id:"23-lagrangian-formulation",style:{position:"relative"}},i.createElement(t.a,{href:"#23-lagrangian-formulation","aria-label":"23 lagrangian formulation permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.3. Lagrangian formulation"),"\n",i.createElement(t.p,null,"Because the above is a (convex) quadratic optimization problem with linear constraints, we can apply standard Lagrangian methods. We define Lagrange multipliers ",i.createElement(o.A,{text:"\\(\\lambda_i \\geq 0\\)"})," (one for each constraint), and write the ",i.createElement(s.A,null,"Lagrangian"),":"),"\n",i.createElement(o.A,{text:"\\[\n\\mathcal{L}(\\mathbf{w}, b, \\boldsymbol{\\lambda}) = \\frac{1}{2}\\|\\mathbf{w}\\|^2 - \\sum_{i=1}^{\\ell} \\lambda_i \n\\bigl( y_i(\\langle \\mathbf{w}, \\mathbf{x}_i \\rangle - b) - 1 \\bigr).\n\\]"}),"\n",i.createElement(t.p,null,"We want to find the saddle point of ",i.createElement(o.A,{text:"\\(\\mathcal{L}(\\mathbf{w}, b, \\boldsymbol{\\lambda})\\)"}),". After taking partial derivatives with respect to ",i.createElement(o.A,{text:"\\(\\mathbf{w}\\)"})," and ",i.createElement(o.A,{text:"\\(b\\)"})," and enforcing stationarity, one can re-express the problem in a ",i.createElement(s.A,null,"dual form")," that depends solely on the Lagrange multipliers ",i.createElement(o.A,{text:"\\(\\lambda_i\\)"}),". The dual problem reads:"),"\n",i.createElement(o.A,{text:"\\[\n\\begin{cases}\nW(\\boldsymbol{\\lambda}) = \\sum_{i=1}^{\\ell}\\lambda_i \n- \\frac{1}{2}\\sum_{i=1}^\\ell \\sum_{j=1}^\\ell \\lambda_i \\lambda_j y_i y_j \\langle \\mathbf{x}_i,\\mathbf{x}_j\\rangle \n\\rightarrow \\max_{\\boldsymbol{\\lambda}}, \\\\\n\\text{subject to } \\lambda_i \\geq 0,\\; \\sum_{i=1}^{\\ell} \\lambda_i y_i = 0.\n\\end{cases}\n\\]"}),"\n",i.createElement(t.p,null,"This ",i.createElement(s.A,null,"dual optimization")," is also a quadratic program, but interestingly it involves only dot products ",i.createElement(o.A,{text:"\\(\\langle \\mathbf{x}_i, \\mathbf{x}_j\\rangle\\)"})," of data points. The solution yields a vector of optimal multipliers ",i.createElement(o.A,{text:"\\(\\boldsymbol{\\lambda}^*\\)"}),". Only a portion of these multipliers (those corresponding to the so-called ",i.createElement(s.A,null,"support vectors"),") turn out to be nonzero. The final classifier can be expressed as:"),"\n",i.createElement(o.A,{text:"\\( \na(\\mathbf{x}) = \\text{sign}\\!\\Bigl(\\sum_{i=1}^\\ell \\lambda_i^* \\, y_i \\,\\langle \\mathbf{x}_i, \\mathbf{x}\\rangle - b\\Bigr).\n\\)"}),"\n",i.createElement(t.h3,{id:"24-kkt-conditions",style:{position:"relative"}},i.createElement(t.a,{href:"#24-kkt-conditions","aria-label":"24 kkt conditions permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.4. KKT conditions"),"\n",i.createElement(t.p,null,"A crucial theoretical component behind SVM optimization are the ",i.createElement(s.A,null,"KKT (Karush-Kuhn-Tucker) conditions"),". They provide necessary and sufficient conditions for optimality in constrained optimization problems. For our primal-dual setup, the KKT conditions revolve around:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Primal feasibility"),": ",i.createElement(o.A,{text:"\\(y_i (\\langle \\mathbf{w}, \\mathbf{x}_i\\rangle - b) \\geq 1\\)"}),"."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Dual feasibility"),": ",i.createElement(o.A,{text:"\\(\\lambda_i \\geq 0\\)"}),"."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Complementary slackness"),": ",i.createElement(o.A,{text:"\\(\\lambda_i \\bigl(y_i(\\langle \\mathbf{w}, \\mathbf{x}_i\\rangle - b) - 1\\bigr) = 0\\)"}),"."),"\n"),"\n",i.createElement(t.p,null,"The final condition implies that if ",i.createElement(o.A,{text:"\\(\\lambda_i > 0\\)"}),", then ",i.createElement(o.A,{text:"\\(y_i(\\langle \\mathbf{w}, \\mathbf{x}_i\\rangle - b) = 1\\)"}),"; meaning the point ",i.createElement(o.A,{text:"\\(\\mathbf{x}_i\\)"}),' lies exactly on the boundary of the margin region. These points are the "support vectors." Points for which ',i.createElement(o.A,{text:"\\(\\lambda_i = 0\\)"})," do not affect the boundary directly because they lie strictly outside the margin."),"\n",i.createElement(t.h3,{id:"25-geometric-interpretation-of-margins",style:{position:"relative"}},i.createElement(t.a,{href:"#25-geometric-interpretation-of-margins","aria-label":"25 geometric interpretation of margins permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.5. Geometric interpretation of margins"),"\n",i.createElement(t.p,null,'From a purely geometric perspective, SVMs attempt to find the hyperplane that creates the largest "no man\'s land" between classes. Consider two parallel hyperplanes:'),"\n",i.createElement(o.A,{text:"\\(\n\\langle \\mathbf{w}, \\mathbf{x}\\rangle - b = 1 \n\\quad \\text{and} \\quad\n\\langle \\mathbf{w}, \\mathbf{x}\\rangle - b = -1.\n\\)"}),"\n",i.createElement(t.p,null,"No training points from either class can lie between these hyperplanes in the hard margin scenario. The margin, or width of that forbidden region, is ",i.createElement(o.A,{text:"\\( \\frac{2}{\\|\\mathbf{w}\\|}\\)"}),". Hence, maximizing that margin is the same as minimizing ",i.createElement(o.A,{text:"\\(\\|\\mathbf{w}\\|\\)"}),", which is precisely what the SVM solution accomplishes."),"\n",i.createElement(t.p,null,'Put in simpler terms, the SVM tries to "push apart" the two classes as far as possible, subject to all data constraints. Points that end up right on the boundary hyperplanes are the critical "support vectors." Changes to any points that are not support vectors do not affect the final solution, which is one reason SVM can be quite robust and computationally efficient once the support vectors are identified.'),"\n",i.createElement(t.h2,{id:"3-hard-margin-svm",style:{position:"relative"}},i.createElement(t.a,{href:"#3-hard-margin-svm","aria-label":"3 hard margin svm permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3. Hard margin SVM"),"\n",i.createElement(t.p,null,"When a dataset is perfectly linearly separable (no overlap between classes), the classical approach is the ",i.createElement(s.A,null,"hard margin SVM"),'. We call it "hard margin" because the algorithm insists that no training data points lie on the wrong side of the boundary. The margin must remain entirely free of data points from either class.'),"\n",i.createElement(t.h3,{id:"31-concept-of-the-hard-margin",style:{position:"relative"}},i.createElement(t.a,{href:"#31-concept-of-the-hard-margin","aria-label":"31 concept of the hard margin permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.1. Concept of the hard margin"),"\n",i.createElement(t.p,null,"In the hard margin setup, the constraints:"),"\n",i.createElement(o.A,{text:"\\( y_i (\\langle \\mathbf{w}, \\mathbf{x}_i \\rangle - b) \\geq 1 \\)"}),"\n",i.createElement(t.p,null,"must hold for all data points ",i.createElement(o.A,{text:"\\(\\mathbf{x}_i\\)"}),'. Equivalently, no data point can "violate" the margin. When data is genuinely separable, an infinite number of separating hyperplanes may exist. SVM narrows these down to the single hyperplane that yields the widest margin.'),"\n",i.createElement(t.h3,{id:"32-maximizing-the-margin",style:{position:"relative"}},i.createElement(t.a,{href:"#32-maximizing-the-margin","aria-label":"32 maximizing the margin permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.2. Maximizing the margin"),"\n",i.createElement(t.p,null,"As mentioned, maximizing the margin"),"\n",i.createElement(o.A,{text:"\\( \\frac{2}{\\|\\mathbf{w}\\|} \\)"}),"\n",i.createElement(t.p,null,"is rephrased as minimizing ",i.createElement(o.A,{text:"\\(\\frac{1}{2}\\|\\mathbf{w}\\|^2\\)"}),". Hence, the primal form:"),"\n",i.createElement(o.A,{text:"\\[\n\\begin{cases}\n\\frac{1}{2}\\|\\mathbf{w}\\|^2 \\rightarrow \\min_{\\mathbf{w}, b},\\\\\ny_i(\\langle \\mathbf{w}, \\mathbf{x}_i\\rangle - b) \\geq 1,\\quad \\forall i.\n\\end{cases}\n\\]"}),"\n",i.createElement(t.p,null,"Geometrically, the algorithm tries to push the decision boundary to be as far from each class as possible."),"\n",i.createElement(t.h3,{id:"33-constraints-and-feasibility",style:{position:"relative"}},i.createElement(t.a,{href:"#33-constraints-and-feasibility","aria-label":"33 constraints and feasibility permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.3. Constraints and feasibility"),"\n",i.createElement(t.p,null,"If the dataset is ",i.createElement(r.A,{text:"Meaning that there are no misclassifications or overlapping distributions of classes."},"truly linearly separable"),', then the feasible region for the primal problem is not empty. In other words, there indeed exists some hyperplane that classifies everything perfectly. However, in practice, many (if not most) real-world datasets are not strictly linearly separable — noise, outliers, or inherent class overlap typically cause minor or major violations. That is why the "hard margin" approach, while conceptually enlightening, is too restrictive.'),"\n",i.createElement(t.h3,{id:"34-example-in-a-2d-feature-space",style:{position:"relative"}},i.createElement(t.a,{href:"#34-example-in-a-2d-feature-space","aria-label":"34 example in a 2d feature space permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.4. Example in a 2D feature space"),"\n",i.createElement(t.p,null,"Imagine a simple two-dimensional dataset:"),"\n",i.createElement(a,{alt:"2D dataset with classes",path:"",caption:"A toy 2D dataset with two linearly separable classes.",zoom:"false"}),"\n",i.createElement(t.p,null,"A naive linear classifier might yield many possible separating lines. However, the SVM solution emerges as the unique line maximizing the distance to the nearest points. In the figure, the margin lines are drawn parallel to the central decision boundary, and the points touching those lines are the support vectors."),"\n",i.createElement(t.p,null,"To see this concretely in code, consider the Python snippet below (using scikit-learn). We assume we have some data arrays ",i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">X</code>'}})," (shape ",i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">[n_samples, 2]</code>'}}),") and ",i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">y</code>'}})," (with values -1 or +1):"),"\n",i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">&lt;Code text={`\nimport numpy as np\nfrom sklearn.svm import SVC\nimport matplotlib.pyplot as plt\n\n# Suppose X, y represent a linearly separable 2D dataset\nmodel = SVC(kernel=\'linear\', C=1e10)\nmodel.fit(X, y)\n\n# Let\'s plot the data, decision boundary, and margin\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=\'bwr\')\n\n# Helper function to plot the boundary\ndef plot_svc_decision_function(clf):\n    w = clf.coef_[0]\n    b = clf.intercept_[0]\n    x_coords = np.linspace(X[:,0].min(), X[:,0].max(), 100)\n    # Decision boundary: w0 * x + w1 * y - b = 0 => y = (b - w0*x)/w1\n    y_coords = (b - w[0]*x_coords) / w[1]\n    plt.plot(x_coords, y_coords, "k-")\n\nplot_svc_decision_function(model)\nplt.title("Hard Margin SVM in 2D")\nplt.show()\n`}/></code></pre></div>'}}),"\n",i.createElement(t.p,null,'This simplistic example emphasizes the geometry behind SVM: once a hyperplane is found, the margin is "fixed."'),"\n",i.createElement(t.h2,{id:"4-soft-margin-svm",style:{position:"relative"}},i.createElement(t.a,{href:"#4-soft-margin-svm","aria-label":"4 soft margin svm permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4. Soft margin SVM"),"\n",i.createElement(t.p,null,"Real datasets are rarely perfectly separable. The presence of outliers, inherent overlaps, or minor label noise can make strict separation impossible or unwise. For instance, a single mislabeled training point could force the margin to shrink drastically if we insist upon zero classification error. To address this, the ",i.createElement(s.A,null,"soft margin")," approach was introduced."),"\n",i.createElement(t.h3,{id:"41-handling-non-separable-data",style:{position:"relative"}},i.createElement(t.a,{href:"#41-handling-non-separable-data","aria-label":"41 handling non separable data permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.1. Handling non-separable data"),"\n",i.createElement(t.p,null,i.createElement(s.A,null,"Soft margin SVM")," essentially allows some points to violate the margin constraints if doing so yields a better overall solution. In other words, the algorithm tolerates a limited number of misclassified or near-boundary points but penalizes them in the objective function. This new perspective provides a more flexible margin that can adapt to the presence of outliers."),"\n",i.createElement(t.h3,{id:"42-slack-variables",style:{position:"relative"}},i.createElement(t.a,{href:"#42-slack-variables","aria-label":"42 slack variables permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.2. Slack variables"),"\n",i.createElement(t.p,null,"To implement the concept of soft margins, we introduce ",i.createElement(s.A,null,"slack variables")," ",i.createElement(o.A,{text:"\\(\\{\\xi_i\\}\\)"}),", one per training point, that measure how much each point ",i.createElement(o.A,{text:"\\(\\mathbf{x}_i\\)"})," violates the ideal margin condition. Concretely, we replace the original constraints with:"),"\n",i.createElement(o.A,{text:"\\(\ny_i(\\langle \\mathbf{w}, \\mathbf{x}_i \\rangle - b) \\geq 1 - \\xi_i, \\quad \n\\xi_i \\geq 0, \\quad i = 1,\\ldots,\\ell.\n\\)"}),"\n",i.createElement(t.p,null,"If ",i.createElement(o.A,{text:"\\(\\xi_i = 0\\)"}),", the point is correctly placed outside (or on) the margin boundary. But if ",i.createElement(o.A,{text:"\\(\\xi_i > 0\\)"}),", it means the point ",i.createElement(o.A,{text:"\\(\\mathbf{x}_i\\)"})," is within the margin or even misclassified. Larger ",i.createElement(o.A,{text:"\\(\\xi_i\\)"})," implies a more severe violation."),"\n",i.createElement(t.h3,{id:"43-role-of-the-c-parameter",style:{position:"relative"}},i.createElement(t.a,{href:"#43-role-of-the-c-parameter","aria-label":"43 role of the c parameter permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.3. Role of the C parameter"),"\n",i.createElement(t.p,null,"We must decide how much penalty to assign to slack variables: we do not want to allow too many misclassifications without cost. Thus, the optimization objective becomes:"),"\n",i.createElement(o.A,{text:"\\[\n\\frac{1}{2}\\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^\\ell \\xi_i \\rightarrow \\min_{\\mathbf{w}, b, \\boldsymbol{\\xi}},\n\\]"}),"\n",i.createElement(t.p,null,"subject to ",i.createElement(o.A,{text:"\\(y_i(\\langle \\mathbf{w}, \\mathbf{x}_i\\rangle - b)\\ge 1-\\xi_i\\)"})," and ",i.createElement(o.A,{text:"\\(\\xi_i \\ge 0\\)"}),". The constant ",i.createElement(s.A,null,i.createElement(o.A,{text:"\\(C\\)"}))," is a critical hyperparameter. It balances two conflicting goals:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Minimize ",i.createElement(o.A,{text:"\\(\\|\\mathbf{w}\\|\\)"}))," so that the margin is large."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Minimize ",i.createElement(o.A,{text:"\\(\\sum \\xi_i\\)"}))," so that margin violations are limited."),"\n"),"\n",i.createElement(t.p,null,"When ",i.createElement(o.A,{text:"\\(C\\)"})," is very large, the term ",i.createElement(o.A,{text:"\\(C \\sum \\xi_i\\)"})," dominates, meaning the model tries hard to reduce slack. This typically forces the margin to become narrower, because we want fewer misclassifications. Conversely, a very small ",i.createElement(o.A,{text:"\\(C\\)"})," means we care less about misclassification: the margin can expand at the cost of a few more errors. Balancing ",i.createElement(o.A,{text:"\\(C\\)"})," to reflect the appropriate trade-off is vital in practice."),"\n",i.createElement(t.h3,{id:"44-trade-off-between-margin-and-misclassification",style:{position:"relative"}},i.createElement(t.a,{href:"#44-trade-off-between-margin-and-misclassification","aria-label":"44 trade off between margin and misclassification permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.4. Trade-off between margin and misclassification"),"\n",i.createElement(t.p,null,"Setting ",i.createElement(o.A,{text:"\\(C\\)"}),' too high can lead to overfitting; the SVM can become too "tightly" shaped around outliers in an effort to avoid margin violations at all costs. Setting ',i.createElement(o.A,{text:"\\(C\\)"})," too low may lead to underfitting, because the margin might expand so much that it incorrectly lumps some data in the wrong region."),"\n",i.createElement(t.p,null,"In real-world tasks, one typically tunes ",i.createElement(o.A,{text:"\\(C\\)"})," using cross-validation. By systematically testing multiple values of ",i.createElement(o.A,{text:"\\(C\\)"})," and measuring out-of-sample performance, we identify the sweet spot that balances margin width against classification accuracy on unseen data."),"\n",i.createElement(t.h2,{id:"5-kernel-methods",style:{position:"relative"}},i.createElement(t.a,{href:"#5-kernel-methods","aria-label":"5 kernel methods permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5. Kernel methods"),"\n",i.createElement(t.p,null,"One of the greatest breakthroughs that catapulted SVMs to fame is the ",i.createElement(s.A,null,"kernel trick"),". The main concept is: we can transform a (potentially) low-dimensional input space into a higher-dimensional feature space where linear separation might become easier or more expressive. Instead of explicitly mapping data, we use specialized kernel functions that compute inner products in the higher-dimensional space without constructing that space explicitly."),"\n",i.createElement(t.h3,{id:"51-introduction-to-kernel-functions",style:{position:"relative"}},i.createElement(t.a,{href:"#51-introduction-to-kernel-functions","aria-label":"51 introduction to kernel functions permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.1. Introduction to kernel functions"),"\n",i.createElement(t.p,null,"A ",i.createElement(s.A,null,"kernel function"),", ",i.createElement(o.A,{text:"\\(K(\\mathbf{x}, \\mathbf{x}')\\)"}),", is a function that takes two data points ",i.createElement(o.A,{text:"\\(\\mathbf{x}\\)"})," and ",i.createElement(o.A,{text:"\\(\\mathbf{x}'\\)"})," from the input space and outputs the inner product of their images in some feature space ",i.createElement(o.A,{text:"\\(\\mathcal{H}\\)"}),":"),"\n",i.createElement(o.A,{text:"\\( \nK(\\mathbf{x}, \\mathbf{x}') = \\langle \\phi(\\mathbf{x}), \\phi(\\mathbf{x}') \\rangle_{\\mathcal{H}}.\n\\)"}),"\n",i.createElement(t.p,null,"Because an SVM's dual formulation depends only on dot products of training points, if we replace ",i.createElement(o.A,{text:"\\(\\langle \\mathbf{x}_i, \\mathbf{x}_j\\rangle\\)"})," with ",i.createElement(o.A,{text:"\\(K(\\mathbf{x}_i, \\mathbf{x}_j)\\)"}),', we effectively allow the SVM to "operate" in a possibly very high-dimensional space ',i.createElement(o.A,{text:"\\(\\mathcal{H}\\)"}),' without enumerating it explicitly. The classical name for this procedure is the "kernel trick."'),"\n",i.createElement(t.h3,{id:"52-the-kernel-trick",style:{position:"relative"}},i.createElement(t.a,{href:"#52-the-kernel-trick","aria-label":"52 the kernel trick permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.2. The kernel trick"),"\n",i.createElement(t.p,null,"The kernel trick is best understood by example. Suppose we suspect that the data is not linearly separable in ",i.createElement(o.A,{text:"\\(\\mathbb{R}^2\\)"}),", but mapping it into some ",i.createElement(o.A,{text:"\\(\\mathbb{R}^3\\)"})," with features like ",i.createElement(o.A,{text:"\\((x_1, x_2, x_1^2 + x_2^2)\\)"})," might help. Instead of computing the new coordinates explicitly, we define:"),"\n",i.createElement(o.A,{text:"\\( K(\\mathbf{x}, \\mathbf{x}') = (\\mathbf{x} \\cdot \\mathbf{x}')^2 \\)"}),"\n",i.createElement(t.p,null,"or some function that matches ",i.createElement(o.A,{text:"\\(\\langle \\phi(\\mathbf{x}), \\phi(\\mathbf{x}')\\rangle\\)"}),". A more common example is the ",i.createElement(s.A,null,"Radial Basis Function (RBF) kernel"),":"),"\n",i.createElement(o.A,{text:"\\( \nK(\\mathbf{x}, \\mathbf{x}') = \\exp\\left(-\\gamma \\|\\mathbf{x}-\\mathbf{x}'\\|^2\\right).\n\\)"}),"\n",i.createElement(t.p,null,"Using the RBF kernel, the SVM can effectively place flexible nonlinear boundaries in the original space by implementing linear separation in a much higher (potentially infinite) dimensional feature space."),"\n",i.createElement(t.h3,{id:"53-popular-kernels",style:{position:"relative"}},i.createElement(t.a,{href:"#53-popular-kernels","aria-label":"53 popular kernels permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.3. Popular kernels"),"\n",i.createElement(t.p,null,"Among the many kernel types, some of the most popular are:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Linear kernel"),": ",i.createElement(o.A,{text:"\\(K(\\mathbf{x}, \\mathbf{x}') = \\langle \\mathbf{x}, \\mathbf{x}' \\rangle\\)"}),"."),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"Simpler approach, typically used for linearly separable data or high-dimensional text classification tasks."),"\n"),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Polynomial kernel"),": ",i.createElement(o.A,{text:"\\(K(\\mathbf{x}, \\mathbf{x}') = \\bigl(\\langle \\mathbf{x}, \\mathbf{x}' \\rangle + c\\bigr)^d\\)"}),"."),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"Allows polynomial decision boundaries with adjustable degree ",i.createElement(o.A,{text:"\\(d\\)"}),"."),"\n"),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"RBF (Gaussian) kernel"),": ",i.createElement(o.A,{text:"\\(K(\\mathbf{x}, \\mathbf{x}') = \\exp\\bigl(-\\gamma \\|\\mathbf{x} - \\mathbf{x}'\\|^2\\bigr)\\)"}),"."),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"Most widely used in practice. ",i.createElement(o.A,{text:"\\(\\gamma\\)"}),' controls the "spread" of the kernel.'),"\n"),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Sigmoid kernel"),": ",i.createElement(o.A,{text:"\\(K(\\mathbf{x}, \\mathbf{x}') = \\tanh\\bigl(\\alpha \\langle \\mathbf{x}, \\mathbf{x}'\\rangle + c\\bigr)\\)"}),"."),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"Inspired by neural networks. Less common in standard SVM usage unless certain conditions are met."),"\n"),"\n"),"\n"),"\n",i.createElement(t.h3,{id:"54-choosing-the-right-kernel",style:{position:"relative"}},i.createElement(t.a,{href:"#54-choosing-the-right-kernel","aria-label":"54 choosing the right kernel permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.4. Choosing the right kernel"),"\n",i.createElement(t.p,null,"Selecting a kernel typically involves heuristics or domain knowledge:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"For text classification with extremely high-dimensional but sparse features, a linear kernel often suffices."),"\n",i.createElement(t.li,null,"For moderate dimensional data with potential nonlinear relationships, an RBF kernel is a popular choice."),"\n",i.createElement(t.li,null,"Polynomial kernels may be beneficial if one expects polynomial-like decision boundaries."),"\n",i.createElement(t.li,null,"Sigmoid kernels are seldom used except in specialized circumstances."),"\n"),"\n",i.createElement(t.p,null,"In general, the RBF kernel is a strong default for nonlinear cases, but ",i.createElement(s.A,null,"model selection")," (e.g., cross-validation) is used to fine-tune kernel-specific hyperparameters, such as ",i.createElement(o.A,{text:"\\(\\gamma\\)"})," in the RBF kernel or the degree ",i.createElement(o.A,{text:"\\(d\\)"})," in the polynomial kernel."),"\n",i.createElement(t.h2,{id:"6-nonlinear-svm",style:{position:"relative"}},i.createElement(t.a,{href:"#6-nonlinear-svm","aria-label":"6 nonlinear svm permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6. Nonlinear SVM"),"\n",i.createElement(t.p,null,"The introduction of kernels is essentially the key that transforms a linear SVM into a ",i.createElement(s.A,null,"nonlinear SVM"),". Instead of building linear separators in the raw input space, the SVM effectively builds linear separators in a (potentially) very complicated feature space. Nonlinear SVM thus can achieve extremely flexible decision boundaries."),"\n",i.createElement(t.h3,{id:"61-mapping-to-higher-dimensional-feature-space",style:{position:"relative"}},i.createElement(t.a,{href:"#61-mapping-to-higher-dimensional-feature-space","aria-label":"61 mapping to higher dimensional feature space permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.1. Mapping to higher-dimensional feature space"),"\n",i.createElement(t.p,null,"Formally, we imagine a mapping ",i.createElement(o.A,{text:"\\(\\phi: \\mathbb{R}^n \\to \\mathcal{H}\\)"}),". The dimension of ",i.createElement(o.A,{text:"\\(\\mathcal{H}\\)"})," might be extremely large (possibly infinite). But the SVM never needs to compute ",i.createElement(o.A,{text:"\\(\\phi(\\mathbf{x})\\)"})," explicitly. It uses the kernel function ",i.createElement(o.A,{text:"\\(K(\\mathbf{x}, \\mathbf{x}') = \\langle \\phi(\\mathbf{x}), \\phi(\\mathbf{x}')\\rangle\\)"}),"."),"\n",i.createElement(t.h3,{id:"62-data-transformation-via-kernels",style:{position:"relative"}},i.createElement(t.a,{href:"#62-data-transformation-via-kernels","aria-label":"62 data transformation via kernels permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.2. Data transformation via kernels"),"\n",i.createElement(t.p,null,"By applying the kernel trick, even complicated data distributions in the original space can become more easily separable in ",i.createElement(o.A,{text:"\\(\\mathcal{H}\\)"}),". This conceptual transformation is one of the reasons SVMs became so popular: practitioners were able to produce state-of-the-art results on numerous benchmarks using RBF kernels, for example."),"\n",i.createElement(t.h3,{id:"63-complex-decision-boundaries",style:{position:"relative"}},i.createElement(t.a,{href:"#63-complex-decision-boundaries","aria-label":"63 complex decision boundaries permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.3. Complex decision boundaries"),"\n",i.createElement(t.p,null,'Visually, in 2D or 3D plots, we see that SVM boundaries with RBF kernels can warp around clusters of data, effectively building local "bubbles" of decision regions. This local adaptivity is controlled by the kernel\'s parameters (e.g., ',i.createElement(o.A,{text:"\\(\\gamma\\)"})," in the RBF kernel). Small ",i.createElement(o.A,{text:"\\(\\gamma\\)"})," values produce smoother, more global boundaries, whereas large ",i.createElement(o.A,{text:"\\(\\gamma\\)"})," values create numerous tight local boundaries that can overfit if ",i.createElement(o.A,{text:"\\(\\gamma\\)"})," is not tuned properly."),"\n",i.createElement(t.h2,{id:"7-model-selection-and-hyperparameter-tuning",style:{position:"relative"}},i.createElement(t.a,{href:"#7-model-selection-and-hyperparameter-tuning","aria-label":"7 model selection and hyperparameter tuning permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7. Model selection and hyperparameter tuning"),"\n",i.createElement(t.p,null,"An SVM's performance hinges critically on the choice of hyperparameters, including ",i.createElement(o.A,{text:"\\(C\\)"}),", the kernel type, and kernel-specific parameters such as ",i.createElement(o.A,{text:"\\(\\gamma\\)"})," (in the RBF kernel) or the polynomial degree ",i.createElement(o.A,{text:"\\(d\\)"}),". In practice, one seldom picks these values arbitrarily. Instead, you employ:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Cross-validation")," (e.g., k-fold cross-validation)."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Grid search")," or ",i.createElement(t.strong,null,"random search")," for a structured or random exploration of candidate hyperparameter values."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Bayesian optimization")," or other advanced methods in more demanding scenarios."),"\n"),"\n",i.createElement(t.p,null,"After systematically testing, you select the combination that yields the best validation performance."),"\n",i.createElement(t.p,null,"Some typical guidelines:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,"Start with a moderate grid of ",i.createElement(o.A,{text:"\\((C, \\gamma)\\)"})," pairs if using an RBF kernel (e.g., ",i.createElement(o.A,{text:"\\(C \\in \\{1, 10, 100\\}\\)"}),", ",i.createElement(o.A,{text:"\\(\\gamma \\in \\{0.01, 0.1, 1\\}\\)"}),")."),"\n",i.createElement(t.li,null,"Evaluate performance (accuracy, F1-score, etc.) on validation folds."),"\n",i.createElement(t.li,null,"Narrow the grid or do a finer search around regions of the hyperparameter space that look promising."),"\n"),"\n",i.createElement(t.p,null,"A quick illustration with Python:"),"\n",i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">&lt;Code text={`\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\nparam_grid = {\n    \'C\': [0.1, 1, 10, 100],\n    \'gamma\': [0.001, 0.01, 0.1, 1],\n    \'kernel\': [\'rbf\']\n}\n\nmodel = SVC()\ngrid_search = GridSearchCV(model, param_grid, cv=5)\ngrid_search.fit(X, y)\n\nprint("Best parameters:", grid_search.best_params_)\nprint("Best CV score:", grid_search.best_score_)\n`}/></code></pre></div>'}}),"\n",i.createElement(t.p,null,"With this approach, one systematically explores a variety of hyperparameter combinations to identify those that generalize best to unseen data."),"\n",i.createElement(t.h2,{id:"8-svm-in-high-dimensional-spaces",style:{position:"relative"}},i.createElement(t.a,{href:"#8-svm-in-high-dimensional-spaces","aria-label":"8 svm in high dimensional spaces permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8. SVM in high-dimensional spaces"),"\n",i.createElement(t.p,null,i.createElement(s.A,null,"SVMs")," historically gained popularity for text classification partly because they can handle high-dimensional input spaces surprisingly well, especially if many features are sparse. The interplay of margin-based classification, kernel methods, and the possibility of ignoring irrelevant features can yield robust performance even as ",i.createElement(o.A,{text:"\\(n\\)"})," grows large. However, one must still remain mindful of potential pitfalls."),"\n",i.createElement(t.h3,{id:"81-curse-of-dimensionality",style:{position:"relative"}},i.createElement(t.a,{href:"#81-curse-of-dimensionality","aria-label":"81 curse of dimensionality permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.1. Curse of dimensionality"),"\n",i.createElement(t.p,null,'In extremely high-dimensional settings (tens of thousands or more features), distances between data points can become less interpretable (this is sometimes referred to as the "curse of dimensionality"). While SVMs can remain more stable than some other methods, the risk of overfitting does rise if one does not regularize properly (choose ',i.createElement(o.A,{text:"\\(C\\)"})," suitably) or if the kernel is too flexible."),"\n",i.createElement(t.h3,{id:"82-feature-selection-and-dimensionality-reduction",style:{position:"relative"}},i.createElement(t.a,{href:"#82-feature-selection-and-dimensionality-reduction","aria-label":"82 feature selection and dimensionality reduction permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.2. Feature selection and dimensionality reduction"),"\n",i.createElement(t.p,null,"When ",i.createElement(o.A,{text:"\\(n\\)"})," is extremely large, dimensionality reduction or feature selection can be beneficial before training the SVM:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Principal Component Analysis (PCA)")," or other linear transforms to reduce dimension."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Regularization-based feature selection")," approaches to prune irrelevant features."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Domain-driven feature engineering")," to isolate relevant signals."),"\n"),"\n",i.createElement(t.p,null,"Doing so not only reduces the risk of overfitting but can substantially lower computational costs."),"\n",i.createElement(t.h3,{id:"83-effect-of-sparsity-on-svm",style:{position:"relative"}},i.createElement(t.a,{href:"#83-effect-of-sparsity-on-svm","aria-label":"83 effect of sparsity on svm permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.3. Effect of sparsity on SVM"),"\n",i.createElement(t.p,null,"Many real high-dimensional tasks (like text classification) exhibit sparse feature vectors. The linear SVM with ",i.createElement(s.A,null,"hinge loss")," can handle these efficiently, and kernel-based approaches can be more expensive but still remain feasible if specialized optimizations or approximate kernel expansions are used. Overall, SVM is among the tried-and-true methods for high-dimensional data, but must be carefully tuned to avoid overfitting or excessive computational overhead."),"\n",i.createElement(t.h2,{id:"9-advantages-and-limitations",style:{position:"relative"}},i.createElement(t.a,{href:"#9-advantages-and-limitations","aria-label":"9 advantages and limitations permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9. Advantages and limitations"),"\n",i.createElement(t.p,null,"SVMs come with a range of practical ",i.createElement(s.A,null,"advantages"),":"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Robustness to Overfitting (once tuned)"),": The maximum margin principle helps reduce structural risk."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Flexibility with Kernels"),": You can incorporate domain knowledge by designing or choosing specific kernels."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Unique Global Solution"),": The underlying optimization is convex, meaning you get a single global optimum."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Interpretability"),": The notion of support vectors often yields some interpretability about which data points are key in shaping the decision boundary."),"\n"),"\n",i.createElement(t.p,null,"However, certain ",i.createElement(s.A,null,"limitations")," must be acknowledged:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Sensitivity to Hyperparameters"),": SVM performance can degrade significantly if ",i.createElement(o.A,{text:"\\(C\\)"})," or kernel parameters (",i.createElement(o.A,{text:"\\(\\gamma\\)"}),", polynomial degree, etc.) are not chosen well."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Computational Complexity"),": For extremely large datasets, especially beyond tens or hundreds of thousands of samples, SVM training can become computationally intensive (though specialized methods like SMO help)."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Difficulty with Non-Sparse, High-Dimensional Data"),": Kernel computations can blow up in memory/time cost if not carefully managed."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Interpretation"),": Although simpler than deep nets, the learned boundaries in kernel spaces can still be quite abstract, making it trickier to interpret than, say, a shallow decision tree for some audiences."),"\n"),"\n",i.createElement(t.h2,{id:"10-implementation-details",style:{position:"relative"}},i.createElement(t.a,{href:"#10-implementation-details","aria-label":"10 implementation details permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"10. Implementation details"),"\n",i.createElement(t.p,null,"There are multiple production-quality and research-grade libraries for training and using SVMs. Two widely used ones include:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"scikit-learn (Python)"),": wraps LIBSVM and LIBLINEAR, providing easy access to SVC, SVR, LinearSVC, etc."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"LIBSVM"),": a classic, standalone C++ library that supports a variety of kernels and tasks (classification, regression, one-class)."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"LIBLINEAR"),": specialized for linear SVMs and logistic regression on large-scale problems."),"\n"),"\n",i.createElement(t.p,null,"In Python, the ",i.createElement(s.A,null,"scikit-learn")," library is one of the most common go-tos. For instance:"),"\n",i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">&lt;Code text={`\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\n# Suppose X, y are input features and labels\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create an RBF-kernel SVC with some hyperparameters\nclf = SVC(kernel=\'rbf\', C=10, gamma=0.1)\nclf.fit(X_train, y_train)\n\n# Predict on test\ny_pred = clf.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprint("Test Accuracy = ", acc)\n`}/></code></pre></div>'}}),"\n",i.createElement(t.h3,{id:"practical-coding-tips",style:{position:"relative"}},i.createElement(t.a,{href:"#practical-coding-tips","aria-label":"practical coding tips permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Practical coding tips"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Feature Scaling"),": Many SVM implementations assume features are on a comparable scale. It is usually recommended to standardize or normalize features before fitting an SVM with an RBF or polynomial kernel."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Parameter Sweeps"),": Use cross-validation-based grid or random search for hyperparameters, especially ",i.createElement(o.A,{text:"\\(C\\)"})," and ",i.createElement(o.A,{text:"\\(\\gamma\\)"}),"."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Probability Estimates"),": If you need probability estimates, you can enable ",i.createElement(s.A,null,"Platt scaling")," in scikit-learn (by passing ",i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">probability=True</code>'}})," to the ",i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">SVC</code>'}}),"). This can slow down training somewhat."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Sparse Data"),": If your data is large but sparse, consider using ",i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">LinearSVC</code>'}})," or specialized libraries like LIBLINEAR or vowpal wabbit (though the latter is not strictly an SVM)."),"\n"),"\n",i.createElement(t.h3,{id:"data-preprocessing",style:{position:"relative"}},i.createElement(t.a,{href:"#data-preprocessing","aria-label":"data preprocessing permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Data preprocessing"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Outlier detection"),": SVM can be sensitive to outliers, especially if ",i.createElement(o.A,{text:"\\(C\\)"})," is large. Consider removing outliers or use robust scaling."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Balancing classes"),": If you have severely imbalanced classes, weigh them differently or sample your data to get a balanced training set."),"\n"),"\n",i.createElement(t.h2,{id:"11-use-cases",style:{position:"relative"}},i.createElement(t.a,{href:"#11-use-cases","aria-label":"11 use cases permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"11. Use cases"),"\n",i.createElement(t.p,null,"SVMs have been successfully applied to numerous domains:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Text classification"),": Spam detection, sentiment analysis, news topic classification. Linear SVM with L2 regularization remains a strong baseline in high-dimensional text tasks due to its efficiency and performance."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Image recognition"),": Particularly in the past, before deep learning's dominance, SVMs were popular for tasks like face recognition or object detection. Feature descriptors (HOG, SIFT) fed into an RBF-kernel SVM can deliver robust results."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Biomedical data analysis"),": In gene expression classification (microarray data), SVMs often perform well because these tasks typically have high-dimensional, relatively small sample-size data."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Financial time series forecasting"),": SVM regression (SVR) can be used to predict stock prices or other time-dependent signals. Though tricky and often overshadowed by advanced neural architectures in current practice, SVR is still a candidate for smaller or well-structured time series tasks."),"\n"),"\n",i.createElement(t.p,null,"In many of these scenarios, SVM-based solutions were once the gold standard or near top-of-the-line. Although they sometimes have been outperformed by modern deep models on large-scale tasks, SVMs can still remain extremely competitive or even superior in specialized or small-to-mid scale tasks."),"\n",i.createElement(t.h2,{id:"12-extensions-and-advanced-topics",style:{position:"relative"}},i.createElement(t.a,{href:"#12-extensions-and-advanced-topics","aria-label":"12 extensions and advanced topics permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"12. Extensions and advanced topics"),"\n",i.createElement(t.p,null,"The SVM framework is broad and can be adapted in various ways to meet specialized demands."),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"One-Class SVM")),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,'Targets anomaly or novelty detection. Learns a boundary around "normal" data and can then label outliers or novel points that fall outside. This approach is popular in fraud detection, intrusion detection, or any domain where "normal vs. abnormal" is the key question.'),"\n"),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"SVM ensembles and boosting")),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"While decision trees are commonly used in boosting or bagging, there exist ensemble strategies with SVM base learners. For instance, each SVM might see different subsets of data or features, and their decisions can be combined for a final majority or weighted vote."),"\n"),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Online SVM learning")),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"Traditional SVM training is batch-based, meaning it sees the entire training set at once. Online SVM algorithms incrementally update the model with incoming data. This can be important for streaming or large-scale applications."),"\n"),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Multiclass SVM strategies")),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,'SVMs are fundamentally binary classifiers. For multiclass problems, we often combine multiple SVMs using "one-vs-one" or "one-vs-rest" schemes. In scikit-learn, passing a multiclass label set to an ',i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">SVC</code>'}})," automatically invokes these strategies under the hood."),"\n"),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"SVR (Support Vector Regression)")),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"The same maximum margin principle can be adapted to regression tasks, leading to Support Vector Regression. Instead of classification constraints, one defines an ",i.createElement(o.A,{text:"\\(\\epsilon\\)"}),"-insensitive tube around the regression function, controlling how much deviation is tolerated."),"\n"),"\n"),"\n"),"\n",i.createElement(t.p,null,"Beyond these widely known aspects, there are even more specialized modifications (e.g., ",i.createElement(s.A,null,"structured SVMs")," for sequence labeling, ",i.createElement(s.A,null,"weighted SVMs")," for cost-sensitive tasks, etc.). The fundamental principle remains the same: harness margin-based optimization and, if appropriate, kernel expansions to achieve robust generalization."),"\n",i.createElement(t.h2,{id:"optional-additional-deep-dive-on-svm-formulation",style:{position:"relative"}},i.createElement(t.a,{href:"#optional-additional-deep-dive-on-svm-formulation","aria-label":"optional additional deep dive on svm formulation permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"(Optional) Additional deep-dive on SVM formulation"),"\n",i.createElement(t.p,null,"In more advanced treatments, one can explore the full details of:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Sequential Minimal Optimization (SMO)"),": An efficient algorithm for solving the SVM dual problem incrementally by focusing on pairs of Lagrange multipliers at a time (Platt, 1998)."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Convergence properties"),": SVM solutions are guaranteed to find the global minimum of the convex objective function."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Statistical learning theory"),": SVMs are often seen as implementing ",i.createElement(s.A,null,"Structural Risk Minimization"),", controlling the VC dimension through margin maximization."),"\n"),"\n",i.createElement(t.p,null,"While these topics may be beyond the immediate scope of this course article, they are a fascinating extension for those who want to fully internalize why SVMs have such strong theoretical underpinnings."),"\n",i.createElement(t.h2,{id:"conclusion-or-final-thoughts",style:{position:"relative"}},i.createElement(t.a,{href:"#conclusion-or-final-thoughts","aria-label":"conclusion or final thoughts permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Conclusion (or final thoughts)"),"\n",i.createElement(t.p,null,"Support vector machines remain a pillar of modern machine learning, combining geometric insights, convex optimization, and a flexible kernel framework into a single cohesive method. Their lineage stretches from basic linear classification to sophisticated nonlinear tasks, from small dataset classification to high-dimensional text and bioinformatics, and from classical supervised classification or regression to specialized forms like one-class and structured SVMs."),"\n",i.createElement(t.p,null,"While deep learning has stolen the limelight in many large-scale pattern recognition problems, SVMs endure in multiple applied domains and academic benchmarks alike — particularly when data is not exceedingly large, or interpretability and theoretical guarantees matter. The synergy of margin maximization and kernel expansions ensures that SVMs will remain part of the essential toolbox for scientists, researchers, and ML practitioners in the foreseeable future."),"\n",i.createElement(t.hr),"\n",i.createElement(t.p,null,"Below is a brief summary of essential points one should remember about SVMs:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"They define an optimal hyperplane in a high-dimensional space, separating classes with maximum margin."),"\n",i.createElement(t.li,null,'The "kernel trick" allows them to learn linear boundaries in rich transformed spaces, enabling complex nonlinear boundaries in the original space.'),"\n",i.createElement(t.li,null,"The soft margin extension is crucial for handling non-separable data."),"\n",i.createElement(t.li,null,"Careful tuning of hyperparameters (especially ",i.createElement(o.A,{text:"\\(C\\)"})," and any kernel parameters) is needed for best results."),"\n",i.createElement(t.li,null,"SVMs can handle classification, regression (SVR), outlier detection (one-class SVM), and more."),"\n"),"\n",i.createElement(t.p,null,"The next step for you may be hands-on experimentation with SVM implementations in libraries such as scikit-learn, comparing results with other models, and practicing thorough model selection to see how well SVMs work in your data context."),"\n",i.createElement(a,{alt:"SVM idea illustration",path:"",caption:"An illustration of SVM margins, hyperplane, and support vectors.",zoom:"false"}),"\n",i.createElement(t.p,null,"Enjoy harnessing the power of SVMs in your advanced data science and machine learning projects!"))}var m=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,l.RP)(),e.components);return t?i.createElement(t,e,i.createElement(c,e)):c(e)};var h=a(36710),d=a(58481),p=a.n(d),u=a(36310),g=a(87245),f=a(27042),b=a(59849),y=a(5591),v=a(61122),E=a(9219),x=a(33203),w=a(95751),S=a(94328),M=a(80791),k=a(78137);const _=e=>{let{toc:t}=e;if(!t||!t.items)return null;return i.createElement("nav",{className:M.R},i.createElement("ul",null,t.items.map(((e,t)=>i.createElement("li",{key:t},i.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const a=t.replace("#",""),n=document.getElementById(a);n&&n.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&i.createElement(_,{toc:{items:e.items}}))))))};function V(e){let{data:{mdx:t,allMdx:r,allPostImages:s},children:o}=e;const{frontmatter:c,body:m,tableOfContents:h}=t,d=c.index,b=c.slug.split("/")[1],M=r.nodes.filter((e=>e.frontmatter.slug.includes(`/${b}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),V=M.findIndex((e=>e.frontmatter.index===d)),z=M[V+1],C=M[V-1],A=c.slug.replace(/\/$/,""),H=/[^/]*$/.exec(A)[0],T=`posts/${b}/content/${H}/`,{0:L,1:I}=(0,i.useState)(c.flagWideLayoutByDefault),{0:N,1:j}=(0,i.useState)(!1);var B;(0,i.useEffect)((()=>{j(!0);const e=setTimeout((()=>j(!1)),340);return()=>clearTimeout(e)}),[L]),"adventures"===b?B=E.cb:"research"===b?B=E.Qh:"thoughts"===b&&(B=E.T6);const P=p()(m).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,D=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),a=e%60;return a<=30?`~${t}${a>0?".5":""} h`:`~${t+1} h`}(Math.ceil(P/B)+(c.extraReadTimeMin||0)),R=[{flag:c.flagDraft,component:()=>Promise.all([a.e(3231),a.e(8809)]).then(a.bind(a,28809))},{flag:c.flagMindfuckery,component:()=>Promise.all([a.e(3231),a.e(2471)]).then(a.bind(a,67709))},{flag:c.flagRewrite,component:()=>Promise.all([a.e(3231),a.e(6764)]).then(a.bind(a,62002))},{flag:c.flagOffensive,component:()=>Promise.all([a.e(3231),a.e(2443)]).then(a.bind(a,17681))},{flag:c.flagProfane,component:()=>Promise.all([a.e(3231),a.e(8048)]).then(a.bind(a,53286))},{flag:c.flagMultilingual,component:()=>Promise.all([a.e(3231),a.e(4069)]).then(a.bind(a,78831))},{flag:c.flagUnreliably,component:()=>Promise.all([a.e(3231),a.e(3417)]).then(a.bind(a,8179))},{flag:c.flagPolitical,component:()=>Promise.all([a.e(3231),a.e(5195)]).then(a.bind(a,30433))},{flag:c.flagCognitohazard,component:()=>Promise.all([a.e(3231),a.e(3175)]).then(a.bind(a,8413))},{flag:c.flagHidden,component:()=>Promise.all([a.e(3231),a.e(9556)]).then(a.bind(a,14794))}],{0:O,1:K}=(0,i.useState)([]);return(0,i.useEffect)((()=>{R.forEach((e=>{let{flag:t,component:a}=e;t&&a().then((e=>{K((t=>[].concat((0,n.A)(t),[e.default])))}))}))}),[]),i.createElement(f.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},i.createElement(y.A,{postNumber:c.index,date:c.date,updated:c.updated,readTime:D,difficulty:c.difficultyLevel,title:c.title,desc:c.desc,banner:c.banner,section:b,postKey:H,isMindfuckery:c.flagMindfuckery,mainTag:c.mainTag}),i.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},c.otherTags.map(((e,t)=>i.createElement("span",{key:t,className:`noselect ${k.MW}`,style:{margin:"0 5px 5px 0"}},e)))),i.createElement("div",{class:"postBody"},i.createElement(_,{toc:h})),i.createElement("br"),i.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},i.createElement(f.P.button,{class:"noselect",className:S.pb,id:S.xG,onClick:()=>{I(!L)},whileTap:{scale:.93}},i.createElement(f.P.div,{className:w.DJ,key:L,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},L?"Switch to default layout":"Switch to wide layout"))),i.createElement("br"),i.createElement("div",{class:"postBody",style:{margin:L?"0 -14%":"",maxWidth:L?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},i.createElement("div",{className:`${S.P_} ${N?S.Xn:S.qG}`},O.map(((e,t)=>i.createElement(e,{key:t}))),c.indexCourse?i.createElement(x.A,{index:c.indexCourse,category:c.courseCategoryName}):"",i.createElement(u.Z.Provider,{value:{images:s.nodes,basePath:T.replace(/\/$/,"")+"/"}},i.createElement(l.xA,{components:{Image:g.A}},o)))),i.createElement(v.A,{nextPost:z,lastPost:C,keyCurrent:H,section:b}))}function z(e){return i.createElement(V,e,i.createElement(m,e))}function C(e){var t,a,n,l,r;let{data:s}=e;const{frontmatter:o}=s.mdx,c=o.titleSEO||o.title,m=o.titleOG||c,d=o.titleTwitter||c,p=o.descSEO||o.desc,u=o.descOG||p,g=o.descTwitter||p,f=o.schemaType||"BlogPosting",y=o.keywordsSEO,v=o.date,E=o.updated||v,x=o.imageOG||(null===(t=o.banner)||void 0===t||null===(a=t.childImageSharp)||void 0===a||null===(n=a.gatsbyImageData)||void 0===n||null===(l=n.images)||void 0===l||null===(r=l.fallback)||void 0===r?void 0:r.src),w=o.imageAltOG||u,S=o.imageTwitter||x,M=o.imageAltTwitter||g,k=o.canonicalURL,_=o.flagHidden||!1,V=o.mainTag||"Posts",z=o.slug.split("/")[1]||"posts",{siteUrl:C}=(0,h.Q)(),A={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:C},{"@type":"ListItem",position:2,name:V,item:`${C}/${o.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${C}${o.slug}`}]};return i.createElement(b.A,{title:c+" - avrtt.blog",titleOG:m,titleTwitter:d,description:p,descriptionOG:u,descriptionTwitter:g,schemaType:f,keywords:y,datePublished:v,dateModified:E,imageOG:x,imageAltOG:w,imageTwitter:S,imageAltTwitter:M,canonicalUrl:k,flagHidden:_,mainTag:V,section:z,type:"article"},i.createElement("script",{type:"application/ld+json"},JSON.stringify(A)))}},96098:function(e,t,a){var n=a(96540),l=a(7978);t.A=e=>{let{text:t}=e;return n.createElement(l.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-js-content-file-path-src-pages-posts-research-svm-mdx-e6eb983adc636bfd9708.js.map