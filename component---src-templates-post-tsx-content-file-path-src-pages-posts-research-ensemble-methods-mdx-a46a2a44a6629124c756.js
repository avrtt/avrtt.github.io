"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[6390],{3962:function(e,t){t.A="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iaXNvLTg4NTktMSI/Pgo8IS0tIEdlbmVyYXRvcjogQWRvYmUgSWxsdXN0cmF0b3IgMTYuMC4wLCBTVkcgRXhwb3J0IFBsdWctSW4gLiBTVkcgVmVyc2lvbjogNi4wMCBCdWlsZCAwKSAgLS0+CjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+CjxzdmcgdmVyc2lvbj0iMS4xIiBpZD0iQ2FwYV8xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB4PSIwcHgiIHk9IjBweCIKCSB3aWR0aD0iNDE2Ljk3OXB4IiBoZWlnaHQ9IjQxNi45NzlweCIgdmlld0JveD0iMCAwIDQxNi45NzkgNDE2Ljk3OSIgc3R5bGU9ImVuYWJsZS1iYWNrZ3JvdW5kOm5ldyAwIDAgNDE2Ljk3OSA0MTYuOTc5OyIKCSB4bWw6c3BhY2U9InByZXNlcnZlIj4KPGc+Cgk8cGF0aCBkPSJNMzU2LjAwNCw2MS4xNTZjLTgxLjM3LTgxLjQ3LTIxMy4zNzctODEuNTUxLTI5NC44NDgtMC4xODJjLTgxLjQ3LDgxLjM3MS04MS41NTIsMjEzLjM3OS0wLjE4MSwyOTQuODUKCQljODEuMzY5LDgxLjQ3LDIxMy4zNzgsODEuNTUxLDI5NC44NDksMC4xODFDNDM3LjI5MywyNzQuNjM2LDQzNy4zNzUsMTQyLjYyNiwzNTYuMDA0LDYxLjE1NnogTTIzNy42LDM0MC43ODYKCQljMCwzLjIxNy0yLjYwNyw1LjgyMi01LjgyMiw1LjgyMmgtNDYuNTc2Yy0zLjIxNSwwLTUuODIyLTIuNjA1LTUuODIyLTUuODIyVjE2Ny44ODVjMC0zLjIxNywyLjYwNy01LjgyMiw1LjgyMi01LjgyMmg0Ni41NzYKCQljMy4yMTUsMCw1LjgyMiwyLjYwNCw1LjgyMiw1LjgyMlYzNDAuNzg2eiBNMjA4LjQ5LDEzNy45MDFjLTE4LjYxOCwwLTMzLjc2Ni0xNS4xNDYtMzMuNzY2LTMzLjc2NQoJCWMwLTE4LjYxNywxNS4xNDctMzMuNzY2LDMzLjc2Ni0zMy43NjZjMTguNjE5LDAsMzMuNzY2LDE1LjE0OCwzMy43NjYsMzMuNzY2QzI0Mi4yNTYsMTIyLjc1NSwyMjcuMTA3LDEzNy45MDEsMjA4LjQ5LDEzNy45MDF6Ii8+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPC9zdmc+Cg=="},9360:function(e,t,n){n.d(t,{A:function(){return l}});var a=n(96540),r=n(3962),i="styles-module--tooltiptext--a263b";var l=e=>{let{text:t,isBadge:n=!1}=e;const{0:l,1:s}=(0,a.useState)(!1),o=(0,a.useRef)(null);return(0,a.useEffect)((()=>{function e(e){o.current&&e.target instanceof Node&&!o.current.contains(e.target)&&s(!1)}return document.addEventListener("click",e),()=>{document.removeEventListener("click",e)}}),[]),a.createElement("span",{className:"styles-module--tooltipWrapper--75ebf",ref:o},a.createElement("img",{id:n?"styles-module--infoBadge--e3d66":"styles-module--info--26c1f",src:r.A,alt:"info",onClick:e=>{e.stopPropagation(),s((e=>!e))}}),a.createElement("span",{className:l?`${i} styles-module--visible--c063c`:i},t))}},63255:function(e,t,n){n.r(t),n.d(t,{Head:function(){return H},PostTemplate:function(){return M},default:function(){return A}});var a=n(28453),r=n(96540),i=n(9360),l=n(61992),s=n(62087),o=n(90548);function c(e){const t=Object.assign({p:"p",hr:"hr",h2:"h2",a:"a",span:"span",ol:"ol",li:"li",strong:"strong",h3:"h3",ul:"ul"},(0,a.RP)(),e.components);return r.createElement(r.Fragment,null,"\n",r.createElement("br"),"\n","\n","\n",r.createElement(t.p,null,"The term ",r.createElement(l.A,null,"ensemble methods")," refers to a family of techniques in machine learning and statistical modeling in which multiple models (often called ",r.createElement(i.A,{text:"Any learning algorithm that takes in data (X) and maps it to a target output (y). Examples include linear regression, decision trees, SVMs, etc."}),"learners) are trained and then strategically combined in order to achieve superior predictive performance when compared to using any individual model on its own. Although the foundational concepts date back to theoretical results about the wisdom of crowds (e.g., Condorcet's jury theorem in the late 18th century) and to 20th-century developments in statistics, ensemble methods burst into prominence in the mid-1990s with the introduction of techniques like ",r.createElement(l.A,null,"bagging")," (Breiman, 1996) and ",r.createElement(l.A,null,"boosting")," (Schapire & Freund, 1997). Since then, they have remained some of the most powerful and widely used methodologies in applied machine learning, winning countless Kaggle competitions and revolutionizing both academic and industrial use cases."),"\n",r.createElement(t.p,null,'Ensemble methods are sometimes described as "meta-algorithms" because they do not necessarily assume a specific kind of model to begin with, but rather define ways of combining multiple models or re-training a single type of model multiple times under carefully chosen perturbations. An ensemble can involve ',r.createElement(l.A,null,"homogeneous learners")," (all from the same class of models, e.g. only decision trees) or ",r.createElement(l.A,null,"heterogeneous learners")," (combinations of different model families, e.g. logistic regression, neural networks, gradient-boosted trees, SVMs, etc.)."),"\n",r.createElement(t.p,null,"In practice, employing ensembles often leads to a lower generalization error by reducing the variance of the final predictions or compensating for the biases that hamper individual models. However, the quest for improved predictive performance can come with computational overhead and potential interpretability issues. As the world of machine learning continues to evolve, ensemble-based strategies remain crucial in both classical and cutting-edge settings, often yielding state-of-the-art results even in the era of deep learning."),"\n",r.createElement(t.p,null,"This article provides a thorough, in-depth, and practical exploration of advanced ensemble approaches, focusing on the fundamental concepts, theoretical motivations, and state-of-the-art implementations. We begin by explaining the fundamental strategies (bagging, boosting, voting, stacking) and then dive into more specialized algorithms (gradient boosting frameworks like XGBoost, LightGBM, CatBoost, and others). We will also explore some advanced topics regarding parameter tuning, computational trade-offs, interpretability, and best practices for implementing these methods in real-world data science workflows."),"\n",r.createElement(t.p,null,"By the end of this comprehensive chapter, you should have a clear understanding of why ensembles work, how to implement them in practice, how to tune their parameters to achieve strong performance, and how to handle critical pitfalls like overfitting, interpretability, and computational costs."),"\n",r.createElement(t.hr),"\n",r.createElement(t.h2,{id:"fundamentals-of-ensemble-strategies",style:{position:"relative"}},r.createElement(t.a,{href:"#fundamentals-of-ensemble-strategies","aria-label":"fundamentals of ensemble strategies permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Fundamentals of ensemble strategies"),"\n",r.createElement(t.p,null,"Ensemble methods revolve around the idea of combining ",r.createElement(l.A,null,"weak learners")," or ",r.createElement(l.A,null,"diverse learners")," to form a more robust predictor. Although there are many ways to characterize them, the following are some of the broad theoretical and practical considerations that explain why ensemble methods so frequently outperform single models:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Reduction of variance"),": Multiple learners can average out the noise or erratic behavior of a single learner, thereby reducing overall variance of the predictor."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Reduction (or balancing) of bias"),": In certain ensemble strategies, carefully crafted combinations of learners can reduce the net bias, producing more accurate predictions on average."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,'Exploitation of diverse modeling "views"'),": Combining learners trained on different distributions, different features, or entirely different algorithmic families can sometimes yield synergy if their errors are uncorrelated or partially complementary."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,'Focus on "hard" examples'),": In boosting methods especially, newly added learners may specifically target the misclassified or mispredicted samples from the prior iteration, leading to a refined model that systematically corrects leftover mistakes."),"\n"),"\n",r.createElement(t.h3,{id:"bias-variance-tradeoff-revisited",style:{position:"relative"}},r.createElement(t.a,{href:"#bias-variance-tradeoff-revisited","aria-label":"bias variance tradeoff revisited permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Bias-variance tradeoff revisited"),"\n",r.createElement(t.p,null,"We recall from earlier sections of this course that every machine learning model's generalization error can be decomposed into ",r.createElement(l.A,null,"bias"),", ",r.createElement(l.A,null,"variance"),", and irreducible noise:"),"\n",r.createElement(o.A,{text:"\\[\n\\text{Expected Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Noise}.\n\\]"}),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Bias")," measures how much the average prediction of the model diverges from the true signal in the data; it is often a result of limited model flexibility (underfitting)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Variance")," measures how sensitive the model is to fluctuations in the training set; it tends to be high if the model is extremely flexible (overfitting)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Irreducible noise")," is the noise inherent in the data-generation process that no model can capture perfectly."),"\n"),"\n",r.createElement(t.p,null,"Different ensemble strategies tackle this tradeoff in different ways:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Bagging"),": Typically reduces variance by averaging many models trained on bootstrapped samples of the original dataset, without necessarily changing the bias drastically (especially relevant for tree-based learners)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Boosting"),": Typically reduces bias (and can also reduce variance) by iteratively refining weak learners that adapt to residual errors; but it can be more prone to overfitting if not regularized or monitored carefully."),"\n"),"\n",r.createElement(t.h3,{id:"combining-weak-learners-vs-strong-learners",style:{position:"relative"}},r.createElement(t.a,{href:"#combining-weak-learners-vs-strong-learners","aria-label":"combining weak learners vs strong learners permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Combining weak learners vs. strong learners"),"\n",r.createElement(t.p,null,'The term "weak learner" historically comes from boosting theory and typically refers to a learner that can achieve performance better than random guessing on average. Meanwhile, a "strong learner" is something that can approximate the underlying function at a very high level of accuracy. In practice, many popular ensemble algorithms still rely on relatively simple or shallow learners (e.g., small decision trees a.k.a. "decision stumps" in AdaBoost) because these are cheap to train in large numbers or in iterative sequences.'),"\n",r.createElement(t.p,null,"However, there is no universal rule that an ensemble must rely on strictly weak learners. Some ensembles combine fairly complex sub-models (like deep neural networks combined with gradient-boosted decision trees, or random forests used in mixture with SVMs). The overarching principle is that each additional model contributes a unique perspective or correction of the combined predictor."),"\n",r.createElement(t.h3,{id:"types-of-ensembles-homogeneous-vs-heterogeneous",style:{position:"relative"}},r.createElement(t.a,{href:"#types-of-ensembles-homogeneous-vs-heterogeneous","aria-label":"types of ensembles homogeneous vs heterogeneous permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Types of ensembles (homogeneous vs. heterogeneous)"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Homogeneous"),": All base learners are from the same family (e.g., all are decision trees). This is the case in random forests, gradient boosting on trees, etc."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Heterogeneous"),": Different base learners from different model families (e.g., neural nets, logistic regression, SVMs, random forest, etc.). Stacking and blending approaches often adopt this perspective, layering or blending multiple distinct model classes together."),"\n"),"\n",r.createElement(t.h3,{id:"key-considerations-in-building-an-ensemble",style:{position:"relative"}},r.createElement(t.a,{href:"#key-considerations-in-building-an-ensemble","aria-label":"key considerations in building an ensemble permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Key considerations in building an ensemble"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Diversity"),": The base learners should be sufficiently different from each other that combining them reduces variance. If they are too similar or produce near-identical predictions, the ensemble gain is minimal."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Correlation of errors"),": The ensemble benefits from uncorrelated or anti-correlated errors. If two models systematically make the same mistake, averaging or voting them will not remove that mistake."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Computational cost"),": Training and combining multiple learners can be expensive in CPU, memory, and time, especially if each sub-model is large."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Data availability"),": In low-data regimes, some ensemble methods can risk overfitting or reduce interpretability. Bootstrapping may degrade the effective training data usage if not done carefully."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Hyperparameter complexity"),": Each ensemble approach introduces additional hyperparameters (e.g., number of learners, learning rates, subsampling fractions, or advanced loss function settings), which can complicate the optimization process for practitioners."),"\n"),"\n",r.createElement(t.hr),"\n",r.createElement(t.h2,{id:"bootstrapping-and-bagging",style:{position:"relative"}},r.createElement(t.a,{href:"#bootstrapping-and-bagging","aria-label":"bootstrapping and bagging permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Bootstrapping and bagging"),"\n",r.createElement(t.p,null,"Among the earliest and best-known ensemble approaches is ",r.createElement(l.A,null,"bagging"),", short for ",r.createElement(l.A,null,"bootstrap aggregating"),". It builds upon the statistical technique known as ",r.createElement(l.A,null,"bootstrapping"),"."),"\n",r.createElement(t.h3,{id:"definition-and-purpose-of-bootstrapping",style:{position:"relative"}},r.createElement(t.a,{href:"#definition-and-purpose-of-bootstrapping","aria-label":"definition and purpose of bootstrapping permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Definition and purpose of bootstrapping"),"\n",r.createElement(t.p,null,r.createElement(l.A,null,"Bootstrapping")," is a resampling technique in which multiple datasets, each the same size as the original dataset, are drawn randomly with replacement from the original data. Concretely:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"Let the original dataset have ",r.createElement(o.A,{text:"\\(N\\)"})," samples."),"\n",r.createElement(t.li,null,"We create a new dataset ",r.createElement(o.A,{text:"\\(X_1\\)"})," by randomly drawing ",r.createElement(o.A,{text:"\\(N\\)"})," samples from the original dataset with replacement (meaning that the same sample can appear multiple times)."),"\n",r.createElement(t.li,null,"We repeat this procedure ",r.createElement(o.A,{text:"\\(M\\)"})," times to generate ",r.createElement(o.A,{text:"\\(M\\)"})," bootstrap datasets ",r.createElement(o.A,{text:"\\(X_1, X_2, \\dots, X_M\\)"}),"."),"\n"),"\n",r.createElement(t.p,null,"Each bootstrap dataset typically contains some fraction (about ",r.createElement(o.A,{text:"\\(63.2\\%\\)"}),") of unique samples from the original set, with certain samples duplicated. This method is widely used for estimating variances, building confidence intervals, and, in the context of bagging, for training multiple models to reduce variance."),"\n",r.createElement(t.h3,{id:"how-bagging-utilizes-bootstrapping",style:{position:"relative"}},r.createElement(t.a,{href:"#how-bagging-utilizes-bootstrapping","aria-label":"how bagging utilizes bootstrapping permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"How bagging utilizes bootstrapping"),"\n",r.createElement(t.p,null,"Bagging is straightforward:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,"We draw ",r.createElement(o.A,{text:"\\(M\\)"})," bootstrap samples, each of size ",r.createElement(o.A,{text:"\\(N\\)"}),", from the original dataset."),"\n",r.createElement(t.li,null,"We train a separate base learner (a classification or regression model) on each bootstrap sample. Because each base learner is trained on a different subset, we expect them to vary in their learned parameters."),"\n",r.createElement(t.li,null,"At inference (prediction) time:","\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"\n",r.createElement(t.p,null,"For ",r.createElement(l.A,null,"classification"),": we often combine model outputs through ",r.createElement(l.A,null,"majority voting"),', i.e., the predicted class is the one that gets the most "votes" among the ',r.createElement(o.A,{text:"\\(M\\)"})," learners. In a more advanced weighted-voting approach, each learner might have a weight ",r.createElement(o.A,{text:"\\(\\alpha_i\\)"})," that reflects its estimated accuracy or confidence, so the predicted class is:"),"\n",r.createElement(o.A,{text:"\\(f(x) = \\arg\\max_{k \\in \\{1, \\dots, K\\}} \\sum_{i=1}^M \\alpha_i I(f_i(x) = k)\\)"}),"\n",r.createElement(t.p,null,"Here, ",r.createElement(o.A,{text:"\\(I(\\cdot)\\)"})," is the indicator function that equals 1 if the condition is true, and 0 otherwise, while ",r.createElement(o.A,{text:"\\(\\alpha_i\\)"})," are weighting coefficients."),"\n"),"\n",r.createElement(t.li,null,"\n",r.createElement(t.p,null,"For ",r.createElement(l.A,null,"regression"),": we typically take the average of predictions from the ",r.createElement(o.A,{text:"\\(M\\)"})," learners, for instance"),"\n",r.createElement(o.A,{text:"\\( \\hat{y}(x) = \\frac{1}{M} \\sum_{i=1}^M f_i(x).\\)"}),"\n",r.createElement(t.p,null,"A well-known theoretical result states that if the errors of each individual regressor are uncorrelated and zero-mean, then combining them reduces the variance of the final estimator by a factor of ",r.createElement(o.A,{text:"\\(M\\)"}),"."),"\n"),"\n"),"\n"),"\n"),"\n",r.createElement(t.h3,{id:"random-forest-as-a-bagging-based-ensemble",style:{position:"relative"}},r.createElement(t.a,{href:"#random-forest-as-a-bagging-based-ensemble","aria-label":"random forest as a bagging based ensemble permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Random forest as a bagging-based ensemble"),"\n",r.createElement(t.p,null,"A ",r.createElement(l.A,null,"random forest")," is arguably the most popular bagging-based ensemble of decision trees, introduced by Breiman (2001). In addition to using bootstrapped samples to train each tree, random forests also perform ",r.createElement(l.A,null,"feature (column) subsampling")," at each split, reducing correlation among trees and typically improving generalization. This approach is covered in more detail in the dedicated chapter on decision trees and random forests."),"\n",r.createElement(t.h3,{id:"practical-implementation-tips",style:{position:"relative"}},r.createElement(t.a,{href:"#practical-implementation-tips","aria-label":"practical implementation tips permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Practical implementation tips"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Number of base learners")," (",r.createElement(o.A,{text:"\\(M\\)"}),"): Usually, the more learners, the lower the variance — up to a point. In practice, 100–1000 trees are common for random forests."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Choice of base learner"),": Bagging can be combined with any type of model, but the random forest approach specifically uses decision trees."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Subsampling vs. full bootstrap"),": Some frameworks allow you to specify the fraction of the dataset to sample. For large datasets, sampling even half or two-thirds may be enough for a good ensemble."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Parallelization"),": Each base learner in bagging can be trained independently, enabling highly parallel implementations."),"\n"),"\n",r.createElement(t.p,null,"Below is an example in Python demonstrating bagging with different base estimators using scikit-learn:"),"\n",r.createElement(s.A,{text:'\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_breast_cancer\n\n# Load a sample dataset\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\nseed = 42\nbase_estimators = [\n    RandomForestClassifier(random_state=seed),\n    ExtraTreesClassifier(random_state=seed),\n    KNeighborsClassifier(),\n    SVC(probability=True, random_state=seed),\n    RidgeClassifier()\n]\n\nfor estimator in base_estimators:\n    scores = cross_val_score(estimator, X, y, cv=5, scoring=\'accuracy\')\n    bagging = BaggingClassifier(estimator, max_samples=0.5, max_features=1.0, random_state=seed)\n    bagging_scores = cross_val_score(bagging, X, y, cv=5, scoring=\'accuracy\')\n    print(f"Base {estimator.__class__.__name__}: mean={scores.mean():.3f}, std={scores.std():.3f}")\n    print(f"Bagging {estimator.__class__.__name__}: mean={bagging_scores.mean():.3f}, std={bagging_scores.std():.3f}")\n    print("---------")\n'}),"\n",r.createElement(t.hr),"\n",r.createElement(t.h2,{id:"boosting",style:{position:"relative"}},r.createElement(t.a,{href:"#boosting","aria-label":"boosting permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Boosting"),"\n",r.createElement(t.p,null,"While bagging attempts to reduce variance by training many learners in parallel on bootstrapped samples, boosting takes a different path. It incrementally builds an ensemble by adding learners that address the residual weaknesses of the existing combined model."),"\n",r.createElement(t.h3,{id:"core-idea-behind-boosting",style:{position:"relative"}},r.createElement(t.a,{href:"#core-idea-behind-boosting","aria-label":"core idea behind boosting permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Core idea behind boosting"),"\n",r.createElement(t.p,null,"In boosting, we start with an initial base model ",r.createElement(o.A,{text:"\\(f_0(x)\\)"}),", which might be as simple as a constant prediction (e.g., the mean of the target variable in regression). Then at each iteration ",r.createElement(o.A,{text:"\\(t\\)"}),", we fit a new weak learner ",r.createElement(o.A,{text:"\\(h_t\\)"}),' to the current residuals (or some related notion of "errors") of the combined model so far. The newly fitted learner is scaled by some coefficient ',r.createElement(o.A,{text:"\\(\\alpha_t\\)"})," and added into the existing ensemble:"),"\n",r.createElement(o.A,{text:"\\[\nf_t(x) = f_{t-1}(x) + \\alpha_t \\, h_t(x).\n\\]"}),"\n",r.createElement(t.p,null,'The typical result is that each new learner tries to correct the mistakes of the previous ones, gradually "boosting" the model\'s performance. Over many iterations, the combination of these weak learners grows into a highly accurate predictor — provided that we use appropriate constraints or regularization to avoid overfitting.'),"\n",r.createElement(t.h3,{id:"sequential-training-of-weak-learners",style:{position:"relative"}},r.createElement(t.a,{href:"#sequential-training-of-weak-learners","aria-label":"sequential training of weak learners permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Sequential training of weak learners"),"\n",r.createElement(t.p,null,"Key steps in a generic boosting algorithm:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Initialize")," the ensemble with ",r.createElement(o.A,{text:"\\(f_0(x)\\)"}),", often a constant model."),"\n",r.createElement(t.li,null,"For each iteration ",r.createElement(o.A,{text:"\\(t = 1, 2, \\ldots, T\\)"}),":","\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"Compute some measure of error or residual for the training data with respect to the current combined model ",r.createElement(o.A,{text:"\\(f_{t-1}\\)"}),"."),"\n",r.createElement(t.li,null,"Train a new weak learner ",r.createElement(o.A,{text:"\\(h_t\\)"})," to predict those residuals (or some function of them, like negative gradients)."),"\n",r.createElement(t.li,null,"Compute an optimal multiplier ",r.createElement(o.A,{text:"\\(\\alpha_t\\)"})," that best integrates ",r.createElement(o.A,{text:"\\(h_t\\)"})," into the model."),"\n",r.createElement(t.li,null,"Update the combined model: ",r.createElement(o.A,{text:"\\(f_t(x) = f_{t-1}(x) + \\alpha_t \\, h_t(x)\\)"}),"."),"\n"),"\n"),"\n"),"\n",r.createElement(t.h3,{id:"comparison-of-bagging-and-boosting",style:{position:"relative"}},r.createElement(t.a,{href:"#comparison-of-bagging-and-boosting","aria-label":"comparison of bagging and boosting permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Comparison of bagging and boosting"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Training approach"),":","\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"Bagging trains each learner independently (in parallel)."),"\n",r.createElement(t.li,null,"Boosting trains learners in a sequential manner: each new learner focuses on the mistakes of the ensemble so far."),"\n"),"\n"),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Data sampling"),":","\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"Bagging often uses bootstrap samples to create variability among models."),"\n",r.createElement(t.li,null,"Boosting reweights or re-targets data points: in some boosting algorithms, misclassified points get higher weights so that subsequent learners pay more attention to them."),"\n"),"\n"),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Combination"),":","\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"Bagging generally uses simple averaging or majority voting."),"\n",r.createElement(t.li,null,"Boosting uses weighted sums of learners, where each learner's weight reflects its contribution or confidence."),"\n"),"\n"),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Bias vs. variance"),":","\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"Bagging mostly reduces variance."),"\n",r.createElement(t.li,null,"Boosting can reduce bias substantially and also help reduce variance, but it can be more prone to overfitting if not regulated or if the number of iterations is too large."),"\n"),"\n"),"\n"),"\n",r.createElement(t.p,null,"Below is a minimal illustration in Python, using scikit-learn's AdaBoost and GradientBoostingClassifier. We train them on the Boston Housing dataset or a classification dataset:"),"\n",r.createElement(s.A,{text:"\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.datasets import load_breast_cancer\n\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\nada = AdaBoostClassifier(n_estimators=100, random_state=42)\ngb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n\nada_scores = cross_val_score(ada, X, y, cv=5, scoring='accuracy')\ngb_scores = cross_val_score(gb, X, y, cv=5, scoring='accuracy')\n\nprint(f\"AdaBoost mean accuracy: {ada_scores.mean():.3f} ± {ada_scores.std():.3f}\")\nprint(f\"GradientBoost mean accuracy: {gb_scores.mean():.3f} ± {gb_scores.std():.3f}\")\n"}),"\n",r.createElement(t.hr),"\n",r.createElement(t.h2,{id:"gradient-boosting",style:{position:"relative"}},r.createElement(t.a,{href:"#gradient-boosting","aria-label":"gradient boosting permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Gradient boosting"),"\n",r.createElement(t.p,null,"A special and extremely popular form of boosting is known as ",r.createElement(l.A,null,"gradient boosting"),". Originally pioneered by Friedman (2001, 2002), it is based on the principle of fitting new learners to the ",r.createElement(l.A,null,"gradient")," of the loss function with respect to the predictions of the ensemble."),"\n",r.createElement(t.h3,{id:"overview-of-gradient-boosting-framework",style:{position:"relative"}},r.createElement(t.a,{href:"#overview-of-gradient-boosting-framework","aria-label":"overview of gradient boosting framework permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Overview of gradient-boosting framework"),"\n",r.createElement(t.p,null,"Generally, we assume a differentiable loss function ",r.createElement(o.A,{text:"\\(L(y, \\hat{y})\\)"}),". Let ",r.createElement(o.A,{text:"\\(\\hat{F}_{t-1}(x)\\)"})," be the ensemble model at iteration ",r.createElement(o.A,{text:"\\(t-1\\)"}),". At iteration ",r.createElement(o.A,{text:"\\(t\\)"}),":"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,"\n",r.createElement(t.p,null,"We compute the negative gradient of the loss with respect to the current predictions:"),"\n",r.createElement(o.A,{text:"\\[\nr_{it} = - \\left[ \\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)} \\right]_{F = \\hat{F}_{t-1}}.\n\\]"}),"\n",r.createElement(t.p,null,"This quantity is often called the ",r.createElement(l.A,null,"pseudo-residual"),"."),"\n"),"\n",r.createElement(t.li,null,"\n",r.createElement(t.p,null,"We fit a weak learner ",r.createElement(o.A,{text:"\\(h_t(x)\\)"})," (e.g., a decision tree) to these pseudo-residuals ",r.createElement(o.A,{text:"\\(\\{r_{it}\\}\\)"}),"."),"\n"),"\n",r.createElement(t.li,null,"\n",r.createElement(t.p,null,"We find the optimal multiplier ",r.createElement(o.A,{text:"\\(\\gamma_t\\)"})," by solving:"),"\n",r.createElement(o.A,{text:"\\[\n\\gamma_t = \\arg \\min_{\\gamma} \\sum_{i} L\\bigl(y_i, \\hat{F}_{t-1}(x_i) + \\gamma\\,h_t(x_i)\\bigr).\n\\]"}),"\n"),"\n",r.createElement(t.li,null,"\n",r.createElement(t.p,null,"We update the model:"),"\n",r.createElement(o.A,{text:"\\[\n\\hat{F}_t(x) = \\hat{F}_{t-1}(x) + \\nu\\,\\gamma_t\\,h_t(x),\n\\]"}),"\n",r.createElement(t.p,null,"where ",r.createElement(o.A,{text:"\\(\\nu\\in (0,1]\\)"})," is the ",r.createElement(l.A,null,"learning rate")," or ",r.createElement(l.A,null,"shrinkage parameter")," that helps slow down the learning to avoid overfitting."),"\n"),"\n"),"\n",r.createElement(t.p,null,"Over many iterations, the ensemble converges to a function that (hopefully) minimizes the overall loss on the training set. A variety of modifications and improvements exist: random subsampling of samples (stochastic gradient boosting), random subsampling of features, advanced penalty terms, specialized handling for classification vs. regression, etc."),"\n",r.createElement(t.h3,{id:"explanation-of-gradient-boost-in-regression",style:{position:"relative"}},r.createElement(t.a,{href:"#explanation-of-gradient-boost-in-regression","aria-label":"explanation of gradient boost in regression permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Explanation of gradient boost in regression"),"\n",r.createElement(t.p,null,"For regression with a squared-error loss ",r.createElement(o.A,{text:"\\(L(y, F(x)) = \\frac{1}{2}(y - F(x))^2\\)"}),", the negative gradient with respect to ",r.createElement(o.A,{text:"\\(F(x)\\)"})," is:"),"\n",r.createElement(o.A,{text:"\\[\nr_{it} = y_i - \\hat{F}_{t-1}(x_i).\n\\]"}),"\n",r.createElement(t.p,null,"Hence, at each iteration, the new weak learner is trained to predict the current residuals ",r.createElement(o.A,{text:"\\((y_i - \\hat{F}_{t-1}(x_i))\\)"}),". Once we find the best-fitting tree (or another base learner) for those residuals, we typically compute:"),"\n",r.createElement(o.A,{text:"\\[\n\\gamma_t = \\arg \\min_\\gamma \\sum_i \\frac{1}{2}\\bigl(y_i - (\\hat{F}_{t-1}(x_i) + \\gamma\\,h_t(x_i))\\bigr)^2.\n\\]"}),"\n",r.createElement(t.p,null,"This leads us to a closed-form solution if ",r.createElement(o.A,{text:"\\(h_t\\)"})," is a regression tree with constant values in each leaf. The model is updated by adding ",r.createElement(o.A,{text:"\\(\\nu\\,\\gamma_t h_t(x)\\)"}),". Intuitively, each step tries to fix the gap between the data and the current ensemble's prediction, focusing on the biggest errors."),"\n",r.createElement(t.h3,{id:"explanation-of-gradient-boost-in-classification",style:{position:"relative"}},r.createElement(t.a,{href:"#explanation-of-gradient-boost-in-classification","aria-label":"explanation of gradient boost in classification permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Explanation of gradient boost in classification"),"\n",r.createElement(t.p,null,"For binary classification with a logistic loss, the negative gradient step is more nuanced, but the principle is the same. We define:"),"\n",r.createElement(o.A,{text:"\\[\nL(y, F(x)) = \\log\\bigl(1 + \\exp(-2y\\,F(x))\\bigr),\n\\]"}),"\n",r.createElement(t.p,null,"where typically ",r.createElement(o.A,{text:"\\(y\\in\\{-1, +1\\}\\)"}),". The negative gradient with respect to ",r.createElement(o.A,{text:"\\(F(x)\\)"})," can be derived, and we again train a weak learner to match this gradient. Then we solve for an optimal multiplier that best fits the logistic loss. The final model outputs a score ",r.createElement(o.A,{text:"\\(F_T(x)\\)"})," which can be converted to a probability estimate via the logistic function:"),"\n",r.createElement(o.A,{text:"\\[\n\\hat{p}(x) = \\frac{1}{1 + \\exp\\bigl(-2\\,F_T(x)\\bigr)}.\n\\]"}),"\n",r.createElement(t.p,null,"In practice, popular frameworks like XGBoost, LightGBM, CatBoost, and scikit-learn's GradientBoostingClassifier handle these steps internally. They offer parameters that specify the type of loss, the maximum depth of trees, the learning rate ",r.createElement(o.A,{text:"\\(\\nu\\)"}),", etc."),"\n",r.createElement(t.h3,{id:"loss-functions-and-their-role-in-gradient-boosting",style:{position:"relative"}},r.createElement(t.a,{href:"#loss-functions-and-their-role-in-gradient-boosting","aria-label":"loss functions and their role in gradient boosting permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Loss functions and their role in gradient boosting"),"\n",r.createElement(t.p,null,"Gradient boosting is flexible enough to accommodate a range of differentiable loss functions:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Squared error")," for regression"),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Absolute error")," for robust regression"),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Huber loss")," for outlier-insensitive regression"),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Logistic loss")," for binary classification"),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Cross-entropy loss")," for multi-class classification"),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Ranking losses")," for ranking tasks (e.g., pairwise or listwise approaches)"),"\n"),"\n",r.createElement(t.p,null,"The choice of loss function must be guided by the problem domain and the evaluation metric relevant to that domain. Many libraries (XGBoost, LightGBM, CatBoost) also allow custom losses if you supply the gradient and second-order derivative."),"\n",r.createElement(t.h3,{id:"regularization-techniques",style:{position:"relative"}},r.createElement(t.a,{href:"#regularization-techniques","aria-label":"regularization techniques permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Regularization techniques"),"\n",r.createElement(t.p,null,"Because gradient boosting can easily overfit — especially if you add a large number of learners — several regularization strategies are typically employed:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Shrinking the contributions")," by a learning rate ",r.createElement(o.A,{text:"\\(\\nu\\in (0,1]\\)"}),"."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Limiting the complexity")," of the weak learners, e.g., restricting the maximum depth of each tree, the number of leaf nodes, or the minimum number of samples per leaf."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Using penalization")," of leaf weights or L2 regularization on the leaf outputs (some implementations use L1 or even more advanced forms)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Subsampling")," both rows (stochastic gradient boosting) and columns at each iteration to reduce correlation among learners (similar to random forest ideas)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Early stopping"),' or "overfitting detection" to halt training if validation loss does not improve for a certain number of iterations.'),"\n"),"\n",r.createElement(t.hr),"\n",r.createElement(t.h2,{id:"popular-gradient-boosting-libraries",style:{position:"relative"}},r.createElement(t.a,{href:"#popular-gradient-boosting-libraries","aria-label":"popular gradient boosting libraries permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Popular gradient boosting libraries"),"\n",r.createElement(t.p,null,"A host of well-maintained, robust libraries exist that implement gradient boosting. In modern machine learning, most practical solutions rely on one of the following to achieve strong results in both regression and classification tasks."),"\n",r.createElement(t.h3,{id:"61-adaboost-basics-and-applications",style:{position:"relative"}},r.createElement(t.a,{href:"#61-adaboost-basics-and-applications","aria-label":"61 adaboost basics and applications permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.1. AdaBoost basics and applications"),"\n",r.createElement(t.p,null,r.createElement(l.A,null,"AdaBoost")," (short for Adaptive Boosting) is historically significant in that it popularized the concept of boosting for classification. Proposed by Freund and Schapire (1997), it focuses on reweighting the training examples so that subsequent weak learners pay more attention to examples previously misclassified."),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Initialization"),": The training set is assigned uniform weights ",r.createElement(o.A,{text:"\\(D_i^{(1)} = 1/m\\)"}),"."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Learner training"),": A weak classifier ",r.createElement(o.A,{text:"\\(h_t\\)"})," is trained to minimize the weighted classification error ",r.createElement(o.A,{text:"\\(\\epsilon_t = \\sum D_i^{(t)} I(y_i \\neq h_t(x_i))\\)"}),"."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Coefficient calculation"),": The learner's contribution ",r.createElement(o.A,{text:"\\(\\alpha_t\\)"})," is set to","\n",r.createElement(o.A,{text:"\\( \\alpha_t = \\frac{1}{2}\\,\\ln\\left(\\frac{1-\\epsilon_t}{\\epsilon_t}\\right). \\)"}),"\n"),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Update of weights"),":","\n",r.createElement(o.A,{text:"\\[\nD_i^{(t+1)} = \\frac{D_i^{(t)} \\exp\\bigl(-\\alpha_t\\, y_i\\,h_t(x_i)\\bigr)}{Z_t},\n\\]"}),"\n","where ",r.createElement(o.A,{text:"\\(Z_t\\)"})," is a normalization constant ensuring that ",r.createElement(o.A,{text:"\\( \\sum_i D_i^{(t+1)} = 1.\\)"})),"\n"),"\n",r.createElement(t.p,null,"This procedure is repeated for ",r.createElement(o.A,{text:"\\(t=1,\\dots,T\\)"}),", and the final combined classifier is"),"\n",r.createElement(o.A,{text:"\\[\nH(x) = \\text{sign}\\left(\\sum_{t=1}^T \\alpha_t\\,h_t(x)\\right).\n\\]"}),"\n",r.createElement(t.p,null,"Despite the historical importance, AdaBoost is sometimes overshadowed by the more general frameworks of gradient boosting. However, it remains a simple and effective method — particularly for binary classification. AdaBoost also has a known sensitivity to outliers (since misclassified points accumulate ever-larger weights)."),"\n",r.createElement(t.h3,{id:"62-xgboost-core-concepts",style:{position:"relative"}},r.createElement(t.a,{href:"#62-xgboost-core-concepts","aria-label":"62 xgboost core concepts permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.2. XGBoost core concepts"),"\n",r.createElement(t.p,null,r.createElement(l.A,null,"XGBoost")," (",r.createElement(l.A,null,"eXtreme Gradient Boosting"),") is a high-performance library popularized by Chen and Guestrin (2016). Its success in Kaggle competitions stems from a combination of algorithmic optimizations, highly efficient handling of sparse data, and scale-out capabilities. Notable features include:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"A custom tree learning algorithm that caches sorted feature values for splits."),"\n",r.createElement(t.li,null,"Clever penalization of tree complexity using a regularization term of the form:","\n",r.createElement(o.A,{text:"\\[\n\\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda \\lVert w \\rVert^2\n\\]"}),"\n","where ",r.createElement(o.A,{text:"\\(T\\)"})," is the number of leaves, ",r.createElement(o.A,{text:"\\(\\gamma\\)"})," and ",r.createElement(o.A,{text:"\\(\\lambda\\)"})," are regularization parameters, and ",r.createElement(o.A,{text:"\\(w\\)"})," is the vector of leaf weights."),"\n",r.createElement(t.li,null,"Built-in support for distributed training on clusters via frameworks like Spark and Hadoop Yarn."),"\n",r.createElement(t.li,null,"Rich support for custom objectives, early stopping, and GPU acceleration."),"\n"),"\n",r.createElement(t.p,null,"Example usage (Python):"),"\n",r.createElement(s.A,{text:'\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.datasets import load_breast_cancer\n\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ntrain_dmatrix = xgb.DMatrix(X_train, label=y_train)\ntest_dmatrix = xgb.DMatrix(X_test, label=y_test)\n\nparams = {\n    "objective": "binary:logistic",\n    "max_depth": 4,\n    "eta": 0.1,\n    "eval_metric": "logloss"\n}\nnum_round = 100\n\nevals = [(train_dmatrix, \'train\'), (test_dmatrix, \'eval\')]\nbst = xgb.train(params, train_dmatrix, num_round, evals=evals, early_stopping_rounds=10)\n\ny_pred_prob = bst.predict(test_dmatrix)\ny_pred = [1 if prob > 0.5 else 0 for prob in y_pred_prob]\nprint("XGBoost Accuracy:", accuracy_score(y_test, y_pred))\n'}),"\n",r.createElement(t.h3,{id:"63-catboost-and-its-handling-of-categorical-features",style:{position:"relative"}},r.createElement(t.a,{href:"#63-catboost-and-its-handling-of-categorical-features","aria-label":"63 catboost and its handling of categorical features permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.3. CatBoost and its handling of categorical features"),"\n",r.createElement(t.p,null,r.createElement(l.A,null,"CatBoost"),", developed by Yandex, is a gradient boosting library aimed at addressing one key limitation in many other libraries: the handling of ",r.createElement(l.A,null,"categorical features"),". Traditional boosting libraries often require that the data scientist manually encode categorical variables (e.g., via one-hot or label encoding). CatBoost automates many of these transformations by:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"Employing ",r.createElement(l.A,null,"ordered boosting")," and other strategies to mitigate the target leakage that can occur with naive encoding of categorical variables."),"\n",r.createElement(t.li,null,"Having specialized encodings that produce more robust numerical representations from high-cardinality categories."),"\n",r.createElement(t.li,null,"Tending to have strong out-of-the-box performance with minimal parameter tuning, especially for datasets with many categorical features."),"\n"),"\n",r.createElement(t.p,null,"Below is a minimal example:"),"\n",r.createElement(s.A,{text:'\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\n# Suppose we have a dataset with both numeric and categorical features\ndata = {\n    "color": ["red", "blue", "green", "red", "blue", "blue", "green", "red"],\n    "size": [1, 2, 2, 1, 3, 2, 1, 1],\n    "weight": [10.5, 12.3, 13.1, 9.6, 11.2, 10.1, 9.8, 10.4],\n    "label": [0, 1, 1, 0, 1, 1, 0, 0]\n}\ndf = pd.DataFrame(data)\n\nX = df[["color", "size", "weight"]]\ny = df["label"]\n\n# Identify which features are categorical by index\ncat_features = [0]  # \'color\' is the 0th column\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42)\n\nmodel = CatBoostClassifier(\n    iterations=50,\n    learning_rate=0.1,\n    depth=3,\n    cat_features=cat_features,\n    verbose=False\n)\n\nmodel.fit(X_train, y_train, eval_set=(X_val, y_val))\npreds = model.predict(X_val)\nprint("CatBoost predictions:", preds)\n'}),"\n",r.createElement(t.h3,{id:"64-lightgbm-and-its-efficiency-optimizations",style:{position:"relative"}},r.createElement(t.a,{href:"#64-lightgbm-and-its-efficiency-optimizations","aria-label":"64 lightgbm and its efficiency optimizations permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.4. LightGBM and its efficiency optimizations"),"\n",r.createElement(t.p,null,r.createElement(l.A,null,"LightGBM"),", developed by Microsoft, focuses heavily on computational efficiency and scalability. Among its innovations:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Gradient-based One-Side Sampling (GOSS)"),": Instead of sampling data uniformly, LightGBM retains instances with large gradients and randomly downsamples those with small gradients, speeding up training without significantly compromising accuracy."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Exclusive Feature Bundling (EFB)"),": Merges mutually exclusive features into a single feature to reduce dimensionality, especially beneficial for sparse data."),"\n",r.createElement(t.li,null,"Highly efficient histogram-based splits, multi-threading, and GPU support."),"\n"),"\n",r.createElement(t.p,null,"Use LightGBM if:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"You have a large dataset with high cardinality features."),"\n",r.createElement(t.li,null,"You need faster training or memory efficiency compared to straightforward implementations of gradient boosting."),"\n",r.createElement(t.li,null,"You wish to tune advanced sampling or histogram-based parameters for performance gains."),"\n"),"\n",r.createElement(t.p,null,"Example usage (Python):"),"\n",r.createElement(s.A,{text:'\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.datasets import load_breast_cancer\n\ndata = load_breast_cancer()\nX, y = data.data, data.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.2, \n                                                    random_state=42)\ntrain_data = lgb.Dataset(X_train, label=y_train)\ntest_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n\nparams = {\n    "objective": "binary",\n    "learning_rate": 0.1,\n    "num_leaves": 31,\n    "metric": "binary_logloss"\n}\n\ngbm = lgb.train(params, train_data, num_boost_round=100, \n                valid_sets=[test_data], \n                early_stopping_rounds=10)\n\ny_pred_prob = gbm.predict(X_test, num_iteration=gbm.best_iteration)\ny_pred = [1 if prob > 0.5 else 0 for prob in y_pred_prob]\nprint("LightGBM Accuracy:", accuracy_score(y_test, y_pred))\n'}),"\n",r.createElement(t.hr),"\n",r.createElement(t.h2,{id:"stacking-and-blending",style:{position:"relative"}},r.createElement(t.a,{href:"#stacking-and-blending","aria-label":"stacking and blending permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Stacking and blending"),"\n",r.createElement(t.p,null,r.createElement(l.A,null,"Stacking")," (short for stacked generalization) and ",r.createElement(l.A,null,"blending"),' are ensemble techniques that combine predictions from multiple models (which can be homogeneous or heterogeneous) by training a final "meta-learner" to weigh these predictions. While bagging averages multiple models in a relatively straightforward manner and boosting builds a sequence of dependent learners, stacking sets up a layered structure, often called "Level-1" (base learners) and "Level-2" (meta learner).'),"\n",r.createElement(t.h3,{id:"71-layered-architecture-of-stacking",style:{position:"relative"}},r.createElement(t.a,{href:"#71-layered-architecture-of-stacking","aria-label":"71 layered architecture of stacking permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.1. Layered architecture of stacking"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Level-1"),": We train multiple base models (e.g., a random forest, a gradient boosting regressor, and a neural network). Each model provides an output (e.g., predicted probability for classification or a numeric estimate for regression)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Meta-features"),": We collect these outputs as new features. For example, if you have 3 base models, each sample in the dataset now has 3 new predicted values."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Level-2"),": We train a second-layer model (meta-learner) on these meta-features to produce the final prediction. This meta-learner might be something as simple as linear or logistic regression, or more advanced methods."),"\n"),"\n",r.createElement(t.p,null,"A crucial detail: to avoid overfitting, when constructing meta-features, each base model should be trained on one part of the training set and then validated on a held-out fold. This ensures the meta-learner sees honest predictions that reflect real generalization performance."),"\n",r.createElement(t.h3,{id:"72-practical-tips-for-blending-multiple-models",style:{position:"relative"}},r.createElement(t.a,{href:"#72-practical-tips-for-blending-multiple-models","aria-label":"72 practical tips for blending multiple models permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.2. Practical tips for blending multiple models"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Diversity of base learners")," is key. If all base models are the same, there may be little advantage."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Cross-validation")," is typically used for generating out-of-fold predictions for the meta-learner."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Regularization")," in the meta-learner is often helpful, since the meta-learner can easily overfit."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Blending"),' is a simplified approach in which you train base learners on the entire training set but keep a separate (small) "blending" set to estimate their predictions and tune a simpler combiner (like a weighted average). In practice, blending can be easier to implement but might be less robust than a full cross-validated stacking approach.'),"\n"),"\n",r.createElement(t.p,null,"Below is a high-level Python code snippet illustrating stacking:"),"\n",r.createElement(s.A,{text:'\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\n# Example data\nfrom sklearn.datasets import load_breast_cancer\nX, y = load_breast_cancer(return_X_y=True)\n\n# Level-1 models\nmodel1 = RandomForestClassifier(n_estimators=50, random_state=42)\nmodel2 = GradientBoostingClassifier(n_estimators=50, random_state=42)\nmodel3 = SVC(probability=True, random_state=42)\n\n# Generate out-of-fold predictions\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\nmeta_features = np.zeros((len(X), 3))\nfor train_idx, val_idx in kf.split(X):\n    X_train, X_val = X[train_idx], X[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n    \n    model1.fit(X_train, y_train)\n    model2.fit(X_train, y_train)\n    model3.fit(X_train, y_train)\n    \n    meta_features[val_idx, 0] = model1.predict_proba(X_val)[:, 1]\n    meta_features[val_idx, 1] = model2.predict_proba(X_val)[:, 1]\n    meta_features[val_idx, 2] = model3.predict_proba(X_val)[:, 1]\n\n# Train meta-learner\nmeta_learner = LogisticRegression()\nmeta_learner.fit(meta_features, y)\n\n# Evaluate\nmeta_pred = meta_learner.predict_proba(meta_features)[:, 1] > 0.5\nprint("Stacking training set accuracy:", accuracy_score(y, meta_pred))\n'}),"\n",r.createElement(t.h3,{id:"73-tuning-hyperparameters-for-stacked-ensembles",style:{position:"relative"}},r.createElement(t.a,{href:"#73-tuning-hyperparameters-for-stacked-ensembles","aria-label":"73 tuning hyperparameters for stacked ensembles permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.3. Tuning hyperparameters for stacked ensembles"),"\n",r.createElement(t.p,null,"Stacked ensembles introduce multiple levels of tuning:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Base learners"),": Each one may have its own hyperparameters."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Meta-learner"),": Has its own hyperparameters as well."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Stacking strategy"),": Number of folds, how to generate out-of-fold predictions, and so forth."),"\n"),"\n",r.createElement(t.p,null,"In practice, a recommended approach is:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,"Individually tune each base learner or choose their top hyperparameters from preliminary experiments."),"\n",r.createElement(t.li,null,"Choose a meta-learner that is relatively simple (e.g., linear or logistic regression) for interpretability."),"\n",r.createElement(t.li,null,"Consider advanced strategies such as multi-layer stacking or ensembling multiple meta-learners if computational resources allow."),"\n"),"\n",r.createElement(t.hr),"\n",r.createElement(t.h2,{id:"performance-considerations",style:{position:"relative"}},r.createElement(t.a,{href:"#performance-considerations","aria-label":"performance considerations permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Performance considerations"),"\n",r.createElement(t.h3,{id:"81-overfitting-risks-in-ensemble-methods",style:{position:"relative"}},r.createElement(t.a,{href:"#81-overfitting-risks-in-ensemble-methods","aria-label":"81 overfitting risks in ensemble methods permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.1. Overfitting risks in ensemble methods"),"\n",r.createElement(t.p,null,"Although ensembles are often robust, they are not immune to overfitting:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Boosting")," can overfit if you allow it to iterate for too many rounds without early stopping or if each weak learner is too powerful (e.g., large max-depth for trees)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Stacking")," can overfit if the meta-learner memorizes the base learner predictions in the training set, especially if the out-of-fold predictions are not generated properly."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Bagging")," is typically less prone to overfitting, but if the base learners are extremely flexible and you have limited data, you can still overfit."),"\n"),"\n",r.createElement(t.p,null,"Using validation sets or cross-validation to track an out-of-sample error metric is critical. Many modern implementations have built-in ",r.createElement(l.A,null,"early stopping")," or ",r.createElement(l.A,null,"overfitting detectors"),"."),"\n",r.createElement(t.h3,{id:"82-computation-time-vs-predictive-performance",style:{position:"relative"}},r.createElement(t.a,{href:"#82-computation-time-vs-predictive-performance","aria-label":"82 computation time vs predictive performance permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.2. Computation time vs. predictive performance"),"\n",r.createElement(t.p,null,"Ensembles can drastically increase computational requirements:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"You are training multiple (sometimes hundreds or thousands) of models."),"\n",r.createElement(t.li,null,"For large-scale tasks, the memory overhead can also be significant."),"\n"),"\n",r.createElement(t.p,null,"Pragmatic tips:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"Carefully choose the number of base models (e.g., number of trees in a random forest or gradient boosting)."),"\n",r.createElement(t.li,null,"Take advantage of parallelization or distributed computing frameworks (Spark, multi-GPU setups, etc.)."),"\n",r.createElement(t.li,null,"Use approximate or histogram-based methods (as in LightGBM or XGBoost) for large datasets."),"\n"),"\n",r.createElement(t.h3,{id:"83-interpretability-challenges",style:{position:"relative"}},r.createElement(t.a,{href:"#83-interpretability-challenges","aria-label":"83 interpretability challenges permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.3. Interpretability challenges"),"\n",r.createElement(t.p,null,'A main downside of ensemble methods is that they often yield a "black box." While each individual learner (especially if they are decision trees) might be partially interpretable, ensembling a large set of them can become difficult to interpret:'),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(l.A,null,"Permutation importance"),", ",r.createElement(l.A,null,"SHAP values"),", and other model-agnostic interpretability methods can help identify which features drive predictions."),"\n",r.createElement(t.li,null,"Surrogate modeling or partial dependence plots can help approximate the ensemble's behavior."),"\n",r.createElement(t.li,null,"If interpretability is paramount, consider simpler ensembles (like a small random forest) or a single interpretable model with an accuracy–interpretability tradeoff."),"\n"),"\n",r.createElement(t.h3,{id:"84-when-not-to-use-ensembles",style:{position:"relative"}},r.createElement(t.a,{href:"#84-when-not-to-use-ensembles","aria-label":"84 when not to use ensembles permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.4. When not to use ensembles"),"\n",r.createElement(t.p,null,"Despite their power, you might ",r.createElement(l.A,null,"not")," want an ensemble if:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"You need a very simple, interpretable model"),". A single linear model or shallow tree might suffice (e.g., in some regulated industries)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"You have extremely limited data"),". Some ensemble methods can overfit easily or become unstable without enough samples."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"You have tight resource constraints"),". If you need minimal memory or real-time inference with extremely low latency, a large ensemble might be impractical."),"\n"),"\n",r.createElement(t.p,null,"In most other scenarios, especially if you are looking for top predictive accuracy on sufficiently large data, ensembles are a robust and proven choice."),"\n",r.createElement(t.hr),"\n",r.createElement(t.p,null,"Below is an extended, integrative example that demonstrates a typical workflow using an ensemble:"),"\n",r.createElement(s.A,{text:'\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split, cross_val_predict\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# 1. Create a synthetic classification dataset\nX, y = make_classification(n_samples=2000, n_features=20, n_informative=10,\n                           n_redundant=2, random_state=42)\n\n# 2. Split data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.3, \n                                                    random_state=42)\n\n# 3. Train two base models: random forest and gradient boosting\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\ngb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n\nrf.fit(X_train, y_train)\ngb.fit(X_train, y_train)\n\n# 4. Evaluate individually\nrf_preds = rf.predict(X_test)\ngb_preds = gb.predict(X_test)\nprint("Random Forest accuracy:", accuracy_score(y_test, rf_preds))\nprint("Gradient Boosting accuracy:", accuracy_score(y_test, gb_preds))\n\n# 5. Combine them in a naive "hard" voting ensemble\nensemble_preds = []\nfor i in range(len(X_test)):\n    votes = rf_preds[i] + gb_preds[i]\n    # if sum of votes is >= 1 => majority says class=1\n    # if sum of votes is 0 => both predicted 0\n    ensemble_preds.append(1 if votes >= 1 else 0)\n\nprint("Naive Voting Ensemble accuracy:", accuracy_score(y_test, ensemble_preds))\n\n# 6. Alternatively, use stacking:\n# Generate out-of-fold predictions on the training set for meta-learning\nrf_oof = cross_val_predict(rf, X_train, y_train, cv=5, method=\'predict_proba\')[:, 1]\ngb_oof = cross_val_predict(gb, X_train, y_train, cv=5, method=\'predict_proba\')[:, 1]\nmeta_features = np.column_stack((rf_oof, gb_oof))\n\nmeta_model = LogisticRegression()\nmeta_model.fit(meta_features, y_train)\n\n# 7. Create meta features for test set\nrf_test_probs = rf.predict_proba(X_test)[:, 1]\ngb_test_probs = gb.predict_proba(X_test)[:, 1]\nmeta_test = np.column_stack((rf_test_probs, gb_test_probs))\n\nstacked_preds = meta_model.predict(meta_test)\nprint("Stacking Ensemble accuracy:", accuracy_score(y_test, stacked_preds))\n'}),"\n",r.createElement(t.hr),"\n",r.createElement(t.p,null,"This concludes our deep dive into ensemble methods for machine learning. By now, you should have a nuanced view of how bagging and boosting operate, why they can dramatically outperform a single model in many scenarios, how popular frameworks like AdaBoost, XGBoost, LightGBM, and CatBoost differ, and how advanced stacking techniques can combine heterogeneous models in layered ways."),"\n",r.createElement(t.p,null,"Ensemble approaches remain among the most important and frequently successful paradigms in modern data science, thanks to their flexibility, theoretical foundations, and track record of high performance across diverse tasks."))}var m=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,a.RP)(),e.components);return t?r.createElement(t,e,r.createElement(c,e)):c(e)},d=n(54506),h=n(88864),u=n(58481),g=n.n(u),p=n(5984),f=n(43672),b=n(27042),v=n(72031),y=n(81817),E=n(27105),_=n(17265),w=n(2043),x=n(95751),S=n(94328),k=n(80791),C=n(78137);const z=e=>{let{toc:t}=e;if(!t||!t.items)return null;return r.createElement("nav",{className:k.R},r.createElement("ul",null,t.items.map(((e,t)=>r.createElement("li",{key:t},r.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const n=t.replace("#",""),a=document.getElementById(n);a&&a.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&r.createElement(z,{toc:{items:e.items}}))))))};function M(e){let{data:{mdx:t,allMdx:i,allPostImages:l},children:s}=e;const{frontmatter:o,body:c,tableOfContents:m}=t,h=o.index,u=o.slug.split("/")[1],v=i.nodes.filter((e=>e.frontmatter.slug.includes(`/${u}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),k=v.findIndex((e=>e.frontmatter.index===h)),M=v[k+1],A=v[k-1],H=o.slug.replace(/\/$/,""),L=/[^/]*$/.exec(H)[0],B=`posts/${u}/content/${L}/`,{0:T,1:N}=(0,r.useState)(o.flagWideLayoutByDefault),{0:I,1:V}=(0,r.useState)(!1);var j;(0,r.useEffect)((()=>{V(!0);const e=setTimeout((()=>V(!1)),340);return()=>clearTimeout(e)}),[T]),"adventures"===u?j=_.cb:"research"===u?j=_.Qh:"thoughts"===u&&(j=_.T6);const X=g()(c).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,D=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),n=e%60;return n<=30?`~${t}${n>0?".5":""} h`:`~${t+1} h`}(Math.ceil(X/j)+(o.extraReadTimeMin||0)),G=[{flag:o.flagDraft,component:()=>Promise.all([n.e(5850),n.e(9833)]).then(n.bind(n,49833))},{flag:o.flagMindfuckery,component:()=>Promise.all([n.e(5850),n.e(7805)]).then(n.bind(n,27805))},{flag:o.flagRewrite,component:()=>Promise.all([n.e(5850),n.e(8916)]).then(n.bind(n,78916))},{flag:o.flagOffensive,component:()=>Promise.all([n.e(5850),n.e(6731)]).then(n.bind(n,49112))},{flag:o.flagProfane,component:()=>Promise.all([n.e(5850),n.e(3336)]).then(n.bind(n,83336))},{flag:o.flagMultilingual,component:()=>Promise.all([n.e(5850),n.e(2343)]).then(n.bind(n,62343))},{flag:o.flagUnreliably,component:()=>Promise.all([n.e(5850),n.e(6865)]).then(n.bind(n,11627))},{flag:o.flagPolitical,component:()=>Promise.all([n.e(5850),n.e(4417)]).then(n.bind(n,24417))},{flag:o.flagCognitohazard,component:()=>Promise.all([n.e(5850),n.e(8669)]).then(n.bind(n,18669))},{flag:o.flagHidden,component:()=>Promise.all([n.e(5850),n.e(8124)]).then(n.bind(n,48124))}],{0:P,1:F}=(0,r.useState)([]);return(0,r.useEffect)((()=>{G.forEach((e=>{let{flag:t,component:n}=e;t&&n().then((e=>{F((t=>[].concat((0,d.A)(t),[e.default])))}))}))}),[]),r.createElement(b.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},r.createElement(y.A,{postNumber:o.index,date:o.date,updated:o.updated,readTime:D,difficulty:o.difficultyLevel,title:o.title,desc:o.desc,banner:o.banner,section:u,postKey:L,isMindfuckery:o.flagMindfuckery,mainTag:o.mainTag}),r.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},o.otherTags.map(((e,t)=>r.createElement("span",{key:t,className:`noselect ${C.MW}`,style:{margin:"0 5px 5px 0"}},e)))),r.createElement("div",{className:"postBody"},r.createElement(z,{toc:m})),r.createElement("br",null),r.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},r.createElement(b.P.button,{className:`noselect ${S.pb}`,id:S.xG,onClick:()=>{N(!T)},whileTap:{scale:.93}},r.createElement(b.P.div,{className:x.DJ,key:T,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},T?"Switch to default layout":"Switch to wide layout"))),r.createElement("br",null),r.createElement("div",{className:"postBody",style:{margin:T?"0 -14%":"",maxWidth:T?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},r.createElement("div",{className:`${S.P_} ${I?S.Xn:S.qG}`},P.map(((e,t)=>r.createElement(e,{key:t}))),o.indexCourse?r.createElement(w.A,{index:o.indexCourse,category:o.courseCategoryName}):"",r.createElement(p.Z.Provider,{value:{images:l.nodes,basePath:B.replace(/\/$/,"")+"/"}},r.createElement(a.xA,{components:{Image:f.A}},s)))),r.createElement(E.A,{nextPost:M,lastPost:A,keyCurrent:L,section:u}))}function A(e){return r.createElement(M,e,r.createElement(m,e))}function H(e){var t,n,a,i,l;let{data:s}=e;const{frontmatter:o}=s.mdx,c=o.titleSEO||o.title,m=o.titleOG||c,d=o.titleTwitter||c,u=o.descSEO||o.desc,g=o.descOG||u,p=o.descTwitter||u,f=o.schemaType||"BlogPosting",b=o.keywordsSEO,y=o.date,E=o.updated||y,_=o.imageOG||(null===(t=o.banner)||void 0===t||null===(n=t.childImageSharp)||void 0===n||null===(a=n.gatsbyImageData)||void 0===a||null===(i=a.images)||void 0===i||null===(l=i.fallback)||void 0===l?void 0:l.src),w=o.imageAltOG||g,x=o.imageTwitter||_,S=o.imageAltTwitter||p,k=o.canonicalURL,C=o.flagHidden||!1,z=o.mainTag||"Posts",M=o.slug.split("/")[1]||"posts",{siteUrl:A}=(0,h.Q)(),H={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:A},{"@type":"ListItem",position:2,name:z,item:`${A}/${o.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${A}${o.slug}`}]};return r.createElement(v.A,{title:c+" - avrtt.blog",titleOG:m,titleTwitter:d,description:u,descriptionOG:g,descriptionTwitter:p,schemaType:f,keywords:b,datePublished:y,dateModified:E,imageOG:_,imageAltOG:w,imageTwitter:x,imageAltTwitter:S,canonicalUrl:k,flagHidden:C,mainTag:z,section:M,type:"article"},r.createElement("script",{type:"application/ld+json"},JSON.stringify(H)))}},90548:function(e,t,n){var a=n(96540),r=n(7978);t.A=e=>{let{text:t}=e;return a.createElement(r.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-tsx-content-file-path-src-pages-posts-research-ensemble-methods-mdx-a46a2a44a6629124c756.js.map