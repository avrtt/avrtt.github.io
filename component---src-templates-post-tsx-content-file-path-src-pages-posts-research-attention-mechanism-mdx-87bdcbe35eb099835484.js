"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[9236],{67905:function(e,t,n){n.r(t),n.d(t,{Head:function(){return M},PostTemplate:function(){return T},default:function(){return z}});var a=n(28453),i=n(96540),r=n(61992),o=n(62087),l=n(90548);function s(e){const t=Object.assign({p:"p",em:"em",a:"a",h3:"h3",span:"span",ul:"ul",li:"li",strong:"strong",ol:"ol",h2:"h2",h4:"h4",hr:"hr"},(0,a.RP)(),e.components),{Image:n}=t;return n||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),i.createElement(i.Fragment,null,"\n",i.createElement("br"),"\n","\n","\n",i.createElement(t.p,null,i.createElement(t.em,null,i.createElement(t.a,{href:"https://arxiv.org/pdf/1706.03762"},"Attention is all you need")," (to make money in ML).")),"\n",i.createElement("br"),"\n",i.createElement(t.p,null,"The concept of an attention mechanism has emerged as one of the most transformative innovations in modern deep learning. Put simply, an attention mechanism allows a model to selectively concentrate on certain parts of its input or internal representations rather than treating each element of the input equally. This selective focus serves to highlight the portions of a data sequence that are most relevant to the learning objective at a given moment in time, while de-emphasizing information that is less pertinent. I like to think of it in metaphorical terms: just like humans can focus their attention on a specific word within a sentence or a particular object in a busy scene, neural networks can learn to attend to crucial elements of an input sequence."),"\n",i.createElement(t.p,null,"Historically, deep learning had been dominated by recurrent architectures for handling sequential data, especially in fields like natural language processing (NLP). While recurrent neural networks (RNNs) — and their gated variants such as LSTM or GRU — did represent a major step forward in capturing sequential dependencies, they often struggled with long-range context. The emergence of attention mechanisms largely solved that problem by allowing models to handle dependencies across distant elements in a sequence far more effectively. Nowadays, attention underpins state-of-the-art solutions in machine translation, text summarization, image captioning, speech recognition, and beyond."),"\n",i.createElement(t.p,null,"The attention paradigm is also recognized for its adaptability. Different tasks — ranging from language modeling to image segmentation — can benefit from different ways of computing attention or integrating attention with other computational blocks. For instance, in machine translation, an attention mechanism can decide which source-language words to focus on when generating each word in the target language. In image captioning, a spatial attention mechanism might highlight a specific region of the image to describe it accurately in text. This flexibility opens the door for a broad range of innovations and expansions."),"\n",i.createElement(t.p,null,"In this article, I aim to clarify some of the core ideas behind attention. I'll discuss how attention is computed mathematically, how it alleviates the struggles of classical recurrent approaches to sequence learning, and what a few standard attention architectures look like. I'll also dive into multiple variations — global, local, self, multi-head, cross, hierarchical, and more — and show how these paradigms tackle different computational challenges. Following that, I'll explore advanced and cutting-edge methodologies, including efficient or sparse attention mechanisms designed for extremely long sequences. I'll also walk through practical implementation details in popular deep learning frameworks like PyTorch and TensorFlow/Keras, before covering typical applications in NLP and computer vision. Finally, I'll conclude with an eye toward new directions in the research community, including interpretability, resource constraints, and emergent architectures poised to shape the next generation of attention-based models."),"\n",i.createElement(t.p,null,"Attention mechanisms have become ubiquitous in real-world ML systems. Chatbots, for example, rely on attention to distill relevant context from user input and knowledge repositories. Document classification pipelines incorporate attention to weigh the significance of different sentences or sections in a long text. Even in medical imaging, attention-based models can focus on specific areas of an X-ray or MRI scan that signal pathology. While the impetus for attention first took hold in NLP, it has truly permeated the broader sphere of machine learning."),"\n",i.createElement(t.p,null,"My objective here is twofold: First, I want to demystify the sometimes-intimidating technical details by providing a well-structured guide. Second, I hope to inspire advanced practitioners, who may have experience with simpler models, to adopt and experiment with attention-based approaches. The hype is warranted, but attention is also highly approachable once you grasp the fundamentals and practice with small-scale implementations."),"\n",i.createElement(n,{alt:"High-level diagram illustrating the attention mechanism",path:"",caption:"A simple conceptual illustration of attention, showing how certain segments of the input sequence receive higher weights.",zoom:"false"}),"\n",i.createElement(t.h3,{id:"11-definition-of-attention-mechanism",style:{position:"relative"}},i.createElement(t.a,{href:"#11-definition-of-attention-mechanism","aria-label":"11 definition of attention mechanism permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"1.1 Definition of attention mechanism"),"\n",i.createElement(t.p,null,'An attention mechanism can be defined as a computational framework that learns to assign varying weights (often referred to as attention weights or alignment scores) to different parts of an input sequence (or intermediate representation). These weights reflect how relevant each part is in relation to a specific query. Think of it as a sophisticated lookup: given a query, the system calculates how strongly each input "key" matches that query and uses the matching score to produce a weighted sum of "value" vectors. The result is a context-aware summary that emphasizes essential features.'),"\n",i.createElement(t.h3,{id:"12-importance-of-attention-in-modern-deep-learning",style:{position:"relative"}},i.createElement(t.a,{href:"#12-importance-of-attention-in-modern-deep-learning","aria-label":"12 importance of attention in modern deep learning permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"1.2 Importance of attention in modern deep learning"),"\n",i.createElement(t.p,null,"It would be difficult to overstate the importance of attention in current deep learning research and practice. Some key reasons include:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Better context handling"),": By explicitly modeling relevance across elements of the input, attention-based architectures can capture dependencies that might occur very far apart in a sequence. This is crucial for tasks like document-level translation, where a word at the end of a text can influence the correct translation at the start."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Parallelization"),": Many attention-based systems — exemplified by the Transformer (Vaswani and gang, NeurIPS 2017) — process data in a more parallelizable fashion than RNNs. This translates to significantly faster training, especially on modern GPUs or TPUs."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Interpretability (partial)"),": Although the interpretability of attention is the subject of ongoing debate, attention weights often provide a window into which parts of the input a model found most influential in its prediction. That can serve as a starting point for interpretability analyses."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Versatility"),": Attention isn't confined to textual sequences. There's wide application in speech recognition, image processing, and even structured data scenarios, making it a universal concept in deep learning."),"\n"),"\n",i.createElement(t.h3,{id:"13-objectives-of-this-article",style:{position:"relative"}},i.createElement(t.a,{href:"#13-objectives-of-this-article","aria-label":"13 objectives of this article permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"1.3 Objectives of this article"),"\n",i.createElement(t.p,null,"I'll explore attention from its historical inception to the nuts-and-bolts of standard implementations, while also surveying advanced topics. The main goals are:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,"Provide a conceptual overview of attention and how it fits into sequence modeling."),"\n",i.createElement(t.li,null,"Dive deep into the computations and theoretical underpinnings (keys, queries, values, alignment scores, etc.)."),"\n",i.createElement(t.li,null,"Discuss popular variants of attention, including self-attention and multi-head attention, while explaining their motivations."),"\n",i.createElement(t.li,null,"Offer practical advice for implementing attention in PyTorch and TensorFlow/Keras, with code snippets."),"\n",i.createElement(t.li,null,"Examine advanced and emerging attention mechanisms — like memory-augmented and sparse approaches — alongside their benefits and trade-offs."),"\n",i.createElement(t.li,null,"Highlight real-world applications in natural language understanding, computer vision, recommender systems, and other domains."),"\n"),"\n",i.createElement(t.h3,{id:"14-common-applications",style:{position:"relative"}},i.createElement(t.a,{href:"#14-common-applications","aria-label":"14 common applications permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"1.4 Common applications"),"\n",i.createElement(t.p,null,"Attention has become crucial in a wide range of applied use cases:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Chatbots"),": A dialogue system might attend to certain parts of the conversation context, enabling more coherent, context-sensitive replies."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Document classification"),": By selectively focusing on the most important sentences or paragraphs, attention-based models outperform standard CNN or RNN architectures on tasks like sentiment analysis and topic classification."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Medical image analysis"),": Attention can locate regions within an image that indicate disease or anomalies, aiding diagnostic tasks."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Information retrieval"),": Web search engines can use attention to emphasize the most relevant snippets in a document when matching queries to candidate pages."),"\n"),"\n",i.createElement(t.p,null,"All these scenarios highlight how attention-based methods elegantly integrate with different data modalities and problem setups, enhancing a model's capacity to capture and leverage context."),"\n",i.createElement(t.h2,{id:"2-historical-context",style:{position:"relative"}},i.createElement(t.a,{href:"#2-historical-context","aria-label":"2 historical context permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2. Historical context"),"\n",i.createElement(t.p,null,"To truly appreciate the impact of attention mechanisms, let's revisit the landscape of sequence learning before their inception. Early deep learning solutions for sequence-to-sequence tasks (machine translation, speech recognition, etc.) typically employed encoder-decoder architectures with recurrent layers. Although groundbreaking at the time, these designs had a few critical blind spots."),"\n",i.createElement(t.h3,{id:"21-early-sequence-to-sequence-models",style:{position:"relative"}},i.createElement(t.a,{href:"#21-early-sequence-to-sequence-models","aria-label":"21 early sequence to sequence models permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.1 Early sequence-to-sequence models"),"\n",i.createElement(t.p,null,"In the earliest neural machine translation systems, the typical approach was a vanilla encoder-decoder pipeline using LSTM or GRU units. The encoder produced a fixed-size vector representation of the entire input sentence, then handed that vector off to the decoder to generate the output sequence. While such models outperformed purely statistical machine translation systems on certain benchmarks, they still faced challenges:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Information bottleneck"),": Encoding all the information of a lengthy input sequence into a single hidden vector inevitably caused information loss."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Fixed representation"),": The decoder had to rely on a single context vector for the entire generation process, making it difficult to revisit or refine aspects of the input as the decoding progressed."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Limited capacity for long contexts"),": Even well-tuned LSTMs or GRUs often degrade as the input or output sequences grow in length."),"\n"),"\n",i.createElement(t.h3,{id:"22-limitations-of-purely-recurrent-architectures",style:{position:"relative"}},i.createElement(t.a,{href:"#22-limitations-of-purely-recurrent-architectures","aria-label":"22 limitations of purely recurrent architectures permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.2 Limitations of purely recurrent architectures"),"\n",i.createElement(t.p,null,"Purely recurrent solutions — especially those without gating — struggled with vanishing or exploding gradients when sequences became long. Although LSTM and GRU units introduced gating mechanisms to partially solve the gradient flow problem, capturing extremely long-range dependencies still remained an uphill battle. Additionally, the sequential nature of RNNs imposed an inherent training bottleneck because each time step depends on the previous step, limiting parallelization during training."),"\n",i.createElement(t.h3,{id:"23-emergence-of-attention-in-natural-language-processing",style:{position:"relative"}},i.createElement(t.a,{href:"#23-emergence-of-attention-in-natural-language-processing","aria-label":"23 emergence of attention in natural language processing permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.3 Emergence of attention in natural language processing"),"\n",i.createElement(t.p,null,'The first major shift came with the introduction of the attention-based encoder-decoder model by Bahdanau and gang (ICLR 2015). Their pioneering work in neural machine translation showed that letting the decoder "look back" at the entire input sequence — rather than relying on a single fixed vector — dramatically improved translation quality and overcame many of the aforementioned limitations. The "attention" was conceptualized as a set of alignment weights that indicate which input tokens are most relevant for producing a given output token.'),"\n",i.createElement(t.p,null,"Following Bahdanau's attention mechanism, Luong and gang (2015) developed a multiplicative version (often called dot-product attention), which is computationally simpler and more efficient. These innovations quickly spawned a revolution in how researchers approached sequence modeling across tasks."),"\n",i.createElement(t.h3,{id:"24-key-milestones",style:{position:"relative"}},i.createElement(t.a,{href:"#24-key-milestones","aria-label":"24 key milestones permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.4 Key milestones"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Bahdanau and gang, ICLR 2015"),": Introduced the additive attention approach, popularizing the use of attention weights in neural machine translation."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Luong and gang, 2015"),": Proposed a multiplicative (dot-product) attention that reduced computational overhead compared to additive attention."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Vaswani and gang, NeurIPS 2017"),': Published "Attention is All You Need," unveiling the Transformer architecture. This was a watershed moment, demonstrating that RNNs were not strictly necessary for state-of-the-art performance on tasks like machine translation, if one used attention effectively.'),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Child and gang, 2019"),': Explored sparse attention in "Generating Long Sequences with Sparse Transformers," thereby addressing the high computational costs of full self-attention for extremely long texts.'),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Katharopoulos and gang, 2020"),": Introduced linear attention approaches that reduce the quadratic complexity of self-attention to linear time."),"\n"),"\n",i.createElement(t.p,null,"These breakthroughs ushered in a new era where attention mechanisms lie at the heart of many of the best-performing language, vision, and multimodal models."),"\n",i.createElement(t.h2,{id:"3-foundational-concepts",style:{position:"relative"}},i.createElement(t.a,{href:"#3-foundational-concepts","aria-label":"3 foundational concepts permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3. Foundational concepts"),"\n",i.createElement(t.p,null,"The core philosophy of attention can be conveyed through the fundamental notion of queries, keys, and values. Though the precise functional forms can differ across implementations, the conceptual framework remains largely consistent."),"\n",i.createElement(t.h3,{id:"31-key-value-and-query",style:{position:"relative"}},i.createElement(t.a,{href:"#31-key-value-and-query","aria-label":"31 key value and query permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.1 Key, value, and query"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Key (K)"),": A representation of an element in the input sequence that the model can compare against a query."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Value (V)"),": Another representation or feature set for the same element, which will be weighted and aggregated based on how relevant the key is to a particular query."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Query (Q)"),": A vector that expresses what the model is currently looking for or focusing on."),"\n"),"\n",i.createElement(t.p,null,"Each element in the input can have an associated key and value. When the model processes a particular position in the sequence (or a decoding state in the context of encoder-decoder models), it forms a query to decide how to weight or attend to each possible key."),"\n",i.createElement(t.h3,{id:"32-alignment-scores-and-weight-distribution",style:{position:"relative"}},i.createElement(t.a,{href:"#32-alignment-scores-and-weight-distribution","aria-label":"32 alignment scores and weight distribution permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.2 Alignment scores and weight distribution"),"\n",i.createElement(t.p,null,"The alignment score is computed between a query and each key. It measures how well they match. Typically, this is a dot-product or a small neural network that outputs a single scalar. Once the model obtains alignment scores for all elements of the sequence, it normalizes them — commonly via a softmax function — to form a probability distribution. These normalized scores become the attention weights, which are then applied to the corresponding value vectors. Summing them yields a context vector that captures the relevant information."),"\n",i.createElement(t.h3,{id:"33-soft-vs-hard-attention",style:{position:"relative"}},i.createElement(t.a,{href:"#33-soft-vs-hard-attention","aria-label":"33 soft vs hard attention permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.3 Soft vs. hard attention"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Soft attention"),": Uses a differentiable weighting strategy — often a softmax — to compute a weighted average over all elements. This is fully trainable via standard backpropagation."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Hard attention"),": Picks a single element (or a small subset of elements) from the input sequence stochastically. While potentially more interpretable and computationally cheaper in some cases, hard attention is often not differentiable, requiring techniques like reinforcement learning or specialized gradient estimators."),"\n"),"\n",i.createElement(t.h3,{id:"34-additive-vs-multiplicative-scoring",style:{position:"relative"}},i.createElement(t.a,{href:"#34-additive-vs-multiplicative-scoring","aria-label":"34 additive vs multiplicative scoring permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.4 Additive vs. multiplicative scoring"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Additive attention (Bahdanau)"),": Aligns queries and keys by feeding their concatenated representations into a small feed-forward neural network. Mathematically, it uses a function:","\n",i.createElement(l.A,{text:"\\( \\alpha = W_{a} [q; k] + b_{a} \\)"}),"\n","The idea is that the network can learn an appropriate similarity measure."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Multiplicative (dot-product) attention (Luong)"),": Computes alignment scores via:","\n",i.createElement(l.A,{text:"\\[\n\\alpha = q \\cdot k \n\\]"}),"\n","or a scaled variant. This approach is simpler and often faster, particularly when the dimensionality of q and k is large."),"\n"),"\n",i.createElement(t.h3,{id:"35-relationship-to-gating-mechanisms",style:{position:"relative"}},i.createElement(t.a,{href:"#35-relationship-to-gating-mechanisms","aria-label":"35 relationship to gating mechanisms permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.5 Relationship to gating mechanisms"),"\n",i.createElement(t.p,null,"Interestingly, attention can be thought of as an external gating mechanism, operating outside the recurrent loop. Traditional gating (like in LSTM or GRU) is limited to controlling the flow of information within a single hidden state. In contrast, attention gates the flow of information across the entire sequence, allowing for a more flexible distribution of focus."),"\n",i.createElement(n,{alt:"Key, Value, and Query illustration",path:"",caption:"Conceptual depiction of how Q, K, and V interact in attention. Each token has its own K, V, and we compute an attention score with the query.",zoom:"false"}),"\n",i.createElement(t.h2,{id:"4-types-of-attention",style:{position:"relative"}},i.createElement(t.a,{href:"#4-types-of-attention","aria-label":"4 types of attention permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4. Types of attention"),"\n",i.createElement(t.p,null,"Numerous variants and subcategories of attention mechanisms exist. While they share the same high-level premise, each variant addresses unique computational or conceptual challenges."),"\n",i.createElement(t.h3,{id:"41-global-attention",style:{position:"relative"}},i.createElement(t.a,{href:"#41-global-attention","aria-label":"41 global attention permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.1 Global attention"),"\n",i.createElement(t.p,null,"Global attention (often associated with Bahdanau's original formulation) considers all possible positions in the input sequence for each output token. This means that when generating a specific output, the model calculates attention weights across the entire range of input tokens, summing them up in a weighted manner to form the context vector. The main advantage is completeness: the model theoretically never misses any part of the input. However, global attention scales poorly with long sequences, as it requires computing attention for each output token across every input token."),"\n",i.createElement(t.h3,{id:"42-local-attention",style:{position:"relative"}},i.createElement(t.a,{href:"#42-local-attention","aria-label":"42 local attention permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.2 Local attention"),"\n",i.createElement(t.p,null,'Local attention tries to reduce the computational overhead by restricting the attention scope to a subset (window) of the input sequence. For instance, in "local-m" approach, for each output position, the model decides on a center position in the input and attends to only a small window around it. This can significantly lower computational costs for longer sequences while maintaining strong performance if the relevant information typically lies close to the position of interest.'),"\n",i.createElement(t.h3,{id:"43-self-attention",style:{position:"relative"}},i.createElement(t.a,{href:"#43-self-attention","aria-label":"43 self attention permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.3 Self attention"),"\n",i.createElement(t.p,null,"Self attention (or intra-attention) is the bedrock of the Transformer architecture (Vaswani and gang, 2017). In self attention, each token in the sequence forms a query, key, and value from its own representation and attends to other tokens (including itself). This fosters a high degree of parallelism and eliminates the need for recurrent connections. It also drastically improves the capacity to capture long-range dependencies."),"\n",i.createElement(t.p,null,"Self attention is especially potent in tasks where the relationship among all elements in a sequence is crucial. For example, in language modeling or text classification, every word can be relevant to every other word's context, so the model benefits from the ability to attend globally at each layer."),"\n",i.createElement(t.h3,{id:"44-multi-head-attention",style:{position:"relative"}},i.createElement(t.a,{href:"#44-multi-head-attention","aria-label":"44 multi head attention permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.4 Multi-head attention"),"\n",i.createElement(t.p,null,'Multi-head attention extends self attention by splitting the query, key, and value matrices into multiple "heads." Each head performs attention independently, focusing on potentially different aspects of the input. The results are concatenated and then linearly transformed to form the final output. This approach allows the model to capture different types of relationships — perhaps one head focuses on syntactic clues, while another zeroes in on semantic relationships.'),"\n",i.createElement(t.p,null,"Formally, for ",i.createElement(l.A,{text:"\\( h \\)"})," heads, queries (",i.createElement(l.A,{text:"\\( Q \\)"}),"), keys (",i.createElement(l.A,{text:"\\( K \\)"}),"), and values (",i.createElement(l.A,{text:"\\( V \\)"}),") are linearly projected into sub-spaces of smaller dimensionalities. Each head computes attention in these sub-spaces. The outputs are concatenated and projected back to the original dimension. This multi-head strategy has been instrumental in enabling the rich representational capacity of the Transformer family of models."),"\n",i.createElement(t.h3,{id:"45-cross-attention",style:{position:"relative"}},i.createElement(t.a,{href:"#45-cross-attention","aria-label":"45 cross attention permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.5 Cross attention"),"\n",i.createElement(t.p,null,"Cross attention typically appears in encoder-decoder architectures, such as those used for machine translation or text-to-image generation. In cross attention, the query is derived from the decoder states, while the keys and values come from the encoder outputs. This design allows the decoder to selectively focus on relevant encoder information at each decoding step. Cross attention can be layered after a self-attention block in the decoder, ensuring that the decoder has both an internal representation (via self attention) and external context (via cross attention)."),"\n",i.createElement(t.h3,{id:"46-hierarchical-attention",style:{position:"relative"}},i.createElement(t.a,{href:"#46-hierarchical-attention","aria-label":"46 hierarchical attention permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.6 Hierarchical attention"),"\n",i.createElement(t.p,null,"In tasks where inputs are organized in multiple layers — like words within sentences, or sentences within paragraphs — hierarchical attention can be employed. At the word level, the model attends to each token to generate a sentence-level embedding. At the sentence level, it attends to each sentence representation to produce a document-level embedding. This hierarchical approach allows the model to refine attention at multiple scales, which is especially valuable for tasks like document classification or summarization, where higher-level structure plays a significant role."),"\n",i.createElement(t.h2,{id:"5-mathematical-formulation",style:{position:"relative"}},i.createElement(t.a,{href:"#5-mathematical-formulation","aria-label":"5 mathematical formulation permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5. Mathematical formulation"),"\n",i.createElement(t.p,null,"The essence of attention is often distilled into a few key equations, particularly in the context of dot-product (multiplicative) attention. Let's outline the formula, interpret each variable, and discuss common variations."),"\n",i.createElement(t.h3,{id:"51-calculating-attention-scores",style:{position:"relative"}},i.createElement(t.a,{href:"#51-calculating-attention-scores","aria-label":"51 calculating attention scores permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.1 Calculating attention scores"),"\n",i.createElement(t.p,null,"Suppose we have:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"A set of queries, ",i.createElement(l.A,{text:"\\(Q\\)"}),"; dimension: ",i.createElement(l.A,{text:"\\(\\text{batch size} \\times \\text{sequence length}_Q \\times d\\)"}),"."),"\n",i.createElement(t.li,null,"A set of keys, ",i.createElement(l.A,{text:"\\(K\\)"}),"; dimension: ",i.createElement(l.A,{text:"\\(\\text{batch size} \\times \\text{sequence length}_K \\times d\\)"}),"."),"\n",i.createElement(t.li,null,"A set of values, ",i.createElement(l.A,{text:"\\(V\\)"}),"; dimension: ",i.createElement(l.A,{text:"\\(\\text{batch size} \\times \\text{sequence length}_K \\times d_v\\)"}),"."),"\n"),"\n",i.createElement(t.p,null,"The raw alignment scores ",i.createElement(l.A,{text:"\\(\\alpha\\)"})," for a given query-key pair are computed as a dot-product:"),"\n",i.createElement(l.A,{text:"\\[\n\\alpha_{ij} = Q_i \\cdot K_j^\\top\n\\]"}),"\n",i.createElement(t.p,null,"where ",i.createElement(l.A,{text:"\\( i \\)"})," iterates over query positions and ",i.createElement(l.A,{text:"\\( j \\)"})," iterates over key positions. For simplicity, let's skip batch indexing in the notation. The bigger ",i.createElement(l.A,{text:"\\(\\alpha_{ij}\\)"})," is, the stronger the alignment between the ",i.createElement(l.A,{text:"\\(i^\\text{th}\\)"})," query and the ",i.createElement(l.A,{text:"\\(j^\\text{th}\\)"})," key."),"\n",i.createElement(t.h3,{id:"52-normalizing-with-softmax",style:{position:"relative"}},i.createElement(t.a,{href:"#52-normalizing-with-softmax","aria-label":"52 normalizing with softmax permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.2 Normalizing with softmax"),"\n",i.createElement(t.p,null,"Before applying these scores to the values, we typically apply a softmax over the key dimension to convert them into a probability distribution:"),"\n",i.createElement(l.A,{text:"\\[\na_{ij} = \\frac{\\exp(\\alpha_{ij})}{\\sum_{k=1}^{\\text{seqLen}_K} \\exp(\\alpha_{ik})}\n\\]"}),"\n",i.createElement(t.p,null,"These coefficients ",i.createElement(l.A,{text:"\\( a_{ij} \\)"})," are the attention weights. Intuitively, ",i.createElement(l.A,{text:"\\( a_{ij} \\)"})," measures how much attention the ",i.createElement(l.A,{text:"\\(i^\\text{th}\\)"})," query pays to the ",i.createElement(l.A,{text:"\\(j^\\text{th}\\)"})," key (and its corresponding value)."),"\n",i.createElement(t.h3,{id:"53-scaled-dot-product-attention",style:{position:"relative"}},i.createElement(t.a,{href:"#53-scaled-dot-product-attention","aria-label":"53 scaled dot product attention permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.3 Scaled dot-product attention"),"\n",i.createElement(t.p,null,"When the dimensionality ",i.createElement(l.A,{text:"\\(d\\)"})," of the query and key vectors is large, the dot-product can grow significantly in magnitude, leading to gradients that might become unstable. The scaled dot-product attention (Vaswani and gang, 2017) introduces a scaling factor of ",i.createElement(l.A,{text:"\\( 1 / \\sqrt{d} \\)"}),":"),"\n",i.createElement(l.A,{text:"\\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\Bigl(\\frac{Q K^\\top}{\\sqrt{d}}\\Bigr) V \n\\]"}),"\n",i.createElement(t.p,null,"Here, ",i.createElement(l.A,{text:"\\( d \\)"})," is the dimension of the query/key vectors. This scaling keeps the values of the logits ",i.createElement(l.A,{text:"\\(\\alpha_{ij}\\)"})," at a more moderate level and aids in better gradient flow."),"\n",i.createElement(t.h3,{id:"54-alternative-scoring-functions",style:{position:"relative"}},i.createElement(t.a,{href:"#54-alternative-scoring-functions","aria-label":"54 alternative scoring functions permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.4 Alternative scoring functions"),"\n",i.createElement(t.p,null,"In additive (Bahdanau) attention, the alignment score is computed using a small feed-forward network with a single hidden layer, typically described as:"),"\n",i.createElement(l.A,{text:"\\[\ne_{ij} = v_a^\\top \\tanh(W_q Q_i^\\top + W_k K_j^\\top)\n\\]"}),"\n",i.createElement(t.p,null,"where ",i.createElement(l.A,{text:"\\( v_a, W_q, W_k \\)"})," are learnable parameters. This approach can sometimes capture more intricate interactions between queries and keys but is more computationally expensive than a dot-product."),"\n",i.createElement(t.h3,{id:"55-gradient-flow-considerations",style:{position:"relative"}},i.createElement(t.a,{href:"#55-gradient-flow-considerations","aria-label":"55 gradient flow considerations permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.5 Gradient flow considerations"),"\n",i.createElement(t.p,null,"Attention often alleviates issues with vanishing or exploding gradients in long sequences because the gradient can flow directly through the attention weights to any part of the sequence, bypassing the recurrent path that might hamper standard RNNs. While this isn't a panacea for all optimization issues, it does help networks learn relationships that span large sections of the input or internal representations."),"\n",i.createElement(t.h2,{id:"6-implementation-details",style:{position:"relative"}},i.createElement(t.a,{href:"#6-implementation-details","aria-label":"6 implementation details permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6. Implementation details"),"\n",i.createElement(t.p,null,"Having laid out the conceptual and theoretical underpinnings, it's worth examining how to implement attention in practice. I'll focus on PyTorch and TensorFlow/Keras, as they're among the most popular deep learning frameworks. However, the concepts generalize to other libraries as well (e.g., JAX, Flax, or MXNet)."),"\n",i.createElement(t.h3,{id:"61-data-preparation-and-input-representation",style:{position:"relative"}},i.createElement(t.a,{href:"#61-data-preparation-and-input-representation","aria-label":"61 data preparation and input representation permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.1 Data preparation and input representation"),"\n",i.createElement(t.p,null,"Attention-based models generally start with some form of embedding:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Tokenization"),": For NLP tasks, we convert textual data into discrete tokens (e.g., subwords, words, or characters)."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Embedding layer"),": Transform each token into a continuous vector. Many implementations also add positional encodings or learned positional embeddings to inject information about the order of tokens."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Batching"),": Large batch sizes are possible with attention models, but we should be mindful of memory consumption, particularly for sequences of large length."),"\n"),"\n",i.createElement(t.h3,{id:"62-coding-attention-in-popular-frameworks",style:{position:"relative"}},i.createElement(t.a,{href:"#62-coding-attention-in-popular-frameworks","aria-label":"62 coding attention in popular frameworks permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.2 Coding attention in popular frameworks"),"\n",i.createElement(t.p,null,"The essential building blocks in these frameworks are a linear projection for queries, keys, and values, followed by the scaled dot-product formula. Let's show a simplified custom attention layer in PyTorch and then in TensorFlow/Keras."),"\n",i.createElement(t.h4,{id:"621-pytorch-custom-attention",style:{position:"relative"}},i.createElement(t.a,{href:"#621-pytorch-custom-attention","aria-label":"621 pytorch custom attention permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.2.1 PyTorch custom attention"),"\n",i.createElement(o.A,{text:"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SimpleAttention(nn.Module):\n    def __init__(self, d_model):\n        super(SimpleAttention, self).__init__()\n        self.d_model = d_model\n        \n        # Linear layers to transform inputs into Q, K, and V\n        self.query_layer = nn.Linear(d_model, d_model)\n        self.key_layer = nn.Linear(d_model, d_model)\n        self.value_layer = nn.Linear(d_model, d_model)\n\n    def forward(self, x):\n        # x shape: (batch_size, seq_len, d_model)\n        Q = self.query_layer(x)  # (batch_size, seq_len, d_model)\n        K = self.key_layer(x)    # (batch_size, seq_len, d_model)\n        V = self.value_layer(x)  # (batch_size, seq_len, d_model)\n\n        # Calculate attention scores: QK^T\n        # We'll do a batch matrix multiplication\n        scores = torch.matmul(Q, K.transpose(-2, -1))  # shape: (batch_size, seq_len, seq_len)\n\n        # Scale by sqrt(d_model)\n        scores = scores / (self.d_model ** 0.5)\n        \n        # Apply softmax to get the attention weights\n        attn_weights = F.softmax(scores, dim=-1)  # shape: (batch_size, seq_len, seq_len)\n\n        # Multiply weights by the values\n        out = torch.matmul(attn_weights, V)  # shape: (batch_size, seq_len, d_model)\n\n        return out, attn_weights\n"}),"\n",i.createElement(t.p,null,"In this simplistic example, I've used the same input ",i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">x</code>'}})," for queries, keys, and values (i.e., self attention). However, we could easily pass in different tensors for Q, K, and V to implement cross attention. The module returns both the output of the attention mechanism (",i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">out</code>'}}),") and the attention weight matrix (",i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">attn_weights</code>'}}),") for potential interpretability or subsequent processing."),"\n",i.createElement(t.h4,{id:"622-using-built-in-pytorch-nnmultiheadattention",style:{position:"relative"}},i.createElement(t.a,{href:"#622-using-built-in-pytorch-nnmultiheadattention","aria-label":"622 using built in pytorch nnmultiheadattention permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.2.2 Using built-in PyTorch ",i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">nn.MultiheadAttention</code>'}})),"\n",i.createElement(t.p,null,"For multi-head attention, PyTorch offers a built-in layer: ",i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">nn.MultiheadAttention</code>'}}),". It handles all the splitting into heads, projection, scaling, and concatenation under the hood. Here is a minimal usage example:"),"\n",i.createElement(o.A,{text:"\nimport torch\nimport torch.nn as nn\n\n# Suppose we have d_model=128, 8 heads\nmha = nn.MultiheadAttention(embed_dim=128, num_heads=8, batch_first=True)\n\n# Dummy input: batch_size=2, seq_len=10, d_model=128\nx = torch.rand(2, 10, 128)  # This will be our Q, K, V for self-attention\nattn_output, attn_weights = mha(x, x, x)\nprint(attn_output.shape)  # (2, 10, 128)\nprint(attn_weights.shape) # (2, 8, 10, 10)\n"}),"\n",i.createElement(t.h3,{id:"63-tensorflowkeras-custom-attention-layer",style:{position:"relative"}},i.createElement(t.a,{href:"#63-tensorflowkeras-custom-attention-layer","aria-label":"63 tensorflowkeras custom attention layer permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.3 TensorFlow/Keras custom attention layer"),"\n",i.createElement(t.p,null,"Below is a custom attention layer that can be used within a Keras model:"),"\n",i.createElement(o.A,{text:"\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nclass SimpleAttention(layers.Layer):\n    def __init__(self, d_model):\n        super(SimpleAttention, self).__init__()\n        self.d_model = d_model\n        \n        self.query_dense = layers.Dense(d_model)\n        self.key_dense = layers.Dense(d_model)\n        self.value_dense = layers.Dense(d_model)\n\n    def call(self, x):\n        # x shape: (batch_size, seq_len, d_model)\n        Q = self.query_dense(x)\n        K = self.key_dense(x)\n        V = self.value_dense(x)\n\n        # Scaled dot-product\n        scores = tf.matmul(Q, K, transpose_b=True) \n        scores = scores / tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n\n        # Softmax over the last axis\n        attn_weights = tf.nn.softmax(scores, axis=-1)\n\n        # Weighted sum\n        output = tf.matmul(attn_weights, V)\n\n        return output, attn_weights\n"}),"\n",i.createElement(t.p,null,"Like in the PyTorch example, this is a bare-bones demonstration of self attention within a single head. For multi-head attention, Keras has a built-in layer named ",i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">MultiHeadAttention</code>'}})," (from TensorFlow 2.4+), making it quite straightforward to integrate attention into a model."),"\n",i.createElement(t.h3,{id:"64-complexity-considerations",style:{position:"relative"}},i.createElement(t.a,{href:"#64-complexity-considerations","aria-label":"64 complexity considerations permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.4 Complexity considerations"),"\n",i.createElement(t.p,null,"Naive attention has a computational complexity of ",i.createElement(l.A,{text:"\\(O(n^2)\\)"})," with respect to the sequence length ",i.createElement(l.A,{text:"\\(n\\)"}),", because we compute a dot-product for every query-key pair. This is not a major issue for moderate sequence lengths, but it becomes burdensome for extremely large ",i.createElement(l.A,{text:"\\(n\\)"}),". Researchers have proposed a variety of sparse or approximate methods to address this issue (see section on advanced variations)."),"\n",i.createElement(t.h3,{id:"65-gputpu-usage",style:{position:"relative"}},i.createElement(t.a,{href:"#65-gputpu-usage","aria-label":"65 gputpu usage permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.5 GPU/TPU usage"),"\n",i.createElement(t.p,null,"Attention layers are highly parallelizable, especially self attention, which can be computed in a single matrix multiplication step for each of QK^T and subsequent operations. Modern hardware accelerators (GPUs, TPUs) significantly speed up these matrix multiplications. However, the memory usage can be substantial, since storing attention weights for a batch of sequences can consume a lot of GPU/TPU memory."),"\n",i.createElement(t.h2,{id:"7-advanced-variations",style:{position:"relative"}},i.createElement(t.a,{href:"#7-advanced-variations","aria-label":"7 advanced variations permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7. Advanced variations"),"\n",i.createElement(t.p,null,"As attention has soared in popularity, numerous extensions and refinements have been introduced, targeting everything from efficiency to interpretability."),"\n",i.createElement(t.h3,{id:"71-memory-augmented-attention",style:{position:"relative"}},i.createElement(t.a,{href:"#71-memory-augmented-attention","aria-label":"71 memory augmented attention permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.1 Memory-augmented attention"),"\n",i.createElement(t.p,null,"Some architectures incorporate an external memory bank (e.g., Neural Turing Machines or differentiable memory structures). In these designs, the attention mechanism is extended to read from and write to a large external memory, allowing the model to keep track of far more context than a standard hidden state or even multi-head attention might allow. For instance, a language model could maintain a memory of previously seen paragraphs, effectively enabling it to handle extremely long documents."),"\n",i.createElement(t.h3,{id:"72-sparse-and-efficient-attention",style:{position:"relative"}},i.createElement(t.a,{href:"#72-sparse-and-efficient-attention","aria-label":"72 sparse and efficient attention permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.2 Sparse and efficient attention"),"\n",i.createElement(t.p,null,"A major challenge for attention-based models is the quadratic complexity with respect to sequence length. Various methods address this challenge:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Sparse Transformers (Child and gang, 2019)"),": Restrict attention to certain pattern-based subsets of tokens (e.g., a local window or strided pattern)."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Longformer (Beltagy and gang, 2020)"),": Employ local windowed attention augmented with global tokens that attend to every position."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"BigBird (Zaheer and gang, 2020)"),": Combines local windowed, random, and global attention to achieve sub-quadratic complexity."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Linformer (Wang and gang, 2020)"),": Projects keys and values to a lower-dimensional representation, reducing the computational cost."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Performer (Choromanski and gang, 2021)"),": Uses kernel-based feature maps to approximate softmax attention, achieving linear time complexity under certain conditions."),"\n"),"\n",i.createElement(t.p,null,"These approaches, collectively, are driving the ability of attention models to handle contexts with lengths in the tens or hundreds of thousands of tokens."),"\n",i.createElement(t.h3,{id:"73-attention-in-graph-neural-networks",style:{position:"relative"}},i.createElement(t.a,{href:"#73-attention-in-graph-neural-networks","aria-label":"73 attention in graph neural networks permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.3 Attention in graph neural networks"),"\n",i.createElement(t.p,null,"Graph attention networks (GATs) leverage the attention paradigm to weigh the importance of neighboring nodes in a graph. Instead of a global sequence, each node attends to its neighbors, computing attention coefficients that reflect the relative importance of each neighbor's features (Velickovic and gang, 2018). This has proven particularly successful in tasks like node classification, link prediction, and even molecular property prediction."),"\n",i.createElement(t.h3,{id:"74-adaptivestructured-attention",style:{position:"relative"}},i.createElement(t.a,{href:"#74-adaptivestructured-attention","aria-label":"74 adaptivestructured attention permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.4 Adaptive/structured attention"),"\n",i.createElement(t.p,null,"Adaptive attention mechanisms can learn to prune heads or the dimension of certain attention layers dynamically, saving computation and sometimes improving generalization. Another line of work applies structured constraints (like low-rank or block-sparse constraints) to the attention patterns, aiming to reduce complexity and possibly improve interpretability."),"\n",i.createElement(t.h3,{id:"75-low-rank-factorization-approaches",style:{position:"relative"}},i.createElement(t.a,{href:"#75-low-rank-factorization-approaches","aria-label":"75 low rank factorization approaches permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.5 Low-rank factorization approaches"),"\n",i.createElement(t.p,null,"Orthogonal or low-rank factorizations of the ",i.createElement(l.A,{text:"\\(Q\\)"}),", ",i.createElement(l.A,{text:"\\(K\\)"}),", and ",i.createElement(l.A,{text:"\\(V\\)"})," matrices can significantly compress the parameters of attention layers. Such techniques can be crucial in resource-constrained settings (e.g., mobile devices) and are often used in model distillation or compression pipelines. For instance, a model can approximate the original attention matrix with a factorization that reduces memory consumption without significantly sacrificing performance."),"\n",i.createElement(t.h3,{id:"76-hybrid-attention-strategies",style:{position:"relative"}},i.createElement(t.a,{href:"#76-hybrid-attention-strategies","aria-label":"76 hybrid attention strategies permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.6 Hybrid attention strategies"),"\n",i.createElement(t.p,null,"While Transformers rely primarily on self attention, there's a growing body of research showing that combining attention with convolutional or recurrent blocks can boost performance in certain specialized tasks. For example, a hybrid model might use convolutional layers to capture local context (especially beneficial for signals with strong locality, like images or audio) while employing attention to integrate long-range dependencies."),"\n",i.createElement(t.h2,{id:"8-applications-and-case-studies",style:{position:"relative"}},i.createElement(t.a,{href:"#8-applications-and-case-studies","aria-label":"8 applications and case studies permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8. Applications and case studies"),"\n",i.createElement(t.p,null,"Attention has firmly integrated itself into a broad spectrum of tasks that span various modalities — text, speech, images, and even more structured or domain-specific data."),"\n",i.createElement(t.h3,{id:"81-machine-translation",style:{position:"relative"}},i.createElement(t.a,{href:"#81-machine-translation","aria-label":"81 machine translation permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.1 Machine translation"),"\n",i.createElement(t.p,null,"Neural machine translation (NMT) was the incubator of attention mechanisms (Bahdanau and gang, 2015). In typical attention-based NMT, the decoder attends to different parts of the source sentence for each target token it generates, enabling it to handle complicated linguistic structures and long sentences with greater ease than purely recurrent or convolution-based models. This approach has become the standard blueprint for many industrial translation systems, achieving robust improvements in translation quality and fluency."),"\n",i.createElement(t.h3,{id:"82-text-summarization",style:{position:"relative"}},i.createElement(t.a,{href:"#82-text-summarization","aria-label":"82 text summarization permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.2 Text summarization"),"\n",i.createElement(t.p,null,"Summarizing lengthy documents requires a model to identify the critical points that best represent the overall theme. Attention allows the model to weigh each segment or sentence of the source text accordingly. In abstractive summarization, the model can generate novel sentences rather than just extracting original text chunks, and attention helps it do so by focusing on the most salient content. This is particularly valuable for domains like legal texts or research articles, where clarity and conciseness are paramount."),"\n",i.createElement(t.h3,{id:"83-image-captioning",style:{position:"relative"}},i.createElement(t.a,{href:"#83-image-captioning","aria-label":"83 image captioning permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.3 Image captioning"),"\n",i.createElement(t.p,null,'In an image captioning pipeline (e.g., the Show, Attend and Tell approach by Xu and gang, 2015), the model uses attention to highlight spatial regions of an image that correspond to the words it is generating. For instance, if the model is generating the phrase "brown dog," it might attend to the portion of the image containing the dog, ignoring irrelevant backgrounds. This leads to more accurate and coherent captioning.'),"\n",i.createElement(n,{alt:"Illustration of attention maps in image captioning",path:"",caption:"Visual attention focusing on specific regions of an image during caption generation.",zoom:"false"}),"\n",i.createElement(t.h3,{id:"84-speech-recognition",style:{position:"relative"}},i.createElement(t.a,{href:"#84-speech-recognition","aria-label":"84 speech recognition permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.4 Speech recognition"),"\n",i.createElement(t.p,null,"In sequence-to-sequence speech recognition systems, the encoder processes acoustic frames and produces a hidden representation. The decoder, step by step, attends to the encoder outputs to generate phonemes or characters. Attention helps the decoder to align each output token with the correct region of the acoustic input, which can be especially important in languages with variable-length phoneme or subword structures."),"\n",i.createElement(t.h3,{id:"85-recommender-systems",style:{position:"relative"}},i.createElement(t.a,{href:"#85-recommender-systems","aria-label":"85 recommender systems permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.5 Recommender systems"),"\n",i.createElement(t.p,null,"Recent recommender system architectures have begun to employ attention for capturing user-item interactions. For instance, in a session-based recommendation scenario, each user session can be treated as a sequence of item interactions, and a self-attention model can highlight items in the session history that most strongly predict the user's next choice. This yields more context-aware recommendations."),"\n",i.createElement(t.h3,{id:"86-additional-domains",style:{position:"relative"}},i.createElement(t.a,{href:"#86-additional-domains","aria-label":"86 additional domains permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.6 Additional domains"),"\n",i.createElement(t.p,null,"Beyond these core applications, attention has found its way into time-series forecasting, question answering, knowledge graphs, and even robotics. The fundamental concept of weighting relevant context while ignoring the extraneous is broadly applicable and continues to drive innovations across ML subfields."),"\n",i.createElement(t.h2,{id:"9-future-directions-and-open-challenges",style:{position:"relative"}},i.createElement(t.a,{href:"#9-future-directions-and-open-challenges","aria-label":"9 future directions and open challenges permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9. Future directions and open challenges"),"\n",i.createElement(t.p,null,"Despite its current ubiquity, attention-based modeling is still evolving. Researchers continue to tackle new frontiers, refine existing architectures, and explore entirely new directions."),"\n",i.createElement(t.h3,{id:"91-emerging-architectures",style:{position:"relative"}},i.createElement(t.a,{href:"#91-emerging-architectures","aria-label":"91 emerging architectures permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9.1 Emerging architectures"),"\n",i.createElement(t.p,null,"Proposals like Performer, Linformer, and Reformer illustrate a vigorous push toward more efficient architectures that can handle extremely long sequences without succumbing to quadratic complexity. Moreover, ",i.createElement(r.A,null,"Mixture of Experts"),' strategies have been integrated into Transformer backbones, distributing computations across multiple "expert" subnetworks to handle specialized tasks. Some future attention architectures may incorporate advanced forms of reasoning or external knowledge bases, further pushing the boundaries of what deep models can achieve.'),"\n",i.createElement(t.h3,{id:"92-long-context-attention-improvements",style:{position:"relative"}},i.createElement(t.a,{href:"#92-long-context-attention-improvements","aria-label":"92 long context attention improvements permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9.2 Long-context attention improvements"),"\n",i.createElement(t.p,null,"As models scale to thousands or even tens of thousands of tokens, attention patterns and memory usage become pressing concerns. Sparse, block-sparse, or kernel-based approximate methods appear poised to make these large contexts feasible. There is also excitement around hierarchical or chunk-based approaches, in which attention is computed locally within segments and then aggregated at higher levels."),"\n",i.createElement(t.h3,{id:"93-interpretability-and-explainability",style:{position:"relative"}},i.createElement(t.a,{href:"#93-interpretability-and-explainability","aria-label":"93 interpretability and explainability permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9.3 Interpretability and explainability"),"\n",i.createElement(t.p,null,'While attention weights might naively be interpreted as "explanations," researchers (e.g., Jain and Wallace, 2019) have noted that these weights do not necessarily correlate with the model\'s overall decision-making process. A deeper understanding of how attention interacts with other components of the network is crucial for building trustworthy AI systems. Future directions may involve coupling attention with causal interpretability frameworks or combining attention with model-agnostic explanation methods.'),"\n",i.createElement(t.h3,{id:"94-ethical-considerations",style:{position:"relative"}},i.createElement(t.a,{href:"#94-ethical-considerations","aria-label":"94 ethical considerations permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9.4 Ethical considerations"),"\n",i.createElement(t.p,null,"Large attention-based models (like GPT-like architectures) have raised issues around bias, fairness, and the carbon footprint of training at scale. Bias can be introduced by training data — if the data contain social biases, the attention model can inadvertently amplify them. Likewise, the computational resources required for large models have an environmental impact. Responsible research and deployment involve addressing these concerns through careful data curation, algorithmic debiasing, and energy-efficient model design."),"\n",i.createElement(t.h3,{id:"95-resource-constraints-and-model-deployment",style:{position:"relative"}},i.createElement(t.a,{href:"#95-resource-constraints-and-model-deployment","aria-label":"95 resource constraints and model deployment permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9.5 Resource constraints and model deployment"),"\n",i.createElement(t.p,null,"Although large Transformers are extremely powerful, not every application can justify the requisite compute resources. Many practitioners are investigating compression techniques (quantization, pruning, knowledge distillation, etc.) to bring attention-based models down to a practical size for on-device or embedded deployment. There is also an active community examining how to deploy Transformers efficiently on server clusters, incorporating advanced scheduling or parallelization strategies to reduce latency and cost."),"\n",i.createElement(t.hr),"\n",i.createElement(t.p,null,"Given all that, it's no wonder the attention mechanism has become a foundational pillar of modern machine learning. By actively focusing on relevant parts of the data, it addresses key bottlenecks that previously hindered many tasks. From breakthroughs in machine translation to the unstoppable rise of Transformer-based large language models, attention is at the heart of numerous state-of-the-art systems. Going forward, I expect we will continue to see the refinement and diversification of attention paradigms, combining the best of concurrency, interpretability, efficiency, and scale to tackle even more challenging tasks."),"\n",i.createElement(t.p,null,"I encourage those with a background in simpler models — like standard RNNs or CNNs — to explore attention-based methods in their workflows. Whether you are building a new text classifier, an image captioning system, or a real-time speech recognizer, attention mechanisms can provide a powerful upgrade in both performance and expressiveness. With the rapid growth of publicly available codebases and pre-trained models, it's easier than ever to get hands-on with attention. Indeed, \"attention\" deserves your attention if you're not already using it in your machine learning practice."))}var c=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,a.RP)(),e.components);return t?i.createElement(t,e,i.createElement(s,e)):s(e)};var h=n(54506),d=n(88864),m=n(58481),u=n.n(m),p=n(5984),g=n(43672),f=n(27042),v=n(72031),y=n(81817),b=n(27105),w=n(17265),E=n(2043),S=n(95751),x=n(94328),k=n(80791),H=n(78137);const _=e=>{let{toc:t}=e;if(!t||!t.items)return null;return i.createElement("nav",{className:k.R},i.createElement("ul",null,t.items.map(((e,t)=>i.createElement("li",{key:t},i.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const n=t.replace("#",""),a=document.getElementById(n);a&&a.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&i.createElement(_,{toc:{items:e.items}}))))))};function T(e){let{data:{mdx:t,allMdx:r,allPostImages:o},children:l}=e;const{frontmatter:s,body:c,tableOfContents:d}=t,m=s.index,v=s.slug.split("/")[1],k=r.nodes.filter((e=>e.frontmatter.slug.includes(`/${v}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),T=k.findIndex((e=>e.frontmatter.index===m)),z=k[T+1],M=k[T-1],C=s.slug.replace(/\/$/,""),q=/[^/]*$/.exec(C)[0],V=`posts/${v}/content/${q}/`,{0:I,1:A}=(0,i.useState)(s.flagWideLayoutByDefault),{0:L,1:N}=(0,i.useState)(!1);var B;(0,i.useEffect)((()=>{N(!0);const e=setTimeout((()=>N(!1)),340);return()=>clearTimeout(e)}),[I]),"adventures"===v?B=w.cb:"research"===v?B=w.Qh:"thoughts"===v&&(B=w.T6);const P=u()(c).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,j=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),n=e%60;return n<=30?`~${t}${n>0?".5":""} h`:`~${t+1} h`}(Math.ceil(P/B)+(s.extraReadTimeMin||0)),K=[{flag:s.flagDraft,component:()=>Promise.all([n.e(5850),n.e(9833)]).then(n.bind(n,49833))},{flag:s.flagMindfuckery,component:()=>Promise.all([n.e(5850),n.e(7805)]).then(n.bind(n,27805))},{flag:s.flagRewrite,component:()=>Promise.all([n.e(5850),n.e(8916)]).then(n.bind(n,78916))},{flag:s.flagOffensive,component:()=>Promise.all([n.e(5850),n.e(6731)]).then(n.bind(n,49112))},{flag:s.flagProfane,component:()=>Promise.all([n.e(5850),n.e(3336)]).then(n.bind(n,83336))},{flag:s.flagMultilingual,component:()=>Promise.all([n.e(5850),n.e(2343)]).then(n.bind(n,62343))},{flag:s.flagUnreliably,component:()=>Promise.all([n.e(5850),n.e(6865)]).then(n.bind(n,11627))},{flag:s.flagPolitical,component:()=>Promise.all([n.e(5850),n.e(4417)]).then(n.bind(n,24417))},{flag:s.flagCognitohazard,component:()=>Promise.all([n.e(5850),n.e(8669)]).then(n.bind(n,18669))},{flag:s.flagHidden,component:()=>Promise.all([n.e(5850),n.e(8124)]).then(n.bind(n,48124))}],{0:F,1:R}=(0,i.useState)([]);return(0,i.useEffect)((()=>{K.forEach((e=>{let{flag:t,component:n}=e;t&&n().then((e=>{R((t=>[].concat((0,h.A)(t),[e.default])))}))}))}),[]),i.createElement(f.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},i.createElement(y.A,{postNumber:s.index,date:s.date,updated:s.updated,readTime:j,difficulty:s.difficultyLevel,title:s.title,desc:s.desc,banner:s.banner,section:v,postKey:q,isMindfuckery:s.flagMindfuckery,mainTag:s.mainTag}),i.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},s.otherTags.map(((e,t)=>i.createElement("span",{key:t,className:`noselect ${H.MW}`,style:{margin:"0 5px 5px 0"}},e)))),i.createElement("div",{className:"postBody"},i.createElement(_,{toc:d})),i.createElement("br",null),i.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},i.createElement(f.P.button,{className:`noselect ${x.pb}`,id:x.xG,onClick:()=>{A(!I)},whileTap:{scale:.93}},i.createElement(f.P.div,{className:S.DJ,key:I,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},I?"Switch to default layout":"Switch to wide layout"))),i.createElement("br",null),i.createElement("div",{className:"postBody",style:{margin:I?"0 -14%":"",maxWidth:I?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},i.createElement("div",{className:`${x.P_} ${L?x.Xn:x.qG}`},F.map(((e,t)=>i.createElement(e,{key:t}))),s.indexCourse?i.createElement(E.A,{index:s.indexCourse,category:s.courseCategoryName}):"",i.createElement(p.Z.Provider,{value:{images:o.nodes,basePath:V.replace(/\/$/,"")+"/"}},i.createElement(a.xA,{components:{Image:g.A}},l)))),i.createElement(b.A,{nextPost:z,lastPost:M,keyCurrent:q,section:v}))}function z(e){return i.createElement(T,e,i.createElement(c,e))}function M(e){var t,n,a,r,o;let{data:l}=e;const{frontmatter:s}=l.mdx,c=s.titleSEO||s.title,h=s.titleOG||c,m=s.titleTwitter||c,u=s.descSEO||s.desc,p=s.descOG||u,g=s.descTwitter||u,f=s.schemaType||"BlogPosting",y=s.keywordsSEO,b=s.date,w=s.updated||b,E=s.imageOG||(null===(t=s.banner)||void 0===t||null===(n=t.childImageSharp)||void 0===n||null===(a=n.gatsbyImageData)||void 0===a||null===(r=a.images)||void 0===r||null===(o=r.fallback)||void 0===o?void 0:o.src),S=s.imageAltOG||p,x=s.imageTwitter||E,k=s.imageAltTwitter||g,H=s.canonicalURL,_=s.flagHidden||!1,T=s.mainTag||"Posts",z=s.slug.split("/")[1]||"posts",{siteUrl:M}=(0,d.Q)(),C={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:M},{"@type":"ListItem",position:2,name:T,item:`${M}/${s.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${M}${s.slug}`}]};return i.createElement(v.A,{title:c+" - avrtt.blog",titleOG:h,titleTwitter:m,description:u,descriptionOG:p,descriptionTwitter:g,schemaType:f,keywords:y,datePublished:b,dateModified:w,imageOG:E,imageAltOG:S,imageTwitter:x,imageAltTwitter:k,canonicalUrl:H,flagHidden:_,mainTag:T,section:z,type:"article"},i.createElement("script",{type:"application/ld+json"},JSON.stringify(C)))}},90548:function(e,t,n){var a=n(96540),i=n(7978);t.A=e=>{let{text:t}=e;return a.createElement(i.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-tsx-content-file-path-src-pages-posts-research-attention-mechanism-mdx-87bdcbe35eb099835484.js.map