"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[5841],{42516:function(e,t,n){n.r(t),n.d(t,{Head:function(){return z},PostTemplate:function(){return T},default:function(){return L}});var a=n(54506),l=n(28453),r=n(96540),i=n(16886),s=(n(46295),n(96098));function o(e){const t=Object.assign({p:"p",ol:"ol",li:"li",strong:"strong",h3:"h3",a:"a",span:"span",ul:"ul",h2:"h2",h4:"h4"},(0,l.RP)(),e.components),{Image:n}=t;return n||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),r.createElement(r.Fragment,null,"\n",r.createElement("br"),"\n","\n","\n","\n",r.createElement(t.p,null,"Natural language processing (NLP) is a field at the intersection of ",r.createElement(i.A,null,"machine learning"),' and mathematical linguistics, dedicated to the analysis and generation of text and speech in human language. At its heart, NLP combines sophisticated algorithmic techniques with linguistic theory to enable computers to "understand" or manipulate human language input in meaningful ways. Over the last few decades, NLP has become a cornerstone in modern data science and artificial intelligence, serving as a foundational approach for tasks like automated translation, speech recognition, sentiment analysis, topic modeling, dialogue systems, and more.'),"\n",r.createElement(t.p,null,"Modern NLP methods are employed in a wide spectrum of applications — from personal voice assistants that transcribe and interpret spoken commands, to advanced text classification systems that filter spam or identify offensive content, to large-scale language models that generate coherent text or answer complex questions."),"\n",r.createElement(t.p,null,"The historical evolution of NLP reflects a tension between two broad paradigms:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Rule-based")," or symbolic approaches, which rely on handcrafted rules about language structure, syntax, and semantics."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Statistical")," or machine-learning-based approaches, which derive patterns from vast corpora of textual data."),"\n"),"\n",r.createElement(t.p,null,"Today, the pendulum has swung decisively toward deep learning frameworks that rely on large pretrained models, massive textual corpora, and high-performance hardware. However, both symbolic insights and earlier statistical methods remain invaluable, especially in certain specialized tasks or resource-constrained domains."),"\n",r.createElement(t.h3,{id:"definition-of-nlp",style:{position:"relative"}},r.createElement(t.a,{href:"#definition-of-nlp","aria-label":"definition of nlp permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Definition of NLP"),"\n",r.createElement(t.p,null,"At a high level, NLP can be described as:"),"\n",r.createElement(i.A,null,r.createElement(t.p,null,'"The set of computational techniques aimed at analyzing, understanding, and generating texts (or speech) in natural language."')),"\n",r.createElement(t.p,null,'By "natural language," we typically mean human languages as opposed to formal languages like programming languages or mathematical notation. This broad definition encompasses the subfields of:'),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Speech recognition"),", or how to transform raw audio signals into text."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Natural language understanding"),", or how to parse, interpret, and reason about the meaning behind text."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Natural language generation"),", or how to generate text in ways that reflect human-like fluency and coherence."),"\n"),"\n",r.createElement(t.h3,{id:"importance-in-data-science",style:{position:"relative"}},r.createElement(t.a,{href:"#importance-in-data-science","aria-label":"importance in data science permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Importance in data science"),"\n",r.createElement(t.p,null,"NLP is crucial in modern data science for a variety of reasons:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Text classification")," (e.g., spam detection, sentiment analysis, or topic labeling): Many real-world datasets include unstructured textual data, and classifying these at scale can reveal valuable insights."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Sentiment analysis"),": Businesses and researchers routinely extract sentiments and opinions from social media data, customer reviews, and forums to assess product reception or user experience."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Machine translation"),": Globalization has intensified the need for robust translation systems, which rely heavily on NLP. Neural machine translation has improved rapidly, showcasing how advanced NLP can facilitate cross-lingual communication."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Voice assistants"),": Virtual assistants (e.g., Alexa, Siri, Google Assistant) parse natural language commands, respond to user queries, and sometimes even maintain short dialogues."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Automated text filtering and content moderation"),": As online content grows exponentially, automated systems for filtering offensive or harmful text become indispensable."),"\n"),"\n",r.createElement(t.h3,{id:"historical-milestones",style:{position:"relative"}},r.createElement(t.a,{href:"#historical-milestones","aria-label":"historical milestones permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Historical milestones"),"\n",r.createElement(t.p,null,"NLP has undergone multiple transformations in its short but vibrant history:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"1950s — 1960s: The dawn of symbolic AI"),". Early systems (like ELIZA, created by Joseph Weizenbaum in the mid-1960s) used pattern matching and rule-based heuristics. Researchers pursued fully symbolic approaches: they attempted to encode grammar rules and dictionary definitions by hand."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"1970s — 1980s: Rule-based and knowledge-based methods"),". In these decades, symbolic approaches and expert systems were still popular. Researchers built vast knowledge bases, manually encoding domain-specific rules. However, language's inherent ambiguity and complexity created difficulties in scaling or generalizing these methods."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"1990s — early 2000s: Emergence of statistical NLP"),". With increased computing power and availability of large corpora, researchers began applying Bayesian models, Hidden Markov Models (HMMs), and other statistical approaches to tasks such as part-of-speech tagging, syntactic parsing, and named entity recognition (NER)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Early 2010s: Neural networks"),". The rise of deep learning transformed how we approach NLP. Word embeddings such as Word2Vec (Mikolov and gang) became popular, revealing how neural networks capture semantic nuances in vector spaces."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Mid–late 2010s: Sequence models and attention"),". Recurrent neural networks (RNNs), especially LSTM (Long Short-Term Memory) networks, dominated tasks like machine translation, only to be superseded by attention-based architectures such as the Transformer (Vaswani and gang)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Late 2010s — 2020s: Large-scale pretrained language models"),". Transformer-based models like BERT (Devlin and gang), GPT (OpenAI), and T5 (Raffel and gang) significantly advanced state-of-the-art performance in many NLP tasks, sometimes even surpassing human benchmarks in carefully controlled tasks. Today, these large language models often form the backbone of production-level NLP systems."),"\n"),"\n",r.createElement(t.h3,{id:"main-nlp-tasks",style:{position:"relative"}},r.createElement(t.a,{href:"#main-nlp-tasks","aria-label":"main nlp tasks permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Main NLP tasks"),"\n",r.createElement(t.p,null,"There are numerous subfields within NLP, but commonly cited tasks include:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Speech recognition"),": Converting spoken audio into textual form."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Text synthesis (text-to-speech)"),": Generating spoken output from text."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Morphological analysis"),": Understanding word forms, inflections, and morphological features such as tense, case, gender."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Tokenization"),": Splitting text into smaller units (tokens), such as words or subwords."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Part-of-speech (POS) tagging"),": Labeling words in a sentence with their grammatical role (noun, verb, etc.)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Named entity recognition (NER)"),": Identifying references to entities like persons, organizations, locations."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Syntactic parsing"),": Constructing a syntactic tree or dependency graph for a sentence."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Topic analysis"),": Grouping documents or text segments into broad thematic categories."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Machine translation"),": Translating from one language to another using rule-based, statistical, or neural methods."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Question answering (QA)"),": Automatically answering user queries based on knowledge bases or relevant documents."),"\n"),"\n",r.createElement(t.h3,{id:"core-definitions-eg-corpora",style:{position:"relative"}},r.createElement(t.a,{href:"#core-definitions-eg-corpora","aria-label":"core definitions eg corpora permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Core definitions (e.g., corpora)"),"\n",r.createElement(t.p,null,"A fundamental resource for all NLP tasks is the ",r.createElement(i.A,null,"corpus"),", defined as a systematically collected body of text that often comes with specific processing and annotation rules. Corpora are integral because they serve as the data foundation for both training and evaluating NLP models. Examples range from small curated text sets to massive web-scraped corpora containing billions of tokens."),"\n",r.createElement(t.p,null,"Because so many advanced methods rely on large training datasets, the availability and quality of corpora frequently determine how effective an NLP model can be in practice."),"\n",r.createElement(t.h2,{id:"nlp-fundamentals-text-preprocessing-and-morphological-analysis",style:{position:"relative"}},r.createElement(t.a,{href:"#nlp-fundamentals-text-preprocessing-and-morphological-analysis","aria-label":"nlp fundamentals text preprocessing and morphological analysis permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"NLP fundamentals: text preprocessing and morphological analysis"),"\n",r.createElement(t.p,null,'Text preprocessing and morphological analysis are critical first steps for any downstream NLP pipeline. Even cutting-edge neural architectures benefit from well-prepared, consistent input. The general objective of preprocessing is to "clean" and normalize textual input, remove extraneous information (like excessive punctuation or HTML tags), handle morphological variations, and split text into segments that are easier for algorithms to process.'),"\n",r.createElement(t.h3,{id:"text-preprocessing-steps",style:{position:"relative"}},r.createElement(t.a,{href:"#text-preprocessing-steps","aria-label":"text preprocessing steps permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Text preprocessing steps"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Case normalization"),": For many tasks, it is standard to convert all letters to lowercase (or uppercase) to reduce vocabulary size. However, caution is advised: in certain tasks (like NER), uppercase letters can carry vital information (e.g., the presence of initial capital letters for proper nouns)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Digit handling"),": Either removing or mapping digits to a placeholder (like ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">&lt;num></code>'}}),") is common, especially if numeric values do not convey significant meaning. In some contexts, though, preserving numbers is essential (e.g., in financial or scientific texts)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Punctuation removal"),": Often, punctuation marks are stripped or replaced with special tokens. But tasks like sentiment analysis or question answering might need punctuation as signals for sentiment or question boundaries."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Whitespace trimming"),": Collapsing consecutive whitespace characters into a single space is typically performed to reduce noise."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Basic noise cleaning"),": Removing URLs, HTML tags, emojis, or special characters can help if these do not directly contribute to the learning objective."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Language-specific expansions"),': In English, for instance, mapping contractions (e.g., "don\'t") to expanded forms ("do not") can help unify forms that otherwise appear distinct in a model\'s vocabulary.'),"\n"),"\n",r.createElement(t.p,null,"A minimal example in Python might look like:"),"\n",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:"<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">&lt;Code text={`\nimport re\n\ndef basic_preprocess(text):\n    # Convert to lowercase\n    text = text.lower()\n    # Remove URLs\n    text = re.sub(r'https?://\\\\S+|www\\\\.\\\\S+', '', text)\n    # Remove punctuation\n    text = re.sub(r'[\\\\p{P}+]', '', text)\n    # Remove extra whitespace\n    text = re.sub(r'\\\\s+', ' ', text).strip()\n    return text\n\nsample = \"Check out https://example.com! It's amazing, right??\"\nprint(basic_preprocess(sample))\n`}/></code></pre></div>"}}),"\n",r.createElement(t.h3,{id:"tokenization",style:{position:"relative"}},r.createElement(t.a,{href:"#tokenization","aria-label":"tokenization permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Tokenization"),"\n",r.createElement(t.p,null,"Tokenization is the process of segmenting text into smaller units called tokens. These tokens may be words, subwords, or even individual characters, depending on the approach. Tokenization is essential because it transforms raw text into discrete elements that can be mapped to embeddings or processed by machine learning algorithms."),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Rule-based tokenization"),': Relies on whitespace and punctuation. Quick to implement but can struggle with languages lacking whitespace-delimited words (e.g., Chinese) or with contractions (e.g., "I\'m").'),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Regex-based tokenization"),": Leverages regular expressions to handle more complex patterns."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Subword tokenization")," (e.g., Byte Pair Encoding, WordPiece): Splits rare or unknown words into smaller units, improving handling of morphological or lexical variety."),"\n"),"\n",r.createElement(t.p,null,"For instance, using the ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">nltk</code>'}})," library:"),"\n",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">&lt;Code text={`\nimport nltk\nfrom nltk.tokenize import word_tokenize\n\ntext = "In this sentence, we have many words, let\'s split them!"\ntokens = word_tokenize(text)\nprint(tokens)\n`}/></code></pre></div>'}}),"\n",r.createElement(t.h3,{id:"stop-word-removal",style:{position:"relative"}},r.createElement(t.a,{href:"#stop-word-removal","aria-label":"stop word removal permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Stop word removal"),"\n",r.createElement(t.p,null,'Many words, such as "the," "and," "or," appear extremely frequently but carry relatively little semantic content. Removing them can simplify models like TF-IDF or bag-of-words. However, in advanced contexts, discarding stop words outright may remove relevant context (especially in tasks requiring an understanding of function words).'),"\n",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">&lt;Code text={`\nfrom nltk.corpus import stopwords\n\nstop_words = set(stopwords.words(\'english\'))\ntokens_no_stops = [w for w in tokens if w.lower() not in stop_words]\nprint(tokens_no_stops)\n`}/></code></pre></div>'}}),"\n",r.createElement(t.h3,{id:"stemming-and-lemmatization",style:{position:"relative"}},r.createElement(t.a,{href:"#stemming-and-lemmatization","aria-label":"stemming and lemmatization permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Stemming and lemmatization"),"\n",r.createElement(t.p,null,"Morphological variations of words can increase the size and sparsity of a vocabulary. Two important strategies to address this:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Stemming"),': Reduces words to their "stem" by chopping off word endings using heuristics. (Example: "crying" → "cri" or "cry.")'),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Lemmatization"),': Maps words to their dictionary form (lemma) using morphological analysis (e.g., POS tags). "Came" with part-of-speech as a verb → lemma is "come."'),"\n"),"\n",r.createElement(t.p,null,"Popular stemmers include Porter, Snowball, and Lancaster; a popular lemmatizer is WordNetLemmatizer (English). For languages with richer morphology (e.g., Russian), specialized tools like ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">pymorphy2</code>'}})," are employed."),"\n",r.createElement(t.h3,{id:"morphological-analysis",style:{position:"relative"}},r.createElement(t.a,{href:"#morphological-analysis","aria-label":"morphological analysis permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Morphological analysis"),"\n",r.createElement(t.p,null,"Languages differ in their morphological complexity. English uses relatively simple morphological inflections compared to Slavic languages (like Russian or Polish). ",r.createElement(i.A,null,"Morphological analyzers")," interpret words by assigning grammatical features such as part of speech, case, gender, number, or tense."),"\n",r.createElement(t.p,null,"In Russian, for instance, libraries like ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">pymorphy2</code>'}})," can handle declensions and conjugations:"),"\n",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">&lt;Code text={`\nimport pymorphy2\n\nmorph = pymorphy2.MorphAnalyzer()\nparsed_word = morph.parse("словами")[0]\nprint(parsed_word.normal_form, parsed_word.tag.POS)\n`}/></code></pre></div>'}}),"\n",r.createElement(t.p,null,"In languages with rich morphology, morphological analysis is highly beneficial for tasks like entity recognition or topic modeling, where naive tokenization alone can miss crucial morphological signals."),"\n",r.createElement(t.h3,{id:"pos-tagging",style:{position:"relative"}},r.createElement(t.a,{href:"#pos-tagging","aria-label":"pos tagging permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"POS tagging"),"\n",r.createElement(t.p,null,"Part-of-speech tagging assigns each word its grammatical category (noun, verb, adjective, etc.). Traditional POS tagging approaches rely on:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Rule-based methods"),": A set of handcrafted rules."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Stochastic methods"),": Counting how frequently words appear with certain tags (and bigram or trigram transitions)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Hidden Markov models (HMMs)"),": Modeling tags as hidden states and words as emissions."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Neural approaches"),": Leveraging deep architectures (e.g., BiLSTM + CRF layers) for state-of-the-art performance."),"\n"),"\n",r.createElement(t.p,null,"POS tagging is fundamental: it provides structure and disambiguation for many advanced tasks, including syntactic parsing, named entity recognition, and more."),"\n",r.createElement(t.h3,{id:"deduplication",style:{position:"relative"}},r.createElement(t.a,{href:"#deduplication","aria-label":"deduplication permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Deduplication"),"\n",r.createElement(t.p,null,"When dealing with large corpora, near-duplicate documents can inflate dataset sizes and bias model training. One solution is to leverage a similarity measure (e.g., cosine similarity on TF-IDF vectors) to detect duplicates, though at scale this can become computationally expensive. Locality-sensitive hashing (LSH) can accelerate such comparisons by hashing semantically similar documents into the same bucket, reducing pairwise checks."),"\n",r.createElement(t.h2,{id:"word-embeddings--factor-analysis",style:{position:"relative"}},r.createElement(t.a,{href:"#word-embeddings--factor-analysis","aria-label":"word embeddings  factor analysis permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Word embeddings & factor analysis"),"\n",r.createElement(t.p,null,"Moving from preprocessing to feature extraction, the next step in many NLP pipelines is to represent text in a form suitable for numerical machine learning algorithms. Historically, bag-of-words and TF-IDF have been popular. However, these lose word order information and cannot capture synonyms or semantic relationships well. Modern NLP relies on ",r.createElement(i.A,null,"word embeddings"),": distributed vector representations that place semantically similar words close together in a high-dimensional space."),"\n",r.createElement(t.h3,{id:"feature-extraction-techniques",style:{position:"relative"}},r.createElement(t.a,{href:"#feature-extraction-techniques","aria-label":"feature extraction techniques permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Feature extraction techniques"),"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"Bag-of-Words (BoW)"),": Each document is represented as a vector over the vocabulary, storing the counts (or frequencies) of words present. Simple but discards word order entirely."),"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"TF-IDF")," (",r.createElement(s.A,{text:"\\( \\text{term frequency – inverse document frequency} \\)"}),"): Weights each word's importance by how often it appears in a particular document (TF) and how rare it is across the entire corpus (IDF). Words that appear in many documents get reduced weight."),"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"n-grams"),": Instead of unigrams (single words), n-grams capture sequences of length ",r.createElement(s.A,{text:"\\(n\\)"}),". A bigram approach, for example, keeps pairs of consecutive words, partly addressing the limitation of bag-of-words by incorporating local context."),"\n",r.createElement(t.h3,{id:"factor-analysis-for-text-data",style:{position:"relative"}},r.createElement(t.a,{href:"#factor-analysis-for-text-data","aria-label":"factor analysis for text data permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Factor analysis for text data"),"\n",r.createElement(t.p,null,"Traditional factor analysis and principal component analysis (PCA) can be applied to text data to reduce dimensionality. When we build a term-document matrix ",r.createElement(s.A,{text:"\\( M \\)"})," of shape ",r.createElement(s.A,{text:"\\( (\\text{documents} \\times \\text{vocabulary}) \\)"}),", we can apply SVD or other factorization techniques to discover latent semantic factors. This is at the heart of Latent Semantic Analysis (LSA)."),"\n",r.createElement(t.h3,{id:"word-embeddings",style:{position:"relative"}},r.createElement(t.a,{href:"#word-embeddings","aria-label":"word embeddings permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Word embeddings"),"\n",r.createElement(t.p,null,"In more advanced approaches, each word is embedded into a vector space, typically of dimension 50 to 1000 (depending on the model). Some influential methods:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Word2Vec")," (Mikolov and gang, Google): Learns embeddings using either skip-gram (predict context from a target word) or CBOW (predict a target word from its context)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"GloVe")," (Pennington and gang, Stanford): Uses aggregated global word-word co-occurrence statistics to learn embeddings."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"FastText")," (Bojanowski and gang): Extends Word2Vec by incorporating subword information, beneficial for handling rare words or morphological variations."),"\n"),"\n",r.createElement(t.h4,{id:"word2vec-fundamentals",style:{position:"relative"}},r.createElement(t.a,{href:"#word2vec-fundamentals","aria-label":"word2vec fundamentals permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Word2Vec fundamentals"),"\n",r.createElement(t.p,null,"The skip-gram model tries to predict context words given a center word. Formally, let ",r.createElement(s.A,{text:"\\( w_{t} \\)"})," be the center word at position ",r.createElement(s.A,{text:"\\( t \\)"})," in a sequence, and let ",r.createElement(s.A,{text:"\\( w_{t+j} \\)"})," be a context word (where ",r.createElement(s.A,{text:"\\(j\\)"})," is the offset within a window). The skip-gram training objective is to maximize:"),"\n",r.createElement(s.A,{text:"\\[\n\\sum_{t=1}^{T} \\sum_{-c \\leq j \\leq c, j \\neq 0} \\log p(w_{t+j} \\mid w_t)\n\\]"}),"\n",r.createElement(t.p,null,"where ",r.createElement(s.A,{text:"\\( p(w_{t+j} \\mid w_t) \\)"})," is modeled via a neural network that learns embeddings. Each variable:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(s.A,{text:"\\( T \\)"})," is the total number of words in the corpus."),"\n",r.createElement(t.li,null,r.createElement(s.A,{text:"\\( c \\)"})," is the window size (e.g., 5)."),"\n",r.createElement(t.li,null,r.createElement(s.A,{text:"\\( w_{t} \\)"})," is the input (center) word at position ",r.createElement(s.A,{text:"\\( t \\)"}),"."),"\n",r.createElement(t.li,null,r.createElement(s.A,{text:"\\( w_{t+j} \\)"})," is a context word within the window around ",r.createElement(s.A,{text:"\\( t \\)"}),"."),"\n"),"\n",r.createElement(t.p,null,'The result is that semantically related words (like "dog" and "cat") end up close together in the embedding space.'),"\n",r.createElement(t.h2,{id:"advanced-transformations-lsa-plsa-and-glsa",style:{position:"relative"}},r.createElement(t.a,{href:"#advanced-transformations-lsa-plsa-and-glsa","aria-label":"advanced transformations lsa plsa and glsa permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Advanced transformations: LSA, pLSA, and GLSA"),"\n",r.createElement(t.p,null,"Beyond simple embeddings, researchers have explored more sophisticated factorization methods to identify latent topics or semantic aspects in text."),"\n",r.createElement(t.h3,{id:"latent-semantic-analysis-lsa",style:{position:"relative"}},r.createElement(t.a,{href:"#latent-semantic-analysis-lsa","aria-label":"latent semantic analysis lsa permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Latent semantic analysis (LSA)"),"\n",r.createElement(t.p,null,"LSA uses singular value decomposition (SVD) on a term-document matrix to discover latent semantic dimensions. Let ",r.createElement(s.A,{text:"\\( X \\)"})," be the matrix where each row is a term and each column is a document. SVD decomposes ",r.createElement(s.A,{text:"\\( X \\)"})," as:"),"\n",r.createElement(s.A,{text:"\\[\nX = U \\Sigma V^T\n\\]"}),"\n",r.createElement(t.p,null,"where:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(s.A,{text:"\\( U \\)"})," is an orthonormal matrix of dimension ",r.createElement(s.A,{text:"\\( (\\text{terms} \\times r) \\)"}),"."),"\n",r.createElement(t.li,null,r.createElement(s.A,{text:"\\( \\Sigma \\)"})," is a diagonal matrix of singular values (sorted descending)."),"\n",r.createElement(t.li,null,r.createElement(s.A,{text:"\\( V \\)"})," is an orthonormal matrix of dimension ",r.createElement(s.A,{text:"\\( (\\text{documents} \\times r) \\)"}),"."),"\n",r.createElement(t.li,null,r.createElement(s.A,{text:"\\( r \\)"})," is the rank of ",r.createElement(s.A,{text:"\\( X \\)"})," or a chosen lower dimension if we truncate the SVD."),"\n"),"\n",r.createElement(t.p,null,"Truncating to the top ",r.createElement(s.A,{text:"\\( k \\)"})," singular values yields a low-dimensional representation capturing the most important semantic relationships."),"\n",r.createElement(t.h3,{id:"probabilistic-lsa-plsa",style:{position:"relative"}},r.createElement(t.a,{href:"#probabilistic-lsa-plsa","aria-label":"probabilistic lsa plsa permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Probabilistic LSA (pLSA)"),"\n",r.createElement(t.p,null,"Probabilistic latent semantic analysis reinterprets LSA with a probabilistic model. We assume each word ",r.createElement(s.A,{text:"\\( w \\)"})," in a document ",r.createElement(s.A,{text:"\\( d \\)"})," is generated by a latent topic ",r.createElement(s.A,{text:"\\( z \\)"}),". So:"),"\n",r.createElement(s.A,{text:"\\[\np(w \\mid d) = \\sum_{z \\in Z} p(w \\mid z)p(z \\mid d)\n\\]"}),"\n",r.createElement(t.p,null,"where:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(s.A,{text:"\\( p(z \\mid d) \\)"})," is a per-document distribution over topics."),"\n",r.createElement(t.li,null,r.createElement(s.A,{text:"\\( p(w \\mid z) \\)"})," is a per-topic distribution over words."),"\n"),"\n",r.createElement(t.p,null,"We learn ",r.createElement(s.A,{text:"\\( p(w \\mid z) \\)"})," and ",r.createElement(s.A,{text:"\\( p(z \\mid d) \\)"})," using the EM algorithm. pLSA underlies many topic modeling approaches, although more advanced Bayesian variants (e.g., Latent Dirichlet Allocation, LDA) often surpass pLSA in real-world tasks."),"\n",r.createElement(t.h3,{id:"glsa-techniques",style:{position:"relative"}},r.createElement(t.a,{href:"#glsa-techniques","aria-label":"glsa techniques permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"GLSA techniques"),"\n",r.createElement(t.p,null,"Generalized Latent Semantic Analysis (GLSA) extends LSA by incorporating more advanced weighting or additional external resources like lexical databases or morphological analyzers. These advanced factorization approaches often preserve more nuanced relationships, especially in languages where morphological or syntactic cues are critical."),"\n",r.createElement(t.h3,{id:"relationship-to-factor-analysis",style:{position:"relative"}},r.createElement(t.a,{href:"#relationship-to-factor-analysis","aria-label":"relationship to factor analysis permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Relationship to factor analysis"),"\n",r.createElement(t.p,null,"LSA and pLSA are direct analogs to classical factorization methods. Instead of analyzing correlations in numeric data, we treat word-document co-occurrence or term frequency as the basis for discovering latent factors. This conceptual link has spurred widespread use of matrix factorization or neural factorization in modern NLP pipelines, especially for tasks like topic modeling or text clustering."),"\n",r.createElement(t.h2,{id:"model-architectures-seq2seq-attention-and-positional-encoding",style:{position:"relative"}},r.createElement(t.a,{href:"#model-architectures-seq2seq-attention-and-positional-encoding","aria-label":"model architectures seq2seq attention and positional encoding permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Model architectures: Seq2Seq, attention, and positional encoding"),"\n",r.createElement(t.p,null,"In earlier decades, natural language generation or translation tasks used phrase-based statistical systems or RNN-based seq2seq models. The last few years have seen a massive shift to architectures involving attention and Transformers."),"\n",r.createElement(t.h3,{id:"exploring-seq2seq-architecture",style:{position:"relative"}},r.createElement(t.a,{href:"#exploring-seq2seq-architecture","aria-label":"exploring seq2seq architecture permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Exploring seq2seq architecture"),"\n",r.createElement(t.p,null,"The encoder-decoder or seq2seq framework is a neural approach originally popularized for machine translation. An ",r.createElement(t.strong,null,"encoder")," network processes input tokens (e.g., words in the source language) and produces a latent representation. A ",r.createElement(t.strong,null,"decoder")," network then generates the output sequence (e.g., words in the target language), one token at a time. Recurrent neural networks (LSTM or GRU variants) were once the standard building block."),"\n",r.createElement(t.h3,{id:"the-role-of-attention-in-nlp",style:{position:"relative"}},r.createElement(t.a,{href:"#the-role-of-attention-in-nlp","aria-label":"the role of attention in nlp permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"The role of attention in NLP"),"\n",r.createElement(t.p,null,"A key innovation that improved seq2seq models is the ",r.createElement(i.A,null,"attention mechanism")," (Bahdanau and gang, 2015). Attention allows a model to focus on specific parts of the encoder's output for each step of the decoder, learning alignment automatically rather than compressing an entire sentence into a single vector."),"\n",r.createElement(t.p,null,"Formally, an attention mechanism produces context vectors:"),"\n",r.createElement(s.A,{text:"\\[\n\\text{context}_t = \\sum_{i=1}^{T_{enc}} \\alpha_{t,i} h_i\n\\]"}),"\n",r.createElement(t.p,null,"where:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(s.A,{text:"\\( h_i \\)"})," are encoder hidden states."),"\n",r.createElement(t.li,null,r.createElement(s.A,{text:"\\( \\alpha_{t,i} \\)"})," is a learned weight that shows how strongly the decoder at time ",r.createElement(s.A,{text:"\\( t \\)"})," attends to encoder position ",r.createElement(s.A,{text:"\\( i \\)"}),"."),"\n"),"\n",r.createElement(t.p,null,"This mechanism significantly improves performance in translation, summarization, and other generation tasks."),"\n",r.createElement(t.h3,{id:"positional-encoding-for-sequence-models",style:{position:"relative"}},r.createElement(t.a,{href:"#positional-encoding-for-sequence-models","aria-label":"positional encoding for sequence models permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Positional encoding for sequence models"),"\n",r.createElement(t.p,null,"As we move away from purely recurrent approaches, the Transformer architecture (Vaswani and gang, 2017) does not rely on recurrence. Instead, it processes a sequence in parallel, but must be aware of the token order. This awareness is introduced via ",r.createElement(i.A,null,"positional encoding"),", which uses sinusoidal or learned embeddings to encode relative positions:"),"\n",r.createElement(s.A,{text:"\\[\nPE_{(pos,2i)} = \\sin\\left(\\frac{\\text{pos}}{10000^{2i/d_{\\text{model}}}}\\right), \\quad\nPE_{(pos,2i+1)} = \\cos\\left(\\frac{\\text{pos}}{10000^{2i/d_{\\text{model}}}}\\right)\n\\]"}),"\n",r.createElement(t.p,null,"where:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(s.A,{text:"\\( pos \\)"})," is the position in the sequence."),"\n",r.createElement(t.li,null,r.createElement(s.A,{text:"\\( i \\)"})," indexes each dimension."),"\n",r.createElement(t.li,null,r.createElement(s.A,{text:"\\( d_{\\text{model}} \\)"})," is the embedding dimension size."),"\n"),"\n",r.createElement(t.h3,{id:"transformers",style:{position:"relative"}},r.createElement(t.a,{href:"#transformers","aria-label":"transformers permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Transformers"),"\n",r.createElement(t.p,null,"Transformers represent the current state-of-the-art in many NLP tasks. They rely entirely on attention modules (multi-head self-attention) to handle context, and often come in extremely large pretrained variants. Examples include:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"BERT")," (Devlin and gang, 2018): A bidirectional Transformer for masked language modeling and next-sentence prediction."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"GPT series")," (OpenAI): A unidirectional Transformer specialized in generative tasks."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"T5")," (Raffel and gang, 2020): A text-to-text framework unifying multiple NLP tasks under a single Transformer-based architecture."),"\n"),"\n",r.createElement(t.p,null,"These large Transformer-based models have revolutionized NLP tasks, enabling low-data solutions (few-shot learning) and driving new research in interpretability, fairness, and domain adaptation."),"\n",r.createElement(t.h2,{id:"core-nlp-tasks",style:{position:"relative"}},r.createElement(t.a,{href:"#core-nlp-tasks","aria-label":"core nlp tasks permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Core NLP tasks"),"\n",r.createElement(t.p,null,"Though countless tasks exist, a few major categories stand out in both industrial and research settings."),"\n",r.createElement(t.h3,{id:"text-classification--sentiment-analysis",style:{position:"relative"}},r.createElement(t.a,{href:"#text-classification--sentiment-analysis","aria-label":"text classification  sentiment analysis permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Text classification & sentiment analysis"),"\n",r.createElement(t.p,null,"Classification is among the most common tasks. Examples include labeling a news article by topic or determining user sentiment (positive, negative, neutral). Traditional machine learning approaches (e.g., Naive Bayes, SVMs) remain used in small or specialized contexts. However, large pretrained Transformers often dominate benchmarks."),"\n",r.createElement(t.p,null,"In ",r.createElement(t.strong,null,"sentiment analysis"),", the goal is to assess how positive, negative, or neutral a piece of text is. One can also do ",r.createElement(t.strong,null,"emotion analysis"),", extracting finer-grained categories such as joy, anger, sadness, fear, or disgust. Lexicon-based techniques (dictionary-based), rule-based approaches, or supervised machine learning on labeled data are typical solutions."),"\n",r.createElement(t.h3,{id:"named-entity-recognition-ner",style:{position:"relative"}},r.createElement(t.a,{href:"#named-entity-recognition-ner","aria-label":"named entity recognition ner permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Named entity recognition (NER)"),"\n",r.createElement(t.p,null,"NER focuses on labeling occurrences of real-world entities in text, usually with classes like ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">&lt;Person></code>'}}),", ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">&lt;Organization></code>'}}),", ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">&lt;Location></code>'}}),", ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">&lt;Date></code>'}}),", ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">&lt;Misc></code>'}}),", etc. Neural architectures, especially BiLSTM + CRF or Transformers fine-tuned for token-level classification, achieve excellent performance on standard benchmarks like CoNLL-2003."),"\n",r.createElement(t.h3,{id:"machine-translation",style:{position:"relative"}},r.createElement(t.a,{href:"#machine-translation","aria-label":"machine translation permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Machine translation"),"\n",r.createElement(t.p,null,"Early systems were rule-based, then statistical phrase-based, and now neural. Neural machine translation (NMT) started with RNN-based seq2seq and advanced to Transformer-based approaches, which currently define the state-of-the-art in many language pairs."),"\n",r.createElement(t.h3,{id:"question-answering-qa",style:{position:"relative"}},r.createElement(t.a,{href:"#question-answering-qa","aria-label":"question answering qa permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Question answering (QA)"),"\n",r.createElement(t.p,null,"QA systems answer user queries by either extracting relevant spans from a reference text (extractive QA) or generating new responses (generative QA). Modern large language models can do open-domain QA with minimal additional training, though specialized architectures for retrieval plus reading comprehension remain popular (e.g., RAG — Retrieval-Augmented Generation)."),"\n",r.createElement(t.h3,{id:"text-mining-approaches",style:{position:"relative"}},r.createElement(t.a,{href:"#text-mining-approaches","aria-label":"text mining approaches permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Text mining approaches"),"\n",r.createElement(t.p,null,"Text mining tasks range from ",r.createElement(t.strong,null,"relation extraction")," (deriving semantic relationships between entities) and ",r.createElement(t.strong,null,"topic modeling")," (like LDA) to ",r.createElement(t.strong,null,"classification")," and ",r.createElement(t.strong,null,"semantic role labeling")," (who did what to whom). Another relevant use case is plagiarism detection, often employing n-gram overlap and/or more advanced similarity measures."),"\n",r.createElement(t.h3,{id:"emotion-analysis",style:{position:"relative"}},r.createElement(t.a,{href:"#emotion-analysis","aria-label":"emotion analysis permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Emotion analysis"),"\n",r.createElement(t.p,null,"Emotion analysis (closely linked to sentiment analysis but more fine-grained) attempts to categorize text into emotional states. Research in this area has expanded in recent years, as businesses and social scientists look to measure consumer or societal emotion at scale. Tools can rely on keyword-based approaches (using sets of emotional words) or neural methods that classify text into a discrete set of emotions (e.g., Ekman's basic emotions: anger, disgust, fear, happiness, sadness, surprise)."),"\n",r.createElement(t.h2,{id:"evaluating-nlp-systems",style:{position:"relative"}},r.createElement(t.a,{href:"#evaluating-nlp-systems","aria-label":"evaluating nlp systems permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Evaluating NLP systems"),"\n",r.createElement(t.p,null,"Proper evaluation is vital for reliable progress. The metrics and evaluation methodology chosen can radically influence how a system is perceived or improved."),"\n",r.createElement(t.h3,{id:"common-nlp-metrics",style:{position:"relative"}},r.createElement(t.a,{href:"#common-nlp-metrics","aria-label":"common nlp metrics permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Common NLP metrics"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Accuracy"),": Fraction of test examples correctly predicted."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Precision"),": Of all predicted positives, how many are correct?"),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Recall"),": Of all actual positives, how many did we predict correctly?"),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"F1-score"),": Harmonic mean of precision and recall, used when there is an uneven class distribution."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Confusion matrix"),": A table that shows counts of actual versus predicted classes, highlighting misclassifications."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"BLEU")," (Bilingual Evaluation Understudy): Common for machine translation, measuring n-gram overlap between system output and reference translations."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"ROUGE")," (Recall-Oriented Understudy for Gisting Evaluation): Favored in summarization tasks, focusing on overlap in n-grams or longest common subsequences."),"\n"),"\n",r.createElement(t.h3,{id:"best-practices-for-model-evaluation",style:{position:"relative"}},r.createElement(t.a,{href:"#best-practices-for-model-evaluation","aria-label":"best practices for model evaluation permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Best practices for model evaluation"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Use a held-out test set"),": Helps ensure your model does not just memorize the training data."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Cross-validation"),": Helpful for smaller datasets."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Hyperparameter tuning"),": Tools such as cross-validation or specialized search strategies (grid search, Bayesian optimization) can significantly improve performance."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Reproducibility"),": Always keep track of random seeds, library versions, and data preprocessing steps."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Error analysis"),": Go beyond metrics to understand specific failure modes. For instance, do errors cluster on specific syntactic structures or domain-specific vocabulary?"),"\n"),"\n",r.createElement(t.h3,{id:"multi-class-and-multi-label-tasks",style:{position:"relative"}},r.createElement(t.a,{href:"#multi-class-and-multi-label-tasks","aria-label":"multi class and multi label tasks permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Multi-class and multi-label tasks"),"\n",r.createElement(t.p,null,'In multi-class classification (e.g., five sentiment categories from "highly negative" to "highly positive"), metrics like macro-average or weighted-average precision/recall can measure system performance across multiple classes. In multi-label tasks (where a text may belong to multiple categories simultaneously), one uses metrics like the Hamming loss, subset accuracy, or F1-scores at the label level.'),"\n",r.createElement(t.h2,{id:"advanced-breakthroughs--conclusion",style:{position:"relative"}},r.createElement(t.a,{href:"#advanced-breakthroughs--conclusion","aria-label":"advanced breakthroughs  conclusion permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Advanced breakthroughs & conclusion"),"\n",r.createElement(t.p,null,"NLP is one of the fastest-moving fields within machine learning, and each year sees new innovations and refinements. Below are some of the more recent breakthroughs beyond the core tasks discussed."),"\n",r.createElement(t.h3,{id:"the-clip-model",style:{position:"relative"}},r.createElement(t.a,{href:"#the-clip-model","aria-label":"the clip model permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"The CLIP model"),"\n",r.createElement(t.p,null,"CLIP (Contrastive Language-Image Pretraining, Radford and gang) from OpenAI is a powerful multimodal approach that aligns images and textual descriptions in a shared embedding space. Although not strictly an NLP model, it demonstrates how text encoding can interface with image encoding in tasks like zero-shot image classification. These multimodal approaches are increasingly popular, bridging NLP and computer vision."),"\n",r.createElement(t.h3,{id:"future-directions-in-nlp",style:{position:"relative"}},r.createElement(t.a,{href:"#future-directions-in-nlp","aria-label":"future directions in nlp permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Future directions in NLP"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Continual learning"),": Models that adapt to new tasks without forgetting old ones."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Zero-shot/few-shot learning"),": With large pretrained language models, practitioners can rapidly adapt to new tasks with minimal labeled data, sometimes simply by providing instructions or prompts."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Interpretability and fairness"),": As NLP systems are deployed in sensitive areas (e.g., hiring, lending, legal analysis), efforts are ongoing to interpret black-box models and mitigate bias."),"\n"),"\n",r.createElement(t.h3,{id:"libraries--tools",style:{position:"relative"}},r.createElement(t.a,{href:"#libraries--tools","aria-label":"libraries  tools permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Libraries & tools"),"\n",r.createElement(t.p,null,"Commonly used Python libraries for NLP include:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"NLTK (Natural Language Toolkit)"),": An older but comprehensive library containing tokenizers, POS taggers, chunkers, and more. Good for educational settings."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"spaCy"),": A modern, efficient library with a faster tokenizer, named entity recognizer, and pretrained pipelines for many languages."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"scikit-learn"),": Provides classical machine learning algorithms for classification, regression, and clustering; includes straightforward text-processing modules (like ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">CountVectorizer</code>'}})," or ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">TfidfVectorizer</code>'}}),")."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"gensim"),": Focused on topic modeling (LDA) and word embedding methods (Word2Vec, Doc2Vec)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"pymorphy2 & Natasha")," (for Russian): Tools to handle morphological inflection and named entity recognition for Russian."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"PyTorch"),", ",r.createElement(t.strong,null,"TensorFlow"),", ",r.createElement(t.strong,null,"Keras"),": General deep learning frameworks used to implement advanced custom NLP architectures."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Hugging Face Transformers"),": A widely used library for pretrained models (BERT, GPT-2, GPT-3-like models, T5, etc.) with straightforward APIs for training, fine-tuning, and inference."),"\n"),"\n",r.createElement(t.p,null,"These tools reflect both tradition (rule-based, carefully engineered components) and modern approaches (neural networks, Transformers, large-scale pretraining)."),"\n",r.createElement(t.h3,{id:"summary-of-key-points",style:{position:"relative"}},r.createElement(t.a,{href:"#summary-of-key-points","aria-label":"summary of key points permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Summary of key points"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Preprocessing"),": Basic cleaning, tokenization, and morphological normalization remain essential steps."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Embeddings"),": Distributed word representations (Word2Vec, GloVe, FastText) and advanced factorization or topic modeling approaches (LSA, pLSA, etc.) capture lexical and semantic relationships better than older bag-of-words."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Advanced architectures"),": Transformers have revolutionized how we approach sequence tasks, offering better parallelization and superior performance compared to RNN-based seq2seq."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Evaluation"),": Thorough metric-based evaluation (accuracy, precision, recall, BLEU, ROUGE) and proper methodological rigor ensure that improvements are genuine."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Modern breakthroughs"),": Large language models have opened doors to zero-shot and few-shot learning, while multimodal models like CLIP expand NLP into new territory."),"\n"),"\n",r.createElement(t.p,null,"NLP underpins a huge spectrum of real-world applications and has become central to many data science workflows. As the field continues to advance, data scientists and ML engineers gain ever more powerful tools for extracting insights from textual data. Yet there remain formidable open problems around interpretability, bias, low-resource languages, code-switching, and real-world robustness. Mastery of both foundational concepts — like preprocessing, morphological analysis, embeddings, and factorization — and advanced techniques — such as attention-driven Transformers — provides an excellent foundation for building cutting-edge NLP solutions."),"\n",r.createElement(n,{alt:"nlp_architecture_simplified",path:"",caption:"An illustrative (and highly simplified) diagram of an NLP pipeline: from data ingestion and preprocessing to advanced neural networks and final evaluation.",zoom:"false"}),"\n",r.createElement(t.p,null,"Ultimately, NLP sits at a marvelous intersection of linguistics, mathematics, and computer science. It challenges us to understand the nuances of human language while employing advanced algorithms for large-scale text processing. In the broader machine learning & data science course, you will see how these NLP principles connect with deep learning, generative modeling, specialized domains (like dialogue systems), and more."),"\n",r.createElement(t.p,null,"If there is one key takeaway, it is the recognition that robust language understanding and generation require both the ",r.createElement(t.strong,null,"linguistic grounding")," that symbolic approaches once emphasized and the ",r.createElement(t.strong,null,"scalability")," and ",r.createElement(t.strong,null,"adaptability")," that modern neural architectures deliver. The synergy of these insights — plus a wealth of new research — promises to drive NLP forward to even more remarkable capabilities in the near future."))}var c=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,l.RP)(),e.components);return t?r.createElement(t,e,r.createElement(o,e)):o(e)};var m=n(36710),d=n(58481),h=n.n(d),u=n(36310),g=n(87245),p=n(27042),f=n(59849),v=n(5591),E=n(61122),y=n(9219),b=n(33203),w=n(95751),x=n(94328),S=n(80791),k=n(78137);const H=e=>{let{toc:t}=e;if(!t||!t.items)return null;return r.createElement("nav",{className:S.R},r.createElement("ul",null,t.items.map(((e,t)=>r.createElement("li",{key:t},r.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const n=t.replace("#",""),a=document.getElementById(n);a&&a.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&r.createElement(H,{toc:{items:e.items}}))))))};function T(e){let{data:{mdx:t,allMdx:i,allPostImages:s},children:o}=e;const{frontmatter:c,body:m,tableOfContents:d}=t,f=c.index,S=c.slug.split("/")[1],T=i.nodes.filter((e=>e.frontmatter.slug.includes(`/${S}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),L=T.findIndex((e=>e.frontmatter.index===f)),z=T[L+1],_=T[L-1],M=c.slug.replace(/\/$/,""),A=/[^/]*$/.exec(M)[0],C=`posts/${S}/content/${A}/`,{0:N,1:P}=(0,r.useState)(c.flagWideLayoutByDefault),{0:V,1:I}=(0,r.useState)(!1);var q;(0,r.useEffect)((()=>{I(!0);const e=setTimeout((()=>I(!1)),340);return()=>clearTimeout(e)}),[N]),"adventures"===S?q=y.cb:"research"===S?q=y.Qh:"thoughts"===S&&(q=y.T6);const B=h()(m).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,R=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),n=e%60;return n<=30?`~${t}${n>0?".5":""} h`:`~${t+1} h`}(Math.ceil(B/q)+(c.extraReadTimeMin||0)),O=[{flag:c.flagDraft,component:()=>Promise.all([n.e(3231),n.e(8809)]).then(n.bind(n,28809))},{flag:c.flagMindfuckery,component:()=>Promise.all([n.e(3231),n.e(2471)]).then(n.bind(n,67709))},{flag:c.flagRewrite,component:()=>Promise.all([n.e(3231),n.e(6764)]).then(n.bind(n,62002))},{flag:c.flagOffensive,component:()=>Promise.all([n.e(3231),n.e(2443)]).then(n.bind(n,17681))},{flag:c.flagProfane,component:()=>Promise.all([n.e(3231),n.e(8048)]).then(n.bind(n,53286))},{flag:c.flagMultilingual,component:()=>Promise.all([n.e(3231),n.e(4069)]).then(n.bind(n,78831))},{flag:c.flagUnreliably,component:()=>Promise.all([n.e(3231),n.e(3417)]).then(n.bind(n,8179))},{flag:c.flagPolitical,component:()=>Promise.all([n.e(3231),n.e(5195)]).then(n.bind(n,30433))},{flag:c.flagCognitohazard,component:()=>Promise.all([n.e(3231),n.e(3175)]).then(n.bind(n,8413))},{flag:c.flagHidden,component:()=>Promise.all([n.e(3231),n.e(9556)]).then(n.bind(n,14794))}],{0:G,1:F}=(0,r.useState)([]);return(0,r.useEffect)((()=>{O.forEach((e=>{let{flag:t,component:n}=e;t&&n().then((e=>{F((t=>[].concat((0,a.A)(t),[e.default])))}))}))}),[]),r.createElement(p.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},r.createElement(v.A,{postNumber:c.index,date:c.date,updated:c.updated,readTime:R,difficulty:c.difficultyLevel,title:c.title,desc:c.desc,banner:c.banner,section:S,postKey:A,isMindfuckery:c.flagMindfuckery,mainTag:c.mainTag}),r.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},c.otherTags.map(((e,t)=>r.createElement("span",{key:t,className:`noselect ${k.MW}`,style:{margin:"0 5px 5px 0"}},e)))),r.createElement("div",{className:"postBody"},r.createElement(H,{toc:d})),r.createElement("br"),r.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},r.createElement(p.P.button,{className:`noselect ${x.pb}`,id:x.xG,onClick:()=>{P(!N)},whileTap:{scale:.93}},r.createElement(p.P.div,{className:w.DJ,key:N,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},N?"Switch to default layout":"Switch to wide layout"))),r.createElement("br"),r.createElement("div",{className:"postBody",style:{margin:N?"0 -14%":"",maxWidth:N?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},r.createElement("div",{className:`${x.P_} ${V?x.Xn:x.qG}`},G.map(((e,t)=>r.createElement(e,{key:t}))),c.indexCourse?r.createElement(b.A,{index:c.indexCourse,category:c.courseCategoryName}):"",r.createElement(u.Z.Provider,{value:{images:s.nodes,basePath:C.replace(/\/$/,"")+"/"}},r.createElement(l.xA,{components:{Image:g.A}},o)))),r.createElement(E.A,{nextPost:z,lastPost:_,keyCurrent:A,section:S}))}function L(e){return r.createElement(T,e,r.createElement(c,e))}function z(e){var t,n,a,l,i;let{data:s}=e;const{frontmatter:o}=s.mdx,c=o.titleSEO||o.title,d=o.titleOG||c,h=o.titleTwitter||c,u=o.descSEO||o.desc,g=o.descOG||u,p=o.descTwitter||u,v=o.schemaType||"BlogPosting",E=o.keywordsSEO,y=o.date,b=o.updated||y,w=o.imageOG||(null===(t=o.banner)||void 0===t||null===(n=t.childImageSharp)||void 0===n||null===(a=n.gatsbyImageData)||void 0===a||null===(l=a.images)||void 0===l||null===(i=l.fallback)||void 0===i?void 0:i.src),x=o.imageAltOG||g,S=o.imageTwitter||w,k=o.imageAltTwitter||p,H=o.canonicalURL,T=o.flagHidden||!1,L=o.mainTag||"Posts",z=o.slug.split("/")[1]||"posts",{siteUrl:_}=(0,m.Q)(),M={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:_},{"@type":"ListItem",position:2,name:L,item:`${_}/${o.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${_}${o.slug}`}]};return r.createElement(f.A,{title:c+" - avrtt.blog",titleOG:d,titleTwitter:h,description:u,descriptionOG:g,descriptionTwitter:p,schemaType:v,keywords:E,datePublished:y,dateModified:b,imageOG:w,imageAltOG:x,imageTwitter:S,imageAltTwitter:k,canonicalUrl:H,flagHidden:T,mainTag:L,section:z,type:"article"},r.createElement("script",{type:"application/ld+json"},JSON.stringify(M)))}},96098:function(e,t,n){var a=n(96540),l=n(7978);t.A=e=>{let{text:t}=e;return a.createElement(l.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-js-content-file-path-src-pages-posts-research-intro-to-nlp-mdx-100457fd6ded85a14ca6.js.map