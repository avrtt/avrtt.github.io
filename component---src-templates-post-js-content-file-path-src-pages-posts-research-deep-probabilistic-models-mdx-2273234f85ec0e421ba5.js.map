{"version":3,"file":"component---src-templates-post-js-content-file-path-src-pages-posts-research-deep-probabilistic-models-mdx-2273234f85ec0e421ba5.js","mappings":"8RAsGA,SAASA,EAAkBC,GACzB,MAAMC,EAAcC,OAAOC,OAAO,CAChCC,EAAG,IACHC,GAAI,KACJC,EAAG,IACHC,KAAM,OACNC,GAAI,KACJC,GAAI,KACJC,OAAQ,SACRC,GAAI,KACJC,GAAI,KACJC,GAAI,KACJC,GAAI,OACHC,EAAAA,EAAAA,MAAsBf,EAAMgB,YAC/B,OAAOC,EAAAA,cAAoBA,EAAAA,SAAgB,KAAM,KAAMA,EAAAA,cAAoB,MAAO,KAAM,KAAM,KAAMA,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,qqBAAsqB,KAAMa,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,qeAA4e,KAAMa,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,kHAAmHa,EAAAA,cAAoBC,EAAAA,EAAO,CACzgDC,KAAM,YACJ,4HAA6HF,EAAAA,cAAoBC,EAAAA,EAAO,CAC1JC,KAAM,YACJ,gCAAiCF,EAAAA,cAAoBC,EAAAA,EAAO,CAC9DC,KAAM,eACJ,sBAAuBF,EAAAA,cAAoBC,EAAAA,EAAO,CACpDC,KAAM,uBACJ,iKAAkKF,EAAAA,cAAoBC,EAAAA,EAAO,CAC/LC,KAAM,YACJ,WAAY,KAAMF,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,2eAA4e,KAAMa,EAAAA,cAAoBhB,EAAYI,GAAI,CACnlBe,GAAI,8BACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,+BACN,aAAc,wCACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,+BAAgC,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,gOAAiO,KAAMa,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,+BAAgC,6JAA8J,KAAMO,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,UAAW,oMAAqM,KAAMO,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,0BAA2B,wLAAyL,KAAMO,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,eAAgB,wIAAyI,KAAMO,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,2BAA4B,kQAAmQ,MAAO,KAAMO,EAAAA,cAAoBhB,EAAYI,GAAI,CACp4De,GAAI,mBACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,oBACN,aAAc,6BACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,oBAAqB,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,qKAAsK,KAAMa,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,iCAAkC,uRAAwR,KAAMO,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,0CAA2C,sYAAuY,MAAO,KAAMO,EAAAA,cAAoBhB,EAAYU,IAAK,KAAMM,EAAAA,cAAoBhB,EAAYW,GAAI,CACjyCQ,GAAI,sCACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,uCACN,aAAc,gDACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,uCAAwC,KAAMT,EAAAA,cAAoBhB,EAAYI,GAAI,CACrFe,GAAI,mBACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,oBACN,aAAc,6BACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,oBAAqB,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,8JAA+J,KAAMa,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,6BAA8B,gOAAiO,KAAMO,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,+BAAgC,8MAA+M,MAAO,KAAMO,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,4KAA6K,KAAMa,EAAAA,cAAoBhB,EAAYI,GAAI,CAC7sCe,GAAI,mCACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,oCACN,aAAc,6CACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,qCAAsC,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,wBAAyBa,EAAAA,cAAoBC,EAAAA,EAAO,CAC5IC,KAAM,YACJ,QAASF,EAAAA,cAAoBC,EAAAA,EAAO,CACtCC,KAAM,YACJ,4BAA6BF,EAAAA,cAAoBC,EAAAA,EAAO,CAC1DC,KAAM,kBACJ,+DAAgEF,EAAAA,cAAoBC,EAAAA,EAAO,CAC7FC,KAAM,WACJ,+EAAgFF,EAAAA,cAAoBC,EAAAA,EAAO,CAC7GC,KAAM,uBACJ,kEAAmE,KAAMF,EAAAA,cAAoBC,EAAAA,EAAO,CACtGC,KAAM,sEACJ,KAAMF,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,SAAUa,EAAAA,cAAoBC,EAAAA,EAAO,CACtFC,KAAM,YACJ,kDAAmDF,EAAAA,cAAoBC,EAAAA,EAAO,CAChFC,KAAM,uBACJ,oBAAqBF,EAAAA,cAAoBC,EAAAA,EAAO,CAClDC,KAAM,+BACJ,gFAAiF,KAAMF,EAAAA,cAAoBhB,EAAYI,GAAI,CAC7He,GAAI,iCACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,kCACN,aAAc,2CACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,mCAAoC,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,2GAA4Ga,EAAAA,cAAoBC,EAAAA,EAAO,CAC7NC,KAAM,uBACJ,eAAgB,KAAMF,EAAAA,cAAoBC,EAAAA,EAAO,CACnDC,KAAM,+FACJ,KAAMF,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,MAAO,KAAMa,EAAAA,cAAoBC,EAAAA,EAAO,CACzFC,KAAM,oGACJ,KAAMF,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,sLAAuL,KAAMa,EAAAA,cAAoBhB,EAAYU,IAAK,KAAMM,EAAAA,cAAoBhB,EAAYW,GAAI,CAC7TQ,GAAI,4BACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,6BACN,aAAc,sCACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,8BAA+B,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,2BAA4Ba,EAAAA,cAAoBC,EAAAA,EAAO,CACxIC,KAAM,qBACJ,sBAAuBF,EAAAA,cAAoBC,EAAAA,EAAO,CACpDC,KAAM,YACJ,cAAeF,EAAAA,cAAoBC,EAAAA,EAAO,CAC5CC,KAAM,uBACJ,4BAA6BF,EAAAA,cAAoBC,EAAAA,EAAO,CAC1DC,KAAM,kBACJ,gBAAiBF,EAAAA,cAAoBC,EAAAA,EAAO,CAC9CC,KAAM,uBACJ,0EAA+EF,EAAAA,cAAoBC,EAAAA,EAAO,CAC5GC,KAAM,kBACJ,KAAM,KAAMF,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,uCAAwC,eAAgBO,EAAAA,cAAoBC,EAAAA,EAAO,CACzOC,KAAM,kBACJ,gBAAiBF,EAAAA,cAAoBC,EAAAA,EAAO,CAC9CC,KAAM,uBACJ,KAAM,KAAMF,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,kBAAmB,mDAAoDO,EAAAA,cAAoBC,EAAAA,EAAO,CACxMC,KAAM,6BACJ,sGAAuG,MAAO,KAAMF,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,sBAAuBa,EAAAA,cAAoBC,EAAAA,EAAO,CACjNC,KAAM,6BACJ,mDAAoDF,EAAAA,cAAoBC,EAAAA,EAAO,CACjFC,KAAM,wCACJ,oBAAqBF,EAAAA,cAAoBC,EAAAA,EAAO,CAClDC,KAAM,6CACJ,iNAAkN,KAAMF,EAAAA,cAAoBhB,EAAYU,IAAK,KAAMM,EAAAA,cAAoBhB,EAAYW,GAAI,CACzSQ,GAAI,yCACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,0CACN,aAAc,mDACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,0CAA2C,KAAMT,EAAAA,cAAoBhB,EAAYI,GAAI,CACxFe,GAAI,4BACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,6BACN,aAAc,sCACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,6BAA8B,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,mNAAoN,KAAMa,EAAAA,cAAoBC,EAAAA,EAAO,CACrUC,KAAM,uFACJ,KAAMF,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,qDAAsDa,EAAAA,cAAoBC,EAAAA,EAAO,CAClIC,KAAM,gDACJ,oEAAqE,KAAMF,EAAAA,cAAoBC,EAAAA,EAAO,CACxGC,KAAM,6NACJ,KAAMF,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,wHAAyH,KAAMa,EAAAA,cAAoBhB,EAAYI,GAAI,CACpNe,GAAI,iCACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,kCACN,aAAc,2CACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,kCAAmC,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,4BAA6Ba,EAAAA,cAAoBC,EAAAA,EAAO,CAC7IC,KAAM,8CACJ,aAAcF,EAAAA,cAAoBC,EAAAA,EAAO,CAC3CC,KAAM,mDACJ,oBAAqBF,EAAAA,cAAoBC,EAAAA,EAAO,CAClDC,KAAM,iCACJ,yCAA0CF,EAAAA,cAAoBC,EAAAA,EAAO,CACvEC,KAAM,sBACJ,oLAAqL,KAAMF,EAAAA,cAAoBhB,EAAYI,GAAI,CACjOe,GAAI,2BACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,4BACN,aAAc,qCACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,4BAA6B,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,2aAA4a,KAAMa,EAAAA,cAAoBhB,EAAYU,IAAK,KAAMM,EAAAA,cAAoBhB,EAAYW,GAAI,CAChlBQ,GAAI,qDACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,sDACN,aAAc,+DACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,sDAAuD,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,+GAAgHa,EAAAA,cAAoBC,EAAAA,EAAO,CACpPC,KAAM,qEACJ,QAASF,EAAAA,cAAoBC,EAAAA,EAAO,CACtCC,KAAM,8BACJ,QAASF,EAAAA,cAAoBC,EAAAA,EAAO,CACtCC,KAAM,0BACJ,uBAAwBF,EAAAA,cAAoBC,EAAAA,EAAO,CACrDC,KAAM,cACJ,kCAAmCF,EAAAA,cAAoBC,EAAAA,EAAO,CAChEC,KAAM,cACJ,sCAAuCF,EAAAA,cAAoBC,EAAAA,EAAO,CACpEC,KAAM,cACJ,KAAM,KAAMF,EAAAA,cAAoBhB,EAAYG,EAAG,KAAMa,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,aAAc,kGAAmGO,EAAAA,cAAoBC,EAAAA,EAAO,CACjPC,KAAM,mCACJ,gPAAiP,KAAMF,EAAAA,cAAoBhB,EAAYU,IAAK,KAAMM,EAAAA,cAAoBhB,EAAYW,GAAI,CACxUQ,GAAI,0CACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,2CACN,aAAc,oDACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,2CAA4C,KAAMT,EAAAA,cAAoBhB,EAAYI,GAAI,CACzFe,GAAI,qBACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,sBACN,aAAc,+BACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,sBAAuB,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,2GAA4Ga,EAAAA,cAAoBC,EAAAA,EAAO,CAChNC,KAAM,kBACJ,kCAAmCF,EAAAA,cAAoBC,EAAAA,EAAO,CAChEC,KAAM,kBACJ,kBAAmBF,EAAAA,cAAoBC,EAAAA,EAAO,CAChDC,KAAM,sCACJ,oFAAqF,KAAMF,EAAAA,cAAoBC,EAAAA,EAAO,CACxHC,KAAM,+GACJ,KAAMF,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,sBAAuBa,EAAAA,cAAoBC,EAAAA,EAAO,CACnGC,KAAM,uBACJ,4EAA6EF,EAAAA,cAAoBC,EAAAA,EAAO,CAC1GC,KAAM,YACJ,YAAaF,EAAAA,cAAoBC,EAAAA,EAAO,CAC1CC,KAAM,YACJ,mBAAoB,KAAMF,EAAAA,cAAoBC,EAAAA,EAAO,CACvDC,KAAM,gIACJ,KAAMF,EAAAA,cAAoBhB,EAAYI,GAAI,CAC5Ce,GAAI,YACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,aACN,aAAc,sBACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,aAAc,KAAMT,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,0BAA2B,2CAA4CO,EAAAA,cAAoBC,EAAAA,EAAO,CACjQC,KAAM,kBACJ,gCAAiCF,EAAAA,cAAoBC,EAAAA,EAAO,CAC9DC,KAAM,kBACJ,KAAM,KAAMF,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,sBAAuB,mEAAoE,KAAMO,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,mCAAoC,4DAA6D,MAAO,KAAMO,EAAAA,cAAoBhB,EAAYI,GAAI,CACjbe,GAAI,yCACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,0CACN,aAAc,mDACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,0CAA2C,KAAMT,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,UAAW,iFAAkF,KAAMO,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,cAAe,kDAAmDO,EAAAA,cAAoBC,EAAAA,EAAO,CACpdC,KAAM,YACJ,4BAA6BF,EAAAA,cAAoBC,EAAAA,EAAO,CAC1DC,KAAM,YACJ,KAAM,KAAMF,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,WAAY,2BAA4BO,EAAAA,cAAoBC,EAAAA,EAAO,CACzKC,KAAM,qDACJ,uBAAwB,MAAO,KAAMF,EAAAA,cAAoBhB,EAAYU,IAAK,KAAMM,EAAAA,cAAoBhB,EAAYW,GAAI,CACtHQ,GAAI,oDACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,qDACN,aAAc,8DACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,qDAAsD,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,yFAA0F,KAAMa,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,wBAAyB,oGAAqGO,EAAAA,cAAoBC,EAAAA,EAAO,CACzeC,KAAM,kBACJ,yDAA0D,KAAMF,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,yBAA0B,YAAaO,EAAAA,cAAoBC,EAAAA,EAAO,CAC5NC,KAAM,kBACJ,iGAAkG,MAAO,KAAMF,EAAAA,cAAoBhB,EAAYI,GAAI,CACrJe,GAAI,cACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,eACN,aAAc,wBACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,eAAgB,KAAMT,EAAAA,cAAoBC,EAAAA,EAAO,CACpDC,KAAM,qIACJ,KAAMF,EAAAA,cAAoBC,EAAAA,EAAO,CACnCC,KAAM,yGACJ,KAAMF,EAAAA,cAAoBhB,EAAYI,GAAI,CAC5Ce,GAAI,yBACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,0BACN,aAAc,mCACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,0BAA2B,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,QAASa,EAAAA,cAAoBC,EAAAA,EAAO,CACjHC,KAAM,YACJ,gKAAiK,KAAMF,EAAAA,cAAoBhB,EAAYI,GAAI,CAC7Me,GAAI,wBACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,yBACN,aAAc,kCACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,yBAA0B,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,sGAAuGa,EAAAA,cAAoBC,EAAAA,EAAO,CAC9MC,KAAM,YACJ,oHAAqH,KAAMF,EAAAA,cAAoBhB,EAAYU,IAAK,KAAMM,EAAAA,cAAoBhB,EAAYW,GAAI,CAC5MQ,GAAI,oDACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,qDACN,aAAc,8DACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,qDAAsD,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,8BAA+B,KAAMa,EAAAA,cAAoBhB,EAAYY,GAAI,KAAM,KAAMI,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAM,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,kBAAmB,iCAAkCO,EAAAA,cAAoBC,EAAAA,EAAO,CAC3WC,KAAM,cACJ,aAAc,KAAMF,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,wBAAyB,0EAA2E,MAAO,KAAMO,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,aAAc,KAAMa,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,2BAA4B,wGAAyG,KAAMO,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,kBAAmB,0KAA2K,MAAO,KAAMO,EAAAA,cAAoBhB,EAAYU,IAAK,KAAMM,EAAAA,cAAoBhB,EAAYW,GAAI,CACj6BQ,GAAI,+CACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,gDACN,aAAc,yDACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,gDAAiD,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,oEAAqEa,EAAAA,cAAoBC,EAAAA,EAAO,CACnMC,KAAM,YACJ,2DAA4D,KAAMF,EAAAA,cAAoBhB,EAAYI,GAAI,CACxGe,GAAI,8BACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,+BACN,aAAc,wCACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,gCAAiC,KAAMT,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,gBAAiB,wGAAyGO,EAAAA,cAAoBC,EAAAA,EAAO,CACvUC,KAAM,4BACJ,KAAM,KAAMF,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,iBAAkB,mGAAoG,MAAO,KAAMO,EAAAA,cAAoBhB,EAAYI,GAAI,CAC7Qe,GAAI,4CACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,6CACN,aAAc,sDACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,8CAA+C,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,wGAAyG,KAAMa,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAM,UAAWQ,EAAAA,cAAoBU,EAAAA,EAAW,KAAM,aAAc,+BAAgC,KAAMV,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAM,oFAAqF,KAAMQ,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAM,wGAAyG,MAAO,KAAMQ,EAAAA,cAAoBhB,EAAYI,GAAI,CACvtBe,GAAI,iDACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,kDACN,aAAc,2DACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,mDAAoD,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,qTAAsT,KAAMa,EAAAA,cAAoBhB,EAAYU,IAAK,KAAMM,EAAAA,cAAoBhB,EAAYW,GAAI,CACjfQ,GAAI,oDACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,qDACN,aAAc,8DACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,qDAAsD,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,2GAA4Ga,EAAAA,cAAoBC,EAAAA,EAAO,CAC/OC,KAAM,cACJ,qCAAsCF,EAAAA,cAAoBC,EAAAA,EAAO,CACnEC,KAAM,cACJ,+BAAgCF,EAAAA,cAAoBC,EAAAA,EAAO,CAC7DC,KAAM,cACJ,KAAM,KAAMF,EAAAA,cAAoBhB,EAAYI,GAAI,CAClDe,GAAI,sBACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,uBACN,aAAc,gCACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,uBAAwB,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,yCAA0Ca,EAAAA,cAAoBC,EAAAA,EAAO,CAC/IC,KAAM,+BACJ,kBAAmBF,EAAAA,cAAoBC,EAAAA,EAAO,CAChDC,KAAM,2BACJ,0CAA2CF,EAAAA,cAAoBC,EAAAA,EAAO,CACxEC,KAAM,kBACJ,KAAM,KAAMF,EAAAA,cAAoBhB,EAAYI,GAAI,CAClDe,GAAI,qBACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,sBACN,aAAc,+BACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,sBAAuB,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,cAAea,EAAAA,cAAoBC,EAAAA,EAAO,CACnHC,KAAM,uBACJ,0BAA2BF,EAAAA,cAAoBC,EAAAA,EAAO,CACxDC,KAAM,YACJ,gBAAiBF,EAAAA,cAAoBC,EAAAA,EAAO,CAC9CC,KAAM,YACJ,+FAAgG,KAAMF,EAAAA,cAAoBhB,EAAYI,GAAI,CAC5Ie,GAAI,kBACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,mBACN,aAAc,4BACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,mBAAoB,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,kRAAmR,KAAMa,EAAAA,cAAoBhB,EAAYU,IAAK,KAAMM,EAAAA,cAAoBhB,EAAYW,GAAI,CAC9aQ,GAAI,iDACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,kDACN,aAAc,2DACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,kDAAmD,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,mJAAqJ,KAAMa,EAAAA,cAAoBhB,EAAYI,GAAI,CACpSe,GAAI,uBACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,wBACN,aAAc,iCACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,yBAA0B,KAAMT,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,wBAAyB,iEAAkE,KAAMO,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,WAAY,0IAA2I,KAAMO,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,2BAA4B,mIAAoI,MAAO,KAAMO,EAAAA,cAAoBhB,EAAYI,GAAI,CAC1yBe,GAAI,yBACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,0BACN,aAAc,mCACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,0BAA2B,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,sKAAwKa,EAAAA,cAAoBC,EAAAA,EAAO,CAChRC,KAAM,YACJ,0CAA2CF,EAAAA,cAAoBC,EAAAA,EAAO,CACxEC,KAAM,YACJ,kBAAoB,KAAMF,EAAAA,cAAoBhB,EAAYI,GAAI,CAChEe,GAAI,kBACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,mBACN,aAAc,4BACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,mBAAoB,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,+QAAgR,KAAMa,EAAAA,cAAoBhB,EAAYU,IAAK,KAAMM,EAAAA,cAAoBhB,EAAYW,GAAI,CAC3aQ,GAAI,4BACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,6BACN,aAAc,sCACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,6BAA8B,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,+LAAgM,KAAMa,EAAAA,cAAoBhB,EAAYI,GAAI,CAC1Te,GAAI,0BACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,2BACN,aAAc,oCACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,2BAA4B,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,YAAaa,EAAAA,cAAoBC,EAAAA,EAAO,CACtHC,KAAM,YACJ,UAAWF,EAAAA,cAAoBC,EAAAA,EAAO,CACxCC,KAAM,YACJ,yMAA0M,KAAMF,EAAAA,cAAoBC,EAAAA,EAAO,CAC7OC,KAAM,kGACJ,KAAMF,EAAAA,cAAoBhB,EAAYI,GAAI,CAC5Ce,GAAI,8CACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,+CACN,aAAc,wDACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,+CAAgD,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,aAAca,EAAAA,cAAoBC,EAAAA,EAAO,CAC3IC,KAAM,+BACJ,6DAA8DF,EAAAA,cAAoBC,EAAAA,EAAO,CAC3FC,KAAM,kBACJ,oDAAqDF,EAAAA,cAAoBC,EAAAA,EAAO,CAClFC,KAAM,qBACJ,KAAM,KAAMF,EAAAA,cAAoBC,EAAAA,EAAO,CACzCC,KAAM,8EACJ,KAAMF,EAAAA,cAAoBhB,EAAYI,GAAI,CAC5Ce,GAAI,2CACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,4CACN,aAAc,qDACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,6CAA8C,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,kCAAmCa,EAAAA,cAAoBC,EAAAA,EAAO,CAC9JC,KAAM,+BACJ,wCAAyCF,EAAAA,cAAoBC,EAAAA,EAAO,CACtEC,KAAM,YACJ,kCAAmC,KAAMF,EAAAA,cAAoBC,EAAAA,EAAO,CACtEC,KAAM,0EACJ,KAAMF,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,SAAUa,EAAAA,cAAoBC,EAAAA,EAAO,CACtFC,KAAM,kBACJ,6BAA8BF,EAAAA,cAAoBC,EAAAA,EAAO,CAC3DC,KAAM,YACJ,6BAA8BF,EAAAA,cAAoBC,EAAAA,EAAO,CAC3DC,KAAM,YACJ,qEAAsEF,EAAAA,cAAoBC,EAAAA,EAAO,CACnGC,KAAM,6CACJ,wHAAyH,KAAMF,EAAAA,cAAoBhB,EAAYU,IAAK,KAAMM,EAAAA,cAAoBhB,EAAYW,GAAI,CAChNQ,GAAI,gDACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,iDACN,aAAc,0DACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,iDAAkD,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,gBAAiBa,EAAAA,cAAoBC,EAAAA,EAAO,CAChJC,KAAM,YACJ,mIAAoIF,EAAAA,cAAoBC,EAAAA,EAAO,CACjKC,KAAM,kBACJ,uBAAwB,KAAMF,EAAAA,cAAoBC,EAAAA,EAAO,CAC3DC,KAAM,6FACJ,KAAMF,EAAAA,cAAoBhB,EAAYI,GAAI,CAC5Ce,GAAI,yBACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,0BACN,aAAc,mCACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,0BAA2B,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,8EAA+E,KAAMa,EAAAA,cAAoBC,EAAAA,EAAO,CAC7LC,KAAM,uGACJ,KAAMF,EAAAA,cAAoBhB,EAAYI,GAAI,CAC5Ce,GAAI,gCACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,iCACN,aAAc,0CACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,kCAAmC,KAAMT,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAM,oFAAqFQ,EAAAA,cAAoBC,EAAAA,EAAO,CACtPC,KAAM,0BACJ,oBAAqBF,EAAAA,cAAoBC,EAAAA,EAAO,CAClDC,KAAM,cACJ,KAAM,KAAMF,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAM,mGAAoGQ,EAAAA,cAAoBC,EAAAA,EAAO,CACvLC,KAAM,uBACJ,iFAAkF,MAAO,KAAMF,EAAAA,cAAoBhB,EAAYI,GAAI,CACrIe,GAAI,iCACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,kCACN,aAAc,2CACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,mCAAoC,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,yMAA0M,KAAMa,EAAAA,cAAoBhB,EAAYU,IAAK,KAAMM,EAAAA,cAAoBhB,EAAYW,GAAI,CACrXQ,GAAI,wBACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,yBACN,aAAc,kCACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,yBAA0B,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,oGAAqGa,EAAAA,cAAoBC,EAAAA,EAAO,CAC5MC,KAAM,YACJ,OAAQF,EAAAA,cAAoBC,EAAAA,EAAO,CACrCC,KAAM,0BACJ,mBAAoBF,EAAAA,cAAoBC,EAAAA,EAAO,CACjDC,KAAM,+BACJ,iHAAkHF,EAAAA,cAAoBC,EAAAA,EAAO,CAC/IC,KAAM,6BACJ,qEAAsEF,EAAAA,cAAoBC,EAAAA,EAAO,CACnGC,KAAM,+BACJ,KAAM,KAAMF,EAAAA,cAAoBhB,EAAYI,GAAI,CAClDe,GAAI,uCACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,wCACN,aAAc,iDACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,wCAAyC,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,iGAAkGa,EAAAA,cAAoBC,EAAAA,EAAO,CACxNC,KAAM,uBACJ,OAAQF,EAAAA,cAAoBC,EAAAA,EAAO,CACrCC,KAAM,sCACJ,oBAAqBF,EAAAA,cAAoBC,EAAAA,EAAO,CAClDC,KAAM,YACJ,QAASF,EAAAA,cAAoBC,EAAAA,EAAO,CACtCC,KAAM,kCACJ,kDAAmD,KAAMF,EAAAA,cAAoBhB,EAAYI,GAAI,CAC/Fe,GAAI,mBACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,oBACN,aAAc,6BACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,oBAAqB,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,OAAQa,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,+BAAgC,iBAAkB,KAAMO,EAAAA,cAAoBC,EAAAA,EAAO,CAChNC,KAAM,qLACJ,KAAMF,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,sCAAuCa,EAAAA,cAAoBC,EAAAA,EAAO,CACnHC,KAAM,kBACJ,QAASF,EAAAA,cAAoBC,EAAAA,EAAO,CACtCC,KAAM,gBACJ,kEAAmEF,EAAAA,cAAoBC,EAAAA,EAAO,CAChGC,KAAM,oBACJ,yBAA0BF,EAAAA,cAAoBC,EAAAA,EAAO,CACvDC,KAAM,kBACJ,+CAAgD,KAAMF,EAAAA,cAAoBhB,EAAYI,GAAI,CAC5Fe,GAAI,iCACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,kCACN,aAAc,2CACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,kCAAmC,KAAMT,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,sCAAuC,sEAAuEO,EAAAA,cAAoBC,EAAAA,EAAO,CAC7TC,KAAM,6BACJ,wFAA2FF,EAAAA,cAAoBC,EAAAA,EAAO,CACxHC,KAAM,YACJ,KAAM,KAAMF,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,4BAA6B,+EAAgFO,EAAAA,cAAoBC,EAAAA,EAAO,CAC9OC,KAAM,gEACJ,KAAMF,EAAAA,cAAoBC,EAAAA,EAAO,CACnCC,KAAM,6CACJ,8DAA+D,MAAO,KAAMF,EAAAA,cAAoBhB,EAAYU,IAAK,KAAMM,EAAAA,cAAoBhB,EAAYW,GAAI,CAC7JQ,GAAI,2DACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,4DACN,aAAc,qEACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,4DAA6D,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,QAASa,EAAAA,cAAoBC,EAAAA,EAAO,CACnJC,KAAM,YACJ,6HAA8HF,EAAAA,cAAoBC,EAAAA,EAAO,CAC3JC,KAAM,qCACJ,iBAAkBF,EAAAA,cAAoBC,EAAAA,EAAO,CAC/CC,KAAM,cACJ,sFAAuFF,EAAAA,cAAoBC,EAAAA,EAAO,CACpHC,KAAM,6BACJ,6FAA8F,KAAMF,EAAAA,cAAoBhB,EAAYI,GAAI,CAC1Ie,GAAI,yBACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,0BACN,aAAc,mCACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,2BAA4B,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,wWAAyW,KAAMa,EAAAA,cAAoBhB,EAAYI,GAAI,CACjee,GAAI,+BACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,gCACN,aAAc,yCACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,gCAAiC,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,wCAAyCa,EAAAA,cAAoBC,EAAAA,EAAO,CACvJC,KAAM,6BACJ,0MAA2M,KAAMF,EAAAA,cAAoBhB,EAAYI,GAAI,CACvPe,GAAI,uBACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,wBACN,aAAc,iCACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,wBAAyB,KAAMT,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,sBAAuB,KAAMO,EAAAA,cAAoBC,EAAAA,EAAO,CAClOC,KAAM,YACJ,iFAAoF,KAAMF,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,yBAA0B,KAAMO,EAAAA,cAAoBC,EAAAA,EAAO,CAC/OC,KAAM,YACJ,6CAA8C,KAAMF,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,mCAAoC,KAAMO,EAAAA,cAAoBC,EAAAA,EAAO,CACnNC,KAAM,YACJ,oFAAqFF,EAAAA,cAAoBC,EAAAA,EAAO,CAClHC,KAAM,YACJ,KAAM,MAAO,KAAMF,EAAAA,cAAoBhB,EAAYU,IAAK,KAAMM,EAAAA,cAAoBhB,EAAYW,GAAI,CACpGQ,GAAI,yCACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,0CACN,aAAc,mDACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,4CAA6C,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,MAAOa,EAAAA,cAAoBC,EAAAA,EAAO,CACjIC,KAAM,YACJ,qHAAsHF,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,iCAAkC,KAAM,KAAMO,EAAAA,cAAoBhB,EAAYI,GAAI,CACxPe,GAAI,4BACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,6BACN,aAAc,sCACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,8BAA+B,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,2BAA4B,KAAMa,EAAAA,cAAoBC,EAAAA,EAAO,CAC9IC,KAAM,wFACJ,KAAMF,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,mBAAoBa,EAAAA,cAAoBC,EAAAA,EAAO,CAChGC,KAAM,+BACJ,2BAA4BF,EAAAA,cAAoBC,EAAAA,EAAO,CACzDC,KAAM,6BACJ,uEAAwE,KAAMF,EAAAA,cAAoBC,EAAAA,EAAO,CAC3GC,KAAM,+GACJ,KAAMF,EAAAA,cAAoBhB,EAAYI,GAAI,CAC5Ce,GAAI,wBACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,yBACN,aAAc,kCACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,yBAA0B,KAAMT,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,QAAS,4CAA6CO,EAAAA,cAAoBC,EAAAA,EAAO,CAC5PC,KAAM,YACJ,cAAeF,EAAAA,cAAoBC,EAAAA,EAAO,CAC5CC,KAAM,YACJ,KAAM,KAAMF,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,0BAA2B,4FAA6FO,EAAAA,cAAoBC,EAAAA,EAAO,CACzPC,KAAM,YACJ,UAAWF,EAAAA,cAAoBC,EAAAA,EAAO,CACxCC,KAAM,+CACJ,WAAY,MAAO,KAAMF,EAAAA,cAAoBhB,EAAYI,GAAI,CAC/De,GAAI,iDACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,kDACN,aAAc,2DACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,oDAAqD,KAAMT,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,qBAAsB,sNAAuN,KAAMO,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,qBAAsB,kCAAmCO,EAAAA,cAAoBC,EAAAA,EAAO,CACrmBC,KAAM,4BACJ,sEAAuE,MAAO,KAAMF,EAAAA,cAAoBhB,EAAYU,IAAK,KAAMM,EAAAA,cAAoBhB,EAAYW,GAAI,CACrKQ,GAAI,gCACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,iCACN,aAAc,0CACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,iCAAkC,KAAMT,EAAAA,cAAoBhB,EAAYI,GAAI,CAC/Ee,GAAI,0BACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,2BACN,aAAc,oCACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,2BAA4B,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,uEAAwE,KAAMa,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,eAAgB,6IAA8I,KAAMO,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,mBAAoB,iIAAkI,MAAO,KAAMO,EAAAA,cAAoBhB,EAAYI,GAAI,CACvuBe,GAAI,kCACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,mCACN,aAAc,4CACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,oCAAqC,KAAMT,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,qBAAsB,kBAAmBO,EAAAA,cAAoBC,EAAAA,EAAO,CAC1PC,KAAM,iDACJ,8BAA+BF,EAAAA,cAAoBC,EAAAA,EAAO,CAC5DC,KAAM,iBACJ,4GAA6G,KAAMF,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,kBAAmB,kEAAmE,KAAMO,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,kBAAmB,4JAA6J,MAAO,KAAMO,EAAAA,cAAoBhB,EAAYI,GAAI,CAClmBe,GAAI,wBACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,yBACN,aAAc,kCACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,yBAA0B,KAAMT,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,sBAAuB,sCAAuCO,EAAAA,cAAoBC,EAAAA,EAAO,CACpQC,KAAM,6BACJ,cAAeF,EAAAA,cAAoBC,EAAAA,EAAO,CAC5CC,KAAM,YACJ,8BAA+B,KAAMF,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,yBAA0B,oDAAqD,KAAMO,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,oCAAqC,+DAAgE,MAAO,KAAMO,EAAAA,cAAoBhB,EAAYU,IAAK,KAAMM,EAAAA,cAAoBhB,EAAYW,GAAI,CAC7eQ,GAAI,gCACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,iCACN,aAAc,0CACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,kCAAmC,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,4FAA6F,KAAMa,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,sBAAuB,sGAAuG,KAAMO,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,4BAA6B,+GAAgH,KAAMO,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,uBAAwB,gHAAiH,MAAO,KAAMO,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,ulBAAwlB,KAAMa,EAAAA,cAAoBhB,EAAYU,IAAK,KAAMM,EAAAA,cAAoBhB,EAAYW,GAAI,CACnnDQ,GAAI,iCACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,kCACN,aAAc,2CACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,kCAAmC,KAAMT,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAM,+DAAkEQ,EAAAA,cAAoBhB,EAAYa,GAAI,KAAM,QAAS,WAAY,KAAMG,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAM,8EAAiFQ,EAAAA,cAAoBhB,EAAYa,GAAI,KAAM,QAAS,WAAY,KAAMG,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAM,4DAA+DQ,EAAAA,cAAoBhB,EAAYa,GAAI,KAAM,YAAa,WAAY,KAAMG,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAM,+HAAkIQ,EAAAA,cAAoBhB,EAAYa,GAAI,KAAM,oBAAqB,WAAY,KAAMG,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAM,uDAA0DQ,EAAAA,cAAoBhB,EAAYa,GAAI,KAAM,gBAAiB,WAAY,KAAMG,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAM,wGAA2GQ,EAAAA,cAAoBhB,EAAYa,GAAI,KAAM,uCAAwC,WAAY,KAAMG,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAM,qGAAwGQ,EAAAA,cAAoBhB,EAAYa,GAAI,KAAM,WAAY,WAAY,KAAMG,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAM,gGAAmGQ,EAAAA,cAAoBhB,EAAYa,GAAI,KAAM,mBAAoB,WAAY,MAAO,KAAMG,EAAAA,cAAoBhB,EAAYU,IAAK,KAAMM,EAAAA,cAAoBhB,EAAYW,GAAI,CACnzDQ,GAAI,wCACJC,MAAO,CACLC,SAAU,aAEXL,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCiB,KAAM,yCACN,aAAc,kDACdC,UAAW,iBACVP,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,0CAA2C,KAAMT,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,0OAA2Oa,EAAAA,cAAoBC,EAAAA,EAAO,CACnWC,KAAM,uBACJ,4BAA6BF,EAAAA,cAAoBC,EAAAA,EAAO,CAC1DC,KAAM,+BACJ,oDAAqDF,EAAAA,cAAoBC,EAAAA,EAAO,CAClFC,KAAM,6BACJ,2BAA4B,KAAMF,EAAAA,cAAoBhB,EAAYG,EAAG,KAAMa,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,QAAS,6LAA8L,KAAMO,EAAAA,cAAoBW,EAAAA,EAAM,CAClWT,KAAM,4jHAmHJ,KAAMF,EAAAA,cAAoBhB,EAAYG,EAAG,KAAMa,EAAAA,cAAoBhB,EAAYa,GAAI,KAAM,wKAAyK,KAAMG,EAAAA,cAAoBhB,EAAYU,IAAK,KAAMM,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,2EAA4Ea,EAAAA,cAAoBhB,EAAYS,OAAQ,KAAM,6BAA8B,kTAC1f,CAKA,MAJA,SAAoBV,QAAK,IAALA,IAAAA,EAAQ,CAAC,GAC3B,MAAO6B,QAASC,GAAa5B,OAAOC,OAAO,CAAC,GAAGY,EAAAA,EAAAA,MAAsBf,EAAMgB,YAC3E,OAAOc,EAAYb,EAAAA,cAAoBa,EAAW9B,EAAOiB,EAAAA,cAAoBlB,EAAmBC,IAAUD,EAAkBC,EAC9H,E,iKCnvCA,MAAM+B,EAAkBC,IAAW,IAAV,IAACC,GAAID,EAC5B,IAAKC,IAAQA,EAAIC,MAAO,OAAO,KAY/B,OAAOjB,EAAAA,cAAoB,MAAO,CAChCO,UAAWW,EAAAA,GACVlB,EAAAA,cAAoB,KAAM,KAAMgB,EAAIC,MAAME,KAAI,CAACC,EAAMC,IAAUrB,EAAAA,cAAoB,KAAM,CAC1FsB,IAAKD,GACJrB,EAAAA,cAAoB,IAAK,CAC1BM,KAAMc,EAAKG,IACXC,QAASC,GAjBSC,EAACD,EAAGF,KACtBE,EAAEE,iBACF,MAAMC,EAAWL,EAAIM,QAAQ,IAAK,IAC5BC,EAAgBC,SAASC,eAAeJ,GAC1CE,GACFA,EAAcG,eAAe,CAC3BC,SAAU,SACVC,MAAO,SAEX,EAQcT,CAAYD,EAAGL,EAAKG,MACjCH,EAAKgB,OAAQhB,EAAKH,OAASjB,EAAAA,cAAoBc,EAAiB,CACjEE,IAAK,CACHC,MAAOG,EAAKH,aAEV,EAED,SAASoB,EAAYC,GAAiD,IAA/CC,MAAM,IAACC,EAAG,OAAEC,EAAM,cAAEC,GAAc,SAAEC,GAASL,EACzE,MAAM,YAACM,EAAW,KAAEC,EAAI,gBAAEC,GAAmBN,EACvCnB,EAAQuB,EAAYvB,MAEpB0B,EADOH,EAAYI,KACJC,MAAM,KAAK,GAE1BC,EADQT,EAAOU,MAAMC,QAAOC,GAAQA,EAAKT,YAAYI,KAAKM,SAAS,IAAIP,QACnDQ,MAAK,CAAClE,EAAGmE,IAAMnE,EAAEuD,YAAYvB,MAAQmC,EAAEZ,YAAYvB,QACvEoC,EAAeP,EAAYQ,WAAUL,GAAQA,EAAKT,YAAYvB,QAAUA,IACxEsC,EAAWT,EAAYO,EAAe,GACtCG,EAAWV,EAAYO,EAAe,GACtCI,EAAcjB,EAAYI,KAAKnB,QAAQ,MAAO,IAC9CiC,EAAc,SAAUC,KAAKF,GAAa,GAC1CG,EAAW,SAASjB,aAAmBe,MACvC,EAACG,EAAY,EAAEC,IAAmBC,EAAAA,EAAAA,UAASvB,EAAYwB,0BACvD,EAACC,EAAW,EAAEC,IAAkBH,EAAAA,EAAAA,WAAS,GAS/C,IAAII,GALJC,EAAAA,EAAAA,YAAU,KACRF,GAAe,GACf,MAAMG,EAAQC,YAAW,IAAMJ,GAAe,IAAQ,KACtD,MAAO,IAAMK,aAAaF,EAAM,GAC/B,CAACR,IAEY,eAAZlB,EACFwB,EAAiBK,EAAAA,GACI,aAAZ7B,EACTwB,EAAiBM,EAAAA,GACI,aAAZ9B,IACTwB,EAAiBO,EAAAA,IAEnB,MACMC,EADgBC,IAAenC,GAAMhB,QAAQ,wBAAyB,IAAIA,QAAQ,SAAU,IAAIA,QAAQ,wBAAyB,IAAIoD,OAC3GhC,MAAM,OAAOiC,OAIvCC,EA5ER,SAAwBC,GACtB,GAAIA,GAAW,GAAI,MAAO,UAC1B,GAAIA,GAAW,GAAI,MAAO,UAC1B,GAAIA,GAAW,GAAI,MAAO,UAC1B,GAAIA,GAAW,GAAI,MAAO,UAC1B,GAAIA,GAAW,GAAI,MAAO,UAC1B,GAAIA,GAAW,GAAI,MAAO,OAC1B,MAAMC,EAAQC,KAAKC,MAAMH,EAAU,IAC7BI,EAAYJ,EAAU,GAC5B,OAAII,GAAa,GACR,IAAIH,IAAQG,EAAY,EAAI,KAAO,OAErC,IAAIH,EAAQ,KACrB,CA+DmBI,CAHWH,KAAKI,KAAKX,EAAYR,IAChC3B,EAAY+C,kBAAoB,IAG5CC,EAAU,CAAC,CACfC,KAAMjD,EAAYkD,UAClBC,UAAWA,IAAM,0DAChB,CACDF,KAAMjD,EAAYoD,gBAClBD,UAAWA,IAAM,0DAChB,CACDF,KAAMjD,EAAYqD,YAClBF,UAAWA,IAAM,0DAChB,CACDF,KAAMjD,EAAYsD,cAClBH,UAAWA,IAAM,0DAChB,CACDF,KAAMjD,EAAYuD,YAClBJ,UAAWA,IAAM,0DAChB,CACDF,KAAMjD,EAAYwD,iBAClBL,UAAWA,IAAM,0DAChB,CACDF,KAAMjD,EAAYyD,eAClBN,UAAWA,IAAM,yDAChB,CACDF,KAAMjD,EAAY0D,cAClBP,UAAWA,IAAM,0DAChB,CACDF,KAAMjD,EAAY2D,kBAClBR,UAAWA,IAAM,yDAChB,CACDF,KAAMjD,EAAY4D,WAClBT,UAAWA,IAAM,4DAEb,EAACU,EAAa,EAAEC,IAAoBvC,EAAAA,EAAAA,UAAS,IAUnD,OATAK,EAAAA,EAAAA,YAAU,KACRoB,EAAQe,SAAQC,IAAuB,IAAtB,KAACf,EAAI,UAAEE,GAAUa,EAC5Bf,GACFE,IAAYc,MAAKC,IACfJ,GAAiBK,GAAQ,GAAJC,QAAAC,EAAAA,EAAAA,GAAQF,GAAI,CAAED,EAAOI,WAAS,GAEvD,GACA,GACD,IACIlH,EAAAA,cAAoBmH,EAAAA,EAAOC,IAAK,CACrCC,QAAS,CACPC,QAAS,GAEXC,QAAS,CACPD,QAAS,GAEXE,KAAM,CACJF,QAAS,GAEXG,WAAY,CACVC,SAAU,MAEX1H,EAAAA,cAAoB2H,EAAAA,EAAY,CACjCC,WAAYhF,EAAYvB,MACxBwG,KAAMjF,EAAYiF,KAClBC,QAASlF,EAAYkF,QACrB3C,SAAUA,EACV4C,WAAYnF,EAAYoF,gBACxB5F,MAAOQ,EAAYR,MACnB6F,KAAMrF,EAAYqF,KAClBC,OAAQtF,EAAYsF,OACpBnF,QAASA,EACToF,QAASrE,EACTsE,cAAexF,EAAYoD,gBAC3BqC,QAASzF,EAAYyF,UACnBrI,EAAAA,cAAoB,MAAO,CAC7BI,MAAO,CACLkI,QAAS,OACTC,eAAgB,WAChBC,SAAU,OACVC,SAAU,MACVC,WAAY,OACZC,aAAc,MACdC,UAAW,OACXC,aAAc,QAEfjG,EAAYkG,UAAU3H,KAAI,CAAC4H,EAAK1H,IAAUrB,EAAAA,cAAoB,OAAQ,CACvEsB,IAAKD,EACLd,UAAW,YAAYyI,EAAAA,KACvB5I,MAAO,CACL6I,OAAQ,gBAETF,MAAQ/I,EAAAA,cAAoB,MAAO,CACpCkJ,MAAO,YACNlJ,EAAAA,cAAoBc,EAAiB,CACtCE,IAAK8B,KACF9C,EAAAA,cAAoB,MAAOA,EAAAA,cAAoB,MAAO,CACzDI,MAAO,CACL6I,OAAQ,iBACRE,UAAW,UAEZnJ,EAAAA,cAAoBmH,EAAAA,EAAOiC,OAAQ,CACpCF,MAAO,WACP3I,UAAW8I,EAAAA,GACXlJ,GAAIkJ,EAAAA,GACJ7H,QAvHmB8H,KACnBpF,GAAiBD,EAAa,EAuH9BsF,SAAU,CACRC,MAAO,MAERxJ,EAAAA,cAAoBmH,EAAAA,EAAOC,IAAK,CACjC7G,UAAWkJ,EAAAA,GACXnI,IAAK2C,EACLoD,QAAS,CACPC,QAAS,GAEXC,QAAS,CACPD,QAAS,GAEXE,KAAM,CACJF,QAAS,GAEXG,WAAY,CACVC,SAAU,GACVgC,KAAM,cAEPzF,EAAe,2BAA6B,2BAA4BjE,EAAAA,cAAoB,MAAOA,EAAAA,cAAoB,MAAO,CAC/HkJ,MAAO,WACP9I,MAAO,CACL6I,OAAQhF,EAAe,SAAW,GAClCwE,SAAUxE,EAAe,OAAS,GAClCwD,WAAY,uDAEbzH,EAAAA,cAAoB,MAAO,CAC5BO,UAAW,GAAG8I,EAAAA,MAAuChF,EAAcgF,EAAAA,GAAkCA,EAAAA,MACpG5C,EAActF,KAAI,CAACwI,EAAiBtI,IAAUrB,EAAAA,cAAoB2J,EAAiB,CACpFrI,IAAKD,MACFuB,EAAYgH,YAAc5J,EAAAA,cAAoB6J,EAAAA,EAAoB,CACrExI,MAAOuB,EAAYgH,YACnBE,SAAUlH,EAAYmH,qBACnB,GAAI/J,EAAAA,cAAoBgK,EAAAA,EAAaC,SAAU,CAClDC,MAAO,CACLC,OAAQzH,EAAcS,MACtBa,SAAUA,EAASnC,QAAQ,MAAO,IAAM,MAEzC7B,EAAAA,cAAoBoK,EAAAA,GAAa,CAClCrK,WAAY,CACVsK,MAAKA,EAAAA,IAEN1H,MAAc3C,EAAAA,cAAoBsK,EAAAA,EAAY,CAC/C3G,SAAUA,EACVC,SAAUA,EACVE,WAAYA,EACZf,QAASA,IAEb,CAEe,SAASwH,EAAiBxL,GACvC,OAAOiB,EAAAA,cAAoBqC,EAActD,EAAOiB,EAAAA,cAAoBwK,EAAqBzL,GAC3F,CACO,SAAS0L,EAAIC,GAAS,IAAAC,EAAAC,EAAAC,EAAAC,EAAAC,EAAA,IAAR,KAACxI,GAAKmI,EACzB,MAAM,YAAC9H,GAAeL,EAAKC,IACrBJ,EAAQQ,EAAYoI,UAAYpI,EAAYR,MAC5C6I,EAAUrI,EAAYqI,SAAW7I,EACjC8I,EAAetI,EAAYsI,cAAgB9I,EAC3C+I,EAAcvI,EAAYwI,SAAWxI,EAAYqF,KACjDoD,EAAgBzI,EAAY0I,QAAUH,EACtCI,EAAqB3I,EAAY4I,aAAeL,EAChDM,EAAa7I,EAAY6I,YAAc,cACvCC,EAAW9I,EAAY+I,YACvBC,EAAgBhJ,EAAYiF,KAC5BgE,EAAejJ,EAAYkF,SAAW8D,EACtCE,EAAUlJ,EAAYkJ,UAA6B,QAAtBnB,EAAI/H,EAAYsF,cAAM,IAAAyC,GAAiB,QAAjBC,EAAlBD,EAAoBoB,uBAAe,IAAAnB,GAAiB,QAAjBC,EAAnCD,EAAqCoB,uBAAe,IAAAnB,GAAQ,QAARC,EAApDD,EAAsDV,cAAM,IAAAW,GAAU,QAAVC,EAA5DD,EAA8DmB,gBAAQ,IAAAlB,OAApD,EAAlBA,EAAwEmB,KACzGC,EAAavJ,EAAYuJ,YAAcd,EACvCe,EAAexJ,EAAYwJ,cAAgBN,EAC3CO,EAAkBzJ,EAAYyJ,iBAAmBd,EACjDe,EAAe1J,EAAY2J,aAC3B/F,EAAa5D,EAAY4D,aAAc,EACvC6B,EAAUzF,EAAYyF,SAAW,QACjCtF,EAAUH,EAAYI,KAAKC,MAAM,KAAK,IAAM,SAE5C,QAACuJ,IAAWC,EAAAA,EAAAA,KACZC,EAAiB,CACrB,WAAY,qBACZ,QAAS,iBACT,gBAAmB,CAAC,CAClB,QAAS,WACT,SAAY,EACZ,KAAQ,OACR,KAAQF,GACP,CACD,QAAS,WACT,SAAY,EACZ,KAAQnE,EACR,KAAQ,GAAGmE,KAAW5J,EAAYI,KAAKC,MAAM,KAAK,MACjD,CACD,QAAS,WACT,SAAY,EACZ,KAAQb,EACR,KAAQ,GAAGoK,IAAU5J,EAAYI,UAGrC,OAAOhD,EAAAA,cAAoB2M,EAAAA,EAAK,CAC9BvK,MAAOA,EAAQ,gBACf6I,QAASA,EACTC,aAAcA,EACdC,YAAaA,EACbE,cAAeA,EACfE,mBAAoBA,EACpBE,WAAYA,EACZC,SAAUA,EACVE,cAAeA,EACfC,aAAcA,EACdC,QAASA,EACTK,WAAYA,EACZC,aAAcA,EACdC,gBAAiBA,EACjBC,aAAcA,EACd9F,WAAYA,EACZ6B,QAASA,EACTtF,QAASA,EACT6J,KAzCW,WA0CV5M,EAAAA,cAAoB,SAAU,CAC/B4M,KAAM,uBACLC,KAAKC,UAAUJ,IACpB,C,iDC9SA,IALU3L,IAAe,IAAd,KAAEb,GAAMa,EACjB,OACEf,EAAAA,cAACC,EAAAA,EAAK,KAAEC,EAAa,C","sources":["webpack://avrtt.blog/./src/pages/posts/research/deep_probabilistic_models.mdx","webpack://avrtt.blog/./src/templates/post.js","webpack://avrtt.blog/./src/components/Latex/index.js"],"sourcesContent":["/*@jsxRuntime classic @jsx React.createElement @jsxFrag React.Fragment*/\n/**(intro: a quote, catchphrase, joke, etc.)**/\n/*\n\n- [DPM1 - Deep Probabilistic Models I](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/deep_probabilistic_models_I/tutorial_1.html)\n- [0. General notes](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/deep_probabilistic_models_I/tutorial_1.html#0.-General-notes)\n- [1. Data](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/deep_probabilistic_models_I/tutorial_1.html#1.-Data)\n- [2. Text encoders](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/deep_probabilistic_models_I/tutorial_1.html#2.-Text-encoders)\n- [3. Probabilistic model](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/deep_probabilistic_models_I/tutorial_1.html#3.-Probabilistic-model)\n- [4. Parameter estimation](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/deep_probabilistic_models_I/tutorial_1.html#4.-Parameter-estimation)\n- [5. Decision rules](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/deep_probabilistic_models_I/tutorial_1.html#5.-Decision-rules)\n- [6. Training procedure](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/deep_probabilistic_models_I/tutorial_1.html#6.-Training-procedure)\n- [7. Examples](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/deep_probabilistic_models_I/tutorial_1.html#7.-Examples)\n- [8. Exercise: design your own deep probabilistic model (optional, non-graded)](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/deep_probabilistic_models_I/tutorial_1.html#8.-Exercise:-design-your-own-deep-probabilistic-model-(optional,-non-graded))\n- [DPM2 - Variational inference for deep discrete latent variable models](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/deep_probabilistic_models_II/tutorial_2a.html)\n- [0. Intended Learning Outcomes](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/deep_probabilistic_models_II/tutorial_2a.html#0.-Intended-Learning-Outcomes)\n- [1. Data](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/deep_probabilistic_models_II/tutorial_2a.html#1.-Data)\n- [2. Latent variable models](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/deep_probabilistic_models_II/tutorial_2a.html#2.-Latent-variable-models)\n- [3. Learning](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/deep_probabilistic_models_II/tutorial_2a.html#3.-Learning)\n- [4. Beyond](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/deep_probabilistic_models_II/tutorial_2a.html#4.-Beyond)\n- [DPM 2 - Variational Inference for Deep Continuous LVMs](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/deep_probabilistic_models_II/tutorial_2b.html)\n- [0. Intended Learning Outcomes](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/deep_probabilistic_models_II/tutorial_2b.html#0.-Intended-Learning-Outcomes)\n- [1. Data](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/deep_probabilistic_models_II/tutorial_2b.html#1.-Data)\n- [2. Latent variable models](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/deep_probabilistic_models_II/tutorial_2b.html#2.-Latent-variable-models)\n- [3. Learning](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/deep_probabilistic_models_II/tutorial_2b.html#3.-Learning)\n- [4. Beyond](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/deep_probabilistic_models_II/tutorial_2b.html#4.-Beyond)\n\n*/\n/*\n\n1. Introduction to Deep Probabilistic Models\n- What Are Deep Probabilistic Models?: Overview of how \"deep\" methods merge with probability theory to create flexible, interpretable models.\n- Motivation and Applications: Why uncertainty matters in machine learning, and how deep probabilistic approaches can help in NLP, computer vision, time-series, etc.\n- Key Distinctions: Differences between purely deterministic neural networks and probabilistic/bayesian frameworks.\n2. Topic-Related Probability Refresher\n- Random Variables: Importance of discrete vs. continuous variables, and why we need both in deep probabilistic modeling.\n- Joint & Conditional Distributions: How to define joint distributions; conditional probability as a cornerstone of deep probabilistic methods.\n- Marginalization & Factorization: Summation and integration as fundamental operations; connecting them to model parameterization.\n3. Likelihood Function, Again\n- How maximum likelihood estimation (MLE) underpins many deep probabilistic models.\n- Log-Likelihood vs. Likelihood: Numerical stability considerations; the significance of log-likelihood in large-scale training.\n4. Bayesian Networks and Graphical Models\n- Directed Graphical Models: How Bayesian networks encode conditional dependencies among variables.\n- Inference in Bayesian Networks: Summation over latent variables, message-passing interpretations, and approximate methods.\n- Modeling Complex Systems: Examples where Bayesian networks help factorize complicated distributions (e.g., disease diagnosis, sensor fusion).\n5. Hidden Markov Models and Deep Probabilistic Models\n6. Viterbi Algorithm for Sequence Decoding\n- Viterbi Recurrence: Explanation of how the algorithm finds the most likely hidden state sequence in an HMM.\n- Use Cases: Part-of-speech tagging in NLP, speech recognition decoding, other sequence prediction tasks.\n- Comparison with Other Decoding Methods: Greedy vs. exhaustive decoding, and the complexities involved.\n7. Baum-Welch Algorithm for HMM Parameter Estimation\n- EM Approach: The Expectation-Maximization principle behind Baum-Welch; iterative refinement of transition/emission parameters.\n- Implementation Details: Key steps in practical code, dealing with underflow and large state spaces.\n- Extensions Beyond HMM: How the same EM perspective generalizes to other latent variable models.\n8. Deep Probabilistic Models in Time-Series Analysis\n9. Text Pre-processing for Probabilistic Models\n- Tokenization & Normalization: Ensuring consistent input to deep probabilistic NLP pipelines.\n- Handling Unknown Words/Out-of-Vocabulary: Influence on building robust probability distributions for text.\n- Feature Engineering vs. Learned Representations: Transition from manual text features to embedding-based approaches.\n10. Part-of-Speech Tagging with Probabilistic Methods\n- HMM for POS Tagging: Classic application showcasing hidden states as POS labels and emissions as words.\n- Viterbi in Tagging: How the algorithm finds best POS sequences, including constraints (e.g., unknown tokens).\n- Deep Extensions: Incorporating neural encoders or deep CRFs for improved performance.\n11. IBM Watson and Practical Large-Scale Inference\n- Watson's Architecture: High-level overview of how probabilistic reasoning integrates with large corpora in QA systems.\n- ML Pipelines in Watson: Combining text pre-processing, search-based modules, and scoring in a probabilistic ensemble.\n- Lessons Learned: Why focusing on uncertainty estimation can improve real-world systems.\n12. Deep Probabilistic Models\n- Univariate Conditionals: Designing conditional probability distributions for discrete and continuous targets.\n- Parameter Estimation via Maximum Likelihood: Implementation details, PyTorch or similar frameworks.\n- Decision Rules & Bayesian Decision Theory: From utility-based decisions to maximizing expected gains.\n13. Advanced Autoregressive and Structured Models\n- Autoregressive Taggers: Sequence labeling and factorization of joint probabilities (chain rule).\n- Exact vs. Approximate Decoding: Why exact search is often intractable for large output spaces.\n- Beam Search & Greedy Approaches: Practical solutions and their trade-offs.\n14. Variational Inference\n- Importance of Latent Variable Models: When latent dimensions are large or continuous, direct marginalization becomes infeasible.\n- ELBO Formulation: The evidence lower bound as the cornerstone for approximate inference.\n- Gradient Estimation Techniques: Score-function vs. reparameterization trick; trade-offs in variance.\n15. Variational Inference for Deep Discrete Latent Variables\n- Discrete vs. Continuous: Challenges arising from discrete latent spaces, enumerations, and high variance gradient estimates.\n- Neural Variational Inference: Using neural networks to parameterize both generative and inference models.\n- Examples in Practice: Mixture models, discrete autoencoders, or VQ-VAE architectures.\n16. Continuous Latent Variable Models (VAEs)\n- Gaussian Prior & Posterior: Classic setup for a variational autoencoder with reparameterizable distributions.\n- Decoder Architectures: FFNN vs. CNN decoders; how the choice of architecture impacts reconstruction fidelity.\n- Extensions: Normalizing Flows, Hierarchical VAEs: Achieving richer latent distributions or deeper hierarchical structures.\n17. Practical Implementation Tips\n- Hardware Considerations: GPU memory usage, batch sizes, and stable training for large-scale data.\n- Hyperparameters & Regularization: Balancing the KL term vs. reconstruction term, setting learning rates, weight decay.\n- Debugging Convergence: Common pitfalls such as posterior collapse, mode-seeking behavior, or vanishing gradients.\n18. Future Directions & Conclusion\n- Scalable Inference: Stochastic, distributed, or streaming approaches for extremely large datasets.\n- Structured Latent Spaces: Incorporating domain knowledge into the latent variable structure for interpretability.\n- Research Frontier: Hybrid models (e.g., combining HMM and deep networks), bridging symbolic AI (IBM Watson) with advanced DPMs.\n\n*/\nimport {useMDXComponents as _provideComponents} from \"@mdx-js/react\";\nimport React from \"react\";\nimport Highlight from \"../../../components/Highlight\";\nimport Code from \"../../../components/Code\";\nimport Latex from \"../../../components/Latex\";\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    h3: \"h3\",\n    a: \"a\",\n    span: \"span\",\n    ul: \"ul\",\n    li: \"li\",\n    strong: \"strong\",\n    hr: \"hr\",\n    h2: \"h2\",\n    ol: \"ol\",\n    em: \"em\"\n  }, _provideComponents(), props.components);\n  return React.createElement(React.Fragment, null, \"\\n\", React.createElement(\"br\"), \"\\n\", \"\\n\", \"\\n\", React.createElement(_components.p, null, \"Deep probabilistic models are machine learning methods that systematically combine the representational power of deep neural networks with principled probabilistic frameworks. On one hand, neural networks excel at modeling complex functions over high-dimensional data; on the other hand, probability theory provides a robust foundation for handling uncertainty and for reasoning under incomplete information. A deep probabilistic model, in essence, leverages both: it includes a deep architecture (e.g., a feed-forward network, convolutional layers, recurrent cells, or more advanced structures) and a probabilistic formulation for latent variables, observed data, or both.\"), \"\\n\", React.createElement(_components.p, null, \"In classical machine learning, a neural network typically gives you a single-point estimate (a deterministic mapping from inputs to outputs). Deep probabilistic models generalize this viewpoint. Instead of asking, \\\"\\\"What is the single best output?\\\" we ask, \\\"\\\"What is the probability distribution over possible outputs (or latent states), given the observed data?\\\" This is particularly valuable in scenarios where the data may be noisy, partially observed, or very high dimensional.\"), \"\\n\", React.createElement(_components.p, null, \"Furthermore, many deep probabilistic models adopt latent variable frameworks. A latent variable (often denoted \", React.createElement(Latex, {\n    text: \"\\\\(z\\\\)\"\n  }), \") is a hidden random variable that we do not directly observe but believe can explain important regularities in the data \", React.createElement(Latex, {\n    text: \"\\\\(x\\\\)\"\n  }), \". By positing a distribution \", React.createElement(Latex, {\n    text: \"\\\\(p(z)\\\\)\"\n  }), \" and a conditional \", React.createElement(Latex, {\n    text: \"\\\\(p(x \\\\mid z)\\\\)\"\n  }), \", we create flexible and interpretable generative models that can capture complex data distributions without relying solely on direct parameterization in the \", React.createElement(Latex, {\n    text: \"\\\\(x\\\\)\"\n  }), \"-space.\"), \"\\n\", React.createElement(_components.p, null, \"As we progress through this article, we will encounter many specific examples of deep probabilistic models: from Bayesian neural networks and graphical models to deep latent variable models such as variational autoencoders (VAEs) and deep generative approaches used in large-scale systems. Our focus will be on the underlying probability theory, the algorithmic frameworks for inference (both exact and approximate), and the interplay between deep architectures and uncertainty modeling.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"motivation-and-applications\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#motivation-and-applications\",\n    \"aria-label\": \"motivation and applications permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"motivation and applications\"), \"\\n\", React.createElement(_components.p, null, \"The motivation for adopting a probabilistic (rather than purely deterministic) perspective in deep learning is rooted in a need for uncertainty quantification and structured representations. Some example domains include:\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Natural language processing\"), \": Words and sentences are often ambiguous, and their interpretations can be best captured in a probabilistic sense (e.g., multiple meanings of a phrase).\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Vision\"), \": An image may have occlusions, multiple objects in uncertain positions, or otherwise incomplete evidence. A probabilistic framework can model the variety of plausible scenes or segmentations.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Reinforcement learning\"), \": In sequential decision-making, the environment's states and transitions are typically uncertain. A deep probabilistic viewpoint can handle partial observability or belief states.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Time-series\"), \": Future events in a sequence can be modeled with predictive distributions, capturing the variance and possible future trajectories.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Large-scale web systems\"), \": For example, in recommendation or question-answering systems (think IBM Watson), we often combine multiple candidate sources of evidence in a probabilistic ensemble. This can help calibrate confidence scores or guide the search among candidate answers.\"), \"\\n\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"key-distinctions\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#key-distinctions\",\n    \"aria-label\": \"key distinctions permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"key distinctions\"), \"\\n\", React.createElement(_components.p, null, \"The main difference between purely deterministic neural networks and deep probabilistic or Bayesian frameworks lies in how they treat parameters and predictions:\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Deterministic neural networks\"), \": They learn a single set of network weights. Once trained, they output a single deterministic prediction (though they can appear stochastic if some dropout or random data augmentation is used at inference, that typically is not part of a principled probabilistic mechanism).\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Probabilistic/Bayesian neural networks\"), \": They treat weights (and/or outputs) as random variables. In a Bayesian approach, you maintain a distribution over weights and integrate over that distribution to make predictions. In many latent variable models, part of the model is a distribution that explains unobserved factors. The prediction is a probability distribution over possible outcomes, not just a single point estimate.\"), \"\\n\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.h2, {\n    id: \"topic-related-probability-refresher\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#topic-related-probability-refresher\",\n    \"aria-label\": \"topic related probability refresher permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"topic-related probability refresher\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"random-variables\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#random-variables\",\n    \"aria-label\": \"random variables permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"random variables\"), \"\\n\", React.createElement(_components.p, null, \"A random variable is a variable that can take on different values according to some probability distribution. In the context of deep probabilistic models:\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Discrete random variables\"), \": typically used for categorical phenomena (e.g., a class label for classification, or the presence/absence of certain attributes). For instance, in text generation, you might have discrete variables representing tokens.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Continuous random variables\"), \": typically used for real-valued phenomena (e.g., the location of an object in an image, or a latent code in a variational autoencoder). Gaussian or related distributions often appear in these settings.\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, \"Many deep latent variable models, like VAEs, contain continuous latent variables, while other deep models for text and NLP might incorporate discrete latent structures.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"joint--conditional-distributions\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#joint--conditional-distributions\",\n    \"aria-label\": \"joint  conditional distributions permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"joint & conditional distributions\"), \"\\n\", React.createElement(_components.p, null, \"For random variables \", React.createElement(Latex, {\n    text: \"\\\\(X\\\\)\"\n  }), \" and \", React.createElement(Latex, {\n    text: \"\\\\(Y\\\\)\"\n  }), \", the joint distribution \", React.createElement(Latex, {\n    text: \"\\\\(p(x, y)\\\\)\"\n  }), \" encodes the probabilities or densities for pairs of values \", React.createElement(Latex, {\n    text: \"(x, y)\"\n  }), \". Conditional distributions appear when we condition on one variable to get \", React.createElement(Latex, {\n    text: \"\\\\(p(y \\\\mid x)\\\\)\"\n  }), \". In a deep model, we often define a distribution of the form:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\np_\\\\theta(x, z) = p_\\\\theta(z)\\\\, p_\\\\theta(x \\\\mid z),\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"where \", React.createElement(Latex, {\n    text: \"\\\\(z\\\\)\"\n  }), \" is a latent variable. This factorization into \", React.createElement(Latex, {\n    text: \"\\\\(p_\\\\theta(z)\\\\)\"\n  }), \" (the prior) and \", React.createElement(Latex, {\n    text: \"\\\\(p_\\\\theta(x \\\\mid z)\\\\)\"\n  }), \" (the likelihood or observation model) is central to many generative models.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"marginalization--factorization\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#marginalization--factorization\",\n    \"aria-label\": \"marginalization  factorization permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"marginalization & factorization\"), \"\\n\", React.createElement(_components.p, null, \"Marginalization is the operation of integrating or summing out hidden variables. For example, to obtain \", React.createElement(Latex, {\n    text: \"\\\\(p_\\\\theta(x)\\\\)\"\n  }), \", we write:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\np_\\\\theta(x) = \\\\sum_{z} p_\\\\theta(x, z) \\\\quad \\\\text{(if \\\\(z\\\\) is discrete)}\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"or\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\np_\\\\theta(x) = \\\\int p_\\\\theta(x, z)\\\\, dz \\\\quad \\\\text{(if \\\\(z\\\\) is continuous).}\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"In large-scale deep probabilistic models, exactly performing this sum or integral is often intractable, which motivates approximate inference strategies like variational methods.\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.h2, {\n    id: \"likelihood-function-again\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#likelihood-function-again\",\n    \"aria-label\": \"likelihood function again permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"likelihood function, again\"), \"\\n\", React.createElement(_components.p, null, \"The likelihood function \", React.createElement(Latex, {\n    text: \"\\\\(L(\\\\theta)\\\\)\"\n  }), \" for observed data \", React.createElement(Latex, {\n    text: \"\\\\(x\\\\)\"\n  }), \" is simply \", React.createElement(Latex, {\n    text: \"\\\\(p_\\\\theta(x)\\\\)\"\n  }), \" viewed as a function of \", React.createElement(Latex, {\n    text: \"\\\\(\\\\theta\\\\)\"\n  }), \". Maximizing \", React.createElement(Latex, {\n    text: \"\\\\(p_\\\\theta(x)\\\\)\"\n  }), \" typically corresponds to \\\"fitting\\\" or \\\"training\\\" the model parameters \", React.createElement(Latex, {\n    text: \"\\\\(\\\\theta\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"MLE (maximum likelihood estimation)\"), \": we choose \", React.createElement(Latex, {\n    text: \"\\\\(\\\\theta\\\\)\"\n  }), \" to maximize \", React.createElement(Latex, {\n    text: \"\\\\(p_\\\\theta(x)\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Log-likelihood\"), \": often used for numerical stability. We prefer \", React.createElement(Latex, {\n    text: \"\\\\(\\\\log p_\\\\theta(x)\\\\)\"\n  }), \" in optimization, which turns products into sums and can help avoid underflow in large-scale data.\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, \"Consider a dataset \", React.createElement(Latex, {\n    text: \"\\\\( \\\\{x_i\\\\}_{i=1}^N\\\\)\"\n  }), \". Under an i.i.d. assumption, the likelihood is \", React.createElement(Latex, {\n    text: \"\\\\(\\\\prod_{i=1}^N p_\\\\theta(x_i)\\\\)\"\n  }), \", or in log-form \", React.createElement(Latex, {\n    text: \"\\\\(\\\\sum_{i=1}^N \\\\log p_\\\\theta(x_i)\\\\)\"\n  }), \". Almost all modern large-scale approaches in deep probabilistic models rely on gradient-based optimization of this log-likelihood or some suitable proxy objective (like the evidence lower bound, or ELBO).\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.h2, {\n    id: \"bayesian-networks-and-graphical-models\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#bayesian-networks-and-graphical-models\",\n    \"aria-label\": \"bayesian networks and graphical models permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"bayesian networks and graphical models\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"directed-graphical-models\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#directed-graphical-models\",\n    \"aria-label\": \"directed graphical models permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"directed graphical models\"), \"\\n\", React.createElement(_components.p, null, \"A Bayesian network is a directed acyclic graph whose nodes represent random variables, and edges encode direct conditional dependencies. It factorizes a joint distribution as a product of local conditionals:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\np(x_1, \\\\ldots, x_n) = \\\\prod_{i=1}^n p(x_i \\\\mid \\\\text{Parents}(x_i)).\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"In a deep setting, imagine you have hidden layers \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathbf{z}_1, \\\\mathbf{z}_2, \\\\ldots\\\\)\"\n  }), \" forming a deep generative chain. A simplified example might be:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\mathbf{z}_1 \\\\sim p_\\\\theta(\\\\mathbf{z}_1), \\\\quad\\n\\\\mathbf{z}_2 \\\\sim p_\\\\theta(\\\\mathbf{z}_2 \\\\mid \\\\mathbf{z}_1), \\\\quad\\n\\\\ldots, \\\\quad\\n\\\\mathbf{x} \\\\sim p_\\\\theta(\\\\mathbf{x} \\\\mid \\\\mathbf{z}_k).\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"Deep Bayesian networks can represent complicated dependencies, but often come at the cost of more complex inference.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"inference-in-bayesian-networks\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#inference-in-bayesian-networks\",\n    \"aria-label\": \"inference in bayesian networks permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"inference in bayesian networks\"), \"\\n\", React.createElement(_components.p, null, \"Given a Bayesian network \", React.createElement(Latex, {\n    text: \"\\\\(p_\\\\theta(\\\\mathbf{x}, \\\\mathbf{z})\\\\)\"\n  }), \", we want \", React.createElement(Latex, {\n    text: \"\\\\(p_\\\\theta(\\\\mathbf{z} \\\\mid \\\\mathbf{x})\\\\)\"\n  }), \" or the marginal \", React.createElement(Latex, {\n    text: \"\\\\(p_\\\\theta(\\\\mathbf{x})\\\\)\"\n  }), \". Exact summation or integration over \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathbf{z}\\\\)\"\n  }), \" can be expensive or entirely intractable, especially as the dimension or structure grows. Instead, approximate methods (message passing, MCMC, variational inference) are used.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"modeling-complex-systems\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#modeling-complex-systems\",\n    \"aria-label\": \"modeling complex systems permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"modeling complex systems\"), \"\\n\", React.createElement(_components.p, null, \"Graphical models shine when you want to incorporate domain knowledge in conditional structure. For instance, in sensor fusion or medical diagnosis, you might structure your Bayesian network so it captures well-known conditional independencies. Or in large-scale QA systems (like IBM Watson), a Bayesian network can orchestrate how multiple candidate evidence sources combine into a final answer with a model of uncertainty.\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.h2, {\n    id: \"hidden-markov-models-and-deep-probabilistic-models\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#hidden-markov-models-and-deep-probabilistic-models\",\n    \"aria-label\": \"hidden markov models and deep probabilistic models permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"hidden markov models and deep probabilistic models\"), \"\\n\", React.createElement(_components.p, null, \"A Hidden Markov Model (HMM) is a type of Bayesian network specialized for sequence data, with the structure \", React.createElement(Latex, {\n    text: \"\\\\(z_1 \\\\rightarrow z_2 \\\\rightarrow \\\\cdots \\\\rightarrow z_T\\\\)\"\n  }), \" and \", React.createElement(Latex, {\n    text: \"\\\\(x_t \\\\leftarrow z_t\\\\)\"\n  }), \" for \", React.createElement(Latex, {\n    text: \"\\\\(t=1,2,\\\\ldots,T\\\\)\"\n  }), \". The latent states \", React.createElement(Latex, {\n    text: \"\\\\(z_t\\\\)\"\n  }), \" form a Markov chain, and each \", React.createElement(Latex, {\n    text: \"\\\\(x_t\\\\)\"\n  }), \" depends only on the corresponding \", React.createElement(Latex, {\n    text: \"\\\\(z_t\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Deep HMMs\"), \" can incorporate deep neural layers in the emission or transition probabilities. For instance, \", React.createElement(Latex, {\n    text: \"\\\\(p_\\\\theta(x_t \\\\mid z_t)\\\\)\"\n  }), \" might be parameterized by a neural network. Alternatively, we can chain multiple layers of hidden states. Although standard HMMs are limited in expressiveness, adding neural architectures can yield significantly richer sequence models.\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.h2, {\n    id: \"viterbi-algorithm-for-sequence-decoding\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#viterbi-algorithm-for-sequence-decoding\",\n    \"aria-label\": \"viterbi algorithm for sequence decoding permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"viterbi algorithm for sequence decoding\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"viterbi-recurrence\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#viterbi-recurrence\",\n    \"aria-label\": \"viterbi recurrence permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"viterbi recurrence\"), \"\\n\", React.createElement(_components.p, null, \"The Viterbi algorithm is a dynamic programming method for finding the most likely hidden state sequence \", React.createElement(Latex, {\n    text: \"\\\\(z_{1:T}\\\\)\"\n  }), \" given an observation sequence \", React.createElement(Latex, {\n    text: \"\\\\(x_{1:T}\\\\)\"\n  }), \" in an HMM. If \", React.createElement(Latex, {\n    text: \"\\\\(p(\\\\mathbf{z}, \\\\mathbf{x})\\\\)\"\n  }), \" denotes the joint likelihood of states and observations, Viterbi aims to solve:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\arg \\\\max_{z_{1:T}} p(z_1)\\\\prod_{t=2}^T p(z_t \\\\mid z_{t-1}) \\\\prod_{t=1}^T p(x_t \\\\mid z_t).\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"The recurrence for \", React.createElement(Latex, {\n    text: \"\\\\(\\\\delta_t(j)\\\\)\"\n  }), \", which denotes the highest probability of any state path reaching state \", React.createElement(Latex, {\n    text: \"\\\\(j\\\\)\"\n  }), \" at time \", React.createElement(Latex, {\n    text: \"\\\\(t\\\\)\"\n  }), \", is typically:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\delta_{t}(j) = \\\\Bigl[\\\\max_i \\\\delta_{t-1}(i) \\\\, p(z_t = j \\\\mid z_{t-1} = i)\\\\Bigr]\\\\, p(x_t \\\\mid z_t = j).\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.h3, {\n    id: \"use-cases\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#use-cases\",\n    \"aria-label\": \"use cases permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"use cases\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Part-of-speech tagging\"), \": Identify the most likely POS sequence \", React.createElement(Latex, {\n    text: \"\\\\(z_{1:T}\\\\)\"\n  }), \" for the words in a sentence \", React.createElement(Latex, {\n    text: \"\\\\(x_{1:T}\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Speech recognition\"), \": Find the best word or phoneme sequence given acoustic frames.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Other sequence prediction tasks\"), \": Any domain with Markov assumptions over hidden states.\"), \"\\n\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"comparison-with-other-decoding-methods\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#comparison-with-other-decoding-methods\",\n    \"aria-label\": \"comparison with other decoding methods permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"comparison with other decoding methods\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Greedy\"), \": picks the locally best state at each step; not guaranteed globally optimal.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Exhaustive\"), \": enumerates all possible sequences; for large \", React.createElement(Latex, {\n    text: \"\\\\(T\\\\)\"\n  }), \", this is exponential in \", React.createElement(Latex, {\n    text: \"\\\\(T\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Viterbi\"), \": polynomial complexity \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathcal{O}(T \\\\cdot \\\\#\\\\text{states}^2)\\\\)\"\n  }), \" for standard HMMs.\"), \"\\n\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.h2, {\n    id: \"baum-welch-algorithm-for-hmm-parameter-estimation\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#baum-welch-algorithm-for-hmm-parameter-estimation\",\n    \"aria-label\": \"baum welch algorithm for hmm parameter estimation permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"baum-welch algorithm for hmm parameter estimation\"), \"\\n\", React.createElement(_components.p, null, \"The Baum-Welch algorithm is an application of Expectation-Maximization (EM) for HMMs:\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Expectation step (E)\"), \": Compute posterior probabilities over latent state sequences given the current model parameters \", React.createElement(Latex, {\n    text: \"\\\\(\\\\theta\\\\)\"\n  }), \". This typically uses the forward-backward procedure.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Maximization step (M)\"), \": Update \", React.createElement(Latex, {\n    text: \"\\\\(\\\\theta\\\\)\"\n  }), \" by maximizing the expected complete-data log-likelihood under those posterior probabilities.\"), \"\\n\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"em-approach\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#em-approach\",\n    \"aria-label\": \"em approach permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"em approach\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\text{E-step}: \\\\; Q(\\\\theta, \\\\theta^{(old)}) = \\\\mathbb{E}_{p_{\\\\theta^{(old)}}(z \\\\mid x)}[\\\\log p_\\\\theta(x, z)].\\n\\\\]\"\n  }), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\text{M-step}: \\\\; \\\\theta^{(new)} = \\\\arg \\\\max_\\\\theta \\\\, Q(\\\\theta, \\\\theta^{(old)}).\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.h3, {\n    id: \"implementation-details\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#implementation-details\",\n    \"aria-label\": \"implementation details permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"implementation details\"), \"\\n\", React.createElement(_components.p, null, \"When \", React.createElement(Latex, {\n    text: \"\\\\(T\\\\)\"\n  }), \" or the number of states is large, numerical stability issues (underflow) become central. Typically, logs are used throughout forward-backward computations.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"extensions-beyond-hmm\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#extensions-beyond-hmm\",\n    \"aria-label\": \"extensions beyond hmm permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"extensions beyond hmm\"), \"\\n\", React.createElement(_components.p, null, \"The same iterative refinement idea extends to other latent variable models — any model with hidden \", React.createElement(Latex, {\n    text: \"\\\\(z\\\\)\"\n  }), \" can in principle use EM if exact computations are tractable or can be approximated (leading to variational EM).\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.h2, {\n    id: \"deep-probabilistic-models-in-time-series-analysis\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#deep-probabilistic-models-in-time-series-analysis\",\n    \"aria-label\": \"deep probabilistic models in time series analysis permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"deep probabilistic models in time-series analysis\"), \"\\n\", React.createElement(_components.p, null, \"Time-series often combine:\"), \"\\n\", React.createElement(_components.ol, null, \"\\n\", React.createElement(_components.li, null, \"A \", React.createElement(_components.strong, null, \"latent process\"), \" that evolves over time (like \", React.createElement(Latex, {\n    text: \"\\\\(z_t\\\\)\"\n  }), \" states).\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Deep neural networks\"), \" that model transitions or emissions in a flexible, high-capacity way.\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, \"Examples:\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Deep Markov Model (DMM)\"), \": a continuous-state generalization of HMM, but using neural networks for transitions and emissions.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Recurrent VAEs\"), \": a variational autoencoder that processes sequential data, capturing high-level features in a latent space but also modeling the time evolution in a flexible manner.\"), \"\\n\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.h2, {\n    id: \"text-pre-processing-for-probabilistic-models\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#text-pre-processing-for-probabilistic-models\",\n    \"aria-label\": \"text pre processing for probabilistic models permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"text pre-processing for probabilistic models\"), \"\\n\", React.createElement(_components.p, null, \"In natural language processing contexts, we often feed text data \", React.createElement(Latex, {\n    text: \"\\\\(x\\\\)\"\n  }), \" into deep probabilistic models. Typical steps include:\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"tokenization--normalization\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#tokenization--normalization\",\n    \"aria-label\": \"tokenization  normalization permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"tokenization & normalization\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Tokenization\"), \": Splitting text into tokens (e.g., words, subwords, or characters). This yields a discrete sequence \", React.createElement(Latex, {\n    text: \"\\\\(x_1, x_2, \\\\ldots\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Normalization\"), \": Lowercasing, removing punctuation, possibly lemmatizing. This ensures consistent input forms.\"), \"\\n\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"handling-unknown-words--out-of-vocabulary\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#handling-unknown-words--out-of-vocabulary\",\n    \"aria-label\": \"handling unknown words  out of vocabulary permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"handling unknown words / out-of-vocabulary\"), \"\\n\", React.createElement(_components.p, null, \"In a purely discrete model, an out-of-vocabulary (OOV) word leads to immediate mismatch. Approaches:\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, \"Use an \", React.createElement(Highlight, null, \"UNK token\"), \" to represent unseen words.\"), \"\\n\", React.createElement(_components.li, null, \"Use subword or character-based tokenization to drastically reduce OOV frequency.\"), \"\\n\", React.createElement(_components.li, null, \"In a probabilistic language model, the system might place a small probability on all unknown tokens.\"), \"\\n\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"feature-engineering-vs-learned-representations\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#feature-engineering-vs-learned-representations\",\n    \"aria-label\": \"feature engineering vs learned representations permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"feature engineering vs. learned representations\"), \"\\n\", React.createElement(_components.p, null, \"Older pipelines might rely on hand-designed text features (e.g., TF-IDF). Modern deep probabilistic text models directly learn embeddings that better preserve semantic or syntactic structure. E.g., a deep Bayesian text classifier might embed text into a latent space and place a prior on those embeddings.\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.h2, {\n    id: \"part-of-speech-tagging-with-probabilistic-methods\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#part-of-speech-tagging-with-probabilistic-methods\",\n    \"aria-label\": \"part of speech tagging with probabilistic methods permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"part-of-speech tagging with probabilistic methods\"), \"\\n\", React.createElement(_components.p, null, \"POS tagging is a canonical example for introducing hidden variable models in NLP. We can treat POS tags \", React.createElement(Latex, {\n    text: \"\\\\(z_t\\\\)\"\n  }), \" as hidden states, with each word \", React.createElement(Latex, {\n    text: \"\\\\(x_t\\\\)\"\n  }), \" conditionally dependent on \", React.createElement(Latex, {\n    text: \"\\\\(z_t\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"hmm-for-pos-tagging\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#hmm-for-pos-tagging\",\n    \"aria-label\": \"hmm for pos tagging permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"hmm for pos tagging\"), \"\\n\", React.createElement(_components.p, null, \"The classic approach uses transitions \", React.createElement(Latex, {\n    text: \"\\\\(p(z_t \\\\mid z_{t-1})\\\\)\"\n  }), \" and emissions \", React.createElement(Latex, {\n    text: \"\\\\(p(x_t \\\\mid z_t)\\\\)\"\n  }), \". The Viterbi algorithm finds the best \", React.createElement(Latex, {\n    text: \"\\\\(z_{1:T}\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"viterbi-in-tagging\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#viterbi-in-tagging\",\n    \"aria-label\": \"viterbi in tagging permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"viterbi in tagging\"), \"\\n\", React.createElement(_components.p, null, \"We compute \", React.createElement(Latex, {\n    text: \"\\\\(\\\\delta_t(j)\\\\)\"\n  }), \" for each possible tag \", React.createElement(Latex, {\n    text: \"\\\\(j\\\\)\"\n  }), \" at position \", React.createElement(Latex, {\n    text: \"\\\\(t\\\\)\"\n  }), \". The final result is the path of tags maximizing the product of transitions and emissions.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"deep-extensions\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#deep-extensions\",\n    \"aria-label\": \"deep extensions permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"deep extensions\"), \"\\n\", React.createElement(_components.p, null, \"State-of-the-art taggers often incorporate deep neural networks (e.g., BiLSTMs or Transformers) for richer feature extraction, with a CRF or HMM-like layer on top. This can be interpreted as a deep probabilistic approach if we keep a well-defined distribution over tags.\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.h2, {\n    id: \"ibm-watson-and-practical-large-scale-inference\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#ibm-watson-and-practical-large-scale-inference\",\n    \"aria-label\": \"ibm watson and practical large scale inference permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"ibm watson and practical large-scale inference\"), \"\\n\", React.createElement(_components.p, null, \"IBM Watson's \\\"DeepQA\\\" system (famous for playing Jeopardy!) illustrates how multiple probabilistic modules can be combined with large corpora:\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"watsons-architecture\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#watsons-architecture\",\n    \"aria-label\": \"watsons architecture permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"watson's architecture\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Search-based modules\"), \" identify candidate documents or passages for an input query.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Scoring\"), \": Each candidate answer is scored with learned models that incorporate textual features, structured knowledge, and confidence metrics.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Probabilistic ensembles\"), \": The overall confidence in an answer is an aggregate of multiple features, often computed in a log-linear or Bayesian fashion.\"), \"\\n\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"ml-pipelines-in-watson\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#ml-pipelines-in-watson\",\n    \"aria-label\": \"ml pipelines in watson permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"ml pipelines in watson\"), \"\\n\", React.createElement(_components.p, null, \"Text pre-processing, search, candidate generation, scoring, and re-ranking happen in stages. Each stage can be framed probabilistically, e.g., \\\"Given the question \", React.createElement(Latex, {\n    text: \"\\\\(q\\\\)\"\n  }), \", what is the probability that snippet \", React.createElement(Latex, {\n    text: \"\\\\(s\\\\)\"\n  }), \" is relevant?\\\"\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"lessons-learned\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#lessons-learned\",\n    \"aria-label\": \"lessons learned permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"lessons learned\"), \"\\n\", React.createElement(_components.p, null, \"In large-scale systems, robust uncertainty estimation can be vital. Overconfident or miscalibrated modules lead to poor overall performance. A well-designed probabilistic ensemble can sometimes offset mistakes from individual modules and lead to better final answers.\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.h2, {\n    id: \"deep-probabilistic-models\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#deep-probabilistic-models\",\n    \"aria-label\": \"deep probabilistic models permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"deep probabilistic models\"), \"\\n\", React.createElement(_components.p, null, \"Up to now, we have seen or mentioned discrete latent variable models (e.g., HMM) and simpler parametric structures. We now discuss advanced deep probabilistic models more comprehensively:\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"univariate-conditionals\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#univariate-conditionals\",\n    \"aria-label\": \"univariate conditionals permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"univariate conditionals\"), \"\\n\", React.createElement(_components.p, null, \"A single \", React.createElement(Latex, {\n    text: \"\\\\(y\\\\)\"\n  }), \" given \", React.createElement(Latex, {\n    text: \"\\\\(x\\\\)\"\n  }), \" might be discrete (like a classification) or continuous (like a real-valued measurement). A neural network can parameterize a probability distribution in either case. For instance, for regression:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\ny \\\\mid x \\\\sim \\\\mathcal{N}\\\\bigl(\\\\mu_\\\\theta(x),\\\\, \\\\sigma^2_\\\\theta(x)\\\\bigr).\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.h3, {\n    id: \"parameter-estimation-via-maximum-likelihood\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#parameter-estimation-via-maximum-likelihood\",\n    \"aria-label\": \"parameter estimation via maximum likelihood permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"parameter estimation via maximum likelihood\"), \"\\n\", React.createElement(_components.p, null, \"We define \", React.createElement(Latex, {\n    text: \"\\\\(p_\\\\theta(y \\\\mid x)\\\\)\"\n  }), \" as a distribution from the neural predictor. Then we fit \", React.createElement(Latex, {\n    text: \"\\\\(\\\\theta\\\\)\"\n  }), \" to maximize the log-likelihood of observed data \", React.createElement(Latex, {\n    text: \"\\\\((x_i, y_i)\\\\)\"\n  }), \":\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\mathcal{L}(\\\\theta) = \\\\sum_i \\\\log p_\\\\theta(y_i \\\\mid x_i).\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.h3, {\n    id: \"decision-rules--bayesian-decision-theory\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#decision-rules--bayesian-decision-theory\",\n    \"aria-label\": \"decision rules  bayesian decision theory permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"decision rules & bayesian decision theory\"), \"\\n\", React.createElement(_components.p, null, \"Given a predicted distribution \", React.createElement(Latex, {\n    text: \"\\\\(p_\\\\theta(y \\\\mid x)\\\\)\"\n  }), \", you might want to choose an action \", React.createElement(Latex, {\n    text: \"\\\\(a\\\\)\"\n  }), \" to maximize expected utility:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\na^* = \\\\arg\\\\max_a \\\\sum_y u(a, y)\\\\, p_\\\\theta(y \\\\mid x),\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"where \", React.createElement(Latex, {\n    text: \"\\\\(u(a, y)\\\\)\"\n  }), \" is the utility of action \", React.createElement(Latex, {\n    text: \"\\\\(a\\\\)\"\n  }), \" when the true outcome is \", React.createElement(Latex, {\n    text: \"\\\\(y\\\\)\"\n  }), \". In many classification tasks with 0-1 utility, we take the mode \", React.createElement(Latex, {\n    text: \"\\\\(\\\\arg \\\\max_y p_\\\\theta(y \\\\mid x)\\\\)\"\n  }), \". In other tasks, we might prefer the mean or median if we measure losses like squared error or absolute deviations.\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.h2, {\n    id: \"advanced-autoregressive-and-structured-models\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#advanced-autoregressive-and-structured-models\",\n    \"aria-label\": \"advanced autoregressive and structured models permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"advanced autoregressive and structured models\"), \"\\n\", React.createElement(_components.p, null, \"When outputs \", React.createElement(Latex, {\n    text: \"\\\\(y\\\\)\"\n  }), \" are sequences, trees, or graphs, a factorized approach is possible. For instance, we can express the probability of a sequence \", React.createElement(Latex, {\n    text: \"\\\\(y_{1:T}\\\\)\"\n  }), \" by the chain rule:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\np_\\\\theta(y_{1:T} \\\\mid x) = \\\\prod_{t=1}^T p_\\\\theta(y_t \\\\mid y_{1:t-1}, x).\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.h3, {\n    id: \"autoregressive-taggers\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#autoregressive-taggers\",\n    \"aria-label\": \"autoregressive taggers permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"autoregressive taggers\"), \"\\n\", React.createElement(_components.p, null, \"In POS tagging or other labeling tasks, some advanced taggers approximate:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\np_\\\\theta(z_1, \\\\ldots, z_T \\\\mid x) = \\\\prod_{t=1}^T p_\\\\theta(z_t \\\\mid z_{1:t-1}, x).\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.h3, {\n    id: \"exact-vs-approximate-decoding\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#exact-vs-approximate-decoding\",\n    \"aria-label\": \"exact vs approximate decoding permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"exact vs. approximate decoding\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, \"For purely factorized or conditionally independent structures, you can decode in \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathcal{O}(T)\\\\)\"\n  }), \" by picking each \", React.createElement(Latex, {\n    text: \"\\\\(z_t\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(_components.li, null, \"For fully autoregressive or other advanced factorization, searching for the exact mode might be \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathsf{NP}\\\\)\"\n  }), \"-hard. Instead, we use approximate methods like greedy search or beam search.\"), \"\\n\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"beam-search--greedy-approaches\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#beam-search--greedy-approaches\",\n    \"aria-label\": \"beam search  greedy approaches permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"beam search & greedy approaches\"), \"\\n\", React.createElement(_components.p, null, \"These are heuristics for approximate decoding, used widely in machine translation, text generation, or structured prediction. They strike a trade-off between computational cost and search accuracy.\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.h2, {\n    id: \"variational-inference\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#variational-inference\",\n    \"aria-label\": \"variational inference permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"variational inference\"), \"\\n\", React.createElement(_components.p, null, \"In many deep probabilistic models, a central challenge is dealing with hidden (latent) variables \", React.createElement(Latex, {\n    text: \"\\\\(z\\\\)\"\n  }), \" in \", React.createElement(Latex, {\n    text: \"\\\\(p_\\\\theta(x, z)\\\\)\"\n  }), \". The posterior \", React.createElement(Latex, {\n    text: \"\\\\(p_\\\\theta(z \\\\mid x)\\\\)\"\n  }), \" is typically intractable. Variational inference addresses this problem by introducing a simpler distribution \", React.createElement(Latex, {\n    text: \"\\\\(q_\\\\phi(z \\\\mid x)\\\\)\"\n  }), \" (the variational distribution or inference model) to approximate \", React.createElement(Latex, {\n    text: \"\\\\(p_\\\\theta(z \\\\mid x)\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"importance-of-latent-variable-models\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#importance-of-latent-variable-models\",\n    \"aria-label\": \"importance of latent variable models permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"importance of latent variable models\"), \"\\n\", React.createElement(_components.p, null, \"Latent variables capture hidden structure, making the model more expressive. But the marginal \", React.createElement(Latex, {\n    text: \"\\\\(p_\\\\theta(x)\\\\)\"\n  }), \" is \", React.createElement(Latex, {\n    text: \"\\\\(\\\\int p_\\\\theta(x, z)\\\\, dz\\\\)\"\n  }), \" (for continuous \", React.createElement(Latex, {\n    text: \"\\\\(z\\\\)\"\n  }), \") or \", React.createElement(Latex, {\n    text: \"\\\\(\\\\sum_z p_\\\\theta(x, z)\\\\)\"\n  }), \" (discrete). That integral or sum can be huge.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"elbo-formulation\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#elbo-formulation\",\n    \"aria-label\": \"elbo formulation permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"elbo formulation\"), \"\\n\", React.createElement(_components.p, null, \"The \", React.createElement(_components.strong, null, \"Evidence Lower BOund (ELBO)\"), \" is given by:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\log p_\\\\theta(x) \\\\ge \\\\mathbb{E}_{q_\\\\phi(z \\\\mid x)}\\\\bigl[\\\\log p_\\\\theta(x \\\\mid z)\\\\bigr] - \\\\mathrm{KL}\\\\bigl[q_\\\\phi(z \\\\mid x) \\\\;||\\\\; p_\\\\theta(z)\\\\bigr].\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"Maximizing this lower bound w.r.t. \", React.createElement(Latex, {\n    text: \"\\\\(\\\\theta\\\\)\"\n  }), \" and \", React.createElement(Latex, {\n    text: \"\\\\(\\\\phi\\\\)\"\n  }), \" is equivalent to performing approximate maximum likelihood on \", React.createElement(Latex, {\n    text: \"\\\\(p_\\\\theta\\\\)\"\n  }), \" while also improving \", React.createElement(Latex, {\n    text: \"\\\\(q_\\\\phi\\\\)\"\n  }), \" as an approximation to the true posterior.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"gradient-estimation-techniques\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#gradient-estimation-techniques\",\n    \"aria-label\": \"gradient estimation techniques permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"gradient estimation techniques\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Score-function (REINFORCE or NVIL)\"), \": Directly estimates the gradient of the ELBO by seeing the log of \", React.createElement(Latex, {\n    text: \"\\\\(q_\\\\phi(z \\\\mid x)\\\\)\"\n  }), \" as a \\\"policy.\\\" Often exhibits high variance, but can handle discrete or complicated \", React.createElement(Latex, {\n    text: \"\\\\(z\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Reparameterization trick\"), \": For continuous reparameterizable distributions (like Gaussians), we write \", React.createElement(Latex, {\n    text: \"\\\\(z = \\\\mu_\\\\phi(x) + \\\\sigma_\\\\phi(x) \\\\odot \\\\epsilon\\\\)\"\n  }), \", \", React.createElement(Latex, {\n    text: \"\\\\(\\\\epsilon \\\\sim \\\\mathcal{N}(0, I)\\\\)\"\n  }), \". This typically yields lower-variance gradient estimates.\"), \"\\n\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.h2, {\n    id: \"variational-inference-for-deep-discrete-latent-variables\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#variational-inference-for-deep-discrete-latent-variables\",\n    \"aria-label\": \"variational inference for deep discrete latent variables permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"variational inference for deep discrete latent variables\"), \"\\n\", React.createElement(_components.p, null, \"When \", React.createElement(Latex, {\n    text: \"\\\\(z\\\\)\"\n  }), \" is discrete, we often rely on score-function or related gradient estimators. For example, in a discrete autoencoder with \", React.createElement(Latex, {\n    text: \"\\\\(z \\\\in \\\\{1,\\\\ldots,K\\\\}^D\\\\)\"\n  }), \", enumerating \", React.createElement(Latex, {\n    text: \"\\\\(K^D\\\\)\"\n  }), \" possibilities is usually impossible. Instead we define a factorized or structured \", React.createElement(Latex, {\n    text: \"\\\\(q_\\\\phi(z \\\\mid x)\\\\)\"\n  }), \" (like a product of categorical distributions) and do a typical policy-gradient approach.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"discrete-vs-continuous\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#discrete-vs-continuous\",\n    \"aria-label\": \"discrete vs continuous permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"discrete vs. continuous\"), \"\\n\", React.createElement(_components.p, null, \"Discrete latent spaces cannot typically exploit the reparameterization trick. There are advanced methods (e.g., Gumbel-Softmax, straight-through estimators, or more sophisticated relaxations) that attempt to approximate discrete sampling with continuous surrogates. But a standard fallback is the score-function approach plus variance reduction techniques.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"neural-variational-inference\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#neural-variational-inference\",\n    \"aria-label\": \"neural variational inference permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"neural variational inference\"), \"\\n\", React.createElement(_components.p, null, \"This phrase often means constructing \", React.createElement(Latex, {\n    text: \"\\\\(q_\\\\phi(z \\\\mid x)\\\\)\"\n  }), \" with a neural network, plus using gradient-based optimization of the ELBO. Many architectures show up in tasks like neural clustering, discrete sequence autoencoders, or generative models for text.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"examples-in-practice\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#examples-in-practice\",\n    \"aria-label\": \"examples in practice permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"examples in practice\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Mixture of experts\"), \": \", React.createElement(Latex, {\n    text: \"\\\\(z\\\\)\"\n  }), \" might be an indicator for which \\\"expert\\\" neural network processes the input.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Discrete autoencoders\"), \": \", React.createElement(Latex, {\n    text: \"\\\\(z\\\\)\"\n  }), \" is a code from a codebook (like VQ-VAE).\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Latent classification variables\"), \": \", React.createElement(Latex, {\n    text: \"\\\\(z\\\\)\"\n  }), \" might represent class membership, combined with deeper generative structure for \", React.createElement(Latex, {\n    text: \"\\\\(x\\\\)\"\n  }), \".\"), \"\\n\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.h2, {\n    id: \"continuous-latent-variable-models-vaes\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#continuous-latent-variable-models-vaes\",\n    \"aria-label\": \"continuous latent variable models vaes permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"continuous latent variable models (vaes)\"), \"\\n\", React.createElement(_components.p, null, \"If \", React.createElement(Latex, {\n    text: \"\\\\(z\\\\)\"\n  }), \" is continuous, we can often exploit reparameterization-based variational inference. The classical example is the \", React.createElement(_components.strong, null, \"Variational Autoencoder (VAE)\"), \".\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"gaussian-prior--posterior\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#gaussian-prior--posterior\",\n    \"aria-label\": \"gaussian prior  posterior permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"gaussian prior & posterior\"), \"\\n\", React.createElement(_components.p, null, \"A standard approach is:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\nz \\\\sim \\\\mathcal{N}(0, I), \\\\quad\\nx \\\\mid z \\\\sim p_\\\\theta(x \\\\mid z),\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"and approximate \", React.createElement(Latex, {\n    text: \"\\\\(p_\\\\theta(z \\\\mid x)\\\\)\"\n  }), \" by a diagonal Gaussian \", React.createElement(Latex, {\n    text: \"\\\\(q_\\\\phi(z \\\\mid x)\\\\)\"\n  }), \" with neural networks for its mean and variance. Then we can sample\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\nz = \\\\mu_\\\\phi(x) + \\\\sigma_\\\\phi(x)\\\\odot \\\\epsilon, \\\\quad \\\\epsilon \\\\sim \\\\mathcal{N}(0, I).\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.h3, {\n    id: \"decoder-architectures\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#decoder-architectures\",\n    \"aria-label\": \"decoder architectures permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"decoder architectures\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"FFNN\"), \": simple flattening from the latent code \", React.createElement(Latex, {\n    text: \"\\\\(z\\\\)\"\n  }), \" to output \", React.createElement(Latex, {\n    text: \"\\\\(x\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Convolutional decoders\"), \": useful for image data, building an up-sampling or transposed convolution pipeline from \", React.createElement(Latex, {\n    text: \"\\\\(z\\\\)\"\n  }), \" to an \", React.createElement(Latex, {\n    text: \"\\\\(H \\\\times W \\\\times \\\\text{channels}\\\\)\"\n  }), \" image.\"), \"\\n\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"extensions-normalizing-flows-hierarchical-vaes\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#extensions-normalizing-flows-hierarchical-vaes\",\n    \"aria-label\": \"extensions normalizing flows hierarchical vaes permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"extensions: normalizing flows, hierarchical vaes\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Normalizing flows\"), \": let you transform a simple distribution (like a diagonal Gaussian) into a more flexible one by applying a series of invertible transformations. This is a powerful method to approximate complicated posteriors.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Hierarchical VAEs\"), \": stack multiple latent layers \", React.createElement(Latex, {\n    text: \"\\\\(z_1, z_2, \\\\ldots\\\\)\"\n  }), \" so each distribution can capture different levels of abstraction.\"), \"\\n\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.h2, {\n    id: \"practical-implementation-tips\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#practical-implementation-tips\",\n    \"aria-label\": \"practical implementation tips permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"practical implementation tips\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"hardware-considerations\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#hardware-considerations\",\n    \"aria-label\": \"hardware considerations permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"hardware considerations\"), \"\\n\", React.createElement(_components.p, null, \"Training deep probabilistic models can be GPU intensive. Some tips:\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Batch sizes\"), \": Large batches can speed up training, but memory usage might blow up (especially if the model enumerates or stores large distributions).\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Mixed precision\"), \": If using libraries that support half-precision, watch for potential numerical instabilities in computing log probabilities.\"), \"\\n\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"hyperparameters--regularization\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#hyperparameters--regularization\",\n    \"aria-label\": \"hyperparameters  regularization permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"hyperparameters & regularization\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"KL-term weighting\"), \": In VAEs, the \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathrm{KL}[q_\\\\phi(z \\\\mid x)||p(z)]\\\\)\"\n  }), \" can be scaled by a factor \", React.createElement(Latex, {\n    text: \"\\\\(\\\\beta\\\\)\"\n  }), \". This is often used to encourage certain properties (e.g., encouraging more or fewer codes to be used).\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Early stopping\"), \": Evaluate the ELBO on validation data to prevent overfitting.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Learning rates\"), \": reparameterized models often do well with Adam or other adaptive optimizers. For SFE-based discrete models, RMSProp might handle high variance better.\"), \"\\n\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"debugging-convergence\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#debugging-convergence\",\n    \"aria-label\": \"debugging convergence permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"debugging convergence\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Posterior collapse\"), \": sometimes the VAE training leads \", React.createElement(Latex, {\n    text: \"\\\\(q_\\\\phi(z \\\\mid x)\\\\)\"\n  }), \" to ignore \", React.createElement(Latex, {\n    text: \"\\\\(x\\\\)\"\n  }), \", collapsing to the prior.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Mode-seeking behavior\"), \": especially in discrete latent variable models.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Vanishing or exploding gradients\"), \": as usual in deep learning, watch for numerical stability.\"), \"\\n\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.h2, {\n    id: \"future-directions--conclusion\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#future-directions--conclusion\",\n    \"aria-label\": \"future directions  conclusion permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"future directions & conclusion\"), \"\\n\", React.createElement(_components.p, null, \"Deep probabilistic models are a rich and rapidly evolving area. Some directions include:\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"scalable inference\"), \": Stochastic, distributed, or streaming approaches for extremely large datasets or streaming data.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"structured latent spaces\"), \": Incorporating domain knowledge (graphs, hierarchies) to achieve interpretability or improved performance.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"advanced expansions\"), \": bridging symbolic AI with deep probabilistic approaches for logic, reasoning, or knowledge representation.\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, \"In conclusion, deep probabilistic models unite the representational depth of neural networks with the interpretability and rigor of probability theory. Through frameworks like Bayesian networks, HMMs, VAEs, and their numerous extensions, we can capture a wide variety of data modalities and structures while still maintaining a principled handle on uncertainty. The combination of approximate inference strategies — variational or otherwise — and high-capacity decoders or prior structures continues to open new frontiers in machine learning research and practical enterprise applications alike.\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.h2, {\n    id: \"references-and-further-reading\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#references-and-further-reading\",\n    \"aria-label\": \"references and further reading permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"references and further reading\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, \"D.P. Kingma, M. Welling. \\\"Auto-Encoding Variational Bayes.\\\" \", React.createElement(_components.em, null, \"ICLR\"), \", 2014.\"), \"\\n\", React.createElement(_components.li, null, \"Rezende, D.J., Mohamed, S. \\\"Variational Inference with Normalizing Flows.\\\" \", React.createElement(_components.em, null, \"ICML\"), \", 2015.\"), \"\\n\", React.createElement(_components.li, null, \"Bishop, C.M. \\\"Pattern Recognition and Machine Learning.\\\" \", React.createElement(_components.em, null, \"Springer\"), \", 2006.\"), \"\\n\", React.createElement(_components.li, null, \"Jordan, M.I., Ghahramani, Z., Jaakkola, T.S., and Saul, L.K. \\\"An Introduction to Variational Methods for Graphical Models.\\\" \", React.createElement(_components.em, null, \"Machine Learning\"), \", 1999.\"), \"\\n\", React.createElement(_components.li, null, \"Neal, R.M. \\\"Bayesian Learning for Neural Networks.\\\" \", React.createElement(_components.em, null, \"Ph.D. Thesis\"), \", 1995.\"), \"\\n\", React.createElement(_components.li, null, \"Blei, D.M., Kucukelbir, A., and McAuliffe, J.D. \\\"Variational Inference: A Review for Statisticians.\\\" \", React.createElement(_components.em, null, \"J. American Statistical Association\"), \", 2017.\"), \"\\n\", React.createElement(_components.li, null, \"J. Ba, R. R. Salakhutdinov, R. Grosse, B. Frey. \\\"Learning Wake-Sleep Recurrent Attention Models.\\\" \", React.createElement(_components.em, null, \"NeurIPS\"), \", 2015.\"), \"\\n\", React.createElement(_components.li, null, \"J. Pearl. \\\"Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.\\\" \", React.createElement(_components.em, null, \"Morgan Kaufmann\"), \", 1988.\"), \"\\n\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.h2, {\n    id: \"code-snippets-an-illustrative-example\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#code-snippets-an-illustrative-example\",\n    \"aria-label\": \"code snippets an illustrative example permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"code snippets: an illustrative example\"), \"\\n\", React.createElement(_components.p, null, \"Below is a simplified code demonstration (in Python) that references the core building blocks used in many deep probabilistic modeling workflows. We wrap it in an example of training a variational autoencoder with a Gaussian prior \", React.createElement(Latex, {\n    text: \"\\\\(p_\\\\theta(z)\\\\)\"\n  }), \" and a Bernoulli decoder \", React.createElement(Latex, {\n    text: \"\\\\(p_\\\\theta(x \\\\mid z)\\\\)\"\n  }), \". We then show how to build an inference network \", React.createElement(Latex, {\n    text: \"\\\\(q_\\\\phi(z \\\\mid x)\\\\)\"\n  }), \" that is also Gaussian.\"), \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Note\"), \": This is a self-contained snippet that demonstrates the essential logic. In a real codebase, you would typically separate modules, handle data loaders more carefully, add logging, etc.\"), \"\\n\", React.createElement(Code, {\n    text: `\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport numpy as np\n\n# Suppose we have a dataset X in shape (N, x_dim).\n# We define a simple VAE with:\n#   - p(z) = N(0, I)\n#   - p(x|z) = Bernoulli( decoder(z) )\n#   - q(z|x) = N(mu(x), diag(sigma^2(x)))\n\nclass Encoder(nn.Module):\n    def __init__(self, x_dim, z_dim, hidden_dim=256):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(x_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n        )\n        # separate heads for mean and log-variance\n        self.mu_head = nn.Linear(hidden_dim, z_dim)\n        self.logvar_head = nn.Linear(hidden_dim, z_dim)\n    \n    def forward(self, x):\n        h = self.net(x)\n        mu = self.mu_head(h)\n        logvar = self.logvar_head(h)\n        return mu, logvar\n\nclass Decoder(nn.Module):\n    def __init__(self, z_dim, x_dim, hidden_dim=256):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(z_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, x_dim),\n        )\n    \n    def forward(self, z):\n        # outputs logits for Bernoulli\n        return self.net(z)\n\nclass VAE(nn.Module):\n    def __init__(self, x_dim, z_dim, hidden_dim=256):\n        super().__init__()\n        self.encoder = Encoder(x_dim, z_dim, hidden_dim)\n        self.decoder = Decoder(z_dim, x_dim, hidden_dim)\n        self.z_dim = z_dim\n    \n    def reparameterize(self, mu, logvar):\n        # z = mu + eps * sigma\n        # logvar is log(sigma^2), so sigma = exp(0.5*logvar)\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def forward(self, x):\n        # encode\n        mu, logvar = self.encoder(x)\n        z = self.reparameterize(mu, logvar)\n        # decode\n        logits = self.decoder(z)\n        return logits, mu, logvar\n\ndef vae_loss(x, logits, mu, logvar):\n    # Reconstruction term: Bernoulli negative log-likelihood\n    # We use F.binary_cross_entropy_with_logits in PyTorch\n    recon_loss = F.binary_cross_entropy_with_logits(\n        logits, x, reduction='sum'\n    )\n    # KL term: D_KL( N(mu, diag(sigma^2)) || N(0, I) )\n    # = 0.5 * sum( exp(logvar) + mu^2 - 1 - logvar )\n    kld = 0.5 * torch.sum(torch.exp(logvar) + mu**2 - 1.0 - logvar)\n    return recon_loss + kld\n\n# Example usage\ndef train_vae(model, dataloader, optimizer, device):\n    model.train()\n    total_loss = 0.\n    for batch_x in dataloader:\n        batch_x = batch_x.to(device)\n        optimizer.zero_grad()\n        logits, mu, logvar = model(batch_x)\n        loss = vae_loss(batch_x, logits, mu, logvar)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(dataloader.dataset)\n\n# Suppose x_dim=784 (like flattened MNIST), z_dim=20\nx_dim = 784\nz_dim = 20\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nvae_model = VAE(x_dim, z_dim).to(device)\noptimizer = optim.Adam(vae_model.parameters(), lr=1e-3)\n\n# Example: a fake dataset\nN = 1000\nfake_data = torch.bernoulli(torch.rand(N, x_dim))  # random Bernoulli\ndloader = DataLoader(fake_data, batch_size=64, shuffle=True)\n\n# training loop\nepochs = 3\nfor ep in range(epochs):\n    loss_val = train_vae(vae_model, dloader, optimizer, device)\n    print(f\"Epoch {ep} Loss = {loss_val:.3f}\")\n\n`\n  }), \"\\n\", React.createElement(_components.p, null, React.createElement(_components.em, null, \"(The above code uses a Bernoulli decoder for demonstration, as would be typical for binarized MNIST. Adjust the reconstruction term for continuous data if needed.)\")), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.p, null, \"I hope this long exploration has helped illuminate the wide spectrum of \", React.createElement(_components.strong, null, \"Deep Probabilistic Models\"), \" — from HMMs and Bayesian networks to advanced VAEs and large-scale inference systems. By blending neural architectures with solid probabilistic reasoning, you can open up a world of interpretability, robustness to noise, and capacity to incorporate domain knowledge or uncertainty in a principled way.\"));\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? React.createElement(MDXLayout, props, React.createElement(_createMdxContent, props)) : _createMdxContent(props);\n}\nexport default MDXContent;\n","import GATSBY_COMPILED_MDX from \"/home/avrtt/Repos/avrtt.github.io/src/pages/posts/research/deep_probabilistic_models.mdx\";\nimport React, {useState, useEffect} from 'react';\nimport {useSiteMetadata} from \"../hooks/useSiteMetadata\";\nimport RemoveMarkdown from 'remove-markdown';\nimport {ImageContext} from '../context/ImageContext';\nimport {MDXProvider} from '@mdx-js/react';\nimport Image from '../components/PostImage';\nimport {motion} from 'framer-motion';\nimport SEO from \"../components/seo\";\nimport PostBanner from '../components/PostBanner';\nimport PostBottom from '../components/PostBottom';\nimport {wordsPerMinuteAdventures, wordsPerMinuteResearch, wordsPerMinuteThoughts} from '../data/commonVariables';\nimport {graphql} from 'gatsby';\nimport PartOfCourseNotice from \"../components/PartOfCourseNotice\";\nimport * as stylesButtonsCommon from \"../styles/buttons_common.module.scss\";\nimport * as stylesCustomPostLayouts from \"../styles/custom_post_layouts.module.scss\";\nimport * as stylesTableOfContents from \"../styles/table_of_contents.module.scss\";\nimport * as stylesTagBadges from \"../styles/tag_badges.module.scss\";\nfunction formatReadTime(minutes) {\n  if (minutes <= 10) return '~10 min';\n  if (minutes <= 20) return '~20 min';\n  if (minutes <= 30) return '~30 min';\n  if (minutes <= 40) return '~40 min';\n  if (minutes <= 50) return '~50 min';\n  if (minutes <= 60) return '~1 h';\n  const hours = Math.floor(minutes / 60);\n  const remainder = minutes % 60;\n  if (remainder <= 30) {\n    return `~${hours}${remainder > 0 ? '.5' : ''} h`;\n  }\n  return `~${hours + 1} h`;\n}\nconst TableOfContents = ({toc}) => {\n  if (!toc || !toc.items) return null;\n  const handleClick = (e, url) => {\n    e.preventDefault();\n    const targetId = url.replace('#', '');\n    const targetElement = document.getElementById(targetId);\n    if (targetElement) {\n      targetElement.scrollIntoView({\n        behavior: 'smooth',\n        block: 'start'\n      });\n    }\n  };\n  return React.createElement(\"nav\", {\n    className: stylesTableOfContents.toc\n  }, React.createElement(\"ul\", null, toc.items.map((item, index) => React.createElement(\"li\", {\n    key: index\n  }, React.createElement(\"a\", {\n    href: item.url,\n    onClick: e => handleClick(e, item.url)\n  }, item.title), item.items && React.createElement(TableOfContents, {\n    toc: {\n      items: item.items\n    }\n  })))));\n};\nexport function PostTemplate({data: {mdx, allMdx, allPostImages}, children}) {\n  const {frontmatter, body, tableOfContents} = mdx;\n  const index = frontmatter.index;\n  const slug = frontmatter.slug;\n  const section = slug.split('/')[1];\n  const posts = allMdx.nodes.filter(post => post.frontmatter.slug.includes(`/${section}/`));\n  const sortedPosts = posts.sort((a, b) => a.frontmatter.index - b.frontmatter.index);\n  const currentIndex = sortedPosts.findIndex(post => post.frontmatter.index === index);\n  const nextPost = sortedPosts[currentIndex + 1];\n  const lastPost = sortedPosts[currentIndex - 1];\n  const trimmedSlug = frontmatter.slug.replace(/\\/$/, '');\n  const keyCurrent = (/[^/]*$/).exec(trimmedSlug)[0];\n  const basePath = `posts/${section}/content/${keyCurrent}/`;\n  const [isWideLayout, setIsWideLayout] = useState(frontmatter.flagWideLayoutByDefault);\n  const [isAnimating, setIsAnimating] = useState(false);\n  const toggleLayout = () => {\n    setIsWideLayout(!isWideLayout);\n  };\n  useEffect(() => {\n    setIsAnimating(true);\n    const timer = setTimeout(() => setIsAnimating(false), 340);\n    return () => clearTimeout(timer);\n  }, [isWideLayout]);\n  var wordsPerMinute;\n  if (section === \"adventures\") {\n    wordsPerMinute = wordsPerMinuteAdventures;\n  } else if (section === \"research\") {\n    wordsPerMinute = wordsPerMinuteResearch;\n  } else if (section === \"thoughts\") {\n    wordsPerMinute = wordsPerMinuteThoughts;\n  }\n  const plainTextBody = RemoveMarkdown(body).replace(/import .*? from .*?;/g, '').replace(/<.*?>/g, '').replace(/\\{\\/\\*[\\s\\S]*?\\*\\/\\}/g, '').trim();\n  const wordCount = plainTextBody.split(/\\s+/).length;\n  const baseReadTimeMinutes = Math.ceil(wordCount / wordsPerMinute);\n  const extraTime = frontmatter.extraReadTimeMin || 0;\n  const totalReadTime = baseReadTimeMinutes + extraTime;\n  const readTime = formatReadTime(totalReadTime);\n  const notices = [{\n    flag: frontmatter.flagDraft,\n    component: () => import(\"../components/NotFinishedNotice\")\n  }, {\n    flag: frontmatter.flagMindfuckery,\n    component: () => import(\"../components/MindfuckeryNotice\")\n  }, {\n    flag: frontmatter.flagRewrite,\n    component: () => import(\"../components/RewriteNotice\")\n  }, {\n    flag: frontmatter.flagOffensive,\n    component: () => import(\"../components/OffensiveNotice\")\n  }, {\n    flag: frontmatter.flagProfane,\n    component: () => import(\"../components/ProfanityNotice\")\n  }, {\n    flag: frontmatter.flagMultilingual,\n    component: () => import(\"../components/MultilingualNotice\")\n  }, {\n    flag: frontmatter.flagUnreliably,\n    component: () => import(\"../components/UnreliablyNotice\")\n  }, {\n    flag: frontmatter.flagPolitical,\n    component: () => import(\"../components/PoliticsNotice\")\n  }, {\n    flag: frontmatter.flagCognitohazard,\n    component: () => import(\"../components/CognitohazardNotice\")\n  }, {\n    flag: frontmatter.flagHidden,\n    component: () => import(\"../components/HiddenNotice\")\n  }];\n  const [loadedNotices, setLoadedNotices] = useState([]);\n  useEffect(() => {\n    notices.forEach(({flag, component}) => {\n      if (flag) {\n        component().then(module => {\n          setLoadedNotices(prev => [...prev, module.default]);\n        });\n      }\n    });\n  }, []);\n  return React.createElement(motion.div, {\n    initial: {\n      opacity: 0\n    },\n    animate: {\n      opacity: 1\n    },\n    exit: {\n      opacity: 0\n    },\n    transition: {\n      duration: 0.15\n    }\n  }, React.createElement(PostBanner, {\n    postNumber: frontmatter.index,\n    date: frontmatter.date,\n    updated: frontmatter.updated,\n    readTime: readTime,\n    difficulty: frontmatter.difficultyLevel,\n    title: frontmatter.title,\n    desc: frontmatter.desc,\n    banner: frontmatter.banner,\n    section: section,\n    postKey: keyCurrent,\n    isMindfuckery: frontmatter.flagMindfuckery,\n    mainTag: frontmatter.mainTag\n  }), React.createElement(\"div\", {\n    style: {\n      display: \"flex\",\n      justifyContent: \"flex-end\",\n      flexWrap: \"wrap\",\n      maxWidth: \"75%\",\n      marginLeft: \"auto\",\n      paddingRight: \"1vw\",\n      marginTop: \"-6vh\",\n      marginBottom: \"4vh\"\n    }\n  }, frontmatter.otherTags.map((tag, index) => React.createElement(\"span\", {\n    key: index,\n    className: `noselect ${stylesTagBadges.tagPosts}`,\n    style: {\n      margin: \"0 5px 5px 0\"\n    }\n  }, tag))), React.createElement(\"div\", {\n    class: \"postBody\"\n  }, React.createElement(TableOfContents, {\n    toc: tableOfContents\n  })), React.createElement(\"br\"), React.createElement(\"div\", {\n    style: {\n      margin: \"0 10% -2vh 30%\",\n      textAlign: \"right\"\n    }\n  }, React.createElement(motion.button, {\n    class: \"noselect\",\n    className: stylesCustomPostLayouts.postButton,\n    id: stylesCustomPostLayouts.postLayoutSwitchButton,\n    onClick: toggleLayout,\n    whileTap: {\n      scale: 0.93\n    }\n  }, React.createElement(motion.div, {\n    className: stylesButtonsCommon.buttonTextWrapper,\n    key: isWideLayout,\n    initial: {\n      opacity: 0\n    },\n    animate: {\n      opacity: 1\n    },\n    exit: {\n      opacity: 0\n    },\n    transition: {\n      duration: 0.3,\n      ease: \"easeInOut\"\n    }\n  }, isWideLayout ? \"Switch to default layout\" : \"Switch to wide layout\"))), React.createElement(\"br\"), React.createElement(\"div\", {\n    class: \"postBody\",\n    style: {\n      margin: isWideLayout ? \"0 -14%\" : \"\",\n      maxWidth: isWideLayout ? \"200%\" : \"\",\n      transition: \"margin 1s ease, max-width 1s ease, padding 1s ease\"\n    }\n  }, React.createElement(\"div\", {\n    className: `${stylesCustomPostLayouts.textContent} ${isAnimating ? stylesCustomPostLayouts.fadeOut : stylesCustomPostLayouts.fadeIn}`\n  }, loadedNotices.map((NoticeComponent, index) => React.createElement(NoticeComponent, {\n    key: index\n  })), frontmatter.indexCourse ? React.createElement(PartOfCourseNotice, {\n    index: frontmatter.indexCourse,\n    category: frontmatter.courseCategoryName\n  }) : \"\", React.createElement(ImageContext.Provider, {\n    value: {\n      images: allPostImages.nodes,\n      basePath: basePath.replace(/\\/$/, '') + '/'\n    }\n  }, React.createElement(MDXProvider, {\n    components: {\n      Image\n    }\n  }, children)))), React.createElement(PostBottom, {\n    nextPost: nextPost,\n    lastPost: lastPost,\n    keyCurrent: keyCurrent,\n    section: section\n  }));\n}\nPostTemplate\nexport default function GatsbyMDXWrapper(props) {\n  return React.createElement(PostTemplate, props, React.createElement(GATSBY_COMPILED_MDX, props));\n}\nexport function Head({data}) {\n  const {frontmatter} = data.mdx;\n  const title = frontmatter.titleSEO || frontmatter.title;\n  const titleOG = frontmatter.titleOG || title;\n  const titleTwitter = frontmatter.titleTwitter || title;\n  const description = frontmatter.descSEO || frontmatter.desc;\n  const descriptionOG = frontmatter.descOG || description;\n  const descriptionTwitter = frontmatter.descTwitter || description;\n  const schemaType = frontmatter.schemaType || \"BlogPosting\";\n  const keywords = frontmatter.keywordsSEO;\n  const datePublished = frontmatter.date;\n  const dateModified = frontmatter.updated || datePublished;\n  const imageOG = frontmatter.imageOG || frontmatter.banner?.childImageSharp?.gatsbyImageData?.images?.fallback?.src;\n  const imageAltOG = frontmatter.imageAltOG || descriptionOG;\n  const imageTwitter = frontmatter.imageTwitter || imageOG;\n  const imageAltTwitter = frontmatter.imageAltTwitter || descriptionTwitter;\n  const canonicalUrl = frontmatter.canonicalURL;\n  const flagHidden = frontmatter.flagHidden || false;\n  const mainTag = frontmatter.mainTag || \"Posts\";\n  const section = frontmatter.slug.split('/')[1] || \"posts\";\n  const type = \"article\";\n  const {siteUrl} = useSiteMetadata();\n  const breadcrumbJSON = {\n    \"@context\": \"https://schema.org\",\n    \"@type\": \"BreadcrumbList\",\n    \"itemListElement\": [{\n      \"@type\": \"ListItem\",\n      \"position\": 1,\n      \"name\": \"Home\",\n      \"item\": siteUrl\n    }, {\n      \"@type\": \"ListItem\",\n      \"position\": 2,\n      \"name\": mainTag,\n      \"item\": `${siteUrl}/${frontmatter.slug.split('/')[1]}`\n    }, {\n      \"@type\": \"ListItem\",\n      \"position\": 3,\n      \"name\": title,\n      \"item\": `${siteUrl}${frontmatter.slug}`\n    }]\n  };\n  return React.createElement(SEO, {\n    title: title + \" - avrtt.blog\",\n    titleOG: titleOG,\n    titleTwitter: titleTwitter,\n    description: description,\n    descriptionOG: descriptionOG,\n    descriptionTwitter: descriptionTwitter,\n    schemaType: schemaType,\n    keywords: keywords,\n    datePublished: datePublished,\n    dateModified: dateModified,\n    imageOG: imageOG,\n    imageAltOG: imageAltOG,\n    imageTwitter: imageTwitter,\n    imageAltTwitter: imageAltTwitter,\n    canonicalUrl: canonicalUrl,\n    flagHidden: flagHidden,\n    mainTag: mainTag,\n    section: section,\n    type: type\n  }, React.createElement(\"script\", {\n    type: \"application/ld+json\"\n  }, JSON.stringify(breadcrumbJSON)));\n}\nexport const query = graphql`\n  query($id: String!, $postsFilterRegex: String!, $imagePathRegex: String!) {\n    mdx(id: { eq: $id }) {\n      frontmatter {\n        index\n        indexCourse\n        title\n        titleSEO\n        titleOG\n        titleTwitter\n        courseCategoryName\n        desc\n        descSEO\n        descOG\n        descTwitter\n        date\n        updated\n        extraReadTimeMin\n        difficultyLevel\n        flagDraft\n        flagMindfuckery\n        flagRewrite\n        flagOffensive\n        flagProfane\n        flagMultilingual\n        flagUnreliably\n        flagPolitical\n        flagCognitohazard\n        flagHidden\n        flagWideLayoutByDefault\n        schemaType\n        mainTag\n        otherTags\n        keywordsSEO\n        banner {\n          childImageSharp {\n            gatsbyImageData(\n\t\t\t\t\t\t\tformats: [JPG, WEBP], \n\t\t\t\t\t\t\tplaceholder: BLURRED, \n\t\t\t\t\t\t\tquality: 100\n\t\t\t\t\t\t)\n          }\n        }\n        imageOG\n        imageAltOG\n        imageTwitter\n        imageAltTwitter\n        canonicalURL\n        slug\n      }\n      body\n      tableOfContents(maxDepth: 3)\n    }\n    allMdx(filter: {frontmatter: {slug: {regex: $postsFilterRegex}}}) {\n      nodes {\n        frontmatter {\n          index\n          slug\n          banner {\n            childImageSharp {\n              gatsbyImageData(\n                formats: [JPG, WEBP],\n                placeholder: BLURRED,\n                quality: 100\n              )\n            }\n          }\n        }\n      }\n    }\n    allPostImages: allFile(\n      filter: { \n        sourceInstanceName: { eq: \"images\" },\n        relativePath: { regex: $imagePathRegex }\n      }\n    ) {\n      nodes {\n        relativePath\n        childImageSharp {\n          gatsbyImageData(\n            layout: CONSTRAINED\n            placeholder: DOMINANT_COLOR\n            quality: 100\n          )\n        }\n      }\n    }\n  }\n`;\n","import React from \"react\";\nimport Latex from 'react-latex-next';\nimport 'katex/dist/katex.min.css'; \n  \nconst L = ({ text }) => {\n  return (\n    <Latex>{text}</Latex>\n  );\n};\nexport default L;\n"],"names":["_createMdxContent","props","_components","Object","assign","p","h3","a","span","ul","li","strong","hr","h2","ol","em","_provideComponents","components","React","Latex","text","id","style","position","href","className","dangerouslySetInnerHTML","__html","Highlight","Code","wrapper","MDXLayout","TableOfContents","_ref","toc","items","stylesTableOfContents","map","item","index","key","url","onClick","e","handleClick","preventDefault","targetId","replace","targetElement","document","getElementById","scrollIntoView","behavior","block","title","PostTemplate","_ref2","data","mdx","allMdx","allPostImages","children","frontmatter","body","tableOfContents","section","slug","split","sortedPosts","nodes","filter","post","includes","sort","b","currentIndex","findIndex","nextPost","lastPost","trimmedSlug","keyCurrent","exec","basePath","isWideLayout","setIsWideLayout","useState","flagWideLayoutByDefault","isAnimating","setIsAnimating","wordsPerMinute","useEffect","timer","setTimeout","clearTimeout","wordsPerMinuteAdventures","wordsPerMinuteResearch","wordsPerMinuteThoughts","wordCount","RemoveMarkdown","trim","length","readTime","minutes","hours","Math","floor","remainder","formatReadTime","ceil","extraReadTimeMin","notices","flag","flagDraft","component","flagMindfuckery","flagRewrite","flagOffensive","flagProfane","flagMultilingual","flagUnreliably","flagPolitical","flagCognitohazard","flagHidden","loadedNotices","setLoadedNotices","forEach","_ref3","then","module","prev","concat","_toConsumableArray","default","motion","div","initial","opacity","animate","exit","transition","duration","PostBanner","postNumber","date","updated","difficulty","difficultyLevel","desc","banner","postKey","isMindfuckery","mainTag","display","justifyContent","flexWrap","maxWidth","marginLeft","paddingRight","marginTop","marginBottom","otherTags","tag","stylesTagBadges","margin","class","textAlign","button","stylesCustomPostLayouts","toggleLayout","whileTap","scale","stylesButtonsCommon","ease","NoticeComponent","indexCourse","PartOfCourseNotice","category","courseCategoryName","ImageContext","Provider","value","images","MDXProvider","Image","PostBottom","GatsbyMDXWrapper","GATSBY_COMPILED_MDX","Head","_ref4","_frontmatter$banner","_frontmatter$banner$c","_frontmatter$banner$c2","_frontmatter$banner$c3","_frontmatter$banner$c4","titleSEO","titleOG","titleTwitter","description","descSEO","descriptionOG","descOG","descriptionTwitter","descTwitter","schemaType","keywords","keywordsSEO","datePublished","dateModified","imageOG","childImageSharp","gatsbyImageData","fallback","src","imageAltOG","imageTwitter","imageAltTwitter","canonicalUrl","canonicalURL","siteUrl","useSiteMetadata","breadcrumbJSON","SEO","type","JSON","stringify"],"sourceRoot":""}