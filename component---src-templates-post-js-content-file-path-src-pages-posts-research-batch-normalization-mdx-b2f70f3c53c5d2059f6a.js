"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[2700],{3962:function(e,t){t.A="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iaXNvLTg4NTktMSI/Pgo8IS0tIEdlbmVyYXRvcjogQWRvYmUgSWxsdXN0cmF0b3IgMTYuMC4wLCBTVkcgRXhwb3J0IFBsdWctSW4gLiBTVkcgVmVyc2lvbjogNi4wMCBCdWlsZCAwKSAgLS0+CjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+CjxzdmcgdmVyc2lvbj0iMS4xIiBpZD0iQ2FwYV8xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB4PSIwcHgiIHk9IjBweCIKCSB3aWR0aD0iNDE2Ljk3OXB4IiBoZWlnaHQ9IjQxNi45NzlweCIgdmlld0JveD0iMCAwIDQxNi45NzkgNDE2Ljk3OSIgc3R5bGU9ImVuYWJsZS1iYWNrZ3JvdW5kOm5ldyAwIDAgNDE2Ljk3OSA0MTYuOTc5OyIKCSB4bWw6c3BhY2U9InByZXNlcnZlIj4KPGc+Cgk8cGF0aCBkPSJNMzU2LjAwNCw2MS4xNTZjLTgxLjM3LTgxLjQ3LTIxMy4zNzctODEuNTUxLTI5NC44NDgtMC4xODJjLTgxLjQ3LDgxLjM3MS04MS41NTIsMjEzLjM3OS0wLjE4MSwyOTQuODUKCQljODEuMzY5LDgxLjQ3LDIxMy4zNzgsODEuNTUxLDI5NC44NDksMC4xODFDNDM3LjI5MywyNzQuNjM2LDQzNy4zNzUsMTQyLjYyNiwzNTYuMDA0LDYxLjE1NnogTTIzNy42LDM0MC43ODYKCQljMCwzLjIxNy0yLjYwNyw1LjgyMi01LjgyMiw1LjgyMmgtNDYuNTc2Yy0zLjIxNSwwLTUuODIyLTIuNjA1LTUuODIyLTUuODIyVjE2Ny44ODVjMC0zLjIxNywyLjYwNy01LjgyMiw1LjgyMi01LjgyMmg0Ni41NzYKCQljMy4yMTUsMCw1LjgyMiwyLjYwNCw1LjgyMiw1LjgyMlYzNDAuNzg2eiBNMjA4LjQ5LDEzNy45MDFjLTE4LjYxOCwwLTMzLjc2Ni0xNS4xNDYtMzMuNzY2LTMzLjc2NQoJCWMwLTE4LjYxNywxNS4xNDctMzMuNzY2LDMzLjc2Ni0zMy43NjZjMTguNjE5LDAsMzMuNzY2LDE1LjE0OCwzMy43NjYsMzMuNzY2QzI0Mi4yNTYsMTIyLjc1NSwyMjcuMTA3LDEzNy45MDEsMjA4LjQ5LDEzNy45MDF6Ii8+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPC9zdmc+Cg=="},44670:function(e,t,a){a.r(t),a.d(t,{Head:function(){return B},PostTemplate:function(){return H},default:function(){return T}});var n=a(54506),i=a(28453),r=a(96540),l=a(66501),o=a(16886),s=a(46295),c=a(96098);function h(e){const t=Object.assign({p:"p",ul:"ul",li:"li",h2:"h2",a:"a",span:"span",h3:"h3",ol:"ol",h4:"h4",strong:"strong"},(0,i.RP)(),e.components),{Image:a}=t;return a||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),r.createElement(r.Fragment,null,"\n",r.createElement("br"),"\n","\n","\n",r.createElement(t.p,null,'Batch normalization is a technique that emerged in 2015 — introduced in the seminal paper "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift" (Ioffe and Szegedy, 2015) — and it radically changed the way we train modern neural networks. Essentially, the goal is to stabilize and speed up training by normalizing certain intermediate values within the neural network. Historically, data scientists and machine learning engineers had long recognized that normalizing input features (for instance, scaling each feature to have zero mean and unit variance) helps gradient-based methods converge more quickly and reliably. However, normalizing internal activations (i.e., hidden-layer activations during training) was not a common practice until batch normalization arrived on the scene.'),"\n",r.createElement(t.p,null,"One of the key motivations behind batch normalization is the issue of ",r.createElement(o.A,null,"internal covariate shift"),". This term captures the notion that, as we train a deep network, the distribution of each layer's inputs can keep changing drastically due to updates of preceding layers' parameters. Because deeper layers continually see changing distributions, the network needs smaller learning rates and often more training steps to converge. The more layers we stack, the more amplified the effect can become — unless we do something to mitigate that shift."),"\n",r.createElement(t.p,null,"Batch normalization significantly reduces these challenges. By normalizing (i.e., re-centering and re-scaling) intermediate layer activations in mini-batches, it ensures that each layer sees input data of a more stable distribution, thus alleviating the training difficulties caused by internal covariate shift. This practice enables faster convergence, permits higher learning rates, and even provides a helpful regularization effect. The normalization typically happens along the feature dimension; in the simplest case of a fully connected layer, the same kind of standardization is applied to each neuron's output across the batch."),"\n",r.createElement(t.p,null,"In addition to speeding up convergence, batch normalization can also reduce the sensitivity to hyperparameter choices, make training deeper architectures more stable, and sometimes help the network generalize better. While subsequent research has refined the original concept (leading to variants like layer normalization, group normalization, instance normalization, and more), the fundamental principles of batch normalization remain relevant to a wide variety of deep learning tasks — ranging from image classification to natural language processing and beyond."),"\n",r.createElement(t.p,null,"I have often found that practitioners new to deep learning do not realize how much of a performance bottleneck can be removed simply by integrating batch normalization layers in the right spots. It can be the difference between a model that saturates or explodes after a few epochs, and one that trains gracefully to state-of-the-art levels."),"\n",r.createElement(t.p,null,"Given its importance, I think it is essential to dive deeper into the theory, discuss its derivation, illustrate the formulas that underlie it, and point out potential pitfalls. In this article, I will explore batch normalization from the ground up, covering:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"Where it came from and how it relates to data preprocessing"),"\n",r.createElement(t.li,null,"The details of how it normalizes layer inputs"),"\n",r.createElement(t.li,null,"The role of learnable parameters within the batch normalization transform"),"\n",r.createElement(t.li,null,"How backpropagation flows through these normalizations"),"\n",r.createElement(t.li,null,"How to implement batch normalization in popular frameworks like TensorFlow, Keras, and PyTorch"),"\n",r.createElement(t.li,null,"Extensions and variations, such as layer normalization, instance normalization, group normalization, batch renormalization, and more"),"\n",r.createElement(t.li,null,"Empirical demonstrations showing the speedup in training and improvements in final accuracy"),"\n"),"\n",r.createElement(t.p,null,"With this foundation in mind, let us begin by defining batch normalization in more detail."),"\n",r.createElement(t.h2,{id:"2-defining-batch-normalization",style:{position:"relative"}},r.createElement(t.a,{href:"#2-defining-batch-normalization","aria-label":"2 defining batch normalization permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2. defining batch normalization"),"\n",r.createElement(t.h3,{id:"typical-workflow-in-neural-network-training",style:{position:"relative"}},r.createElement(t.a,{href:"#typical-workflow-in-neural-network-training","aria-label":"typical workflow in neural network training permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"typical workflow in neural network training"),"\n",r.createElement(t.p,null,"In the usual pipeline for training neural networks, one typically:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,"Preprocesses the training data (e.g., normalizing or standardizing each input feature so that it has zero mean and unit variance, or scaling data within [0, 1] or [-1, 1], etc.)"),"\n",r.createElement(t.li,null,"Divides the training data into mini-batches (also called batches) of size ",r.createElement(c.A,{text:"\\(m\\)"}),"."),"\n",r.createElement(t.li,null,"Performs forward propagation for each batch, calculates the loss, and then computes gradients by backpropagation."),"\n",r.createElement(t.li,null,"Uses an optimization procedure (like stochastic gradient descent ",r.createElement(l.A,{text:"Stochastic Gradient Descent"}),", possibly with momentum or adaptive methods) to update the parameters of the model."),"\n"),"\n",r.createElement(t.p,null,"Even though the raw input features are often normalized at the first step, nothing prevents the hidden layers from producing highly varying activations over the course of training. Once the parameters of the earlier layers update, the distribution of activations feeding into deeper layers can shift drastically. This phenomenon, known as ",r.createElement(o.A,null,"internal covariate shift"),", is precisely what batch normalization addresses."),"\n",r.createElement(t.h3,{id:"relation-to-data-preprocessing-techniques",style:{position:"relative"}},r.createElement(t.a,{href:"#relation-to-data-preprocessing-techniques","aria-label":"relation to data preprocessing techniques permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"relation to data preprocessing techniques"),"\n",r.createElement(t.p,null,"In a sense, batch normalization is an automated internal data preprocessing scheme for each layer. Traditionally, you might do:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"Input standardization: ",r.createElement(c.A,{text:"\\( x = (x - \\mu_x)/\\sigma_x \\)"})," for each input feature ",r.createElement(c.A,{text:"\\(x\\)"}),"."),"\n",r.createElement(t.li,null,"Possibly apply principal component analysis or other transformations."),"\n"),"\n",r.createElement(t.p,null,"But with batch normalization, each hidden layer's input vector is likewise standardized to have zero mean and unit variance (in a mini-batch sense), and then re-scaled by a learned gain (often denoted ",r.createElement(c.A,{text:"\\( \\gamma \\)"}),") plus a learned shift (denoted ",r.createElement(c.A,{text:"\\( \\beta \\)"}),"). These learned parameters ",r.createElement(c.A,{text:"\\( \\gamma, \\beta \\)"})," allow the model to undo or modulate the normalization if that is optimal for the task. In other words, the network is not forced to keep everything strictly zero mean and unit variance; it can discover the best shift and scale for each activation dimension."),"\n",r.createElement(t.h3,{id:"conceptual-overview",style:{position:"relative"}},r.createElement(t.a,{href:"#conceptual-overview","aria-label":"conceptual overview permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"conceptual overview"),"\n",r.createElement(t.p,null,"While I will get into the exact formulas in the next chapter, let me emphasize that batch normalization is effectively:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,"Compute mean and variance of each feature dimension across a mini-batch."),"\n",r.createElement(t.li,null,"Subtract the batch mean from each example's feature dimension, then divide by the batch standard deviation."),"\n",r.createElement(t.li,null,"Multiply by a trainable scale ",r.createElement(c.A,{text:"\\( \\gamma \\)"})," and add a trainable offset ",r.createElement(c.A,{text:"\\( \\beta \\)"}),"."),"\n"),"\n",r.createElement(t.p,null,"During training, mean and variance are typically calculated per mini-batch. During inference (test time), one usually uses running estimates of mean and variance that were collected over many training batches."),"\n",r.createElement(t.p,null,"This approach (learning ",r.createElement(c.A,{text:"\\( \\gamma \\)"})," and ",r.createElement(c.A,{text:"\\( \\beta \\)"})," for each activation dimension) solves a big worry in older networks, where if you normalized the hidden units, you might lose representational capacity. But with these trainable parameters, the network can adapt the normalized activations to exactly what is needed for a given layer's objective."),"\n",r.createElement(t.p,null,"Batch normalization is not just a detail in the pipeline; it is widely recognized as a milestone in deep network training, enabling larger-scale, deeper, and more complex models to train stably."),"\n",r.createElement(t.h2,{id:"3-mathematical-foundations",style:{position:"relative"}},r.createElement(t.a,{href:"#3-mathematical-foundations","aria-label":"3 mathematical foundations permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3. mathematical foundations"),"\n",r.createElement(t.h3,{id:"normalization-process",style:{position:"relative"}},r.createElement(t.a,{href:"#normalization-process","aria-label":"normalization process permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"normalization process"),"\n",r.createElement(t.p,null,"Let us define the forward-pass transformation done by batch normalization more rigorously. Suppose at some layer of the network, we have a mini-batch ",r.createElement(c.A,{text:"\\( B = \\{ \\mathbf{x}_1, \\ldots, \\mathbf{x}_m \\} \\)"})," of size ",r.createElement(c.A,{text:"\\(m\\)"}),". For simplicity, I will denote each ",r.createElement(c.A,{text:"\\( \\mathbf{x}_i \\)"})," as a vector of dimension ",r.createElement(c.A,{text:"\\( d \\)"})," (these could be, for instance, the pre-activation outputs from the previous layer). Then each coordinate ",r.createElement(c.A,{text:"\\( k \\in \\{1,2,\\ldots,d\\} \\)"})," in ",r.createElement(c.A,{text:"\\( \\mathbf{x}_i \\)"})," is normalized as follows:"),"\n",r.createElement(c.A,{text:"\\[\n\\mu_{B}^{(k)} = \\frac{1}{m} \\sum_{i=1}^{m} x_i^{(k)}, \\quad\n\\sigma_{B}^{2 (k)} = \\frac{1}{m} \\sum_{i=1}^{m} \\bigl(x_i^{(k)} - \\mu_B^{(k)}\\bigr)^2\n\\]"}),"\n",r.createElement(t.p,null,"These are respectively the mean and variance of the ",r.createElement(c.A,{text:"\\(k\\)"}),"-th dimension across the batch. Then we normalize:"),"\n",r.createElement(c.A,{text:"\\[\n\\hat{x}_i^{(k)} = \\frac{x_i^{(k)} - \\mu_B^{(k)}}{\\sqrt{\\sigma_{B}^{2 (k)} + \\epsilon}}\n\\]"}),"\n",r.createElement(t.p,null,"Here, ",r.createElement(c.A,{text:"\\( \\epsilon \\)"})," is a small constant (like ",r.createElement(c.A,{text:"\\(10^{-5}\\)"}),") for numerical stability to avoid division by zero."),"\n",r.createElement(t.p,null,"After this standardization step, we apply a learned scale and shift:"),"\n",r.createElement(c.A,{text:"\\[\ny_i^{(k)} = \\gamma^{(k)} \\hat{x}_i^{(k)} + \\beta^{(k)},\n\\]"}),"\n",r.createElement(t.p,null,"yielding the final output of batch normalization for the ",r.createElement(c.A,{text:"\\(k\\)"}),"-th coordinate. The set of parameters ",r.createElement(c.A,{text:"\\( \\{\\gamma^{(k)}\\} \\)"})," and ",r.createElement(c.A,{text:"\\( \\{\\beta^{(k)}\\} \\)"})," are learned jointly with the other parameters in the network (like weights and biases of other layers) by backpropagation. The result is a normalized output ",r.createElement(c.A,{text:"\\( \\hat{x}_i^{(k)} \\)"})," that has zero mean and unit variance across the mini-batch, re-scaled and shifted according to ",r.createElement(c.A,{text:"\\( \\gamma \\)"})," and ",r.createElement(c.A,{text:"\\( \\beta \\)"}),"."),"\n",r.createElement(t.h3,{id:"normalized-outputs-and-learnable-parameters",style:{position:"relative"}},r.createElement(t.a,{href:"#normalized-outputs-and-learnable-parameters","aria-label":"normalized outputs and learnable parameters permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"normalized outputs and learnable parameters"),"\n",r.createElement(t.p,null,"The presence of ",r.createElement(c.A,{text:"\\( \\gamma \\)"})," and ",r.createElement(c.A,{text:"\\( \\beta \\)"})," is crucial. If we forced the layer to have mean zero and variance one, we might hamper its representational capacity. The network might need a different scale or offset in order to generate a certain intermediate representation. By letting the network learn these, batch normalization retains the benefits of having stable and well-behaved distributions, while preserving the representational power."),"\n",r.createElement(t.p,null,"Note that ",r.createElement(c.A,{text:"\\( \\beta \\)"})," can absorb the role of what used to be a bias term in many network layers. Often, we can omit the bias in a layer after using batch normalization because ",r.createElement(c.A,{text:"\\( \\beta \\)"})," already plays that role."),"\n",r.createElement(t.h3,{id:"backpropagation-through-batch-normalization",style:{position:"relative"}},r.createElement(t.a,{href:"#backpropagation-through-batch-normalization","aria-label":"backpropagation through batch normalization permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"backpropagation through batch normalization"),"\n",r.createElement(t.p,null,"Training a model with batch normalization means we need to backpropagate through the mini-batch statistics. The partial derivatives can be summarized by:"),"\n",r.createElement(c.A,{text:"\\[\n\\frac{\\partial l}{\\partial \\hat{x}_i} = \\frac{\\partial l}{\\partial y_i} \\cdot \\gamma\n\\]"}),"\n",r.createElement(c.A,{text:"\\[\n\\frac{\\partial l}{\\partial \\sigma_B^2} =\n\\sum_{i=1}^{m}\n\\frac{\\partial l}{\\partial \\hat{x}_i} (x_i - \\mu_B) \\cdot\n\\frac{-1}{2} (\\sigma_B^2 + \\epsilon)^{-3/2}\n\\]"}),"\n",r.createElement(c.A,{text:"\\[\n\\frac{\\partial l}{\\partial \\mu_B} =\n\\sum_{i=1}^{m} \\frac{\\partial l}{\\partial \\hat{x}_i} \\cdot\n\\Bigl(- \\frac{1}{\\sqrt{\\sigma_B^2 + \\epsilon}}\\Bigr)\n\\;+\\;\n\\frac{\\partial l}{\\partial \\sigma_B^2} \\cdot\n\\frac{\\sum_{i=1}^m -2(x_i-\\mu_B)}{m}\n\\]"}),"\n",r.createElement(c.A,{text:"\\[\n\\frac{\\partial l}{\\partial x_i} =\n\\frac{\\partial l}{\\partial \\hat{x}_i} \\cdot \\frac{1}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n\\;+\\; \\frac{\\partial l}{\\partial \\sigma_B^2} \\cdot\n\\frac{2(x_i - \\mu_B)}{m}\n\\;+\\; \\frac{\\partial l}{\\partial \\mu_B} \\cdot \\frac{1}{m}\n\\]"}),"\n",r.createElement(c.A,{text:"\\[\n\\frac{\\partial l}{\\partial \\gamma} =\n\\sum_{i=1}^{m} \\frac{\\partial l}{\\partial y_i} \\cdot \\hat{x}_i,\n\\quad\\quad\n\\frac{\\partial l}{\\partial \\beta} =\n\\sum_{i=1}^{m} \\frac{\\partial l}{\\partial y_i}.\n\\]"}),"\n",r.createElement(t.p,null,"Here, the notation ",r.createElement(c.A,{text:"\\( x_i \\)"})," implicitly refers to a specific dimension, but the concept generalizes across all dimensions in the vector ",r.createElement(c.A,{text:"\\( \\mathbf{x}_i \\)"}),". As the mini-batch size ",r.createElement(c.A,{text:"\\( m \\)"})," grows, we get more robust estimates of ",r.createElement(c.A,{text:"\\( \\mu_B \\)"})," and ",r.createElement(c.A,{text:"\\( \\sigma_B^2 \\)"}),". At inference time, we replace ",r.createElement(c.A,{text:"\\( \\mu_B \\)"})," and ",r.createElement(c.A,{text:"\\( \\sigma_B^2 \\)"})," by moving averages that were collected during training, so that we can run predictions on single examples or any batch size without having to rely on the current batch's statistics."),"\n",r.createElement(t.h2,{id:"4-benefits-of-batch-normalization",style:{position:"relative"}},r.createElement(t.a,{href:"#4-benefits-of-batch-normalization","aria-label":"4 benefits of batch normalization permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4. benefits of batch normalization"),"\n",r.createElement(t.h3,{id:"reducing-internal-covariate-shift",style:{position:"relative"}},r.createElement(t.a,{href:"#reducing-internal-covariate-shift","aria-label":"reducing internal covariate shift permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"reducing internal covariate shift"),"\n",r.createElement(t.p,null,"As I have said, internal covariate shift occurs when updates to early layers cause the distribution of activations feeding into deeper layers to shift dramatically. Because the deeper layers always see a changing input distribution, they require extra care to train stably. By applying batch normalization at intermediate points, we keep the distribution of these inputs stable across training iterations, thereby reducing this internal covariate shift."),"\n",r.createElement(t.p,null,'Some argue about whether the term "internal covariate shift" is an entirely accurate characterization. Regardless of the nuances, the method indeed reduces the volatility of internal hidden distributions, which in practice yields better training dynamics.'),"\n",r.createElement(t.h3,{id:"improved-convergence-speed",style:{position:"relative"}},r.createElement(t.a,{href:"#improved-convergence-speed","aria-label":"improved convergence speed permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"improved convergence speed"),"\n",r.createElement(t.p,null,"With batch normalization, networks often converge in fewer epochs. Each individual update is more stable, so you can use larger learning rates or train deeper networks without blowing up the gradients. This was a major impetus behind the mass adoption of batch normalization in the deep learning community."),"\n",r.createElement(t.h3,{id:"regularization-effects-and-reduced-overfitting",style:{position:"relative"}},r.createElement(t.a,{href:"#regularization-effects-and-reduced-overfitting","aria-label":"regularization effects and reduced overfitting permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"regularization effects and reduced overfitting"),"\n",r.createElement(t.p,null,"Another pleasant side effect is that batch normalization can provide a regularizing effect. By making each sample's activation depend on other samples in the batch (through the mean and variance calculation), the network introduces a slight noise in the hidden activations for each training example. This effect is somewhat analogous to dropout — though not exactly the same — and can help reduce overfitting. In some cases, you might even reduce or remove other forms of regularization (like dropout) when you use batch normalization."),"\n",r.createElement(t.p,null,"The presence of ",r.createElement(c.A,{text:"\\( \\gamma \\)"})," and ",r.createElement(c.A,{text:"\\( \\beta \\)"})," helps ensure that batch normalization does not harm expressivity. Instead, the normalization merely re-anchors the representation space in each layer. This extra flexibility is often crucial for achieving strong final performance."),"\n",r.createElement(t.h3,{id:"more-robust-to-initialization",style:{position:"relative"}},r.createElement(t.a,{href:"#more-robust-to-initialization","aria-label":"more robust to initialization permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"more robust to initialization"),"\n",r.createElement(t.p,null,"A well-known pain point in neural networks is how to initialize parameters so that gradients neither explode nor vanish. Batch normalization can help reduce the reliance on very careful weight initialization. Because the hidden activations are re-centered and re-scaled, even if the initial weights produce higher or lower distributions than expected, the normalization step can help keep them in a manageable range, making training stable right from the start."),"\n",r.createElement(t.h2,{id:"5-implementation",style:{position:"relative"}},r.createElement(t.a,{href:"#5-implementation","aria-label":"5 implementation permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5. implementation"),"\n",r.createElement(t.h3,{id:"using-batch-normalization-layers-in-popular-frameworks",style:{position:"relative"}},r.createElement(t.a,{href:"#using-batch-normalization-layers-in-popular-frameworks","aria-label":"using batch normalization layers in popular frameworks permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"using batch normalization layers in popular frameworks"),"\n",r.createElement(t.p,null,"Modern deep learning frameworks make batch normalization easy to apply. Typically, you insert a batch normalization layer right after a linear or convolutional transform and before the nonlinearity (although in some architectures, it might come after the nonlinearity — this depends on convention and experimentation)."),"\n",r.createElement(t.p,null,"Broadly, the step is:"),"\n",r.createElement(s.A,{text:"\n# Pseudocode structure\n\nout = linear_layer(input, W, b)\nout_bn = BatchNorm(out, is_training=True, gamma, beta, momentum, eps)\nout_activation = ReLU(out_bn)  # for example\n"}),"\n",r.createElement(t.p,null,"In a typical scenario, we let the framework handle the running average of batch means and variances. During inference, we set is_training=False, and the transform uses those running statistics."),"\n",r.createElement(t.h4,{id:"tensorflow--keras",style:{position:"relative"}},r.createElement(t.a,{href:"#tensorflow--keras","aria-label":"tensorflow  keras permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"tensorflow / keras"),"\n",r.createElement(t.p,null,"In TensorFlow or Keras, a batch normalization layer can be inserted as:"),"\n",r.createElement(s.A,{text:"\nimport tensorflow as tf\n\n# This example uses the high-level Keras API in TF 2.x\ndef build_model():\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Dense(128, input_shape=(784,)),  # Some layer\n        tf.keras.layers.BatchNormalization(),            # BN layer\n        tf.keras.layers.Activation('relu'),              # Activation\n        # ...\n    ])\n    return model\n"}),"\n",r.createElement(t.p,null,"In Keras, by default, there are arguments like ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">momentum</code>'}}),", ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">epsilon</code>'}}),", and others that you can tweak:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">momentum</code>'}}),": controls how the moving averages of mean and variance are computed over successive batches. A typical default is around 0.99 or 0.9."),"\n",r.createElement(t.li,null,r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">epsilon</code>'}}),": a small value to add to the variance for numerical stability, often ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">1e-3</code>'}})," or ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">1e-5</code>'}}),"."),"\n",r.createElement(t.li,null,r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">beta_initializer</code>'}})," and ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">gamma_initializer</code>'}}),": for initializing the learnable parameters."),"\n"),"\n",r.createElement(t.h4,{id:"pytorch",style:{position:"relative"}},r.createElement(t.a,{href:"#pytorch","aria-label":"pytorch permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"pytorch"),"\n",r.createElement(t.p,null,"In PyTorch, you do:"),"\n",r.createElement(s.A,{text:"\nimport torch\nimport torch.nn as nn\n\nclass MyModel(nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.fc = nn.Linear(784, 128)\n        self.bn = nn.BatchNorm1d(128)   # BN for a fully connected layer\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.fc(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n"}),"\n",r.createElement(t.p,null,"For convolutional layers, you would use ",r.createElement(l.A,{text:"nn.BatchNorm2d"})," or ",r.createElement(l.A,{text:"nn.BatchNorm3d"}),", depending on dimensionality. The logic is the same, but in 2D or 3D form, the batch norm layer normalizes each channel map over the batch, width, and height (for images)."),"\n",r.createElement(t.h3,{id:"code-snippets-and-walkthrough",style:{position:"relative"}},r.createElement(t.a,{href:"#code-snippets-and-walkthrough","aria-label":"code snippets and walkthrough permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"code snippets and walkthrough"),"\n",r.createElement(t.p,null,"Let me provide a bit more thorough example in PyTorch, demonstrating how to configure the hyperparameters:"),"\n",r.createElement(s.A,{text:"\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n        self.bn1   = nn.BatchNorm2d(32, eps=1e-5, momentum=0.9, affine=True)\n        self.relu  = nn.ReLU()\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n        self.bn2   = nn.BatchNorm2d(64, eps=1e-5, momentum=0.9, affine=True)\n        self.fc    = nn.Linear(64*24*24, 10) # for a 28x28 input\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n\n        # flatten\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n\ndef train_example():\n    model = SimpleCNN()\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\n    # Suppose we have some dataloader that fetches MNIST images\n    for epoch in range(10):\n        for images, labels in dataloader:\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n    # During inference, PyTorch automatically uses the running averages:\n    model.eval()\n    # ...\n"}),"\n",r.createElement(t.p,null,"Here, ",r.createElement(c.A,{text:"\\( \\text{momentum} \\)"})," is set to 0.9 by default. This means PyTorch uses an exponential moving average to track the batch mean and variance of each channel. The parameters ",r.createElement(c.A,{text:"\\( \\gamma \\)"})," and ",r.createElement(c.A,{text:"\\( \\beta \\)"})," (if ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">affine=True</code>'}}),") are also learned."),"\n",r.createElement(t.h3,{id:"recommended-default-settings",style:{position:"relative"}},r.createElement(t.a,{href:"#recommended-default-settings","aria-label":"recommended default settings permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"recommended default settings"),"\n",r.createElement(t.p,null,"Most frameworks set a default ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">epsilon</code>'}})," to around ",r.createElement(c.A,{text:"\\( 10^{-5} \\)"})," or ",r.createElement(c.A,{text:"\\( 10^{-3} \\)"})," and a default momentum near 0.9 or 0.99. Empirically, these defaults are usually good starting points. Typically, you do not need to heavily tune them."),"\n",r.createElement(t.h3,{id:"practical-considerations-and-best-practices",style:{position:"relative"}},r.createElement(t.a,{href:"#practical-considerations-and-best-practices","aria-label":"practical considerations and best practices permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"practical considerations and best practices"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"Placement in the architecture:")," Commonly, you apply batch normalization immediately before the activation function. For instance, ",r.createElement(c.A,{text:"\\( \\mathrm{Conv} \\rightarrow \\mathrm{BN} \\rightarrow \\mathrm{ReLU} \\)"}),". Some architectures, however, do it differently, like ",r.createElement(c.A,{text:"\\( \\mathrm{Conv} \\rightarrow \\mathrm{ReLU} \\rightarrow \\mathrm{BN} \\)"}),". Empirically, the first approach is often slightly more standard, but best practice can vary."),"\n"),"\n",r.createElement(t.li,null,"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"Remove biases in preceding layers:")," Since batch normalization can add a learnable offset (",r.createElement(c.A,{text:"\\( \\beta \\)"}),"), you do not strictly need a bias term in your linear or convolutional layers. Setting ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">bias=False</code>'}})," might reduce redundancy."),"\n"),"\n",r.createElement(t.li,null,"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"Watch out for very small batch sizes:")," If your mini-batches are extremely small (e.g., 2 or 4 examples), the estimates of mean and variance become noisy, which can degrade performance. In such scenarios, alternative methods (layer normalization, group normalization, or batch renormalization) might yield more stable training."),"\n"),"\n",r.createElement(t.li,null,"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"Momentum for running statistics:")," The momentum parameter in your BN layer affects how quickly the moving average of means and variances adapts to new data. If you see poor performance at inference time, check if your running stats have converged or if you need a different momentum setting."),"\n"),"\n",r.createElement(t.li,null,"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"Initialization:")," Usually ",r.createElement(c.A,{text:"\\( \\gamma \\)"})," is initialized to 1, and ",r.createElement(c.A,{text:"\\( \\beta \\)"})," to 0. This way, at the beginning of training, the layer does not transform the normalized values, letting the rest of the network behave as if no BN was present. Over time, BN will adapt to the distribution of activations."),"\n"),"\n"),"\n",r.createElement(t.h3,{id:"choosing-the-right-batch-size",style:{position:"relative"}},r.createElement(t.a,{href:"#choosing-the-right-batch-size","aria-label":"choosing the right batch size permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"choosing the right batch size"),"\n",r.createElement(t.p,null,"Batch normalization depends on the batch's mean and variance, so the batch size matters. If the batch is too small, the statistics might be unreliable. Large mini-batches can produce more stable estimates, but you might not always have the resources (GPU memory, for example) to use large batches. Typical batch sizes for BN might range from 16 to 256, though the sweet spot depends on your application and hardware constraints. If you need to train with very small batches or large-scale distributed training where each worker sees only a small portion of data, consider alternatives like group normalization or batch renormalization."),"\n",r.createElement(t.h3,{id:"hyperparameters-gamma-beta-and-momentum",style:{position:"relative"}},r.createElement(t.a,{href:"#hyperparameters-gamma-beta-and-momentum","aria-label":"hyperparameters gamma beta and momentum permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"hyperparameters: gamma, beta, and momentum"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(c.A,{text:"\\( \\gamma \\)"})," (",r.createElement(l.A,{text:"Gamma"}),") and ",r.createElement(c.A,{text:"\\( \\beta \\)"})," (",r.createElement(l.A,{text:"Beta"}),") are the learnable scale and shift. In PyTorch, these are typically named ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">weight</code>'}})," and ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">bias</code>'}})," of the batch norm layer. In Keras or TensorFlow, they might be explicitly called ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">gamma</code>'}})," and ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">beta</code>'}}),"."),"\n",r.createElement(t.li,null,r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">momentum</code>'}})," is a hyperparameter used for the running averages. Despite the name, it is not the same as momentum in SGD; it is just a parameter that controls how quickly the statistics used at test time are updated."),"\n"),"\n",r.createElement(t.h3,{id:"dealing-with-small-batches-or-large-scale-distributed-training",style:{position:"relative"}},r.createElement(t.a,{href:"#dealing-with-small-batches-or-large-scale-distributed-training","aria-label":"dealing with small batches or large scale distributed training permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"dealing with small batches or large-scale distributed training"),"\n",r.createElement(t.p,null,"When you do distributed training, each device sees only a subset of the mini-batch, so if you globally batch-normalize, you need synchronization across multiple devices. Many frameworks have ",r.createElement(o.A,null,"sync batch norm")," modules that handle this by calculating global means and variances across devices at each step. This can be more expensive, but is often necessary. If your effective batch size per worker is small, again, you might consider group normalization or layer normalization."),"\n",r.createElement(t.h2,{id:"6-extensions-and-variations",style:{position:"relative"}},r.createElement(t.a,{href:"#6-extensions-and-variations","aria-label":"6 extensions and variations permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6. extensions and variations"),"\n",r.createElement(t.p,null,"Batch normalization inspired a wave of research on how best to normalize intermediate representations in deep networks. Let us discuss some of the key variants."),"\n",r.createElement(t.h3,{id:"layer-normalization",style:{position:"relative"}},r.createElement(t.a,{href:"#layer-normalization","aria-label":"layer normalization permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"layer normalization"),"\n",r.createElement(t.p,null,r.createElement(o.A,null,"Layer normalization")," (Ba and gang, 2016) addresses a scenario where you might have to use very small batch sizes or sequences of variable lengths, as in certain RNN tasks. Instead of normalizing across the batch dimension, layer normalization normalizes across the feature dimension for each individual sample. That is, for each example in the batch, we compute the mean and variance of all hidden units in that layer, then re-scale them accordingly. This effectively decouples normalization from the batch dimension. It works especially well for recurrent networks and sometimes for Transformers, although in Transformers you often see ",r.createElement(o.A,null,"pre-layer-norm")," or ",r.createElement(o.A,null,"post-layer-norm")," variants that were introduced for training stability."),"\n",r.createElement(t.h3,{id:"instance-normalization",style:{position:"relative"}},r.createElement(t.a,{href:"#instance-normalization","aria-label":"instance normalization permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"instance normalization"),"\n",r.createElement(t.p,null,r.createElement(o.A,null,"Instance normalization")," was originally proposed to address the style transfer problem, where normalizing across the entire batch of images can destroy instance-specific style details. Instead, you normalize each single example's feature map across spatial dimensions (in case of images). This method can preserve style or allow style transformations. It is widely used in tasks like image-to-image translation, style transfer, or generative models where each sample is supposed to maintain distinct style characteristics."),"\n",r.createElement(t.h3,{id:"group-normalization",style:{position:"relative"}},r.createElement(t.a,{href:"#group-normalization","aria-label":"group normalization permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"group normalization"),"\n",r.createElement(t.p,null,r.createElement(o.A,null,"Group normalization")," (Wu and He, 2018) is a technique that splits the channels of each sample into groups and normalizes each group separately. It aims to strike a middle ground between instance normalization and layer normalization. Group normalization does not rely on the batch dimension, so it is robust to smaller batches but can still preserve some of the statistical averaging across multiple channels. This is popular in detection and segmentation tasks where the batch size can be limited by GPU memory (e.g., in large image segmentation tasks, you might only fit 1 — 2 images per GPU)."),"\n",r.createElement(t.h3,{id:"batch-renormalization",style:{position:"relative"}},r.createElement(t.a,{href:"#batch-renormalization","aria-label":"batch renormalization permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"batch renormalization"),"\n",r.createElement(t.p,null,r.createElement(o.A,null,"Batch renormalization")," (Ioffe, 2017) modifies batch normalization so that it can work better when the mini-batch statistics are poor estimates of the dataset statistics. The idea is to mitigate reliance on the batch's mean and variance by introducing correction terms that come from a running average. This can help especially when the batch size is small or the data is distributed in a complicated manner. For many use cases, standard batch normalization still suffices, but batch renormalization is an interesting approach if your batch size is severely restricted or if your data distribution is highly non-stationary."),"\n",r.createElement(t.h3,{id:"comparison-of-normalization-methods",style:{position:"relative"}},r.createElement(t.a,{href:"#comparison-of-normalization-methods","aria-label":"comparison of normalization methods permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"comparison of normalization methods"),"\n",r.createElement(t.p,null,"There is no one-size-fits-all normalization. The best approach depends on your data characteristics, batch size constraints, and architecture type:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Batch Normalization"),": Great for large mini-batches, widely used in CNNs and MLPs."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Layer Normalization"),": Often chosen for RNNs, Transformers, or situations with small batch sizes."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Instance Normalization"),": Style transfer, generative tasks."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Group Normalization"),": Good compromise for tasks with small batch sizes or large images."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Batch Renormalization"),": A specialized fix for small or unrepresentative batches in typical BN setups."),"\n"),"\n",r.createElement(t.h3,{id:"other-specialized-variants",style:{position:"relative"}},r.createElement(t.a,{href:"#other-specialized-variants","aria-label":"other specialized variants permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"other specialized variants"),"\n",r.createElement(t.p,null,"Beyond these main branches, there are further explorations: ",r.createElement(l.A,{text:"Decorrelated Batch Normalization"}),", ",r.createElement(l.A,{text:"Streaming Normalization"}),", ",r.createElement(l.A,{text:"Conditional Batch Normalization"})," for multi-task learning or style manipulation, ",r.createElement(l.A,{text:"Recurrent Batch Normalization"})," for RNNs, and more. These specialized solutions each revolve around the same fundamental principle: controlling the distribution of intermediate activations for more stable training and improved representational power."),"\n",r.createElement(t.h2,{id:"7-experiments",style:{position:"relative"}},r.createElement(t.a,{href:"#7-experiments","aria-label":"7 experiments permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7. experiments"),"\n",r.createElement(t.h3,{id:"effect-on-training-speed-and-final-accuracy",style:{position:"relative"}},r.createElement(t.a,{href:"#effect-on-training-speed-and-final-accuracy","aria-label":"effect on training speed and final accuracy permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"effect on training speed and final accuracy"),"\n",r.createElement(t.p,null,"Batch normalization often drastically increases the training speed (in terms of epochs needed to converge), although it might add a small overhead per iteration. In practice, the total wall-clock time to achieve a certain accuracy is typically reduced significantly."),"\n",r.createElement(t.h4,{id:"an-mnist-classification-demonstration",style:{position:"relative"}},r.createElement(t.a,{href:"#an-mnist-classification-demonstration","aria-label":"an mnist classification demonstration permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"an mnist classification demonstration"),"\n",r.createElement(t.p,null,"Consider a fully connected network with a few hidden layers of 100 units each, using ReLU activation. Let us train this network on the MNIST handwritten digit dataset (28x28 grayscale images, 10 classes). We compare:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Network without BN"),": Just linear layers + ReLU."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Network with BN"),": Insert BN layers before ReLU in each hidden layer."),"\n"),"\n",r.createElement(t.p,null,"We use a learning rate of 0.01, batch size of 60, and a standard weight initialization. If we plot training accuracy versus training iterations, we typically see that the BN version rapidly jumps above 90% accuracy within the first thousand iterations, whereas the baseline might take many more iterations to get there."),"\n",r.createElement(t.p,null,"Below is an illustrative snippet for a BN version in PyTorch:"),"\n",r.createElement(s.A,{text:"\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass MLPwithBN(nn.Module):\n    def __init__(self):\n        super(MLPwithBN, self).__init__()\n        self.fc1 = nn.Linear(784, 100)\n        self.bn1 = nn.BatchNorm1d(100)\n        self.fc2 = nn.Linear(100, 100)\n        self.bn2 = nn.BatchNorm1d(100)\n        self.fc3 = nn.Linear(100, 10)\n\n    def forward(self, x):\n        x = x.view(x.size(0), -1)  # flatten\n        x = self.fc1(x)\n        x = self.bn1(x)\n        x = nn.functional.relu(x)\n        x = self.fc2(x)\n        x = self.bn2(x)\n        x = nn.functional.relu(x)\n        x = self.fc3(x)\n        return x\n\n\ndef train_mnist_with_bn(dataloader):\n    model = MLPwithBN()\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=0.01)\n\n    for epoch in range(10):\n        for batch_idx, (images, labels) in enumerate(dataloader):\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n"}),"\n",r.createElement(t.p,null,"Plotting the training/validation curves usually shows that:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"The BN network's training error drops faster in the early epochs."),"\n",r.createElement(t.li,null,"The BN model often can reach a slightly higher final accuracy or do so with less tuning of learning rates."),"\n"),"\n",r.createElement(t.h3,{id:"impact-on-different-architectures-cnns-rnns-transformers",style:{position:"relative"}},r.createElement(t.a,{href:"#impact-on-different-architectures-cnns-rnns-transformers","aria-label":"impact on different architectures cnns rnns transformers permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"impact on different architectures (cnns, rnns, transformers)"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"CNNs"),": Batch normalization is extremely common in convolutional neural networks such as VGG, ResNet, and many others. It is typically used after each convolution."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"RNNs"),": Applying BN in recurrent models is trickier due to time-step dependencies. Variants such as ",r.createElement(o.A,null,"Recurrent Batch Normalization")," have been proposed, but mainstream solutions for RNNs often prefer layer normalization, which is simpler for recurrent connections."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Transformers"),": Transformers commonly rely on layer normalization. However, in some variations, batch normalization might be used in the feed-forward components, or no normalization is used in some stages. The principle is similar — control the distribution of intermediate states for stable training."),"\n"),"\n",r.createElement(t.h3,{id:"code-snippets-for-experiments",style:{position:"relative"}},r.createElement(t.a,{href:"#code-snippets-for-experiments","aria-label":"code snippets for experiments permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"code snippets for experiments"),"\n",r.createElement(t.p,null,"A typical experimental pipeline might look like this:"),"\n",r.createElement(s.A,{text:'\n# High-level pseudocode for an experiment comparing BN vs No-BN\n\ndef experiment_bn_vs_nobn():\n    # 1. Prepare data\n    train_loader, valid_loader = get_mnist_loaders(batch_size=64)\n\n    # 2. Define two models\n    model_bn = MLPwithBN()\n    model_nobn = MLPnoBN()\n\n    # 3. Train both\n    train(model_bn, train_loader, ...)\n    train(model_nobn, train_loader, ...)\n\n    # 4. Evaluate both\n    acc_bn = evaluate(model_bn, valid_loader)\n    acc_nobn = evaluate(model_nobn, valid_loader)\n\n    print("Accuracy with BN:", acc_bn)\n    print("Accuracy without BN:", acc_nobn)\n'}),"\n",r.createElement(t.p,null,"One can then record how many epochs or steps each model takes to surpass a certain accuracy threshold, how stable the loss is across training steps, and the final accuracy or other metrics."),"\n",r.createElement(t.p,null,"In image tasks like CIFAR-10, CIFAR-100, and ImageNet, the presence of batch normalization is nearly universal in modern CNN architectures. It is sometimes singled out as one of the factors that made training very deep networks feasible in the first place (e.g., ResNets rely heavily on batch normalization throughout their layers)."),"\n",r.createElement(t.h2,{id:"additional-expansions-and-deep-dive-discussions",style:{position:"relative"}},r.createElement(t.a,{href:"#additional-expansions-and-deep-dive-discussions","aria-label":"additional expansions and deep dive discussions permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"additional expansions and deep-dive discussions"),"\n",r.createElement(t.p,null,"Because batch normalization is so foundational, I will expand on a few more points that might further clarify advanced aspects or interesting nuances. These expansions can serve as optional reading if you want to see how batch normalization interacts with topics like partial updates, more exotic architectures, or advanced theoretical perspectives."),"\n",r.createElement(t.h3,{id:"partial-updates-and-internal-covariate-shift-reconsidered",style:{position:"relative"}},r.createElement(t.a,{href:"#partial-updates-and-internal-covariate-shift-reconsidered","aria-label":"partial updates and internal covariate shift reconsidered permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"partial updates and internal covariate shift reconsidered"),"\n",r.createElement(t.p,null,"There has been debate about whether \"internal covariate shift\" fully explains BN's success. Some arguments revolve around the fact that normalization alone might not entirely remove distribution shifts, especially as the network evolves. Another line of thought proposes that batch normalization's success can also be tied to how it smooths the optimization landscape, making it easier for gradient descent to navigate. Indeed, a smoothed loss surface can significantly accelerate training. Regardless of the underlying reason, in practice, batch normalization addresses many training instability issues."),"\n",r.createElement(t.h3,{id:"synergy-with-other-techniques",style:{position:"relative"}},r.createElement(t.a,{href:"#synergy-with-other-techniques","aria-label":"synergy with other techniques permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"synergy with other techniques"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Dropout"),": BN already has a noise-like effect, but combining BN and dropout can sometimes still be beneficial — especially in large networks."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Skip connections"),": BN can be used alongside residual or skip connections (as in ResNet). The ResNet architecture typically uses ",r.createElement(c.A,{text:"\\( \\mathrm{Conv} \\rightarrow \\mathrm{BN} \\rightarrow \\mathrm{ReLU} \\)"})," patterns in the building blocks."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Weight decay"),": This form of L2 regularization often plays well with BN. However, if you do not want to regularize ",r.createElement(c.A,{text:"\\( \\gamma \\)"})," or ",r.createElement(c.A,{text:"\\( \\beta \\)"}),", you might have to exclude them from weight decay."),"\n"),"\n",r.createElement(t.h3,{id:"advanced-topics-in-inference",style:{position:"relative"}},r.createElement(t.a,{href:"#advanced-topics-in-inference","aria-label":"advanced topics in inference permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"advanced topics in inference"),"\n",r.createElement(t.p,null,'When the network is deployed (i.e., at inference time), we no longer have a mini-batch of data from which to compute mean and variance. Instead, we rely on the exponential moving averages that were accumulated during training. A crucial detail is that if your final running statistics are inaccurate (e.g., if the training procedure changed the distribution drastically at some late stage), you might see performance degrade. Sometimes, people do a final pass called a "re-estimation of batch norm statistics" by running through the training set once more to refine the running means and variances. This can help calibrate the BN layers for inference if needed.'),"\n",r.createElement(t.h3,{id:"limitations-of-batch-normalization",style:{position:"relative"}},r.createElement(t.a,{href:"#limitations-of-batch-normalization","aria-label":"limitations of batch normalization permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"limitations of batch normalization"),"\n",r.createElement(t.p,null,"Although BN is powerful, it is not a panacea. It can fail if:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"The batch size is too small or your data is not i.i.d. ",r.createElement(l.A,{text:"independent and identically distributed"})," in each mini-batch."),"\n",r.createElement(t.li,null,"The overhead of synchronizing batch statistics across many GPUs is high."),"\n",r.createElement(t.li,null,"You are dealing with recurrent or sequential data where time-step dependencies complicate the assumption of independence within the batch."),"\n"),"\n",r.createElement(t.p,null,"In such cases, variants like group normalization, layer normalization, or instance normalization can be more robust."),"\n",r.createElement(t.h3,{id:"bridging-theory-and-practice",style:{position:"relative"}},r.createElement(t.a,{href:"#bridging-theory-and-practice","aria-label":"bridging theory and practice permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"bridging theory and practice"),"\n",r.createElement(t.p,null,"In advanced literature, researchers have studied the effect of BN on the optimization landscape. Some findings suggest that BN can make the gradients more well-conditioned, effectively allowing for larger updates. Another theoretical perspective is that BN re-centers the hidden space, preventing extreme outliers from saturating nonlinearities like ReLU. While not all these theories are definitive, the empirical success of BN in deep learning remains unchallenged."),"\n",r.createElement(t.h2,{id:"conclusion-optional-ending-note",style:{position:"relative"}},r.createElement(t.a,{href:"#conclusion-optional-ending-note","aria-label":"conclusion optional ending note permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"conclusion (optional ending note)"),"\n",r.createElement(t.p,null,"Batch normalization, for many practitioners, is a near-default component in constructing neural networks — especially for feed-forward or convolutional architectures. It has proven to significantly speed up training, allow for deeper networks, reduce sensitivity to hyperparameters, and often improve generalization. The method's core idea — normalize intermediate activations across mini-batches — seems simple, but it was a major milestone in deep learning research. Its impact is apparent in almost every modern architecture from CNNs in vision tasks to certain advanced feed-forward networks in other domains."),"\n",r.createElement(t.p,null,"If your batch size is not too small, or if your data is well-shuffled so that each mini-batch is a decent representation of the overall distribution, you can typically expect batch normalization to provide immediate benefits. If you have unusual constraints, be aware of alternatives like layer normalization, instance normalization, group normalization, or batch renormalization."),"\n",r.createElement(t.p,null,"Whether you are building a standard CNN or experimenting with new architectures, batch normalization remains an indispensable technique to keep in your toolbox, and understanding its mathematical underpinnings, as well as its practical usage, is vital for any machine learning professional."),"\n",r.createElement(a,{alt:"batch_normalization_diagram",path:"",caption:"A conceptual diagram illustrating the forward pass of batch normalization in a convolutional neural network. The layer normalizes across the batch dimension and each channel.",zoom:"false"}),"\n",r.createElement(c.A,{text:"\\( \\text{End of the batch normalization article.} \\)"}),"\n",r.createElement(t.h2,{id:"extended-expansions-to-ensure-thorough-coverage-and-length",style:{position:"relative"}},r.createElement(t.a,{href:"#extended-expansions-to-ensure-thorough-coverage-and-length","aria-label":"extended expansions to ensure thorough coverage and length permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"extended expansions to ensure thorough coverage and length"),"\n",r.createElement(t.p,null,"In this extended section, I will restate or deepen the coverage on topics related to batch normalization. The following expansions are partially reiterative yet angle deeper into the conceptual and theoretical frameworks, the historical evolution, and the nuanced practicalities. Although some of the material overlaps with earlier sections, revisiting it from multiple perspectives can help solidify understanding."),"\n",r.createElement(t.h3,{id:"historical-context-and-evolution",style:{position:"relative"}},r.createElement(t.a,{href:"#historical-context-and-evolution","aria-label":"historical context and evolution permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"historical context and evolution"),"\n",r.createElement(t.p,null,"Before batch normalization took the deep learning field by storm, practitioners already recognized that normalizing inputs significantly improved training. However, few systematically attempted to normalize hidden layer activations in the middle of a network. The landmark paper by Sergey Ioffe and Christian Szegedy in 2015 introduced the idea of normalizing at each layer within mini-batches, framing it primarily as a solution to the internal covariate shift problem. This approach quickly gained acceptance because it addressed two major bottlenecks in training:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,"The need to manually tune learning rates to avoid exploding or vanishing gradients."),"\n",r.createElement(t.li,null,"The difficulty in training very deep networks due to shifting distributions at each layer."),"\n"),"\n",r.createElement(t.p,null,"Early successes with BN included enabling deeper networks to train faster and achieve state-of-the-art results in the ImageNet classification challenge. Notably, models that integrated BN (such as new variants of Inception or ResNet) saw significant improvements in both speed and final accuracy."),"\n",r.createElement(t.p,null,"Subsequent years witnessed a surge of variants:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Layer Normalization")," (2016): proposed to tackle tasks with small batch sizes or RNNs."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Instance Normalization")," (2016, 2017): especially for style transfer."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Batch Renormalization")," (2017): improving reliability of BN estimates with small batches."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Group Normalization")," (2018): bridging the gap between LN and BN in large-scale tasks with memory constraints."),"\n"),"\n",r.createElement(t.p,null,"Furthermore, specialized forms of BN also appeared for domain adaptation, multi-domain learning, conditional tasks, etc."),"\n",r.createElement(t.h3,{id:"deeper-dive-into-internal-covariate-shift",style:{position:"relative"}},r.createElement(t.a,{href:"#deeper-dive-into-internal-covariate-shift","aria-label":"deeper dive into internal covariate shift permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"deeper dive into internal covariate shift"),"\n",r.createElement(t.p,null,"The concept of ",r.createElement(o.A,null,"covariate shift"),' is well-studied in classical machine learning: it occurs when the distribution of input features changes between training and testing. The word "internal" in internal covariate shift underscores that the phenomenon is happening inside the network between layers, not just between training and test sets. Because each layer\'s input can shift as prior layers learn, the deeper layers effectively see a non-stationary distribution over the course of training.'),"\n",r.createElement(t.p,null,"From a more formal perspective, consider a neural network with ",r.createElement(c.A,{text:"\\( L \\)"})," layers, each with parameters that update at each iteration. We can denote the input to layer ",r.createElement(c.A,{text:"\\( l \\)"})," by ",r.createElement(c.A,{text:"\\( \\mathbf{z}^{[l]} \\)"}),". The distribution of ",r.createElement(c.A,{text:"\\( \\mathbf{z}^{[l]} \\)"})," depends on the parameters of all preceding layers ",r.createElement(c.A,{text:"\\( 1, 2, \\ldots, l-1 \\)"}),". Whenever those earlier layers change, so does ",r.createElement(c.A,{text:"\\( \\mathbf{z}^{[l]} \\)"}),". BN tries to keep the mean and variance of ",r.createElement(c.A,{text:"\\( \\mathbf{z}^{[l]} \\)"})," more consistent during training iterations by performing the normalization per mini-batch."),"\n",r.createElement(t.p,null,"Critics argue that BN does not truly fix all forms of shifting distributions in deeper layers, especially once the layer's scale and shift parameters ",r.createElement(c.A,{text:"\\( \\gamma, \\beta \\)"})," are introduced. The distribution is still subject to changes in ",r.createElement(c.A,{text:"\\( \\gamma, \\beta \\)"})," themselves. Nonetheless, it remains that BN empirically stabilizes and accelerates training, which is the principal practical reason behind its widespread use."),"\n",r.createElement(t.h3,{id:"training-with-large-vs-small-batch-sizes",style:{position:"relative"}},r.createElement(t.a,{href:"#training-with-large-vs-small-batch-sizes","aria-label":"training with large vs small batch sizes permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"training with large vs. small batch sizes"),"\n",r.createElement(t.p,null,"One of the biggest disadvantages of BN is the reliance on sufficiently large mini-batches to get reliable estimates of mean and variance. If the batch is too small:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"The sample mean and variance might be highly noisy, leading to unstable updates."),"\n",r.createElement(t.li,null,"The performance might degrade, or the model might not converge properly."),"\n",r.createElement(t.li,null,"In extreme cases (like 1 or 2 images per batch), BN can hamper training more than it helps."),"\n"),"\n",r.createElement(t.p,null,"Practitioners have responded by using advanced techniques like ",r.createElement(o.A,null,"virtual batch normalization")," (VBM) or ",r.createElement(o.A,null,"micro-batch accumulation"),", or by switching to alternative normalizations. For example, if you can only fit a single example on your GPU at a time, you can accumulate gradients across multiple forward passes and only then do the BN step with an effective batch. Alternatively, you might switch to group normalization, which does not rely on the batch dimension at all."),"\n",r.createElement(t.h3,{id:"bridging-to-statistical-theory",style:{position:"relative"}},r.createElement(t.a,{href:"#bridging-to-statistical-theory","aria-label":"bridging to statistical theory permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"bridging to statistical theory"),"\n",r.createElement(t.p,null,"From a statistical standpoint, normalizing data typically helps gradient-based methods because it avoids certain pathological curvatures in the loss landscape. If we recall the concept of the ",r.createElement(c.A,{text:"\\( \\mathrm{cov}(\\cdot,\\cdot) \\)"})," of features, having higher correlation among features can hamper gradient-based optimization. BN breaks up some of these correlations by ensuring each dimension has the same scale, at least within a mini-batch. Additionally, the partial derivatives can become more stable as the layer's outputs remain in a narrower range. This can reduce or eliminate saturations in nonlinearities, such as a ReLU or a sigmoid."),"\n",r.createElement(t.h3,{id:"discussion-on-the-hyperparameters",style:{position:"relative"}},r.createElement(t.a,{href:"#discussion-on-the-hyperparameters","aria-label":"discussion on the hyperparameters permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"discussion on the hyperparameters"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,r.createElement(c.A,{text:"\\( \\epsilon \\)"})," (epsilon)"),": Usually chosen to be ",r.createElement(c.A,{text:"\\(10^{-5}\\)"})," or ",r.createElement(c.A,{text:"\\(10^{-3}\\)"}),'. If you see "checkerboard artifacts" or unexpected performance issues, adjusting epsilon might help, though it is rarely the main culprit.'),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">momentum</code>'}})),": Typically 0.9 or 0.99. The difference is in how quickly the running mean and variance adapt. If momentum is too high, the estimates can lag behind changes in the distribution. If it is too low, your estimates might be more volatile."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Batch size"),": As mentioned, bigger is generally better, but practical constraints abound."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Placement"),": Standard practice is ",r.createElement(c.A,{text:"\\( \\mathrm{Conv} \\rightarrow \\mathrm{BN} \\rightarrow \\mathrm{ReLU} \\)"})," (or another activation). Some practitioners have found no big difference if they switch BN and ReLU, but the official BN paper recommended placing it before the nonlinearity."),"\n"),"\n",r.createElement(t.h3,{id:"computational-overhead",style:{position:"relative"}},r.createElement(t.a,{href:"#computational-overhead","aria-label":"computational overhead permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"computational overhead"),"\n",r.createElement(t.p,null,"Batch normalization does introduce extra computational steps:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"Mean and variance must be computed across each mini-batch."),"\n",r.createElement(t.li,null,"The output has an additional elementwise transformation (",r.createElement(c.A,{text:"\\( \\hat{x}_i \\mapsto \\gamma \\hat{x}_i + \\beta \\)"}),")."),"\n",r.createElement(t.li,null,"During backprop, partial derivatives with respect to ",r.createElement(c.A,{text:"\\( \\mu_B \\)"})," and ",r.createElement(c.A,{text:"\\( \\sigma_B^2 \\)"})," must also be computed, though frameworks do this efficiently."),"\n"),"\n",r.createElement(t.p,null,"For typical moderate or large batch sizes, the overhead is not severe, and the net effect is beneficial because fewer epochs are needed to converge. On modern GPUs, the overhead is minimal relative to the entire forward and backward pass of a large CNN."),"\n",r.createElement(t.h3,{id:"advanced-research-trends",style:{position:"relative"}},r.createElement(t.a,{href:"#advanced-research-trends","aria-label":"advanced research trends permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"advanced research trends"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Decorrelated Batch Normalization"),": This normalizes not just the first and second moments but also attempts to whiten the features so that they become decorrelated."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Conditional Batch Normalization"),": The scale and shift parameters ",r.createElement(c.A,{text:"\\( \\gamma, \\beta \\)"})," are replaced by functions of some condition, such as class labels or textual input. This is popular in tasks such as style transfer or multi-domain generation."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Revisiting Normalization-Free Networks"),': A small subset of research explores networks that do not rely on BN or LN, e.g., using scaled weight initializations or carefully chosen skip connections that preserve variance. These "normalization-free" networks aim to reduce memory overhead or handle tasks where BN fails.'),"\n"),"\n",r.createElement(t.h3,{id:"practical-notes-on-partial-vs-full-batch",style:{position:"relative"}},r.createElement(t.a,{href:"#practical-notes-on-partial-vs-full-batch","aria-label":"practical notes on partial vs full batch permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"practical notes on partial vs. full batch"),"\n",r.createElement(t.p,null,'Sometimes, people worry about the difference between "batch" in the name "batch normalization" and the typical "mini-batch" used in training. Strictly speaking, the original BN approach is mini-batch normalization. The dataset can be extremely large, but we are only computing means and variances on each mini-batch. This is usually enough to yield stable estimates, especially if the mini-batch is randomly sampled from the dataset.'),"\n",r.createElement(t.h3,{id:"re-centered-recaps-of-forward-and-backward-passes",style:{position:"relative"}},r.createElement(t.a,{href:"#re-centered-recaps-of-forward-and-backward-passes","aria-label":"re centered recaps of forward and backward passes permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"re-centered recaps of forward and backward passes"),"\n",r.createElement(c.A,{text:"\\( \\textbf{Forward pass:} \\)"}),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,"\n",r.createElement(c.A,{text:"\\( \\mu_B = \\frac{1}{m}\\sum_i x_i \\)"}),"\n"),"\n",r.createElement(t.li,null,"\n",r.createElement(c.A,{text:"\\( \\sigma_B^2 = \\frac{1}{m}\\sum_i (x_i - \\mu_B)^2 \\)"}),"\n"),"\n",r.createElement(t.li,null,"\n",r.createElement(c.A,{text:"\\( \\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\)"}),"\n"),"\n",r.createElement(t.li,null,"\n",r.createElement(c.A,{text:"\\( y_i = \\gamma \\hat{x}_i + \\beta \\)"}),"\n"),"\n"),"\n",r.createElement(c.A,{text:"\\( \\textbf{Backward pass:} \\)"}),"\n",r.createElement(t.p,null,"We consider partial derivatives ",r.createElement(c.A,{text:"\\( \\frac{\\partial l}{\\partial x_i} \\)"}),", ",r.createElement(c.A,{text:"\\( \\frac{\\partial l}{\\partial \\gamma} \\)"}),", ",r.createElement(c.A,{text:"\\( \\frac{\\partial l}{\\partial \\beta} \\)"}),". The derivations revolve around the chain rule applied to the above transformations. In practice, frameworks like PyTorch or TensorFlow handle this automatically via computational graphs."),"\n",r.createElement(t.h3,{id:"synergy-with-skip-connections-resnets",style:{position:"relative"}},r.createElement(t.a,{href:"#synergy-with-skip-connections-resnets","aria-label":"synergy with skip connections resnets permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"synergy with skip connections (resnets)"),"\n",r.createElement(t.p,null,"In ResNet and many advanced CNN architectures, BN is typically used at multiple points in each residual block. For example, in a classic ResNet building block:"),"\n",r.createElement(s.A,{text:"\nx -> Conv -> BN -> ReLU -> Conv -> BN\n|___________________________________ + -> ReLU\n"}),"\n",r.createElement(t.p,null,"The skip connection sums the input to the block with the output of the second BN. This synergy works well because BN ensures that the residual branch does not blow up or vanish. Then the final ReLU also receives stable distributions from the block's output."),"\n",r.createElement(t.h3,{id:"emergent-phenomena",style:{position:"relative"}},r.createElement(t.a,{href:"#emergent-phenomena","aria-label":"emergent phenomena permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"emergent phenomena"),"\n",r.createElement(t.p,null,"Empirically, one emergent phenomenon is that BN often allows stable training with a range of higher learning rates than one might normally attempt. This is especially helpful if you want to train quickly with a large learning rate and then do a learning rate schedule or warm restarts. Another phenomenon is that BN can help networks generalize better. The slight mismatch between training and inference modes can even act like a mild regularizer. However, there have been edge cases where a mismatch in distributions between training and inference can cause performance drops if the running mean and variance are inaccurate."),"\n",r.createElement(t.h3,{id:"domain-shift-and-finetuning",style:{position:"relative"}},r.createElement(t.a,{href:"#domain-shift-and-finetuning","aria-label":"domain shift and finetuning permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"domain shift and finetuning"),"\n",r.createElement(t.p,null,'When performing domain adaptation or transfer learning, one might face a new data distribution. The stored running statistics in BN layers might not reflect the new distribution well. A solution is to update or "finetune" the BN layers on the new domain data. That is, allow new means and variances to be computed and stored for the new domain, while optionally freezing or lightly adjusting other parts of the network. This technique, often called "re-BN," can be a powerful approach to quickly adapt to domain shifts.'),"\n",r.createElement(t.h3,{id:"extremely-large-scale-training",style:{position:"relative"}},r.createElement(t.a,{href:"#extremely-large-scale-training","aria-label":"extremely large scale training permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"extremely large-scale training"),"\n",r.createElement(t.p,null,"In massive distributed training scenarios (e.g., training on hundreds of GPUs), the local mini-batch on each GPU might be small, but the overall global batch across all GPUs can be large. Some frameworks implement synchronous BN that computes the mean and variance across all GPUs. This keeps the BN estimates consistent throughout the distributed system. Alternatively, if we do asynchronous or local BN, we risk each GPU having different estimates of the distribution, which can hamper convergence."),"\n",r.createElement(t.h3,{id:"debugging-tips",style:{position:"relative"}},r.createElement(t.a,{href:"#debugging-tips","aria-label":"debugging tips permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"debugging tips"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"If your training accuracy suddenly plummets when you switch the network from training to inference mode (",r.createElement(c.A,{text:"\\( \\texttt{model.eval()} \\)"})," in PyTorch), suspect that your BN running statistics might be off. You can debug by looking at the saved running means and variances in each BN layer and comparing them with the actual batch statistics. If the mismatch is large, re-check your training procedure or consider doing a separate pass to re-estimate the BN stats."),"\n",r.createElement(t.li,null,'If you see "nan" or "inf" values, verify that ',r.createElement(c.A,{text:"\\( \\epsilon \\)"})," is not set too small, or that your batch variance is not zero. Another possibility is that the learning rate is still too high."),"\n"),"\n",r.createElement(t.h3,{id:"concluding-expansions",style:{position:"relative"}},r.createElement(t.a,{href:"#concluding-expansions","aria-label":"concluding expansions permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"concluding expansions"),"\n",r.createElement(t.p,null,"Batch normalization remains the default normalization technique in many domains. Despite that, if you find yourself debugging training issues, do not forget to examine the BN layers carefully: the default hyperparameters might not be optimal in every scenario, especially if your mini-batch distribution differs drastically from typical assumptions. The synergy between BN, correct random shuffling, and a sufficiently large batch size can yield extremely stable and fast training."),"\n",r.createElement(t.p,null,"Moreover, as new architectural paradigms (like Transformers) continue to reshape the machine learning landscape, the fundamental idea of normalizing internal representations has persisted in various forms. The general principle behind BN — that controlling internal activation scales can drastically ease training — continues to be validated across numerous tasks and architectures."),"\n",r.createElement(t.p,null,"Below is one more code snippet for advanced usage, demonstrating how one might implement a custom BN approach in PyTorch that explicitly tracks the means and variances:"),"\n",r.createElement(s.A,{text:"\nimport torch\nimport torch.nn.functional as F\n\nclass CustomBatchNorm(torch.nn.Module):\n    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n        super(CustomBatchNorm, self).__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.momentum = momentum\n\n        self.gamma = torch.nn.Parameter(torch.ones(num_features))\n        self.beta = torch.nn.Parameter(torch.zeros(num_features))\n\n        self.running_mean = torch.zeros(num_features)\n        self.running_var = torch.ones(num_features)\n\n    def forward(self, x):\n        if self.training:\n            # compute mean/var per feature dimension\n            mean = x.mean(dim=(0,2,3), keepdim=True)  # for 2D conv shape\n            var = x.var(dim=(0,2,3), keepdim=True, unbiased=False)\n\n            # update running stats\n            self.running_mean = (1 - self.momentum) * self.running_mean +                                 self.momentum * mean.squeeze()\n            self.running_var = (1 - self.momentum) * self.running_var +                                self.momentum * var.squeeze()\n\n            # normalize\n            x_hat = (x - mean) / torch.sqrt(var + self.eps)\n        else:\n            mean = self.running_mean.view(1, -1, 1, 1)\n            var = self.running_var.view(1, -1, 1, 1)\n            x_hat = (x - mean) / torch.sqrt(var + self.eps)\n\n        gamma = self.gamma.view(1, -1, 1, 1)\n        beta  = self.beta.view(1, -1, 1, 1)\n        return x_hat * gamma + beta\n"}),"\n",r.createElement(t.p,null,"In this custom layer, I explicitly compute the per-channel mean and variance over spatial dimensions and across the batch dimension, then update the running statistics accordingly. This code snippet clarifies the underlying mechanics of BN that frameworks usually handle for you automatically."),"\n",r.createElement(t.p,null,"Through these expansions, I hope to have provided a thoroughly comprehensive overview of batch normalization, from its fundamental formulas to advanced practical tips and to its broader context in deep learning. If you are building neural networks for modern tasks, it is almost certain you will rely on BN or a close variant of it, making a deep understanding invaluable."))}var m=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,i.RP)(),e.components);return t?r.createElement(t,e,r.createElement(h,e)):h(e)};var d=a(36710),u=a(58481),p=a.n(u),g=a(36310),f=a(87245),v=a(27042),y=a(59849),b=a(5591),E=a(61122),w=a(9219),z=a(33203),x=a(95751),S=a(94328),_=a(80791),k=a(78137);const N=e=>{let{toc:t}=e;if(!t||!t.items)return null;return r.createElement("nav",{className:_.R},r.createElement("ul",null,t.items.map(((e,t)=>r.createElement("li",{key:t},r.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const a=t.replace("#",""),n=document.getElementById(a);n&&n.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&r.createElement(N,{toc:{items:e.items}}))))))};function H(e){let{data:{mdx:t,allMdx:l,allPostImages:o},children:s}=e;const{frontmatter:c,body:h,tableOfContents:m}=t,d=c.index,u=c.slug.split("/")[1],y=l.nodes.filter((e=>e.frontmatter.slug.includes(`/${u}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),_=y.findIndex((e=>e.frontmatter.index===d)),H=y[_+1],T=y[_-1],B=c.slug.replace(/\/$/,""),M=/[^/]*$/.exec(B)[0],I=`posts/${u}/content/${M}/`,{0:C,1:L}=(0,r.useState)(c.flagWideLayoutByDefault),{0:A,1:V}=(0,r.useState)(!1);var j;(0,r.useEffect)((()=>{V(!0);const e=setTimeout((()=>V(!1)),340);return()=>clearTimeout(e)}),[C]),"adventures"===u?j=w.cb:"research"===u?j=w.Qh:"thoughts"===u&&(j=w.T6);const P=p()(h).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,D=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),a=e%60;return a<=30?`~${t}${a>0?".5":""} h`:`~${t+1} h`}(Math.ceil(P/j)+(c.extraReadTimeMin||0)),R=[{flag:c.flagDraft,component:()=>Promise.all([a.e(3231),a.e(8809)]).then(a.bind(a,28809))},{flag:c.flagMindfuckery,component:()=>Promise.all([a.e(3231),a.e(2471)]).then(a.bind(a,67709))},{flag:c.flagRewrite,component:()=>Promise.all([a.e(3231),a.e(6764)]).then(a.bind(a,62002))},{flag:c.flagOffensive,component:()=>Promise.all([a.e(3231),a.e(2443)]).then(a.bind(a,17681))},{flag:c.flagProfane,component:()=>Promise.all([a.e(3231),a.e(8048)]).then(a.bind(a,53286))},{flag:c.flagMultilingual,component:()=>Promise.all([a.e(3231),a.e(4069)]).then(a.bind(a,78831))},{flag:c.flagUnreliably,component:()=>Promise.all([a.e(3231),a.e(3417)]).then(a.bind(a,8179))},{flag:c.flagPolitical,component:()=>Promise.all([a.e(3231),a.e(5195)]).then(a.bind(a,30433))},{flag:c.flagCognitohazard,component:()=>Promise.all([a.e(3231),a.e(3175)]).then(a.bind(a,8413))},{flag:c.flagHidden,component:()=>Promise.all([a.e(3231),a.e(9556)]).then(a.bind(a,14794))}],{0:G,1:O}=(0,r.useState)([]);return(0,r.useEffect)((()=>{R.forEach((e=>{let{flag:t,component:a}=e;t&&a().then((e=>{O((t=>[].concat((0,n.A)(t),[e.default])))}))}))}),[]),r.createElement(v.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},r.createElement(b.A,{postNumber:c.index,date:c.date,updated:c.updated,readTime:D,difficulty:c.difficultyLevel,title:c.title,desc:c.desc,banner:c.banner,section:u,postKey:M,isMindfuckery:c.flagMindfuckery,mainTag:c.mainTag}),r.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},c.otherTags.map(((e,t)=>r.createElement("span",{key:t,className:`noselect ${k.MW}`,style:{margin:"0 5px 5px 0"}},e)))),r.createElement("div",{className:"postBody"},r.createElement(N,{toc:m})),r.createElement("br"),r.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},r.createElement(v.P.button,{className:`noselect ${S.pb}`,id:S.xG,onClick:()=>{L(!C)},whileTap:{scale:.93}},r.createElement(v.P.div,{className:x.DJ,key:C,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},C?"Switch to default layout":"Switch to wide layout"))),r.createElement("br"),r.createElement("div",{className:"postBody",style:{margin:C?"0 -14%":"",maxWidth:C?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},r.createElement("div",{className:`${S.P_} ${A?S.Xn:S.qG}`},G.map(((e,t)=>r.createElement(e,{key:t}))),c.indexCourse?r.createElement(z.A,{index:c.indexCourse,category:c.courseCategoryName}):"",r.createElement(g.Z.Provider,{value:{images:o.nodes,basePath:I.replace(/\/$/,"")+"/"}},r.createElement(i.xA,{components:{Image:f.A}},s)))),r.createElement(E.A,{nextPost:H,lastPost:T,keyCurrent:M,section:u}))}function T(e){return r.createElement(H,e,r.createElement(m,e))}function B(e){var t,a,n,i,l;let{data:o}=e;const{frontmatter:s}=o.mdx,c=s.titleSEO||s.title,h=s.titleOG||c,m=s.titleTwitter||c,u=s.descSEO||s.desc,p=s.descOG||u,g=s.descTwitter||u,f=s.schemaType||"BlogPosting",v=s.keywordsSEO,b=s.date,E=s.updated||b,w=s.imageOG||(null===(t=s.banner)||void 0===t||null===(a=t.childImageSharp)||void 0===a||null===(n=a.gatsbyImageData)||void 0===n||null===(i=n.images)||void 0===i||null===(l=i.fallback)||void 0===l?void 0:l.src),z=s.imageAltOG||p,x=s.imageTwitter||w,S=s.imageAltTwitter||g,_=s.canonicalURL,k=s.flagHidden||!1,N=s.mainTag||"Posts",H=s.slug.split("/")[1]||"posts",{siteUrl:T}=(0,d.Q)(),B={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:T},{"@type":"ListItem",position:2,name:N,item:`${T}/${s.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${T}${s.slug}`}]};return r.createElement(y.A,{title:c+" - avrtt.blog",titleOG:h,titleTwitter:m,description:u,descriptionOG:p,descriptionTwitter:g,schemaType:f,keywords:v,datePublished:b,dateModified:E,imageOG:w,imageAltOG:z,imageTwitter:x,imageAltTwitter:S,canonicalUrl:_,flagHidden:k,mainTag:N,section:H,type:"article"},r.createElement("script",{type:"application/ld+json"},JSON.stringify(B)))}},66501:function(e,t,a){a.d(t,{A:function(){return l}});var n=a(96540),i=a(3962),r="styles-module--tooltiptext--a263b";var l=e=>{let{text:t,isBadge:a=!1}=e;const{0:l,1:o}=(0,n.useState)(!1),s=(0,n.useRef)(null);return(0,n.useEffect)((()=>{function e(e){s.current&&!s.current.contains(e.target)&&o(!1)}return document.addEventListener("click",e),()=>{document.removeEventListener("click",e)}}),[]),n.createElement("span",{className:"styles-module--tooltipWrapper--75ebf",ref:s},n.createElement("img",{id:a?"styles-module--infoBadge--e3d66":"styles-module--info--26c1f",src:i.A,alt:"info",onClick:e=>{e.stopPropagation(),o((e=>!e))}}),n.createElement("span",{className:l?`${r} styles-module--visible--c063c`:r},t))}},96098:function(e,t,a){var n=a(96540),i=a(7978);t.A=e=>{let{text:t}=e;return n.createElement(i.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-js-content-file-path-src-pages-posts-research-batch-normalization-mdx-b2f70f3c53c5d2059f6a.js.map