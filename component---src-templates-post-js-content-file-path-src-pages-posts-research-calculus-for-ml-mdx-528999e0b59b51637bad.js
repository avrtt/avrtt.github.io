"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[3056],{1160:function(e,t,a){a.r(t),a.d(t,{Head:function(){return z},PostTemplate:function(){return _},default:function(){return A}});var n=a(54506),i=a(28453),r=a(96540),l=(a(16886),a(46295)),o=a(96098);function s(e){const t=Object.assign({p:"p",h2:"h2",a:"a",span:"span",h3:"h3",ul:"ul",li:"li",strong:"strong",hr:"hr"},(0,i.RP)(),e.components),{Image:a}=t;return a||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),r.createElement(r.Fragment,null,"\n",r.createElement("br"),"\n","\n",r.createElement(t.p,null,"Calculus lies at the heart of modern data science and machine learning, offering a rigorous toolkit for understanding how models change in response to their parameters. Whenever you hear about gradient descent, backpropagation, or continuous optimization methods, you are seeing calculus in action. In deep learning, for instance, the entire training process hinges on taking derivatives of the loss function with respect to millions (or billions) of parameters. Even in simpler machine learning methods, we often rely on partial derivatives to update parameters, evaluate sensitivities, or find maxima and minima. Consequently, a solid understanding of calculus provides both intuitive and formal perspectives on why certain ML methods work."),"\n",r.createElement(t.p,null,"Throughout this article, we explore the fundamentals of single-variable and multivariate calculus, connect them to probabilities and expectations, then conclude with a deep dive into how automatic differentiation is implemented in popular libraries. We also discuss more advanced topics like differential geometry and partial differential equations, highlighting their significance in specialized areas of machine learning. Although we will dive into some sophisticated details, the goal is to make these ideas accessible, focusing on clarity, concrete examples, and a guided tour of key concepts."),"\n",r.createElement(t.h2,{id:"quick-refreshment-of-non-related-fundamentals-of-calculus",style:{position:"relative"}},r.createElement(t.a,{href:"#quick-refreshment-of-non-related-fundamentals-of-calculus","aria-label":"quick refreshment of non related fundamentals of calculus permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"quick refreshment of non-related fundamentals of calculus"),"\n",r.createElement(t.p,null,"Many data scientists learn single-variable calculus early in their studies, but it's easy to lose sight of its direct connections to machine learning applications. Below is a refresher on fundamental topics:"),"\n",r.createElement(t.h3,{id:"limits-and-continuity",style:{position:"relative"}},r.createElement(t.a,{href:"#limits-and-continuity","aria-label":"limits and continuity permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"limits and continuity"),"\n",r.createElement(t.p,null,"A limit ",r.createElement(o.A,{text:"\\( \\lim_{x \\to a} f(x) \\)"}),' asks, "What value does ',r.createElement(o.A,{text:"\\( f(x) \\)"})," approach as ",r.createElement(o.A,{text:"\\( x \\)"})," approaches ",r.createElement(o.A,{text:"\\( a \\)"}),'?" Continuity means a function has no abrupt jumps or holes. Formally, a function ',r.createElement(o.A,{text:"\\( f \\)"})," is continuous at ",r.createElement(o.A,{text:"\\( x = a \\)"})," if its limit at ",r.createElement(o.A,{text:"\\( a \\)"})," exists and equals ",r.createElement(o.A,{text:"\\( f(a) \\)"}),". Continuity and limits underlie the definitions of derivatives and integrals; in machine learning, we often implicitly assume continuity for the functions we differentiate (like loss functions) to avoid pathological behavior."),"\n",r.createElement(t.h3,{id:"differentiation-and-the-chain-rule",style:{position:"relative"}},r.createElement(t.a,{href:"#differentiation-and-the-chain-rule","aria-label":"differentiation and the chain rule permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"differentiation and the chain rule"),"\n",r.createElement(t.p,null,"A derivative measures how fast a function changes when its input changes. If we have a single-variable function ",r.createElement(o.A,{text:"\\( f(x) \\)"}),", its derivative is:"),"\n",r.createElement(o.A,{text:"\\[\nf'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}.\n\\]"}),"\n",r.createElement(t.p,null,"In machine learning, derivatives help us pinpoint the direction and rate of steepest change of a model's error function. Minimizing that error is the driving force in gradient descent."),"\n",r.createElement(t.p,null,"The chain rule is crucial: if ",r.createElement(o.A,{text:"\\( y = f(g(x)) \\)"}),", the derivative ",r.createElement(o.A,{text:"\\( \\frac{dy}{dx} \\)"})," is given by ",r.createElement(o.A,{text:"\\( f'(g(x)) \\cdot g'(x) \\)"}),". This simple principle generalizes elegantly to many-layered functions — an idea at the core of backpropagation. If you have a neural network with multiple layers, you apply the chain rule iteratively from the final output layer all the way back to the initial inputs."),"\n",r.createElement(t.h3,{id:"finding-extrema",style:{position:"relative"}},r.createElement(t.a,{href:"#finding-extrema","aria-label":"finding extrema permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"finding extrema"),"\n",r.createElement(t.p,null,"Machine learning often revolves around finding extrema of objective functions — minima when training a model, maxima in certain other scenarios (e.g., maximum likelihood). To find a local minimum or maximum of a single-variable function, you set its derivative to zero:"),"\n",r.createElement(o.A,{text:"\\[\nf'(x) = 0 \\quad \\text{(candidate for a minimum or maximum).}\n\\]"}),"\n",r.createElement(t.p,null,"From there, you check second derivatives or other methods to confirm whether it's indeed a minimum, maximum, or saddle point."),"\n",r.createElement(t.h3,{id:"taylor-series",style:{position:"relative"}},r.createElement(t.a,{href:"#taylor-series","aria-label":"taylor series permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"taylor series"),"\n",r.createElement(t.p,null,"A Taylor expansion approximates functions near a point ",r.createElement(o.A,{text:"\\( a \\)"}),". For a function ",r.createElement(o.A,{text:"\\( f \\)"})," with sufficient derivatives, its Taylor series around ",r.createElement(o.A,{text:"\\( a \\)"})," is:"),"\n",r.createElement(o.A,{text:"\\[\nf(x) \\approx f(a) + f'(a)(x-a) + \\frac{f''(a)}{2!}(x-a)^2 + \\dots\n\\]"}),"\n",r.createElement(t.p,null,"In machine learning, Taylor expansions help analyze local behavior of loss functions or approximate them near optimum points. This local approximation viewpoint can offer insights into optimization landscapes, convexity, and numerical stability."),"\n",r.createElement(t.h3,{id:"convex-and-non-convex-functions",style:{position:"relative"}},r.createElement(t.a,{href:"#convex-and-non-convex-functions","aria-label":"convex and non convex functions permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"convex and non-convex functions"),"\n",r.createElement(t.p,null,'A function is convex if, informally, it "curves upward" everywhere. In single-variable terms, a convex function has a nonnegative second derivative (',r.createElement(o.A,{text:"\\( f''(x) \\ge 0 \\)"}),"). Convexity in higher dimensions is more nuanced, but the single-variable idea generalizes. Convex optimization problems (e.g., linear regression with L2 regularization) are easier to handle; we can guarantee global minima. Non-convex problems (e.g., deep neural networks, certain clustering algorithms) may have multiple local minima or saddle points, making them trickier to optimize."),"\n",r.createElement(t.h3,{id:"integration",style:{position:"relative"}},r.createElement(t.a,{href:"#integration","aria-label":"integration permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"integration"),"\n",r.createElement(t.p,null,"The integral of a function ",r.createElement(o.A,{text:"\\( f(x) \\)"})," from ",r.createElement(o.A,{text:"\\( a \\)"})," to ",r.createElement(o.A,{text:"\\( b \\)"})," is:"),"\n",r.createElement(o.A,{text:"\\[\n\\int_a^b f(x)\\, dx.\n\\]"}),"\n",r.createElement(t.p,null,"In data science, integrals appear whenever we compute area under curves (e.g., analyzing probability densities or cumulative distributions). Integrals help define the expectation of random variables, which is a fundamental operation in statistical analysis. For instance, the expected value of a continuous random variable ",r.createElement(o.A,{text:"\\( X \\)"})," with density ",r.createElement(o.A,{text:"\\( p(x) \\)"})," is:"),"\n",r.createElement(o.A,{text:"\\[\nE[X] = \\int_{-\\infty}^{\\infty} x\\, p(x)\\, dx.\n\\]"}),"\n",r.createElement(t.p,null,"Line integrals, surface integrals, or multidimensional integrals arise in more advanced applications such as computing probabilities over multiple variables or dealing with certain PDE-based image processing techniques. Even though linear algebra and probability theory are covered in separate parts of this course, it's valuable to see how integrals tie in with gradient methods and expectations in ML."),"\n",r.createElement(t.h2,{id:"multivariate-calculus",style:{position:"relative"}},r.createElement(t.a,{href:"#multivariate-calculus","aria-label":"multivariate calculus permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"multivariate calculus"),"\n",r.createElement(t.p,null,"In real-world machine learning scenarios, we almost always deal with functions of multiple variables — think of a neural network's parameters ",r.createElement(o.A,{text:"\\( \\theta = (\\theta_1, \\theta_2, \\dots, \\theta_n) \\)"}),". The shift from single-variable calculus to multivariate calculus is a leap into higher dimensions, but it follows familiar principles."),"\n",r.createElement(t.h3,{id:"partial-derivatives",style:{position:"relative"}},r.createElement(t.a,{href:"#partial-derivatives","aria-label":"partial derivatives permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"partial derivatives"),"\n",r.createElement(t.p,null,"A partial derivative with respect to ",r.createElement(o.A,{text:"\\( \\theta_j \\)"})," means differentiating the function while keeping all other variables constant. For a function ",r.createElement(o.A,{text:"\\( F(\\theta_1, \\theta_2, \\dots, \\theta_n) \\)"}),":"),"\n",r.createElement(o.A,{text:"\\[\n\\frac{\\partial F}{\\partial \\theta_j} = \\lim_{h \\to 0} \\frac{F(\\theta_1, \\dots, \\theta_j + h, \\dots, \\theta_n) - F(\\theta_1, \\dots, \\theta_j, \\dots, \\theta_n)}{h}.\n\\]"}),"\n",r.createElement(t.p,null,"Each partial derivative measures how sensitive the function is to changes in a specific direction. Summing these partial derivatives into a vector yields the gradient."),"\n",r.createElement(t.h3,{id:"gradient-vectors",style:{position:"relative"}},r.createElement(t.a,{href:"#gradient-vectors","aria-label":"gradient vectors permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"gradient vectors"),"\n",r.createElement(t.p,null,"The gradient ",r.createElement(o.A,{text:"\\( \\nabla F \\)"})," is the vector of partial derivatives:"),"\n",r.createElement(o.A,{text:"\\[\n\\nabla F(\\theta) = \n\\begin{pmatrix}\n\\frac{\\partial F}{\\partial \\theta_1} \\\\\n\\frac{\\partial F}{\\partial \\theta_2} \\\\\n\\vdots \\\\\n\\frac{\\partial F}{\\partial \\theta_n}\n\\end{pmatrix}.\n\\]"}),"\n",r.createElement(t.p,null,"Geometrically, the gradient points in the direction of steepest ascent of the function. In machine learning, especially in gradient-based optimization, we are interested in the negative gradient, which indicates the steepest descent direction."),"\n",r.createElement(a,{alt:"Illustration of gradient vector",path:"",caption:"Visualizing how the gradient points in the direction of steepest ascent",zoom:"false"}),"\n",r.createElement(t.h3,{id:"directional-derivatives",style:{position:"relative"}},r.createElement(t.a,{href:"#directional-derivatives","aria-label":"directional derivatives permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"directional derivatives"),"\n",r.createElement(t.p,null,"A directional derivative measures the instantaneous rate of change of ",r.createElement(o.A,{text:"\\( F \\)"})," in a specific direction. Formally, if ",r.createElement(o.A,{text:"\\( \\mathbf{v} \\)"})," is a unit vector in the direction we care about, then"),"\n",r.createElement(o.A,{text:"\\[\nD_{\\mathbf{v}} F(\\theta) = \\lim_{h \\to 0} \\frac{F(\\theta + h\\mathbf{v}) - F(\\theta)}{h}.\n\\]"}),"\n",r.createElement(t.p,null,"You can show that ",r.createElement(o.A,{text:"\\( D_{\\mathbf{v}} F(\\theta) = \\nabla F(\\theta) \\cdot \\mathbf{v} \\)"})," (the dot product). A key insight is that the gradient is the direction that maximizes the directional derivative — hence why gradient-based algorithms move in that direction to climb or descend the function as needed."),"\n",r.createElement(t.h3,{id:"chain-rule-in-multiple-dimensions",style:{position:"relative"}},r.createElement(t.a,{href:"#chain-rule-in-multiple-dimensions","aria-label":"chain rule in multiple dimensions permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"chain rule in multiple dimensions"),"\n",r.createElement(t.p,null,"Consider a multivariate function ",r.createElement(o.A,{text:"\\( F(g(x), h(x)) \\)"}),". You can generalize the chain rule:"),"\n",r.createElement(o.A,{text:"\\[\n\\frac{dF}{dx} = \n\\frac{\\partial F}{\\partial u} \\frac{du}{dx} + \n\\frac{\\partial F}{\\partial v} \\frac{dv}{dx},\n\\]"}),"\n",r.createElement(t.p,null,"where ",r.createElement(o.A,{text:"\\( u = g(x) \\)"})," and ",r.createElement(o.A,{text:"\\( v = h(x) \\)"}),". For a deep neural network, this extends to many layers, each with multiple inputs and outputs. We apply the chain rule systematically to compute all partial derivatives, step by step. This systematic approach is exactly what frameworks like PyTorch or TensorFlow do automatically when they perform backpropagation."),"\n",r.createElement(t.h3,{id:"jacobians-and-hessians",style:{position:"relative"}},r.createElement(t.a,{href:"#jacobians-and-hessians","aria-label":"jacobians and hessians permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"jacobians and hessians"),"\n",r.createElement(t.p,null,"The Jacobian is a matrix containing all first-order partial derivatives of a vector-valued function. If we have ",r.createElement(o.A,{text:"\\( \\mathbf{F}(\\theta) = (F_1(\\theta), \\dots, F_m(\\theta)) \\)"}),", then the Jacobian is an ",r.createElement(o.A,{text:"\\( m \\times n \\)"})," matrix:"),"\n",r.createElement(o.A,{text:"\\[\nJ(\\theta) =\n\\begin{pmatrix}\n\\frac{\\partial F_1}{\\partial \\theta_1} & \\cdots & \\frac{\\partial F_1}{\\partial \\theta_n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial F_m}{\\partial \\theta_1} & \\cdots & \\frac{\\partial F_m}{\\partial \\theta_n}\n\\end{pmatrix}.\n\\]"}),"\n",r.createElement(t.p,null,"When ",r.createElement(o.A,{text:"\\( m = 1 \\)"})," (i.e., a scalar output, such as a loss function), the Jacobian reduces to the row vector of partial derivatives — just another way to talk about the gradient."),"\n",r.createElement(t.p,null,"The Hessian is the second-order partial derivative matrix of a scalar function. If ",r.createElement(o.A,{text:"\\( F: \\mathbb{R}^n \\to \\mathbb{R} \\)"}),", its Hessian ",r.createElement(o.A,{text:"\\( H(\\theta) \\)"})," is an ",r.createElement(o.A,{text:"\\( n \\times n \\)"})," matrix:"),"\n",r.createElement(o.A,{text:"\\[\nH(\\theta) =\n\\begin{pmatrix}\n\\frac{\\partial^2 F}{\\partial \\theta_1^2} & \\cdots & \\frac{\\partial^2 F}{\\partial \\theta_1 \\partial \\theta_n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 F}{\\partial \\theta_n \\partial \\theta_1} & \\cdots & \\frac{\\partial^2 F}{\\partial \\theta_n^2}\n\\end{pmatrix}.\n\\]"}),"\n",r.createElement(t.p,null,"The Hessian helps us analyze curvature and is integral to second-order optimization methods (e.g., Newton's method). Although exact Hessians can be expensive to compute in high dimensions, approximations (like those in L-BFGS) see use in advanced ML scenarios."),"\n",r.createElement(t.h2,{id:"vector-and-matrix-calculus",style:{position:"relative"}},r.createElement(t.a,{href:"#vector-and-matrix-calculus","aria-label":"vector and matrix calculus permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"vector and matrix calculus"),"\n",r.createElement(t.p,null,"As machine learning models scale, vector and matrix notation becomes essential for representing large parameter spaces and data sets efficiently. Here are some highlights of vector and matrix calculus:"),"\n",r.createElement(t.h3,{id:"vector-fields-divergence-and-curl",style:{position:"relative"}},r.createElement(t.a,{href:"#vector-fields-divergence-and-curl","aria-label":"vector fields divergence and curl permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"vector fields, divergence, and curl"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Vector fields"),": A function ",r.createElement(o.A,{text:"\\( \\mathbf{F}(x, y, z) \\)"})," that outputs a vector for each point in space (e.g., the velocity field of fluid flow). In ML contexts, we may see vector fields for multi-output transformations or embeddings."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Divergence")," ",r.createElement(o.A,{text:"\\( \\nabla \\cdot \\mathbf{F} \\)"}),' measures how much a vector field expands or contracts at a point (like "outflow" in fluid mechanics).'),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Curl")," ",r.createElement(o.A,{text:"\\( \\nabla \\times \\mathbf{F} \\)"})," measures the twisting or rotational component of the field."),"\n"),"\n",r.createElement(t.p,null,"These operations come up in more specialized ML topics (e.g., certain PDE-based methods for images, or physically-inspired approaches to data transformations)."),"\n",r.createElement(t.h3,{id:"vectorization-of-operations",style:{position:"relative"}},r.createElement(t.a,{href:"#vectorization-of-operations","aria-label":"vectorization of operations permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"vectorization of operations"),"\n",r.createElement(t.p,null,"In practice, data scientists vectorize computations to leverage optimized linear algebra routines (e.g., BLAS, CUDA kernels). Rather than computing element-wise operations with loops, we express them as matrix multiplications, vector additions, and so forth. This speeds up training by orders of magnitude, especially for large-scale models. Frameworks like NumPy, PyTorch, and TensorFlow handle these vectorized operations under the hood."),"\n",r.createElement(t.h3,{id:"matrix-and-tensor-derivatives",style:{position:"relative"}},r.createElement(t.a,{href:"#matrix-and-tensor-derivatives","aria-label":"matrix and tensor derivatives permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"matrix and tensor derivatives"),"\n",r.createElement(t.p,null,"A typical example in deep learning is computing ",r.createElement(o.A,{text:"\\( \\frac{\\partial \\mathbf{y}}{\\partial W} \\)"})," where ",r.createElement(o.A,{text:"\\( \\mathbf{y} = W \\cdot \\mathbf{x} \\)"}),". Knowing matrix calculus shortcuts drastically reduces the complexity of deriving these expressions by hand. Common identities you might see:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(o.A,{text:"\\( \\frac{\\partial (W \\mathbf{x})}{\\partial W} = \\mathbf{x}^T \\)"})," (with appropriate dimension ordering)."),"\n",r.createElement(t.li,null,r.createElement(o.A,{text:"\\( \\frac{\\partial (\\mathbf{x}^T W \\mathbf{y})}{\\partial W} = \\mathbf{x} \\mathbf{y}^T \\)"}),"."),"\n"),"\n",r.createElement(t.p,null,"In more advanced networks, parameters are structured as tensors, and the same rules generalize. Automatic differentiation frameworks also rely heavily on well-optimized matrix calculus routines."),"\n",r.createElement(a,{alt:"Matrix calculus concept diagram",path:"",caption:"Representing gradients in matrix/tensor form allows for efficient computation",zoom:"false"}),"\n",r.createElement(t.h3,{id:"a-small-cheat-sheet-example",style:{position:"relative"}},r.createElement(t.a,{href:"#a-small-cheat-sheet-example","aria-label":"a small cheat sheet example permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"a small cheat-sheet example"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Derivative of a scalar w.r.t a vector"),": Yields a row vector (gradient)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Derivative of a vector w.r.t a scalar"),": Yields a vector."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Derivative of a vector w.r.t a vector"),": Yields a Jacobian matrix."),"\n"),"\n",r.createElement(t.p,null,"When dealing with neural networks, each parameter can be seen as an element in a large vector of weights ",r.createElement(o.A,{text:"\\( \\theta \\)"}),". The network outputs might be a scalar (e.g., a loss) or a vector (e.g., multi-class predictions), so you decide whether to form gradients or Jacobians accordingly."),"\n",r.createElement(t.h2,{id:"relation-of-calculus-to-probability-and-expectation",style:{position:"relative"}},r.createElement(t.a,{href:"#relation-of-calculus-to-probability-and-expectation","aria-label":"relation of calculus to probability and expectation permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"relation of calculus to probability and expectation"),"\n",r.createElement(t.p,null,"Calculus and probability interweave closely in machine learning, especially in areas like maximum likelihood estimation, Bayesian inference, and information theory."),"\n",r.createElement(t.h3,{id:"cross-entropy-likelihoods-and-derivatives",style:{position:"relative"}},r.createElement(t.a,{href:"#cross-entropy-likelihoods-and-derivatives","aria-label":"cross entropy likelihoods and derivatives permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"cross-entropy, likelihoods, and derivatives"),"\n",r.createElement(t.p,null,"Cross-entropy is a primary loss function in classification tasks:"),"\n",r.createElement(o.A,{text:"\\[\nH(p, q) = -\\sum_{x} p(x) \\log q(x).\n\\]"}),"\n",r.createElement(t.p,null,"When we differentiate cross-entropy with respect to the parameters of ",r.createElement(o.A,{text:"\\( q \\)"}),", we get updates that push ",r.createElement(o.A,{text:"\\( q \\)"})," to match ",r.createElement(o.A,{text:"\\( p \\)"}),". In practice, ",r.createElement(o.A,{text:"\\( q \\)"})," is a model's predicted probability distribution (often parameterized by a neural network). This approach ties directly into the notion of maximum likelihood. Suppose you have a likelihood function ",r.createElement(o.A,{text:"\\( L(\\theta) \\)"}),"; you often take the log-likelihood ",r.createElement(o.A,{text:"\\( \\ell(\\theta) = \\log L(\\theta) \\)"})," and find the maximum by solving:"),"\n",r.createElement(t.p,null,r.createElement(o.A,{text:"\\( \\nabla \\ell(\\theta) = 0 \\)"}),"."),"\n",r.createElement(t.h3,{id:"integrals-in-probability-densities-and-cdfs",style:{position:"relative"}},r.createElement(t.a,{href:"#integrals-in-probability-densities-and-cdfs","aria-label":"integrals in probability densities and cdfs permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"integrals in probability densities and cdfs"),"\n",r.createElement(t.p,null,"The probability density function (PDF) of a continuous variable must integrate to 1 over its domain. Likewise, the cumulative distribution function (CDF) is the integral of the PDF. Whenever you see expectations, moments, or marginal probabilities, you are performing integrals:"),"\n",r.createElement(o.A,{text:"\\[\n\\int p(x)\\, dx = 1,\n\\quad\nF_X(a) = \\int_{-\\infty}^a p(x)\\, dx.\n\\]"}),"\n",r.createElement(t.p,null,"Moments — like mean, variance, skewness — are specific integrals of the density with different powers of the variable."),"\n",r.createElement(t.h3,{id:"derivatives-of-likelihood-and-log-likelihood",style:{position:"relative"}},r.createElement(t.a,{href:"#derivatives-of-likelihood-and-log-likelihood","aria-label":"derivatives of likelihood and log likelihood permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"derivatives of likelihood and log-likelihood"),"\n",r.createElement(t.p,null,"For a parameterized model with density ",r.createElement(o.A,{text:"\\( p(x|\\theta) \\)"})," over data ",r.createElement(o.A,{text:"\\( x \\)"}),":"),"\n",r.createElement(o.A,{text:"\\[\nL(\\theta) = \\prod_{i=1}^N p(x_i | \\theta),\n\\quad\n\\ell(\\theta) = \\sum_{i=1}^N \\log p(x_i | \\theta).\n\\]"}),"\n",r.createElement(t.p,null,"To find the maximum likelihood estimate, you take the derivative of ",r.createElement(o.A,{text:"\\( \\ell(\\theta) \\)"})," w.r.t. ",r.createElement(o.A,{text:"\\(\\theta\\)"})," and set it to zero. In practice, we might do this via gradient-based iterative methods rather than solving analytically (especially for complex models)."),"\n",r.createElement(t.h3,{id:"expectation-moments-and-gradients",style:{position:"relative"}},r.createElement(t.a,{href:"#expectation-moments-and-gradients","aria-label":"expectation moments and gradients permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"expectation, moments, and gradients"),"\n",r.createElement(t.p,null,"The expectation of a function ",r.createElement(o.A,{text:"\\( g(x) \\)"})," under distribution ",r.createElement(o.A,{text:"\\( p \\)"})," is:"),"\n",r.createElement(o.A,{text:"\\[\nE_p[g(x)] = \\int g(x) p(x)\\, dx.\n\\]"}),"\n",r.createElement(t.p,null,"Sometimes we differentiate these integrals w.r.t. parameters inside ",r.createElement(o.A,{text:"\\( g \\)"})," or ",r.createElement(o.A,{text:"\\( p \\)"}),'. This is key in methods like the "score function" estimator (used in policy gradients or reinforcement learning) and reparameterization tricks (used in variational autoencoders).'),"\n",r.createElement(t.h3,{id:"bayesian-inference-and-variational-methods",style:{position:"relative"}},r.createElement(t.a,{href:"#bayesian-inference-and-variational-methods","aria-label":"bayesian inference and variational methods permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"bayesian inference and variational methods"),"\n",r.createElement(t.p,null,"Bayesian inference often boils down to computing a posterior distribution:"),"\n",r.createElement(o.A,{text:"\\[\np(\\theta | x) \\propto p(x | \\theta)\\, p(\\theta).\n\\]"}),"\n",r.createElement(t.p,null,"Because these posteriors can be intractable, we resort to variational approaches that approximate the posterior with a more tractable distribution ",r.createElement(o.A,{text:"\\( q(\\theta) \\)"}),". We define an objective called the Evidence Lower BOund (ELBO) and optimize it via gradient methods. The derivatives of the ELBO w.r.t. the variational parameters are computed — again employing calculus:"),"\n",r.createElement(o.A,{text:"\\[\n\\mathrm{ELBO}(q) = E_{q(\\theta)}[\\log p(x, \\theta)] - E_{q(\\theta)}[\\log q(\\theta)].\n\\]"}),"\n",r.createElement(t.p,null,"We then take ",r.createElement(o.A,{text:"\\( \\nabla \\mathrm{ELBO}(q) \\)"})," to update ",r.createElement(o.A,{text:"\\( q(\\theta) \\)"}),". In advanced methods like Hamiltonian Monte Carlo (Neal, 2011), we also use gradient information of the posterior to draw efficient samples."),"\n",r.createElement(t.h2,{id:"automatic-differentiation-and-implementation",style:{position:"relative"}},r.createElement(t.a,{href:"#automatic-differentiation-and-implementation","aria-label":"automatic differentiation and implementation permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"automatic differentiation and implementation"),"\n",r.createElement(t.p,null,"For large-scale models, computing all the necessary partial derivatives by hand is impractical. Automatic differentiation (AD) frameworks let us specify a model in code, then obtain accurate derivatives automatically."),"\n",r.createElement(t.h3,{id:"forward-mode-vs-reverse-mode-differentiation",style:{position:"relative"}},r.createElement(t.a,{href:"#forward-mode-vs-reverse-mode-differentiation","aria-label":"forward mode vs reverse mode differentiation permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"forward-mode vs. reverse-mode differentiation"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Forward-mode AD"),": Propagates derivatives from inputs forward to outputs. Good for functions with many inputs and few outputs."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Reverse-mode AD"),": Propagates from outputs backward to inputs. Perfect when you have fewer outputs (e.g., a single scalar loss) but many inputs (model parameters). Hence, reverse-mode AD is standard in machine learning libraries."),"\n"),"\n",r.createElement(t.h3,{id:"symbolic-vs-numeric-differentiation",style:{position:"relative"}},r.createElement(t.a,{href:"#symbolic-vs-numeric-differentiation","aria-label":"symbolic vs numeric differentiation permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"symbolic vs. numeric differentiation"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Symbolic differentiation"),": Tools like Sympy parse an expression symbolically and differentiate it exactly. This is useful for small, closed-form expressions, but can be slow or unwieldy for large, dynamic computational graphs."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Numeric differentiation"),": Uses finite differences to approximate the derivative. It's straightforward but can suffer from numerical instability and requires multiple function evaluations."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Automatic differentiation (AD)"),": Executes the program and compiles a graph of operations, applying the chain rule meticulously. This approach yields high accuracy with moderate overhead and is the backbone of modern ML frameworks."),"\n"),"\n",r.createElement(t.h3,{id:"code-example-computing-derivatives-in-python",style:{position:"relative"}},r.createElement(t.a,{href:"#code-example-computing-derivatives-in-python","aria-label":"code example computing derivatives in python permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"code example: computing derivatives in python"),"\n",r.createElement(t.p,null,"Below is a short example using Python's ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">sympy</code>'}})," for symbolic differentiation. Although symbolic differentiation is not how deep learning frameworks typically run, it demonstrates the principle:"),"\n",r.createElement(l.A,{text:'\nimport sympy as sp\n\n# Define a symbolic variable\nx = sp.Symbol(\'x\', real=True)\n\n# Define a function\nf = x**3 + 2*x - 5\n\n# Compute its derivative\ndf = sp.diff(f, x)\n\nprint("f(x) =", f)\nprint("df/dx =", df)\n'}),"\n",r.createElement(t.p,null,"You would see ",r.createElement(o.A,{text:"\\( 3x^2 + 2 \\)"})," as the output for the derivative. In a framework like PyTorch, you'd define a computational graph for a function and call ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">.backward()</code>'}})," on a final scalar, which uses reverse-mode AD to compute all needed gradients."),"\n",r.createElement(t.h3,{id:"implementation-in-ml-frameworks",style:{position:"relative"}},r.createElement(t.a,{href:"#implementation-in-ml-frameworks","aria-label":"implementation in ml frameworks permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"implementation in ml frameworks"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"PyTorch's autograd"),": Wraps tensors in a structure that records operations. When you call ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">.backward()</code>'}}),", it traverses that recorded graph in reverse to compute gradients."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"TensorFlow's computational graph"),": Builds a static or dynamic graph of ops, then uses reverse-mode AD."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"JAX's functional approach"),": Stages transformations as pure functions and relies on composable derivatives like ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">grad</code>'}}),", ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">vmap</code>'}}),", or ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">jit</code>'}}),"."),"\n"),"\n",r.createElement(t.p,null,"All these frameworks implement graph optimizations: they prune subgraphs not needed for computing the final gradient, merge common operations, and more, improving speed and memory usage."),"\n",r.createElement(t.h2,{id:"further-topics-specialized-applications-optional-stuff",style:{position:"relative"}},r.createElement(t.a,{href:"#further-topics-specialized-applications-optional-stuff","aria-label":"further topics specialized applications optional stuff permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"further topics, specialized applications, optional stuff"),"\n",r.createElement(t.p,null,"We end with a brief look at advanced ideas that connect calculus to specialized domains in machine learning."),"\n",r.createElement(t.h3,{id:"differential-geometry-in-ml",style:{position:"relative"}},r.createElement(t.a,{href:"#differential-geometry-in-ml","aria-label":"differential geometry in ml permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"differential geometry in ml"),"\n",r.createElement(t.p,null,'In many advanced methods, parameters live on manifolds — curved spaces that require Riemannian geometry to handle derivatives properly (Amari, 1998). For example, the natural gradient method uses the Fisher Information Matrix to perform gradient descent in a more "geometry-aware" way. This approach can lead to faster convergence for certain models, notably in Bayesian parameter estimation and large-scale logistic regression.'),"\n",r.createElement(t.h3,{id:"partial-differential-equations-pdes-in-ml",style:{position:"relative"}},r.createElement(t.a,{href:"#partial-differential-equations-pdes-in-ml","aria-label":"partial differential equations pdes in ml permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"partial differential equations (pdes) in ml"),"\n",r.createElement(t.p,null,"Partial differential equations appear in image denoising or in physically-based models. For instance, the Rudin-Osher-Fatemi model (Rudin and gang, 1992) uses PDEs for total variation-based image denoising. In ML, PDE-based approaches can incorporate domain knowledge or physically consistent constraints. They also show up in advanced generative modeling or neural PDE solvers for simulating complex phenomena."),"\n",r.createElement(t.h3,{id:"automatic-mixed-precision-and-gradient-scaling",style:{position:"relative"}},r.createElement(t.a,{href:"#automatic-mixed-precision-and-gradient-scaling","aria-label":"automatic mixed precision and gradient scaling permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"automatic mixed precision and gradient scaling"),"\n",r.createElement(t.p,null,"With the explosion of GPU-based training, we often train with half-precision (float16) or bfloat16 to speed up computations. This sometimes introduces numerical issues (e.g., gradients becoming too small to represent). Techniques like gradient scaling or mixed-precision training compensate by scaling loss and gradient values to remain within valid ranges, then scaling them back at the appropriate stage."),"\n",r.createElement(t.h3,{id:"stochastic-calculus-and-continuous-time-models",style:{position:"relative"}},r.createElement(t.a,{href:"#stochastic-calculus-and-continuous-time-models","aria-label":"stochastic calculus and continuous time models permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"stochastic calculus and continuous-time models"),"\n",r.createElement(t.p,null,"Some areas of ML, particularly those that involve finance or advanced time-series modeling, bring in stochastic calculus. Here, random noise is introduced in continuous-time processes:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Brownian motion"),": A continuous-time stochastic process often used for modeling."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Itô integrals")," and ",r.createElement(t.strong,null,"stochastic differential equations (SDEs)"),": Extend the idea of ordinary derivatives to handle random, time-dependent terms."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Euler-Maruyama, Milstein methods"),": Numerical approximations for SDEs."),"\n"),"\n",r.createElement(t.p,null,"These methods are cutting-edge in certain reinforcement learning scenarios, or in continuous-time models that unify PDEs and dynamics under uncertainty."),"\n",r.createElement(t.hr),"\n",r.createElement(t.p,null,'Whether you are performing simple linear regression or diving into advanced neural network optimization, the foundations of calculus illuminate the underlying mechanics of how model parameters are updated. This grounding in single-variable and multivariate derivatives, integrals, and matrix calculus paves the way for deeper exploration of optimization algorithms, Bayesian inference, and beyond. As you progress through this course, keep in mind how calculus permeates nearly every algorithmic strategy in modern data science and machine learning — knowing the "why" behind the math can guide you in building more robust and insightful models.'))}var c=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,i.RP)(),e.components);return t?r.createElement(t,e,r.createElement(s,e)):s(e)};var m=a(36710),h=a(58481),d=a.n(h),u=a(36310),p=a(87245),f=a(27042),v=a(59849),g=a(5591),E=a(61122),y=a(9219),x=a(33203),b=a(95751),w=a(94328),S=a(80791),H=a(78137);const k=e=>{let{toc:t}=e;if(!t||!t.items)return null;return r.createElement("nav",{className:S.R},r.createElement("ul",null,t.items.map(((e,t)=>r.createElement("li",{key:t},r.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const a=t.replace("#",""),n=document.getElementById(a);n&&n.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&r.createElement(k,{toc:{items:e.items}}))))))};function _(e){let{data:{mdx:t,allMdx:l,allPostImages:o},children:s}=e;const{frontmatter:c,body:m,tableOfContents:h}=t,v=c.index,S=c.slug.split("/")[1],_=l.nodes.filter((e=>e.frontmatter.slug.includes(`/${S}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),A=_.findIndex((e=>e.frontmatter.index===v)),z=_[A+1],T=_[A-1],M=c.slug.replace(/\/$/,""),C=/[^/]*$/.exec(M)[0],I=`posts/${S}/content/${C}/`,{0:V,1:L}=(0,r.useState)(c.flagWideLayoutByDefault),{0:F,1:B}=(0,r.useState)(!1);var N;(0,r.useEffect)((()=>{B(!0);const e=setTimeout((()=>B(!1)),340);return()=>clearTimeout(e)}),[V]),"adventures"===S?N=y.cb:"research"===S?N=y.Qh:"thoughts"===S&&(N=y.T6);const P=d()(m).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,q=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),a=e%60;return a<=30?`~${t}${a>0?".5":""} h`:`~${t+1} h`}(Math.ceil(P/N)+(c.extraReadTimeMin||0)),D=[{flag:c.flagDraft,component:()=>Promise.all([a.e(3231),a.e(8809)]).then(a.bind(a,28809))},{flag:c.flagMindfuckery,component:()=>Promise.all([a.e(3231),a.e(2471)]).then(a.bind(a,67709))},{flag:c.flagRewrite,component:()=>Promise.all([a.e(3231),a.e(6764)]).then(a.bind(a,62002))},{flag:c.flagOffensive,component:()=>Promise.all([a.e(3231),a.e(2443)]).then(a.bind(a,17681))},{flag:c.flagProfane,component:()=>Promise.all([a.e(3231),a.e(8048)]).then(a.bind(a,53286))},{flag:c.flagMultilingual,component:()=>Promise.all([a.e(3231),a.e(4069)]).then(a.bind(a,78831))},{flag:c.flagUnreliably,component:()=>Promise.all([a.e(3231),a.e(3417)]).then(a.bind(a,8179))},{flag:c.flagPolitical,component:()=>Promise.all([a.e(3231),a.e(5195)]).then(a.bind(a,30433))},{flag:c.flagCognitohazard,component:()=>Promise.all([a.e(3231),a.e(3175)]).then(a.bind(a,8413))},{flag:c.flagHidden,component:()=>Promise.all([a.e(3231),a.e(9556)]).then(a.bind(a,14794))}],{0:W,1:O}=(0,r.useState)([]);return(0,r.useEffect)((()=>{D.forEach((e=>{let{flag:t,component:a}=e;t&&a().then((e=>{O((t=>[].concat((0,n.A)(t),[e.default])))}))}))}),[]),r.createElement(f.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},r.createElement(g.A,{postNumber:c.index,date:c.date,updated:c.updated,readTime:q,difficulty:c.difficultyLevel,title:c.title,desc:c.desc,banner:c.banner,section:S,postKey:C,isMindfuckery:c.flagMindfuckery,mainTag:c.mainTag}),r.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},c.otherTags.map(((e,t)=>r.createElement("span",{key:t,className:`noselect ${H.MW}`,style:{margin:"0 5px 5px 0"}},e)))),r.createElement("div",{className:"postBody"},r.createElement(k,{toc:h})),r.createElement("br"),r.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},r.createElement(f.P.button,{className:`noselect ${w.pb}`,id:w.xG,onClick:()=>{L(!V)},whileTap:{scale:.93}},r.createElement(f.P.div,{className:b.DJ,key:V,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},V?"Switch to default layout":"Switch to wide layout"))),r.createElement("br"),r.createElement("div",{className:"postBody",style:{margin:V?"0 -14%":"",maxWidth:V?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},r.createElement("div",{className:`${w.P_} ${F?w.Xn:w.qG}`},W.map(((e,t)=>r.createElement(e,{key:t}))),c.indexCourse?r.createElement(x.A,{index:c.indexCourse,category:c.courseCategoryName}):"",r.createElement(u.Z.Provider,{value:{images:o.nodes,basePath:I.replace(/\/$/,"")+"/"}},r.createElement(i.xA,{components:{Image:p.A}},s)))),r.createElement(E.A,{nextPost:z,lastPost:T,keyCurrent:C,section:S}))}function A(e){return r.createElement(_,e,r.createElement(c,e))}function z(e){var t,a,n,i,l;let{data:o}=e;const{frontmatter:s}=o.mdx,c=s.titleSEO||s.title,h=s.titleOG||c,d=s.titleTwitter||c,u=s.descSEO||s.desc,p=s.descOG||u,f=s.descTwitter||u,g=s.schemaType||"BlogPosting",E=s.keywordsSEO,y=s.date,x=s.updated||y,b=s.imageOG||(null===(t=s.banner)||void 0===t||null===(a=t.childImageSharp)||void 0===a||null===(n=a.gatsbyImageData)||void 0===n||null===(i=n.images)||void 0===i||null===(l=i.fallback)||void 0===l?void 0:l.src),w=s.imageAltOG||p,S=s.imageTwitter||b,H=s.imageAltTwitter||f,k=s.canonicalURL,_=s.flagHidden||!1,A=s.mainTag||"Posts",z=s.slug.split("/")[1]||"posts",{siteUrl:T}=(0,m.Q)(),M={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:T},{"@type":"ListItem",position:2,name:A,item:`${T}/${s.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${T}${s.slug}`}]};return r.createElement(v.A,{title:c+" - avrtt.blog",titleOG:h,titleTwitter:d,description:u,descriptionOG:p,descriptionTwitter:f,schemaType:g,keywords:E,datePublished:y,dateModified:x,imageOG:b,imageAltOG:w,imageTwitter:S,imageAltTwitter:H,canonicalUrl:k,flagHidden:_,mainTag:A,section:z,type:"article"},r.createElement("script",{type:"application/ld+json"},JSON.stringify(M)))}},96098:function(e,t,a){var n=a(96540),i=a(7978);t.A=e=>{let{text:t}=e;return n.createElement(i.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-js-content-file-path-src-pages-posts-research-calculus-for-ml-mdx-528999e0b59b51637bad.js.map