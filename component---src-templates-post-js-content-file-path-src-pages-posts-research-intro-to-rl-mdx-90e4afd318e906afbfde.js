"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[5899],{72789:function(e,t,n){n.r(t),n.d(t,{Head:function(){return z},PostTemplate:function(){return A},default:function(){return _}});var a=n(54506),r=n(28453),i=n(96540),l=n(16886),o=(n(46295),n(96098));function s(e){const t=Object.assign({p:"p",h3:"h3",a:"a",span:"span",ul:"ul",li:"li",strong:"strong",h2:"h2",ol:"ol",hr:"hr",em:"em"},(0,r.RP)(),e.components),{Image:n}=t;return n||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),i.createElement(i.Fragment,null,"\n",i.createElement("br"),"\n","\n","\n",i.createElement(t.p,null,'Reinforcement learning, often referred to simply as RL, is a fascinating sub-field of machine learning that focuses on how an agent can learn to make optimal decisions through trial-and-error interactions with an environment. Unlike in supervised learning, where labeled training data is available, or in unsupervised learning, where one must discover structure in unlabeled data, reinforcement learning presents an agent with a dynamic environment and a scalar reward signal indicating how "good" or "bad" certain actions are in the long run. In other words, the agent seeks to maximize some measure of cumulative reward — often discounted over time or across steps — by adjusting its decision-making strategy, typically called a ',i.createElement(l.A,null,"policy"),"."),"\n",i.createElement(t.p,null,"Because RL is so closely associated with a notion of making sequences of decisions, it has strong relationships with control theory, operations research, and other fields that consider sequential decision-making under uncertainty. In data science and AI, it plays a critical role in developing systems that learn how to act, rather than simply how to classify or cluster. Over the last several decades, especially with the wave of deep learning success, RL has attracted ever-increasing attention and has proven effective in several impressive feats: from learning to play Atari games at a superhuman level to beating world champions in complex board games such as Go."),"\n",i.createElement(t.p,null,"In this chapter, I will start by describing the scope of reinforcement learning within the broader machine learning and data science fields, giving you an idea of where RL typically shines (and some of its limitations). I will then provide a historical perspective — from early insights by researchers such as Richard Bellman, to the formalization of many important concepts by Sutton and Barto, to the modern developments such as Deep Q-Networks (DQN) at DeepMind. Next, you will see how RL differs from supervised and unsupervised paradigms, and I will touch on how RL intersects with control theory and operations research. Finally, I will wrap up this introductory section with a brief set of notable milestones: from TD-Gammon to AlphaGo, AlphaZero, and other marvels."),"\n",i.createElement(t.h3,{id:"scope-of-reinforcement-learning-in-machine-learning-and-data-science",style:{position:"relative"}},i.createElement(t.a,{href:"#scope-of-reinforcement-learning-in-machine-learning-and-data-science","aria-label":"scope of reinforcement learning in machine learning and data science permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"scope of reinforcement learning in machine learning and data science"),"\n",i.createElement(t.p,null,"Reinforcement learning occupies a unique position in the universe of machine learning methods. While data scientists often focus on supervised and unsupervised tasks, RL focuses on learning a policy through interactions with an environment that provide time-delayed feedback (rewards). This characteristic is particularly useful in scenarios such as the following:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Robotics"),": A robot that must navigate and manipulate objects in an environment. The reward might be related to reaching a target location or successfully grasping an object."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Game AI"),": From board games like chess or Go, to real-time strategy games such as StarCraft or even interactive environments like Atari, an RL-based agent learns to make step-by-step decisions to win or achieve a high score."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Recommendation systems"),": While commonly approached as supervised or sequential supervised tasks, RL can also be used to learn an adaptive policy that adjusts its recommendations over time based on user interactions (rewards)."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Healthcare"),": Learning sequential treatment policies. The environment is often complex and partially observable, so RL approaches can optimize patient outcomes across time."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Finance"),": Trading strategies, portfolio management, or algorithmic decision-making, in which the agent tries to maximize returns under uncertainty."),"\n"),"\n",i.createElement(t.p,null,"Despite these successes, RL typically requires careful engineering to ensure stable training, adequate exploration, and a sensible reward design. In modern data science workflows, RL might be used less commonly than supervised or unsupervised approaches, but it nonetheless remains critical for tasks involving sequential decision-making with delayed reward signals."),"\n",i.createElement(t.h3,{id:"historical-perspective-and-evolution",style:{position:"relative"}},i.createElement(t.a,{href:"#historical-perspective-and-evolution","aria-label":"historical perspective and evolution permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"historical perspective and evolution"),"\n",i.createElement(t.p,null,"The roots of reinforcement learning can be traced back to behaviorist psychology, specifically concepts of trial-and-error learning. Over time, these ideas were gradually formalized in mathematics, computer science, and operations research. Some major historical markers:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Bellman's early work (1950s)"),": Richard Bellman introduced the principle of optimality and the idea that many decision-making problems can be broken down via dynamic programming (DP). This gave rise to the Bellman equation, which lies at the core of many RL algorithms."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Control theory and operations research"),": These fields provided significant foundational work, often focusing on Markov Decision Processes (MDPs) to model sequences of decisions under uncertainty."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Sutton and Barto (1980s onward)"),': Rich Sutton and Andy Barto are often credited with formalizing modern RL. Their text, "Reinforcement Learning: An Introduction," remains one of the most influential references in the field. They introduced concepts such as temporal difference (TD) learning and various practical algorithms that shaped the RL landscape.'),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"TD-Gammon (1992)"),": Gerald Tesauro's famous backgammon-playing program was an early demonstration of the power of RL combined with neural networks. TD-Gammon learned to play (and eventually surpass top human players) using temporal difference learning."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"DQN and the Deep Learning Era (2013–2015)"),': A major breakthrough came from DeepMind (Mnih and gang) with the Deep Q-Network algorithm. By combining Q-learning with convolutional neural networks and key innovations such as experience replay and target networks, agents learned to play dozens of Atari games at or above human level from just raw pixel input. This success catalyzed a surge of interest in "deep RL."'),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"AlphaGo (2016)"),": Another iconic milestone from DeepMind combined RL with Monte Carlo Tree Search and deep neural networks to defeat a top-level human professional in the game of Go, a feat previously deemed too difficult for computers in the near future."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"AlphaZero and beyond"),": Building on these ideas, AlphaZero demonstrated that a single algorithmic framework could achieve superhuman performance in chess, shogi, and Go by learning from self-play."),"\n"),"\n",i.createElement(t.h3,{id:"comparison-with-supervised-and-unsupervised-learning",style:{position:"relative"}},i.createElement(t.a,{href:"#comparison-with-supervised-and-unsupervised-learning","aria-label":"comparison with supervised and unsupervised learning permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"comparison with supervised and unsupervised learning"),"\n",i.createElement(t.p,null,"At first glance, it might seem that reinforcement learning sits somewhere between supervised and unsupervised learning. It is true that RL borrows ideas from both, but in practice it is often treated as an entirely separate paradigm:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Supervised learning"),": One is given a labeled dataset ",i.createElement(o.A,{text:"\\(D = \\{(x_i, y_i)\\}\\)"})," and the goal is to find a function ",i.createElement(o.A,{text:"\\(f(x)\\)"})," that maps inputs to outputs accurately. There is no concept of an environment or rewards, and there is a static dataset of examples."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Unsupervised learning"),": One is given unlabeled data ",i.createElement(o.A,{text:"\\(X\\)"})," and aims to discover hidden structure, e.g., clusters or latent factors. There is no concept of action, environment, or reward in the standard sense."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Reinforcement learning"),": One is not given labels for correct decisions. Instead, there is an agent interacting with a possibly evolving environment. When the agent takes an action, it receives a scalar reward (which can be positive or negative). The agent's objective is to maximize the cumulative reward, and thus it must learn not only to predict future rewards but also to select actions that yield high returns over time."),"\n"),"\n",i.createElement(t.p,null,"This difference leads to challenges unique to RL, such as the exploration vs. exploitation trade-off, delayed reward signals, non-stationary data distributions if the agent's policy changes the dynamics of the environment, and so on."),"\n",i.createElement(t.h3,{id:"relationship-to-control-theory-and-operations-research",style:{position:"relative"}},i.createElement(t.a,{href:"#relationship-to-control-theory-and-operations-research","aria-label":"relationship to control theory and operations research permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"relationship to control theory and operations research"),"\n",i.createElement(t.p,null,"Reinforcement learning has deep ties to control theory, which traditionally deals with designing controllers that guide a dynamical system's behavior toward some objective (e.g., setpoint control, trajectory optimization, etc.). In classical control, one typically assumes a known or partially known system model and uses methods like linear–quadratic regulators (LQRs) or robust control approaches. RL extends these ideas to situations in which the environment model is unknown or extremely complex, and we must learn near-optimal control strategies via data-driven approaches."),"\n",i.createElement(t.p,null,"Similarly, in operations research, Markov Decision Processes have been a mainstay for decades, used to solve resource allocation, scheduling, and queue management problems. RL can be seen as a method to solve MDPs or POMDPs (partially observable Markov Decision Processes) empirically, by sampling from the environment."),"\n",i.createElement(t.h3,{id:"notable-milestones-in-rl-research",style:{position:"relative"}},i.createElement(t.a,{href:"#notable-milestones-in-rl-research","aria-label":"notable milestones in rl research permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"notable milestones in rl research"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"TD-Gammon (Tesauro)"),": Demonstrated that TD-based methods combined with function approximators (neural networks) could learn world-class backgammon strategies."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Atari breakthroughs with DQN (2013–2015)"),": By leveraging deep neural networks, experience replay, and target networks, Q-learning scaled to complex environments with high-dimensional state spaces."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"AlphaGo and successors"),": Go had long been considered a pinnacle of human skill. AlphaGo's triumph in 2016 signified a coming-of-age for RL combined with sophisticated search methods. Later improvements from AlphaZero and MuZero broadened the approach and removed the need for domain-specific knowledge."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"AlphaStar and OpenAI Five"),": RL applied to StarCraft II and Dota 2, respectively, showing that RL can handle extremely high-dimensional, partially observable environments in real-time strategy games with multiple agents."),"\n"),"\n",i.createElement(t.p,null,"All of these achievements arose from the same fundamental RL concepts described in the chapters to come: states, actions, rewards, and the quest to maximize long-term returns through iterative learning algorithms."),"\n",i.createElement(t.h2,{id:"key-concepts",style:{position:"relative"}},i.createElement(t.a,{href:"#key-concepts","aria-label":"key concepts permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"key concepts"),"\n",i.createElement(t.p,null,"The basic ideas of reinforcement learning revolve around an ",i.createElement(l.A,null,"agent"),", an ",i.createElement(l.A,null,"environment"),", a set of possible ",i.createElement(l.A,null,"actions"),", and a reward signal. To properly define these ideas, researchers often use the Markov Decision Process (MDP) formalism, which provides a rigorous mathematical foundation for analyzing how an agent should act in uncertain and possibly stochastic domains."),"\n",i.createElement(t.h3,{id:"agent-environment-and-state-definitions",style:{position:"relative"}},i.createElement(t.a,{href:"#agent-environment-and-state-definitions","aria-label":"agent environment and state definitions permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"agent, environment, and state definitions"),"\n",i.createElement(t.p,null,"The agent is the decision-maker. It observes or receives a ",i.createElement(l.A,null,"state")," ",i.createElement(o.A,{text:"\\(s\\)"})," from the environment at each step, selects an action ",i.createElement(o.A,{text:"\\(a\\)"}),", then receives a reward ",i.createElement(o.A,{text:"\\(r\\)"})," and observes a new state ",i.createElement(o.A,{text:"\\(s'\\)"}),". The environment, in turn, is everything outside the agent's decision boundary."),"\n",i.createElement(t.p,null,"Conceptually, you can think of the environment as a system or a world with which the agent interacts. For instance, in a robotics scenario, the environment includes the robot's surroundings, the physics of motion, sensors, etc."),"\n",i.createElement(t.h3,{id:"markov-decision-processes-mdps-and-partially-observable-mdps-pomdps",style:{position:"relative"}},i.createElement(t.a,{href:"#markov-decision-processes-mdps-and-partially-observable-mdps-pomdps","aria-label":"markov decision processes mdps and partially observable mdps pomdps permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"markov decision processes (mdps) and partially observable mdps (pomdps)"),"\n",i.createElement(t.p,null,"An MDP is defined by a tuple ",i.createElement(o.A,{text:"\\( (S, A, P, R, \\gamma) \\)"})," where:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(o.A,{text:"\\(S\\)"}),": The set of states that the agent or environment can be in."),"\n",i.createElement(t.li,null,i.createElement(o.A,{text:"\\(A\\)"}),": The set of actions available to the agent."),"\n",i.createElement(t.li,null,i.createElement(o.A,{text:"\\(P(s' \\mid s, a)\\)"}),": A transition probability function specifying the probability of moving from state ",i.createElement(o.A,{text:"\\(s\\)"})," to ",i.createElement(o.A,{text:"\\(s'\\)"})," after taking action ",i.createElement(o.A,{text:"\\(a\\)"}),"."),"\n",i.createElement(t.li,null,i.createElement(o.A,{text:"\\(R(s, a)\\)"}),": A reward function that provides the expected reward when taking action ",i.createElement(o.A,{text:"\\(a\\)"})," in state ",i.createElement(o.A,{text:"\\(s\\)"}),". In some formulations, ",i.createElement(o.A,{text:"\\(R\\)"})," can also depend on ",i.createElement(o.A,{text:"\\(s'\\)"}),"."),"\n",i.createElement(t.li,null,i.createElement(o.A,{text:"\\(\\gamma \\in [0,1]\\)"}),": A discount factor, which determines how future rewards are weighted compared to immediate rewards."),"\n"),"\n",i.createElement(t.p,null,"The Markov property requires that the environment's response at time ",i.createElement(o.A,{text:"\\(t+1\\)"})," depends only on the state and action at time ",i.createElement(o.A,{text:"\\(t\\)"}),", and not on any earlier states or actions. In practice, many real-world tasks are not fully Markovian when described by a minimal set of observable variables, leading to the notion of partially observable MDPs (POMDPs)."),"\n",i.createElement(t.p,null,"A POMDP extends the MDP framework by including ",i.createElement(o.A,{text:"\\(O(s)\\)"})," (observations) and ",i.createElement(o.A,{text:"\\(Z(o \\mid s)\\)"})," (an observation probability distribution) to handle partial observability. In these scenarios, the agent doesn't necessarily know the true underlying state ",i.createElement(o.A,{text:"\\(s\\)"})," but only sees some noisy observation ",i.createElement(o.A,{text:"\\(o\\)"}),"."),"\n",i.createElement(t.h3,{id:"actions-and-action-spaces",style:{position:"relative"}},i.createElement(t.a,{href:"#actions-and-action-spaces","aria-label":"actions and action spaces permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"actions and action spaces"),"\n",i.createElement(t.p,null,"Actions represent the decisions or moves the agent can make. In a discrete environment, the action space might be something like {Up, Down, Left, Right}, or a set of possible moves in a board game. In a continuous setting (e.g., a robotic arm or self-driving car), the action space might be real-valued vectors specifying forces or torques. There are also hybrid actions that combine discrete and continuous components."),"\n",i.createElement(t.p,null,"Handling large or continuous action spaces is one of the primary challenges in RL, as naive enumeration is impossible when there are infinitely many actions at each decision step. This leads to specialized algorithms, such as policy-gradient-based methods, deterministic policy gradients, and actor-critic schemes, which you will see in later sections."),"\n",i.createElement(t.h3,{id:"rewards-returns-and-episodes",style:{position:"relative"}},i.createElement(t.a,{href:"#rewards-returns-and-episodes","aria-label":"rewards returns and episodes permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"rewards, returns, and episodes"),"\n",i.createElement(t.p,null,"In reinforcement learning, the ",i.createElement(l.A,null,"reward")," is the key training signal. A positive reward encourages the agent to seek similar actions or states in the future, while a negative reward (or cost) deters certain behaviors. Many tasks are described in an ",i.createElement(l.A,null,"episodic")," manner, meaning that interactions happen in episodes (e.g., a single play of a game), each of which starts in some initial state and ends in a terminal state or after a certain number of steps."),"\n",i.createElement(t.p,null,"The agent's goal is to maximize the expected ",i.createElement(l.A,null,"return"),", defined commonly as the discounted cumulative reward:"),"\n",i.createElement(o.A,{text:"\\[\nG_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n\\]"}),"\n",i.createElement(t.p,null,"In this formula:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(o.A,{text:"\\(R_{t+k+1}\\)"})," is the reward at time step ",i.createElement(o.A,{text:"\\(t+k+1\\)"}),"."),"\n",i.createElement(t.li,null,i.createElement(o.A,{text:"\\(\\gamma\\)"})," is the discount factor that trades off the importance of immediate rewards versus future rewards (0 \\≤ ",i.createElement(o.A,{text:"\\(\\gamma\\)"})," \\≤ 1)."),"\n",i.createElement(t.li,null,i.createElement(o.A,{text:"\\(G_t\\)"})," is the return starting from time ",i.createElement(o.A,{text:"\\(t\\)"}),"."),"\n"),"\n",i.createElement(t.p,null,"If the task is ",i.createElement(l.A,null,"continuing")," and does not naturally break down into episodes, the agent still accumulates a discounted return over the long run, or might use other definitions like average reward."),"\n",i.createElement(t.h3,{id:"policy-and-value-functions",style:{position:"relative"}},i.createElement(t.a,{href:"#policy-and-value-functions","aria-label":"policy and value functions permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"policy and value functions"),"\n",i.createElement(t.p,null,"A ",i.createElement(l.A,null,"policy")," ",i.createElement(o.A,{text:"\\(\\pi(a \\mid s)\\)"})," is a mapping from states to probabilities of selecting each action. It essentially defines the agent's behavior. The concept of ",i.createElement(l.A,null,"value functions"),' is central in RL, capturing how "good" it is to be in a certain state or to perform a certain action in that state.'),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,"The ",i.createElement(l.A,null,"state-value function")," under policy ",i.createElement(o.A,{text:"\\(\\pi\\)"})," is:"),"\n",i.createElement(o.A,{text:"\\[\nV^\\pi(s) = \\mathbb{E}_\\pi[G_t \\mid s_t = s]\n\\]"}),"\n",i.createElement(t.p,null,"which is the expected return when starting in state ",i.createElement(o.A,{text:"\\(s\\)"})," and following policy ",i.createElement(o.A,{text:"\\(\\pi\\)"})," thereafter."),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,"The ",i.createElement(l.A,null,"action-value function")," (or Q-function) under policy ",i.createElement(o.A,{text:"\\(\\pi\\)"})," is:"),"\n",i.createElement(o.A,{text:"\\[\nQ^\\pi(s, a) = \\mathbb{E}_\\pi[G_t \\mid s_t = s, a_t = a]\n\\]"}),"\n",i.createElement(t.p,null,"which is the expected return starting from state ",i.createElement(o.A,{text:"\\(s\\)"}),", taking action ",i.createElement(o.A,{text:"\\(a\\)"}),", and then following policy ",i.createElement(o.A,{text:"\\(\\pi\\)"}),"."),"\n"),"\n"),"\n",i.createElement(t.h3,{id:"bellman-equations",style:{position:"relative"}},i.createElement(t.a,{href:"#bellman-equations","aria-label":"bellman equations permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"bellman equations"),"\n",i.createElement(t.p,null,"The Bellman equations express the relationship between the value function of a state and the value functions of subsequent states. They form the backbone of dynamic programming approaches in RL. The Bellman equation for the state-value function is:"),"\n",i.createElement(o.A,{text:"\\[\nV^\\pi(s) = \\sum_{a} \\pi(a \\mid s) \\sum_{s',r} P(s',r \\mid s,a)\\,\\bigl[r + \\gamma V^\\pi(s')\\bigr].\n\\]"}),"\n",i.createElement(t.p,null,"Here, ",i.createElement(o.A,{text:"\\(P(s', r \\mid s, a)\\)"})," is the probability of moving to state ",i.createElement(o.A,{text:"\\(s'\\)"})," and receiving reward ",i.createElement(o.A,{text:"\\(r\\)"})," after taking action ",i.createElement(o.A,{text:"\\(a\\)"})," in state ",i.createElement(o.A,{text:"\\(s\\)"}),"."),"\n",i.createElement(t.p,null,"For the action-value function, we similarly have:"),"\n",i.createElement(o.A,{text:"\\[\nQ^\\pi(s, a) = \\sum_{s',r} P(s',r \\mid s,a)\\,\\bigl[r + \\gamma \\sum_{a'} \\pi(a' \\mid s') Q^\\pi(s', a')\\bigr].\n\\]"}),"\n",i.createElement(t.h3,{id:"on-policy-vs-off-policy-learning",style:{position:"relative"}},i.createElement(t.a,{href:"#on-policy-vs-off-policy-learning","aria-label":"on policy vs off policy learning permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"on-policy vs. off-policy learning"),"\n",i.createElement(t.p,null,"One crucial distinction in RL is whether learning is performed ",i.createElement(l.A,null,"on-policy")," or ",i.createElement(l.A,null,"off-policy"),"."),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"On-policy"),": The agent learns about the policy ",i.createElement(o.A,{text:"\\(\\pi\\)"})," that is being used to make decisions. SARSA is an example of an on-policy method, as it learns action values relative to the agent's current behavior."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Off-policy"),": The agent learns about a target policy ",i.createElement(o.A,{text:"\\(\\pi^*\\)"})," (often the greedy policy) while following some other behavior policy (for exploration). Q-learning is an example of an off-policy algorithm because it learns the optimal action-value function ",i.createElement(o.A,{text:"\\(Q^*\\)"})," regardless of how the agent behaves."),"\n"),"\n",i.createElement(t.h2,{id:"basic-algorithms",style:{position:"relative"}},i.createElement(t.a,{href:"#basic-algorithms","aria-label":"basic algorithms permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"basic algorithms"),"\n",i.createElement(t.p,null,"In this chapter, I will describe the classical algorithms that form the foundation of RL: dynamic programming, Monte Carlo methods, temporal difference learning, and standard off-policy and on-policy control algorithms such as Q-learning and SARSA. These approaches illustrate the core principles and paved the way for more advanced deep RL approaches."),"\n",i.createElement(t.h3,{id:"dynamic-programming-approaches-policy-iteration-value-iteration",style:{position:"relative"}},i.createElement(t.a,{href:"#dynamic-programming-approaches-policy-iteration-value-iteration","aria-label":"dynamic programming approaches policy iteration value iteration permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"dynamic programming approaches (policy iteration, value iteration)"),"\n",i.createElement(t.p,null,"Dynamic programming (DP) methods assume you have a perfect model of the environment — i.e., you know the transition probabilities ",i.createElement(o.A,{text:"\\(P(s' \\mid s, a)\\)"})," and reward function ",i.createElement(o.A,{text:"\\(R(s, a)\\)"})," — and that you can use these to systematically compute value functions."),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Policy Evaluation"),": Given a policy ",i.createElement(o.A,{text:"\\(\\pi\\)"}),", compute ",i.createElement(o.A,{text:"\\(V^\\pi\\)"})," by iterating the Bellman expectation equation until convergence."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Policy Improvement"),": Improve the policy by acting greedily w.r.t. the current value function."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Policy Iteration"),": Alternate between policy evaluation and policy improvement until the policy converges to an optimal policy ",i.createElement(o.A,{text:"\\(\\pi^*\\)"}),"."),"\n"),"\n",i.createElement(t.p,null,"An alternative is ",i.createElement(t.strong,null,"Value Iteration"),", which folds policy improvement steps into each iteration of evaluation, converging potentially faster to the optimal value function."),"\n",i.createElement(t.p,null,"These methods are historically crucial but are limited to relatively small, discrete MDPs where a full model is available. In large or continuous state/action spaces without a known model, DP is not feasible."),"\n",i.createElement(t.h3,{id:"monte-carlo-methods",style:{position:"relative"}},i.createElement(t.a,{href:"#monte-carlo-methods","aria-label":"monte carlo methods permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"monte carlo methods"),"\n",i.createElement(t.p,null,"Monte Carlo (MC) methods estimate values and policies by sampling complete episodes from the environment. An episode ends in a terminal state (or after a fixed horizon). Once an episode is finished, returns from each state-action pair in that episode are computed, and one can update value estimates accordingly."),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Key aspects:")),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"MC methods do not require knowledge of transition probabilities or rewards; they learn directly from sample returns."),"\n",i.createElement(t.li,null,"They need episodes to terminate in order to compute returns."),"\n",i.createElement(t.li,null,"Variance can be high since updates rely on entire episodes."),"\n"),"\n",i.createElement(t.p,null,"In practice, MC methods can be on-policy or off-policy. Off-policy variants might use behavior policies for exploration while learning about a target policy."),"\n",i.createElement(t.h3,{id:"temporal-difference-learning-td",style:{position:"relative"}},i.createElement(t.a,{href:"#temporal-difference-learning-td","aria-label":"temporal difference learning td permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"temporal difference learning (td)"),"\n",i.createElement(t.p,null,"Temporal difference (TD) learning was a landmark conceptual breakthrough, blending the best of dynamic programming (bootstrapping from current value estimates) and Monte Carlo (sampling from the environment)."),"\n",i.createElement(t.p,null,"The ",i.createElement(t.strong,null,"TD(0)")," update rule for the state-value function is:"),"\n",i.createElement(o.A,{text:"\\[\nV(s_t) \\leftarrow V(s_t) + \\alpha\\bigl[r_{t+1} + \\gamma V(s_{t+1}) - V(s_t)\\bigr],\n\\]"}),"\n",i.createElement(t.p,null,"where ",i.createElement(o.A,{text:"\\(\\alpha\\)"})," is the learning rate. This is often called the ",i.createElement(t.strong,null,"TD error"),":"),"\n",i.createElement(o.A,{text:"\\( \\delta_t = r_{t+1} + \\gamma V(s_{t+1}) - V(s_t). \\)"}),"\n",i.createElement(t.p,null,"TD methods do not require waiting for full episodes to finish; they bootstrap from the existing estimate ",i.createElement(o.A,{text:"\\(V(s_{t+1})\\)"}),". This can allow faster, more incremental updates, particularly in continuing or infinite-horizon tasks."),"\n",i.createElement(t.h3,{id:"q-learning",style:{position:"relative"}},i.createElement(t.a,{href:"#q-learning","aria-label":"q learning permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"q-learning"),"\n",i.createElement(t.p,null,i.createElement(l.A,null,"Q-learning")," is one of the most widely known off-policy TD control methods. It updates the action-value function ",i.createElement(o.A,{text:"\\(Q(s,a)\\)"})," using the following rule:"),"\n",i.createElement(o.A,{text:"\\[\nQ(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha\\Bigl(r_{t+1} + \\gamma \\max_{a}Q(s_{t+1}, a) - Q(s_t, a_t)\\Bigr).\n\\]"}),"\n",i.createElement(t.p,null,"This approach estimates the optimal value function ",i.createElement(o.A,{text:"\\(Q^*\\)"}),", regardless of the policy used to sample transitions. To ensure adequate exploration, the behavior policy often includes an ",i.createElement(o.A,{text:"\\(\\epsilon\\)"}),"-greedy approach over ",i.createElement(o.A,{text:"\\(Q\\)"}),"."),"\n",i.createElement(t.p,null,"Below is a simple snippet in Pythonic pseudocode for tabular Q-learning:"),"\n",i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">&lt;Code text={`\nimport numpy as np\nimport random\n\ndef q_learning(env, num_episodes=10000, alpha=0.1, gamma=0.99, epsilon=0.1):\n    # env: environment with discrete states, discrete actions\n    # initialize Q arbitrarily\n    Q = np.zeros((env.num_states, env.num_actions))\n\n    for episode in range(num_episodes):\n        state = env.reset()\n        done = False\n        \n        while not done:\n            # select action using epsilon-greedy policy\n            if random.random() &lt; epsilon:\n                action = random.choice(range(env.num_actions))\n            else:\n                action = np.argmax(Q[state, :])\n            \n            next_state, reward, done, info = env.step(action)\n            \n            # Q-learning update\n            best_next_action = np.argmax(Q[next_state, :])\n            td_target = reward + gamma * Q[next_state, best_next_action]\n            Q[state, action] += alpha * (td_target - Q[state, action])\n            \n            state = next_state\n    \n    return Q\n`}/></code></pre></div>'}}),"\n",i.createElement(t.h3,{id:"sarsa",style:{position:"relative"}},i.createElement(t.a,{href:"#sarsa","aria-label":"sarsa permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"sarsa"),"\n",i.createElement(t.p,null,"<SARSA> is an on-policy alternative to Q-learning. Its update rule looks quite similar, but the crucial difference is that it uses the agent's current policy (which might be ",i.createElement(o.A,{text:"\\(\\epsilon\\)"}),"-greedy w.r.t. Q) to select the next action and updates based on that:"),"\n",i.createElement(o.A,{text:"\\[\nQ(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha\\bigl[r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)\\bigr].\n\\]"}),"\n",i.createElement(t.p,null,"Here, ",i.createElement(o.A,{text:"\\(a_{t+1}\\)"})," is drawn from the same behavior policy used at time ",i.createElement(o.A,{text:"\\(t+1\\)"}),"."),"\n",i.createElement(t.h3,{id:"eligibility-traces-tdλ",style:{position:"relative"}},i.createElement(t.a,{href:"#eligibility-traces-td%CE%BB","aria-label":"eligibility traces tdλ permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"eligibility traces (td(λ))"),"\n",i.createElement(t.p,null,"Eligibility traces provide a unifying framework bridging MC and TD methods. By maintaining a decaying memory trace of which states (and/or actions) have been visited, these methods can update many states from each transition, improving data efficiency. The parameter ",i.createElement(o.A,{text:"\\(\\lambda\\)"})," determines how much credit assignment is spread over time steps."),"\n",i.createElement(t.p,null,"For ",i.createElement(o.A,{text:"\\(\\lambda=0\\)"}),", you recover standard TD(0). For ",i.createElement(o.A,{text:"\\(\\lambda=1\\)"}),", you get something equivalent to Monte Carlo updates (for episodic tasks). In between, you get an interplay of bootstrapping and Monte Carlo backups."),"\n",i.createElement(t.h3,{id:"model-based-vs-model-free-approaches",style:{position:"relative"}},i.createElement(t.a,{href:"#model-based-vs-model-free-approaches","aria-label":"model based vs model free approaches permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"model-based vs. model-free approaches"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Model-Based"),": The agent has or learns an internal model of the environment's dynamics ",i.createElement(o.A,{text:"\\(P(s' \\mid s,a)\\)"})," and reward function ",i.createElement(o.A,{text:"\\(R(s,a)\\)"}),". It can plan by simulating possible trajectories or using dynamic programming."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Model-Free"),": The agent directly learns value functions or policies from experience without explicitly constructing a model. Methods like Q-learning, SARSA, and policy gradient approaches typically fall under this umbrella."),"\n"),"\n",i.createElement(t.p,null,"When you have a reliable model, model-based RL can be more sample-efficient, but in many practical tasks, the environment is too complex, or we do not have access to a perfect simulator, making model-free RL more commonly used."),"\n",i.createElement(t.h2,{id:"value-based-and-policy-based-methods",style:{position:"relative"}},i.createElement(t.a,{href:"#value-based-and-policy-based-methods","aria-label":"value based and policy based methods permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"value-based and policy-based methods"),"\n",i.createElement(t.p,null,"The algorithms presented in the previous chapter are typically considered ",i.createElement(l.A,null,"value-based")," methods, since they focus on estimating and improving an action-value function or state-value function. Another major approach is ",i.createElement(l.A,null,"policy-based"),", which directly parameterizes and optimizes the policy."),"\n",i.createElement(t.h3,{id:"comparing-value-based-vs-policy-based-approaches",style:{position:"relative"}},i.createElement(t.a,{href:"#comparing-value-based-vs-policy-based-approaches","aria-label":"comparing value based vs policy based approaches permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"comparing value-based vs. policy-based approaches"),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Value-Based")," approaches typically find an optimal Q-function (i.e., ",i.createElement(o.A,{text:"\\(Q^*\\)"}),") and then derive a policy by greedily selecting actions that maximize ",i.createElement(o.A,{text:"\\(Q^*\\)"}),". For many tasks, especially with discrete action spaces, this works well. But in large or continuous action spaces, searching for the argmax can be cumbersome, and function approximation can introduce instability in Q-value estimates."),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Policy-Based")," approaches parametrize the policy itself, for example ",i.createElement(o.A,{text:"\\(\\pi_\\theta(a\\mid s)\\)"}),", using some parameters ",i.createElement(o.A,{text:"\\(\\theta\\)"})," (often the weights of a neural network). One can then use gradient-based optimization (e.g., REINFORCE or actor-critic methods) to directly update ",i.createElement(o.A,{text:"\\(\\theta\\)"})," toward maximizing expected returns. Policy-based methods are frequently used for continuous control, as they sidestep the need to explicitly store or approximate a Q-function over a continuous action set."),"\n",i.createElement(t.h3,{id:"actor-critic-methods",style:{position:"relative"}},i.createElement(t.a,{href:"#actor-critic-methods","aria-label":"actor critic methods permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"actor-critic methods"),"\n",i.createElement(t.p,null,"Actor-critic methods combine the best of both worlds. They maintain both:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Actor"),": A parameterized policy ",i.createElement(o.A,{text:"\\(\\pi_\\theta(a \\mid s)\\)"})," that selects actions."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Critic"),": A value function ",i.createElement(o.A,{text:"\\(V_w(s)\\)"})," (or Q-value function) that evaluates how good the chosen actions or states are, guiding the gradient updates for the actor."),"\n"),"\n",i.createElement(t.p,null,"By using a critic, the actor can be updated with lower-variance gradient estimates. By using an explicit policy representation in the actor, the method can seamlessly handle continuous actions."),"\n",i.createElement(t.h3,{id:"advantage-actor-critic-a2c",style:{position:"relative"}},i.createElement(t.a,{href:"#advantage-actor-critic-a2c","aria-label":"advantage actor critic a2c permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"advantage actor-critic (a2c)"),"\n",i.createElement(t.p,null,'A2C is a synchronous, or "batched," version of advantage actor-critic. In advantage-based methods, the critic computes an ',i.createElement(l.A,null,"advantage function"),":"),"\n",i.createElement(o.A,{text:"\\[\nA(s,a) = Q(s,a) - V(s),\n\\]"}),"\n",i.createElement(t.p,null,"which tells how much better or worse an action is, relative to the state's baseline value. The advantage reduces variance in policy gradient updates by not requiring the full return."),"\n",i.createElement(t.h3,{id:"asynchronous-advantage-actor-critic-a3c",style:{position:"relative"}},i.createElement(t.a,{href:"#asynchronous-advantage-actor-critic-a3c","aria-label":"asynchronous advantage actor critic a3c permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"asynchronous advantage actor-critic (a3c)"),"\n",i.createElement(t.p,null,"A3C, proposed by Mnih and gang (2016), is a parallelized version of the advantage actor-critic approach. Instead of training a single agent on experience from one environment, multiple environments and agents run in parallel threads, each computing updates to a shared global set of parameters asynchronously. This improves both the speed of training and the robustness of the learned policy."),"\n",i.createElement(t.h3,{id:"deterministic-policy-gradients-dpg",style:{position:"relative"}},i.createElement(t.a,{href:"#deterministic-policy-gradients-dpg","aria-label":"deterministic policy gradients dpg permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"deterministic policy gradients (dpg)"),"\n",i.createElement(t.p,null,"While standard policy gradient methods often assume a stochastic policy ",i.createElement(o.A,{text:"(\\pi_\\theta(a|s))"})," for exploration, for continuous control tasks it can be advantageous to use a deterministic policy ",i.createElement(o.A,{text:"(\\mu_\\theta(s))"})," that maps states directly to actions. The gradient of the policy performance objective can be computed using chain rule and the Q-function, leading to an algorithm known as ",i.createElement(t.strong,null,"Deterministic Policy Gradient")," (DPG)."),"\n",i.createElement(t.h3,{id:"proximal-policy-optimization-ppo-and-trpo",style:{position:"relative"}},i.createElement(t.a,{href:"#proximal-policy-optimization-ppo-and-trpo","aria-label":"proximal policy optimization ppo and trpo permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"proximal policy optimization (ppo) and trpo"),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"PPO")," (Proximal Policy Optimization) and ",i.createElement(t.strong,null,"TRPO")," (Trust Region Policy Optimization) are popular policy gradient methods designed to improve stability. They limit the size of the policy update at each step. TRPO does so by enforcing a hard constraint on the KL divergence between the old policy and new policy, while PPO uses a clipped surrogate objective. Both methods aim to prevent destructive large updates that destabilize training."),"\n",i.createElement(t.h2,{id:"deep-reinforcement-learning",style:{position:"relative"}},i.createElement(t.a,{href:"#deep-reinforcement-learning","aria-label":"deep reinforcement learning permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"deep reinforcement learning"),"\n",i.createElement(t.p,null,"When RL meets deep learning, we unlock the ability to handle very high-dimensional state spaces, such as raw images in Atari games, or complex continuous observations in robotics. However, naive application of neural networks to RL can lead to instability, so certain key architectural and algorithmic choices are needed to stabilize learning."),"\n",i.createElement(t.h3,{id:"role-of-neural-networks-in-rl",style:{position:"relative"}},i.createElement(t.a,{href:"#role-of-neural-networks-in-rl","aria-label":"role of neural networks in rl permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"role of neural networks in rl"),"\n",i.createElement(t.p,null,"Neural networks serve as powerful function approximators for value functions (e.g., ",i.createElement(o.A,{text:"\\(Q^\\pi(s,a)\\)"}),"), state-value functions (",i.createElement(o.A,{text:"\\(V^\\pi(s)\\)"}),"), or policies (",i.createElement(o.A,{text:"\\(\\pi_\\theta(a\\mid s)\\)"}),"). By adjusting network weights via gradient descent, we can learn representations of states and complex relationships between actions and expected returns."),"\n",i.createElement(t.h3,{id:"deep-q-networks-dqn",style:{position:"relative"}},i.createElement(t.a,{href:"#deep-q-networks-dqn","aria-label":"deep q networks dqn permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"deep q-networks (dqn)"),"\n",i.createElement(t.p,null,"Proposed by Mnih and gang (2013, 2015), DQN revolutionized RL for complex visual tasks. DQN uses a CNN to approximate the Q-function:"),"\n",i.createElement(o.A,{text:"\\[\nQ(s,a; \\theta).\n\\]"}),"\n",i.createElement(t.p,null,"Major innovations that made DQN successful:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Experience Replay"),": Instead of updating from consecutive samples, store transitions (",i.createElement(o.A,{text:"\\(s_t, a_t, r_t, s_{t+1}\\)"}),") in a replay buffer and sample mini-batches randomly. This breaks correlation in consecutive samples and improves data efficiency."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Target Network"),": Maintain a separate set of parameters ",i.createElement(o.A,{text:"\\(\\theta^-\\)"})," for the target Q-network, updated only occasionally, to reduce instability due to constantly shifting targets."),"\n"),"\n",i.createElement(t.h3,{id:"experience-replay",style:{position:"relative"}},i.createElement(t.a,{href:"#experience-replay","aria-label":"experience replay permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"experience replay"),"\n",i.createElement(t.p,null,"With experience replay, we store the agent's experience in a replay memory, then randomly sample from it to update the network. This randomization (rather than purely sequential updates) avoids bias from the highly correlated nature of consecutive data."),"\n",i.createElement(t.h3,{id:"target-networks",style:{position:"relative"}},i.createElement(t.a,{href:"#target-networks","aria-label":"target networks permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"target networks"),"\n",i.createElement(t.p,null,"In Q-learning, the TD target for ",i.createElement(o.A,{text:"\\(Q(s_t,a_t)\\)"})," depends on the next state's Q-values, which are themselves being updated. By using a target network with frozen or slowly updated parameters ",i.createElement(o.A,{text:"\\(\\theta^-\\)"}),", we have a more stable target:"),"\n",i.createElement(o.A,{text:"\\[\ny_t = r_{t+1} + \\gamma \\max_a Q(s_{t+1}, a; \\theta^-).\n\\]"}),"\n",i.createElement(t.h3,{id:"extensions-to-dqn-double-dueling-prioritized-replay",style:{position:"relative"}},i.createElement(t.a,{href:"#extensions-to-dqn-double-dueling-prioritized-replay","aria-label":"extensions to dqn double dueling prioritized replay permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"extensions to dqn (double, dueling, prioritized replay)"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Double DQN"),": Addresses overestimation bias by separately selecting the action that maximizes the Q-function, and evaluating it with a target network."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Dueling DQN"),": Splits the Q-network into two streams: one for the state-value function and one for the advantage function, then combines them to produce Q-values. This helps the agent learn which states are (not) valuable, independent of the chosen action."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Prioritized Replay"),": Samples experiences that the agent is more uncertain about (i.e., those with higher TD error) more frequently, improving learning efficiency."),"\n"),"\n",i.createElement(t.h3,{id:"policy-gradient-with-deep-networks",style:{position:"relative"}},i.createElement(t.a,{href:"#policy-gradient-with-deep-networks","aria-label":"policy gradient with deep networks permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"policy gradient with deep networks"),"\n",i.createElement(t.p,null,"Instead of approximating ",i.createElement(o.A,{text:"\\(Q(s,a)\\)"})," and then selecting ",i.createElement(o.A,{text:"\\(a\\)"})," greedily, we can parametrize a policy ",i.createElement(o.A,{text:"\\(\\pi_\\theta(a\\mid s)\\)"})," with a deep network and optimize for expected return. This is the essence of ",i.createElement(t.strong,null,"Deep Policy Gradients"),"."),"\n",i.createElement(t.p,null,"A canonical example is the ",i.createElement(t.strong,null,"REINFORCE")," algorithm (Williams, 1992), which uses Monte Carlo returns to compute an unbiased estimate of the gradient of performance. Although conceptually straightforward, REINFORCE can suffer from high variance, which actor-critic and advantage-based methods attempt to reduce."),"\n",i.createElement(t.h3,{id:"actor-critic-architectures-ddpg-sac",style:{position:"relative"}},i.createElement(t.a,{href:"#actor-critic-architectures-ddpg-sac","aria-label":"actor critic architectures ddpg sac permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"actor-critic architectures (ddpg, sac)"),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"DDPG")," (Deep Deterministic Policy Gradient) extends DPG to large-scale deep networks, featuring an actor network for the deterministic policy ",i.createElement(o.A,{text:"\\(\\mu_\\theta(s)\\)"})," and a critic network to approximate ",i.createElement(o.A,{text:"\\(Q^\\mu(s,a)\\)"}),". It also uses a replay buffer and target networks."),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"SAC")," (Soft Actor-Critic) is another popular approach for continuous control, optimizing a stochastic policy with the twin goals of maximizing reward and maximizing entropy (exploration)."),"\n",i.createElement(t.h3,{id:"distributional-rl-c51-qr-dqn",style:{position:"relative"}},i.createElement(t.a,{href:"#distributional-rl-c51-qr-dqn","aria-label":"distributional rl c51 qr dqn permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"distributional rl (c51, qr-dqn)"),"\n",i.createElement(t.p,null,"Distributional RL goes beyond learning the expected value of returns, focusing on learning the ",i.createElement(l.A,null,"distribution")," of possible returns. For instance, ",i.createElement(t.strong,null,"C51")," (Bellemare and gang) represents the return distribution with a discrete support of 51 atoms, while ",i.createElement(t.strong,null,"QR-DQN")," uses quantile regression to learn the return distribution at different quantiles. This can give better performance and more insights into risk-sensitive decision-making."),"\n",i.createElement(t.h2,{id:"advanced-techniques",style:{position:"relative"}},i.createElement(t.a,{href:"#advanced-techniques","aria-label":"advanced techniques permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"advanced techniques"),"\n",i.createElement(t.p,null,"Having examined the fundamentals of reinforcement learning and how deep function approximation can help tackle complex, high-dimensional tasks, let's pivot to advanced techniques that address crucial problems such as efficient exploration, hierarchical learning, multi-agent settings, safe RL, and so forth."),"\n",i.createElement(t.h3,{id:"exploration-vs-exploitation-strategies",style:{position:"relative"}},i.createElement(t.a,{href:"#exploration-vs-exploitation-strategies","aria-label":"exploration vs exploitation strategies permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"exploration vs. exploitation strategies"),"\n",i.createElement(t.p,null,"One of the earliest lessons in RL is the need to balance ",i.createElement(t.strong,null,"exploration")," (trying new actions to discover their consequences) and ",i.createElement(t.strong,null,"exploitation")," (leveraging known actions that yield high reward). This fundamental trade-off can be studied in simpler settings through the ",i.createElement(l.A,null,"multi-armed bandit")," problem."),"\n",i.createElement(t.p,null,"Common exploration heuristics in full RL:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"ε-Greedy"),": With probability ",i.createElement(o.A,{text:"\\(\\epsilon\\)"}),", select an action at random; otherwise select the greedy action."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Boltzmann Exploration (Softmax)"),": Sample actions according to a softmax distribution over Q-values."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Upper Confidence Bound (UCB)"),": Maintain an optimism in the face of uncertainty by adding a bonus for actions with fewer visits or higher uncertainty."),"\n"),"\n",i.createElement(t.h3,{id:"ε-greedy-and-boltzmann-exploration",style:{position:"relative"}},i.createElement(t.a,{href:"#%CE%B5-greedy-and-boltzmann-exploration","aria-label":"ε greedy and boltzmann exploration permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"ε-greedy and boltzmann exploration"),"\n",i.createElement(t.p,null,"The ",i.createElement(o.A,{text:"\\(\\epsilon\\)"}),"-greedy approach is straightforward but can be suboptimal if the environment is complex. ",i.createElement(t.strong,null,"Boltzmann exploration")," (or Softmax) tries to allocate exploration probability proportionally to ",i.createElement(o.A,{text:"\\(\\exp(Q(s,a)/\\tau)\\)"}),", where ",i.createElement(o.A,{text:"\\(\\tau\\)"})," is a temperature parameter controlling how sharply differences in Q-values affect selection probabilities."),"\n",i.createElement(t.h3,{id:"upper-confidence-bound-ucb",style:{position:"relative"}},i.createElement(t.a,{href:"#upper-confidence-bound-ucb","aria-label":"upper confidence bound ucb permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"upper confidence bound (ucb)"),"\n",i.createElement(t.p,null,"Originally popularized in multi-armed bandit scenarios, UCB-based techniques incorporate an exploration bonus term that is larger for rarely visited actions. A typical UCB formula for action-value estimates might be:"),"\n",i.createElement(o.A,{text:"\\[\nQ_t(a) + b_t(a), \\quad b_t(a) = \\sqrt{\\frac{2 \\ln \\bigl(\\sum_{a'}P_{a'}\\bigr)}{P_a}},\n\\]"}),"\n",i.createElement(t.p,null,"where ",i.createElement(o.A,{text:"\\(P_a\\)"})," is the number of times action ",i.createElement(o.A,{text:"\\(a\\)"})," has been selected so far, and ",i.createElement(o.A,{text:"\\(\\sum_{a'}P_{a'}\\)"})," is the total number of action selections."),"\n",i.createElement(t.h3,{id:"hierarchical-reinforcement-learning-options-framework-feudal-networks",style:{position:"relative"}},i.createElement(t.a,{href:"#hierarchical-reinforcement-learning-options-framework-feudal-networks","aria-label":"hierarchical reinforcement learning options framework feudal networks permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"hierarchical reinforcement learning (options framework, feudal networks)"),"\n",i.createElement(t.p,null,"In ",i.createElement(t.strong,null,"hierarchical RL"),', one aims to learn or exploit temporal abstractions, such as higher-level actions or "options" that span multiple time steps. This helps break down complex tasks into manageable sub-tasks. The ',i.createElement(l.A,null,"Options Framework")," (Sutton and gang) formalizes such sub-policies, each with its own initiation and termination conditions."),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"FeUdal Networks")," (Vezhnevets and gang, 2017) propose a hierarchy of managers and workers, with managers setting high-level goals in an embedding space, and workers focusing on short-horizon control."),"\n",i.createElement(t.h3,{id:"multi-agent-reinforcement-learning",style:{position:"relative"}},i.createElement(t.a,{href:"#multi-agent-reinforcement-learning","aria-label":"multi agent reinforcement learning permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"multi-agent reinforcement learning"),"\n",i.createElement(t.p,null,"When multiple agents learn and interact in the same environment, we have a multi-agent RL setting. Agents may cooperate, compete, or both. Key challenges include non-stationarity (since the environment changes when other agents change their behavior), communication strategies, and stability. A variety of approaches exist, from independent Q-learning to more advanced methods involving joint action learners or policy gradients with centralized critics and decentralized actors."),"\n",i.createElement(t.h3,{id:"transfer-curriculum-and-meta-learning",style:{position:"relative"}},i.createElement(t.a,{href:"#transfer-curriculum-and-meta-learning","aria-label":"transfer curriculum and meta learning permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"transfer, curriculum, and meta-learning"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Transfer Learning"),": Using knowledge learned in one task to accelerate learning in a new, related task."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Curriculum Learning"),": Presenting simpler tasks first, then gradually increasing difficulty."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Meta-Learning"),": Learning how to learn. An agent might adapt quickly to new tasks after training on a distribution of tasks."),"\n"),"\n",i.createElement(t.p,null,'These methods aim to improve sample efficiency and reduce the need to start learning "from scratch" every time the agent faces a new scenario.'),"\n",i.createElement(t.h3,{id:"inverse-reinforcement-learning",style:{position:"relative"}},i.createElement(t.a,{href:"#inverse-reinforcement-learning","aria-label":"inverse reinforcement learning permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"inverse reinforcement learning"),"\n",i.createElement(t.p,null,"Inverse reinforcement learning (IRL) seeks to infer the reward function ",i.createElement(o.A,{text:"\\(R\\)"})," from expert demonstrations. By observing how experts behave, one can back out what objective they are trying to optimize. IRL is particularly useful when the reward is difficult to specify but we have demonstration data."),"\n",i.createElement(t.h3,{id:"safe-and-robust-rl",style:{position:"relative"}},i.createElement(t.a,{href:"#safe-and-robust-rl","aria-label":"safe and robust rl permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"safe and robust rl"),"\n",i.createElement(t.p,null,"In real-world scenarios, we often need to ensure that the agent avoids catastrophic actions or respects certain constraints. ",i.createElement(t.strong,null,"Safe RL")," studies ways to incorporate constraints into the learning process, or shape exploration so that catastrophic actions are less likely. ",i.createElement(t.strong,null,"Robust RL")," similarly addresses the environment's uncertainty by training or regularizing the agent to handle worst-case scenarios or distribution shifts."),"\n",i.createElement(t.h3,{id:"offline-rl-batch-rl",style:{position:"relative"}},i.createElement(t.a,{href:"#offline-rl-batch-rl","aria-label":"offline rl batch rl permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"offline rl (batch rl)"),"\n",i.createElement(t.p,null,"Offline RL learns policies from a fixed dataset of transitions without additional environment interaction. This setting is especially useful in domains where real-world interaction is costly (e.g., healthcare, autonomous driving). Methods must carefully handle distributional shift issues and avoid extrapolation errors from out-of-distribution state-action pairs."),"\n",i.createElement(t.h2,{id:"applications-and-case-studies",style:{position:"relative"}},i.createElement(t.a,{href:"#applications-and-case-studies","aria-label":"applications and case studies permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"applications and case studies"),"\n",i.createElement(t.p,null,"Reinforcement learning has proven its mettle in numerous fields, both academic and industrial. While many breakthroughs are on benchmark domains like Atari, MuJoCo, or custom simulators, real-world applications continue to proliferate."),"\n",i.createElement(t.h3,{id:"robotics-and-control",style:{position:"relative"}},i.createElement(t.a,{href:"#robotics-and-control","aria-label":"robotics and control permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"robotics and control"),"\n",i.createElement(t.p,null,"Robotic manipulation, locomotion, and navigation are quintessential RL problems, featuring continuous state and action spaces and requiring an agent to handle high-dimensional sensor data (e.g., from cameras, lidar, joint encoders). Techniques such as DDPG, PPO, and SAC are widely tested here, often combined with robust domain randomization and sim-to-real transfer."),"\n",i.createElement(t.h3,{id:"game-ai-and-openai-gym",style:{position:"relative"}},i.createElement(t.a,{href:"#game-ai-and-openai-gym","aria-label":"game ai and openai gym permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"game ai and openai gym"),"\n",i.createElement(t.p,null,"OpenAI Gym provides a standard interface for RL agents and a variety of environments (classic control, Atari, Box2D, MuJoCo). This allows easy benchmarking of algorithms. High-profile game successes (AlphaGo, AlphaStar, OpenAI Five) used RL plus additional techniques (Monte Carlo Tree Search, self-play, etc.) to excel in extremely challenging domains."),"\n",i.createElement(n,{alt:"AlphaGo playing Go",path:"",caption:"AlphaGo's match against Lee Sedol, a hallmark demonstration of RL in complex environments",zoom:"false"}),"\n",i.createElement(t.h3,{id:"recommender-systems",style:{position:"relative"}},i.createElement(t.a,{href:"#recommender-systems","aria-label":"recommender systems permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"recommender systems"),"\n",i.createElement(t.p,null,"Sequential recommendation can be framed as a contextual bandit or RL problem, where the environment is the user or user-model, actions are content recommendations, and rewards might be clicks or engagement metrics. RL can dynamically adapt to evolving user tastes."),"\n",i.createElement(t.h3,{id:"healthcare-and-personalized-medicine",style:{position:"relative"}},i.createElement(t.a,{href:"#healthcare-and-personalized-medicine","aria-label":"healthcare and personalized medicine permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"healthcare and personalized medicine"),"\n",i.createElement(t.p,null,"In healthcare, RL methods can recommend treatment policies that maximize patient health over the long run. For instance, in sepsis management or oncology, where immediate interventions have delayed impacts. Challenges include partial observability, high stakes, and the need for explainability."),"\n",i.createElement(t.h3,{id:"finance-and-algorithmic-trading",style:{position:"relative"}},i.createElement(t.a,{href:"#finance-and-algorithmic-trading","aria-label":"finance and algorithmic trading permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"finance and algorithmic trading"),"\n",i.createElement(t.p,null,"Portfolio optimization, high-frequency trading, and algorithmic strategy design can be framed as RL tasks. The agent must choose trades or adjustments and receives profit/loss as a reward. High volatility, partial observability, and risk constraints complicate the matter, leading to interesting synergy with risk-sensitive or distributional RL."),"\n",i.createElement(t.h3,{id:"autonomous-vehicles",style:{position:"relative"}},i.createElement(t.a,{href:"#autonomous-vehicles","aria-label":"autonomous vehicles permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"autonomous vehicles"),"\n",i.createElement(t.p,null,"Self-driving cars must constantly make decisions about steering, acceleration, lane changes, and so on, balancing collision avoidance, speed, comfort, and traffic rules. RL can help address these decisions in principle, but real-world safety constraints, huge state-action spaces, and interpretability remain open challenges."),"\n",i.createElement(t.h3,{id:"resource-allocation-and-scheduling",style:{position:"relative"}},i.createElement(t.a,{href:"#resource-allocation-and-scheduling","aria-label":"resource allocation and scheduling permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"resource allocation and scheduling"),"\n",i.createElement(t.p,null,"RL can optimize scheduling or resource allocation in data centers, manufacturing, or supply chain management. Instead of coding heuristics, an RL agent can learn from data to handle complexities like changing demand patterns or unexpected disruptions."),"\n",i.createElement(t.h2,{id:"practical-considerations",style:{position:"relative"}},i.createElement(t.a,{href:"#practical-considerations","aria-label":"practical considerations permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"practical considerations"),"\n",i.createElement(t.p,null,"Finally, let's explore the real-world complexities an RL practitioner must consider, such as reward design, hyperparameter tuning, computational constraints, and reproducibility."),"\n",i.createElement(t.h3,{id:"designing-reward-functions",style:{position:"relative"}},i.createElement(t.a,{href:"#designing-reward-functions","aria-label":"designing reward functions permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"designing reward functions"),"\n",i.createElement(t.p,null,"Reward shaping can significantly impact the agent's behavior. A poorly designed reward might inadvertently create ",i.createElement(l.A,null,"reward hacking")," (the agent finds a way to achieve the maximum reward in an unintended manner). It is crucial to craft the reward so that it aligns with the true objectives of the task."),"\n",i.createElement(t.p,null,"Common pitfalls:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"Sparse rewards: The agent seldom receives feedback, making exploration difficult."),"\n",i.createElement(t.li,null,"Delayed rewards: The credit assignment problem becomes severe."),"\n",i.createElement(t.li,null,"Surrogate or proxy rewards: If the reward is poorly correlated with the true objective, the agent might exploit the proxy."),"\n"),"\n",i.createElement(t.h3,{id:"handling-continuous-action-spaces",style:{position:"relative"}},i.createElement(t.a,{href:"#handling-continuous-action-spaces","aria-label":"handling continuous action spaces permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"handling continuous action spaces"),"\n",i.createElement(t.p,null,"In robotics, continuous control, or many real-world tasks, actions are not discrete. Off-the-shelf methods like Q-learning do not directly apply, because ",i.createElement(o.A,{text:"\\(\\max_a Q(s,a)\\)"})," is not well-defined over a continuous domain in a naive sense. Policy gradient or actor-critic methods are more suitable here."),"\n",i.createElement(t.h3,{id:"scalability-and-computational-challenges",style:{position:"relative"}},i.createElement(t.a,{href:"#scalability-and-computational-challenges","aria-label":"scalability and computational challenges permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"scalability and computational challenges"),"\n",i.createElement(t.p,null,"Reinforcement learning can be data-hungry. In some domains, each sample is expensive (e.g., real-world robotic trials). Thus, parallelization and simulation are widely used. Tools like ",i.createElement(t.strong,null,"Ray RLlib")," allow distributing experience collection across many workers."),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Distributed training")," remains an active area of research, especially for tasks requiring huge data throughput, such as large-scale game simulations or complex continuous control tasks."),"\n",i.createElement(t.h3,{id:"common-pitfalls-and-troubleshooting",style:{position:"relative"}},i.createElement(t.a,{href:"#common-pitfalls-and-troubleshooting","aria-label":"common pitfalls and troubleshooting permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"common pitfalls and troubleshooting"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Instability"),": Q-networks can diverge without careful hyperparameters or design choices (learning rates, replay buffer sizes, target updates, etc.)."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Reward hacking"),": The agent finds an unintended strategy to maximize reward."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Lack of exploration"),": If the agent does not explore sufficiently, it will not find good policies."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Overfitting to training environment"),": An agent that performs well in one environment might fail in slightly different but related tasks."),"\n"),"\n",i.createElement(t.h3,{id:"hyperparameter-tuning-and-experimentation",style:{position:"relative"}},i.createElement(t.a,{href:"#hyperparameter-tuning-and-experimentation","aria-label":"hyperparameter tuning and experimentation permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"hyperparameter tuning and experimentation"),"\n",i.createElement(t.p,null,"Reinforcement learning can be quite sensitive to hyperparameters (learning rate, discount factor, exploration schedules, neural network architectures, etc.). I recommend:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"Performing grid or random searches to find good ranges."),"\n",i.createElement(t.li,null,"Using best practices, such as decaying ",i.createElement(o.A,{text:"\\(\\epsilon\\)"})," for ε-greedy or ",i.createElement(o.A,{text:"\\(\\tau\\)"})," for softmax."),"\n",i.createElement(t.li,null,"Carefully monitoring training curves and using multiple seeds."),"\n"),"\n",i.createElement(t.h3,{id:"evaluation-and-benchmarking",style:{position:"relative"}},i.createElement(t.a,{href:"#evaluation-and-benchmarking","aria-label":"evaluation and benchmarking permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"evaluation and benchmarking"),"\n",i.createElement(t.p,null,"Because of the stochastic nature of RL, it is important to run multiple seeds or replicate results multiple times and report average performance plus confidence intervals. The RL community often uses standard benchmark suites like Arcade Learning Environment (Atari), MuJoCo tasks, or continuous control tasks from PyBullet for comparisons."),"\n",i.createElement(t.h3,{id:"reproducibility-and-experiment-tracking",style:{position:"relative"}},i.createElement(t.a,{href:"#reproducibility-and-experiment-tracking","aria-label":"reproducibility and experiment tracking permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"reproducibility and experiment tracking"),"\n",i.createElement(t.p,null,"Due to the complexity and stochasticity of RL algorithms, it is essential to track random seeds, algorithm hyperparameters, environment versions, etc., in a structured manner. Tools like Weights & Biases, MLflow, or TensorBoard can help keep track of experiments."),"\n",i.createElement(t.h3,{id:"frameworks-and-libraries-openai-gym-stable-baselines-rllib",style:{position:"relative"}},i.createElement(t.a,{href:"#frameworks-and-libraries-openai-gym-stable-baselines-rllib","aria-label":"frameworks and libraries openai gym stable baselines rllib permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"frameworks and libraries (openai gym, stable baselines, rllib)"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"OpenAI Gym"),": Defines a standard interface for RL tasks and includes many classic and modern environments."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Stable Baselines"),": Provides popular RL algorithms (PPO, A2C, DDPG, SAC, TD3, etc.) in a user-friendly library."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"RLlib (part of Ray)"),": A scalable RL library that supports distributed training out of the box."),"\n"),"\n",i.createElement(t.p,null,"For specialized tasks (e.g., robotics), frameworks like ",i.createElement(l.A,null,"PyBullet"),", ",i.createElement(l.A,null,"Roboschool"),", or ",i.createElement(l.A,null,"Isaac Gym")," might be used."),"\n",i.createElement(t.hr),"\n",i.createElement(t.p,null,"I hope this extensive overview has given you both the theoretical background and practical knowledge to appreciate how reinforcement learning fits into advanced data science and AI pipelines. It is truly one of the most dynamic fields in modern machine learning, continuing to evolve with cutting-edge research in deep RL, hierarchical approaches, multi-agent settings, and beyond."),"\n",i.createElement(t.p,null,"The chapters here have highlighted fundamental ideas: MDPs, Bellman equations, Q-learning, SARSA, policy gradients, actor-critic methods, and modern breakthroughs like DQN, AlphaGo, or advanced policy optimization algorithms. If you plan to pursue RL further, I strongly recommend exploring in detail the references below, including the classic textbook by Sutton and Barto (which is available online), and some of the seminal papers that launched the deep RL revolution."),"\n",i.createElement(t.p,null,"Reading about these algorithms is helpful, but nothing cements understanding like implementing them. I encourage you to experiment, possibly starting with simpler environments like CartPole or MountainCar in OpenAI Gym, then venturing to more advanced continuous control tasks or even custom domains relevant to your field of interest."),"\n",i.createElement(t.p,null,"If you stay mindful of reward design, keep track of hyperparameters carefully, and remain vigilant about the exploration vs. exploitation dilemma, you will be well on your way to successful applications of reinforcement learning in your projects."),"\n",i.createElement(t.p,null,i.createElement(l.A,null,"References and further reading"),":"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,'Sutton, R. S., & Barto, A. G. (2018). "Reinforcement Learning: An Introduction (2nd edition)."'),"\n",i.createElement(t.li,null,'Bellman, R. (1957). "Dynamic Programming." Princeton University Press.'),"\n",i.createElement(t.li,null,'Mnih, V. and gang (2015). "Human-level control through deep reinforcement learning." ',i.createElement(t.em,null,"Nature"),", 518(7540), 529-533."),"\n",i.createElement(t.li,null,'Silver, D. and gang (2016). "Mastering the game of Go with deep neural networks and tree search." ',i.createElement(t.em,null,"Nature"),", 529(7587), 484-489."),"\n",i.createElement(t.li,null,'Schulman, J. and gang (2015). "Trust Region Policy Optimization." ',i.createElement(t.em,null,"ICML"),"."),"\n",i.createElement(t.li,null,'Schulman, J. and gang (2017). "Proximal Policy Optimization Algorithms." ',i.createElement(t.em,null,"arXiv preprint arXiv:1707.06347.")),"\n",i.createElement(t.li,null,'Hessel, M. and gang (2018). "Rainbow: Combining Improvements in Deep Reinforcement Learning." ',i.createElement(t.em,null,"AAAI.")),"\n",i.createElement(t.li,null,'Vezhnevets, A. S. and gang (2017). "FeUdal Networks for Hierarchical Reinforcement Learning." ',i.createElement(t.em,null,"ICML.")),"\n"),"\n",i.createElement(t.p,null,"And of course, exploring the open-source code bases (OpenAI Baselines, Stable Baselines, RLlib, etc.) can be instructive for practical implementations."),"\n",i.createElement(t.p,null,"All in all, reinforcement learning stands out as a powerful paradigm for sequential decision-making under uncertainty, bridging ideas from control theory, behavioral psychology, and machine learning. By continually refining algorithms and deep architectures, RL researchers and practitioners push the boundaries of what learning-based agents can achieve, from playing complex games at superhuman levels to optimizing resource allocation in large-scale systems, to controlling sophisticated robotic platforms. The future is bright for RL, and as hardware and parallel computing frameworks improve, we can expect further leaps forward in speed, stability, and real-world applicability."))}var c=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,r.RP)(),e.components);return t?i.createElement(t,e,i.createElement(s,e)):s(e)};var m=n(36710),h=n(58481),d=n.n(h),u=n(36310),p=n(87245),g=n(27042),f=n(59849),v=n(5591),y=n(61122),b=n(9219),E=n(33203),w=n(95751),x=n(94328),S=n(80791),k=n(78137);const H=e=>{let{toc:t}=e;if(!t||!t.items)return null;return i.createElement("nav",{className:S.R},i.createElement("ul",null,t.items.map(((e,t)=>i.createElement("li",{key:t},i.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const n=t.replace("#",""),a=document.getElementById(n);a&&a.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&i.createElement(H,{toc:{items:e.items}}))))))};function A(e){let{data:{mdx:t,allMdx:l,allPostImages:o},children:s}=e;const{frontmatter:c,body:m,tableOfContents:h}=t,f=c.index,S=c.slug.split("/")[1],A=l.nodes.filter((e=>e.frontmatter.slug.includes(`/${S}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),_=A.findIndex((e=>e.frontmatter.index===f)),z=A[_+1],M=A[_-1],C=c.slug.replace(/\/$/,""),T=/[^/]*$/.exec(C)[0],L=`posts/${S}/content/${T}/`,{0:I,1:V}=(0,i.useState)(c.flagWideLayoutByDefault),{0:R,1:B}=(0,i.useState)(!1);var P;(0,i.useEffect)((()=>{B(!0);const e=setTimeout((()=>B(!1)),340);return()=>clearTimeout(e)}),[I]),"adventures"===S?P=b.cb:"research"===S?P=b.Qh:"thoughts"===S&&(P=b.T6);const N=d()(m).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,D=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),n=e%60;return n<=30?`~${t}${n>0?".5":""} h`:`~${t+1} h`}(Math.ceil(N/P)+(c.extraReadTimeMin||0)),q=[{flag:c.flagDraft,component:()=>Promise.all([n.e(3231),n.e(8809)]).then(n.bind(n,28809))},{flag:c.flagMindfuckery,component:()=>Promise.all([n.e(3231),n.e(2471)]).then(n.bind(n,67709))},{flag:c.flagRewrite,component:()=>Promise.all([n.e(3231),n.e(6764)]).then(n.bind(n,62002))},{flag:c.flagOffensive,component:()=>Promise.all([n.e(3231),n.e(2443)]).then(n.bind(n,17681))},{flag:c.flagProfane,component:()=>Promise.all([n.e(3231),n.e(8048)]).then(n.bind(n,53286))},{flag:c.flagMultilingual,component:()=>Promise.all([n.e(3231),n.e(4069)]).then(n.bind(n,78831))},{flag:c.flagUnreliably,component:()=>Promise.all([n.e(3231),n.e(3417)]).then(n.bind(n,8179))},{flag:c.flagPolitical,component:()=>Promise.all([n.e(3231),n.e(5195)]).then(n.bind(n,30433))},{flag:c.flagCognitohazard,component:()=>Promise.all([n.e(3231),n.e(3175)]).then(n.bind(n,8413))},{flag:c.flagHidden,component:()=>Promise.all([n.e(3231),n.e(9556)]).then(n.bind(n,14794))}],{0:Q,1:O}=(0,i.useState)([]);return(0,i.useEffect)((()=>{q.forEach((e=>{let{flag:t,component:n}=e;t&&n().then((e=>{O((t=>[].concat((0,a.A)(t),[e.default])))}))}))}),[]),i.createElement(g.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},i.createElement(v.A,{postNumber:c.index,date:c.date,updated:c.updated,readTime:D,difficulty:c.difficultyLevel,title:c.title,desc:c.desc,banner:c.banner,section:S,postKey:T,isMindfuckery:c.flagMindfuckery,mainTag:c.mainTag}),i.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},c.otherTags.map(((e,t)=>i.createElement("span",{key:t,className:`noselect ${k.MW}`,style:{margin:"0 5px 5px 0"}},e)))),i.createElement("div",{className:"postBody"},i.createElement(H,{toc:h})),i.createElement("br"),i.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},i.createElement(g.P.button,{className:`noselect ${x.pb}`,id:x.xG,onClick:()=>{V(!I)},whileTap:{scale:.93}},i.createElement(g.P.div,{className:w.DJ,key:I,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},I?"Switch to default layout":"Switch to wide layout"))),i.createElement("br"),i.createElement("div",{className:"postBody",style:{margin:I?"0 -14%":"",maxWidth:I?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},i.createElement("div",{className:`${x.P_} ${R?x.Xn:x.qG}`},Q.map(((e,t)=>i.createElement(e,{key:t}))),c.indexCourse?i.createElement(E.A,{index:c.indexCourse,category:c.courseCategoryName}):"",i.createElement(u.Z.Provider,{value:{images:o.nodes,basePath:L.replace(/\/$/,"")+"/"}},i.createElement(r.xA,{components:{Image:p.A}},s)))),i.createElement(y.A,{nextPost:z,lastPost:M,keyCurrent:T,section:S}))}function _(e){return i.createElement(A,e,i.createElement(c,e))}function z(e){var t,n,a,r,l;let{data:o}=e;const{frontmatter:s}=o.mdx,c=s.titleSEO||s.title,h=s.titleOG||c,d=s.titleTwitter||c,u=s.descSEO||s.desc,p=s.descOG||u,g=s.descTwitter||u,v=s.schemaType||"BlogPosting",y=s.keywordsSEO,b=s.date,E=s.updated||b,w=s.imageOG||(null===(t=s.banner)||void 0===t||null===(n=t.childImageSharp)||void 0===n||null===(a=n.gatsbyImageData)||void 0===a||null===(r=a.images)||void 0===r||null===(l=r.fallback)||void 0===l?void 0:l.src),x=s.imageAltOG||p,S=s.imageTwitter||w,k=s.imageAltTwitter||g,H=s.canonicalURL,A=s.flagHidden||!1,_=s.mainTag||"Posts",z=s.slug.split("/")[1]||"posts",{siteUrl:M}=(0,m.Q)(),C={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:M},{"@type":"ListItem",position:2,name:_,item:`${M}/${s.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${M}${s.slug}`}]};return i.createElement(f.A,{title:c+" - avrtt.blog",titleOG:h,titleTwitter:d,description:u,descriptionOG:p,descriptionTwitter:g,schemaType:v,keywords:y,datePublished:b,dateModified:E,imageOG:w,imageAltOG:x,imageTwitter:S,imageAltTwitter:k,canonicalUrl:H,flagHidden:A,mainTag:_,section:z,type:"article"},i.createElement("script",{type:"application/ld+json"},JSON.stringify(C)))}},96098:function(e,t,n){var a=n(96540),r=n(7978);t.A=e=>{let{text:t}=e;return a.createElement(r.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-js-content-file-path-src-pages-posts-research-intro-to-rl-mdx-90e4afd318e906afbfde.js.map