"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[7081],{3962:function(e,t){t.A="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iaXNvLTg4NTktMSI/Pgo8IS0tIEdlbmVyYXRvcjogQWRvYmUgSWxsdXN0cmF0b3IgMTYuMC4wLCBTVkcgRXhwb3J0IFBsdWctSW4gLiBTVkcgVmVyc2lvbjogNi4wMCBCdWlsZCAwKSAgLS0+CjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+CjxzdmcgdmVyc2lvbj0iMS4xIiBpZD0iQ2FwYV8xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB4PSIwcHgiIHk9IjBweCIKCSB3aWR0aD0iNDE2Ljk3OXB4IiBoZWlnaHQ9IjQxNi45NzlweCIgdmlld0JveD0iMCAwIDQxNi45NzkgNDE2Ljk3OSIgc3R5bGU9ImVuYWJsZS1iYWNrZ3JvdW5kOm5ldyAwIDAgNDE2Ljk3OSA0MTYuOTc5OyIKCSB4bWw6c3BhY2U9InByZXNlcnZlIj4KPGc+Cgk8cGF0aCBkPSJNMzU2LjAwNCw2MS4xNTZjLTgxLjM3LTgxLjQ3LTIxMy4zNzctODEuNTUxLTI5NC44NDgtMC4xODJjLTgxLjQ3LDgxLjM3MS04MS41NTIsMjEzLjM3OS0wLjE4MSwyOTQuODUKCQljODEuMzY5LDgxLjQ3LDIxMy4zNzgsODEuNTUxLDI5NC44NDksMC4xODFDNDM3LjI5MywyNzQuNjM2LDQzNy4zNzUsMTQyLjYyNiwzNTYuMDA0LDYxLjE1NnogTTIzNy42LDM0MC43ODYKCQljMCwzLjIxNy0yLjYwNyw1LjgyMi01LjgyMiw1LjgyMmgtNDYuNTc2Yy0zLjIxNSwwLTUuODIyLTIuNjA1LTUuODIyLTUuODIyVjE2Ny44ODVjMC0zLjIxNywyLjYwNy01LjgyMiw1LjgyMi01LjgyMmg0Ni41NzYKCQljMy4yMTUsMCw1LjgyMiwyLjYwNCw1LjgyMiw1LjgyMlYzNDAuNzg2eiBNMjA4LjQ5LDEzNy45MDFjLTE4LjYxOCwwLTMzLjc2Ni0xNS4xNDYtMzMuNzY2LTMzLjc2NQoJCWMwLTE4LjYxNywxNS4xNDctMzMuNzY2LDMzLjc2Ni0zMy43NjZjMTguNjE5LDAsMzMuNzY2LDE1LjE0OCwzMy43NjYsMzMuNzY2QzI0Mi4yNTYsMTIyLjc1NSwyMjcuMTA3LDEzNy45MDEsMjA4LjQ5LDEzNy45MDF6Ii8+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPC9zdmc+Cg=="},37774:function(e,t,a){a.r(t),a.d(t,{Head:function(){return k},PostTemplate:function(){return H},default:function(){return C}});var n=a(54506),i=a(28453),l=a(96540),r=a(66501),s=a(16886),o=(a(46295),a(96098));function c(e){const t=Object.assign({p:"p",h2:"h2",a:"a",span:"span",h3:"h3",ol:"ol",li:"li",strong:"strong",ul:"ul",em:"em"},(0,i.RP)(),e.components),{Image:a}=t;return a||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),l.createElement(l.Fragment,null,"\n",l.createElement("br"),"\n","\n",l.createElement(t.p,null,"Machine learning is a subfield of artificial intelligence that focuses on creating algorithms capable of extracting patterns and insights from data — often with minimal explicit human instruction on how to solve a particular task. It is frequently described as being at the intersection of statistics, optimization, and computer science. Because ML models often discover complex, subtle relationships in large datasets, machine learning feels simultaneously daunting (",l.createElement(s.A,null,'it can be tough to "get right"'),") and magical (",l.createElement(s.A,null,"it powers many impressive applications"),")."),"\n",l.createElement(t.p,null,"At its core, ML is all about building models that generalize from past experience (training data) to new, unseen data. This is exciting because it opens the door to solving problems that are difficult or even impossible to tackle with traditional rule-based programming. However, the very flexibility that makes machine learning so powerful also makes it tricky: ML requires substantial care in choosing data, preparing features, selecting model architectures, and applying rigorous validation practices. Even accomplished practitioners acknowledge that learning how to do machine learning well can be quite challenging. Yet this difficulty also makes ML incredibly cool — there's a thrill in watching a model evolve from raw data to robust, near-human-level performance."),"\n",l.createElement(t.p,null,"Below, we'll introduce some key ML concepts that will form the basis for future chapters in this course. We'll discuss what data is in the ML context, how to evaluate models, why we use loss functions, and how to approach different paradigms of machine learning. We'll also give a brief overview of deep learning, widely regarded as one of the most powerful subsets of ML in recent years."),"\n",l.createElement(t.h2,{id:"fundamentals",style:{position:"relative"}},l.createElement(t.a,{href:"#fundamentals","aria-label":"fundamentals permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Fundamentals"),"\n",l.createElement(t.h3,{id:"general-ml-concepts--basics",style:{position:"relative"}},l.createElement(t.a,{href:"#general-ml-concepts--basics","aria-label":"general ml concepts  basics permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"General ml concepts & basics"),"\n",l.createElement(t.p,null,"At a high level, a ",l.createElement(s.A,null,"machine learning algorithm"),' can be viewed as a procedure that "learns" a relationship or rule mapping an input space ',l.createElement(o.A,{text:"\\(X\\)"})," to an output space ",l.createElement(o.A,{text:"\\(Y\\)"}),". For example, if you are predicting housing prices, your input space might be a set of features describing houses (like square footage, number of bedrooms, etc.), and your output space could be a continuous variable representing the price."),"\n",l.createElement(t.p,null,"Key ideas in machine learning:"),"\n",l.createElement(t.ol,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Data"),": ML algorithms rely on data to discover patterns, rather than using hard-coded rules."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameters"),": These are the variables an ML algorithm tunes automatically in order to improve its performance on the given data."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Generalization"),": The ability to apply insights learned from past data to new, unseen data."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Validation"),": Techniques for estimating how well a model is likely to perform on unseen data."),"\n"),"\n",l.createElement(t.h3,{id:"the-importance-of-data",style:{position:"relative"}},l.createElement(t.a,{href:"#the-importance-of-data","aria-label":"the importance of data permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"The importance of data"),"\n",l.createElement(t.p,null,"Data is the fuel powering nearly all machine learning. The more comprehensive and relevant your data, the better your model can be at capturing patterns and performing reliably on new inputs. In some sense, ",l.createElement(s.A,null,"data is everything"),": if you feed poor quality data — even the world's best ML algorithm might fail to generalize. Choosing, curating, and cleaning data are therefore among the most time-consuming yet essential parts of building real-world ML systems."),"\n",l.createElement(t.p,null,"From a theoretical standpoint, the statistical learning approach (Vapnik, 2000) often assumes that data points are drawn from an ",l.createElement(s.A,null,"i.i.d.")," (",l.createElement(r.A,{text:"Independently and Identically Distributed"})," ) distribution. In practice, data may not always follow these assumptions: real datasets often come from multiple sources and can contain biases or correlations. Nonetheless, striving for well-collected, representative data remains crucial."),"\n",l.createElement(t.h3,{id:"what-is-a-sample",style:{position:"relative"}},l.createElement(t.a,{href:"#what-is-a-sample","aria-label":"what is a sample permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"What is a sample"),"\n",l.createElement(t.p,null,"A ",l.createElement(s.A,null,"sample")," in the context of ML is just a single data point (or observation). This data point often appears as a row in a dataset, with columns representing different features of that point. For instance, a single sample of meteorological data might include temperature, humidity, and wind speed at a certain location and time."),"\n",l.createElement(t.p,null,"Because each sample is part of a broader population of interest, it's important that our sample distribution is somewhat reflective of the real-world conditions we expect. If the samples are unrepresentative — say all collected from only one region or date range — then the resulting model might fail to generalize elsewhere."),"\n",l.createElement(t.h3,{id:"train-test-and-validation-sets",style:{position:"relative"}},l.createElement(t.a,{href:"#train-test-and-validation-sets","aria-label":"train test and validation sets permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Train, test, and validation sets"),"\n",l.createElement(t.p,null,"A cornerstone practice in ML is dividing available data into different sets:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Training set"),": The set on which we directly train (fit) our model. The model sees these samples and adjusts its internal parameters accordingly."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Validation set"),": A set used to fine-tune choices like ",l.createElement(s.A,null,"hyperparameters"),", model selection, or even feature choices. The model does not update its parameters on this set, but we use it to evaluate performance across different configurations."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Test set"),": A final set used to measure the performance of the chosen model once all decisions have been made. The test set should be used ",l.createElement(s.A,null,"only once")," to get an unbiased estimate of how well the model generalizes."),"\n"),"\n",l.createElement(t.p,null,"When data is limited, practitioners often adopt specialized ",l.createElement(s.A,null,"cross-validation")," or other splitting schemes (discussed in more detail in Chapter 6) to more efficiently use all available data."),"\n",l.createElement(t.h3,{id:"features-design-matrix-and-feature-space",style:{position:"relative"}},l.createElement(t.a,{href:"#features-design-matrix-and-feature-space","aria-label":"features design matrix and feature space permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Features, design matrix, and feature space"),"\n",l.createElement(t.p,null,"A ",l.createElement(s.A,null,"feature")," refers to an individual measurable property or characteristic of a phenomenon being observed. Features typically populate the columns in a dataset. For example, if you have a dataset of houses, one feature might be the number of bedrooms; another might be the year the house was built."),"\n",l.createElement(t.p,null,"When these features are gathered into a matrix form (with rows representing samples and columns representing features), we call this the ",l.createElement(s.A,null,"design matrix"),". In mathematical terms, if we have ",l.createElement(o.A,{text:"\\(m\\)"})," samples and ",l.createElement(o.A,{text:"\\(n\\)"})," features, the design matrix ",l.createElement(o.A,{text:"\\(X\\)"})," is typically of shape ",l.createElement(o.A,{text:"\\((m \\times n)\\)"}),". Each row ",l.createElement(o.A,{text:"\\(x_i\\)"})," (for ",l.createElement(o.A,{text:"\\(i = 1, \\dots, m\\)"}),") corresponds to a single sample in an ",l.createElement(s.A,null,"n-dimensional feature space"),"."),"\n",l.createElement(t.h3,{id:"data-outliers",style:{position:"relative"}},l.createElement(t.a,{href:"#data-outliers","aria-label":"data outliers permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Data outliers"),"\n",l.createElement(t.p,null,l.createElement(s.A,null,"Outliers")," are data points that deviate markedly from the rest of your dataset. They can arise due to measurement errors, rare events, or even just natural variation. Outliers can heavily distort training, causing models to overemphasize these unusual points. Common strategies include removing or down-weighting outliers, transforming them, or using robust loss functions that are less sensitive to large errors. The decision of how to handle outliers depends on domain knowledge and the specific ML task at hand."),"\n",l.createElement(t.h2,{id:"the-model",style:{position:"relative"}},l.createElement(t.a,{href:"#the-model","aria-label":"the model permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"The model"),"\n",l.createElement(t.h3,{id:"what-is-a-model",style:{position:"relative"}},l.createElement(t.a,{href:"#what-is-a-model","aria-label":"what is a model permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"What is a model"),"\n",l.createElement(t.p,null,"In machine learning, a ",l.createElement(s.A,null,"model")," is a function or process that maps input data (features) to an output (such as a label or numerical value). This function is chosen from a set of possible functions known as the hypothesis space. Concretely, if we denote the feature vector for the ",l.createElement(o.A,{text:"\\(i\\)"}),"-th sample as ",l.createElement(o.A,{text:"\\(x_i\\)"}),", then a model might produce an output ",l.createElement(o.A,{text:"\\(a(x_i)\\)"}),"."),"\n",l.createElement(t.p,null,"Examples of ML models include:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"Linear or polynomial regression,"),"\n",l.createElement(t.li,null,"Decision trees,"),"\n",l.createElement(t.li,null,"Support vector machines,"),"\n",l.createElement(t.li,null,"Neural networks,"),"\n",l.createElement(t.li,null,"Random forests,"),"\n",l.createElement(t.li,null,"and many others."),"\n"),"\n",l.createElement(t.h3,{id:"model-as-a-decision-function-axᵢ",style:{position:"relative"}},l.createElement(t.a,{href:"#model-as-a-decision-function-ax%E1%B5%A2","aria-label":"model as a decision function axᵢ permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Model as a decision function a(xᵢ)"),"\n",l.createElement(t.p,null,"We often talk about a model ",l.createElement(o.A,{text:"\\(a(x)\\)"})," as a ",l.createElement(s.A,null,"decision function")," because it decides (or predicts) a value or category for a given input ",l.createElement(o.A,{text:"\\(x\\)"}),'. For classification tasks (e.g., "Is this email spam?"), the model outputs discrete classes (e.g., spam or not spam). For regression tasks (e.g., "What is the future stock price?"), the model outputs a continuous value (e.g., $150.00).'),"\n",l.createElement(t.p,null,"The relationship is typically parameterized. That is, if ",l.createElement(o.A,{text:"\\( \\theta \\)"})," is a set of parameters (weights, biases, etc.), the model can be written as:"),"\n",l.createElement(o.A,{text:"\\[\na_\\theta(x) = g(x, \\theta),\n\\]"}),"\n",l.createElement(t.p,null,"where ",l.createElement(o.A,{text:"\\( g(\\cdot) \\)"})," is the form of the function (e.g., a linear combination of features or a deep neural network)."),"\n",l.createElement(t.h3,{id:"model-validation",style:{position:"relative"}},l.createElement(t.a,{href:"#model-validation","aria-label":"model validation permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Model validation"),"\n",l.createElement(t.p,null,"Once a model is trained, we need to ask: ",l.createElement(s.A,null,'"Is the model good?"')," We assess this through model validation. The main idea is to evaluate how well the model performs on held-out data that was not used during training. This helps us gauge if the model's decisions generalize beyond the training set. Common practices for validation include:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"Using a simple train/validation/test split."),"\n",l.createElement(t.li,null,"Cross-validation with multiple folds."),"\n",l.createElement(t.li,null,"Other specialized methods (for example, time-series splits)."),"\n"),"\n",l.createElement(t.h3,{id:"overfitting",style:{position:"relative"}},l.createElement(t.a,{href:"#overfitting","aria-label":"overfitting permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Overfitting"),"\n",l.createElement(t.p,null,l.createElement(s.A,null,"Overfitting"),' occurs when a model learns not only the underlying signal but also the random noise present in the training data. In effect, the model "memorizes" the training set rather than capturing generalizable patterns. Overfit models perform extremely well on training data but often fail dismally on unseen data.'),"\n",l.createElement(t.p,null,"Formally, overfitting can be seen as reducing training loss without a corresponding reduction in ",l.createElement(s.A,null,"expected error"),". Modern datasets, especially large high-dimensional ones, can be prone to overfitting, so techniques like regularization (see my ",l.createElement(t.a,{href:"/research/regularization"},'post called "Regularization"'),"), proper validation, or simplifying the model can mitigate it (Blum, NeurIPS 2021)."),"\n",l.createElement(t.h3,{id:"hyperparameters",style:{position:"relative"}},l.createElement(t.a,{href:"#hyperparameters","aria-label":"hyperparameters permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Hyperparameters"),"\n",l.createElement(t.p,null,"Unlike parameters (which a model learns during training), ",l.createElement(s.A,null,"hyperparameters")," are configuration settings selected prior to the learning process. Examples include:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"Learning rate in gradient-based optimization,"),"\n",l.createElement(t.li,null,"The number of layers or hidden units in a neural network,"),"\n",l.createElement(t.li,null,"Regularization strength,"),"\n",l.createElement(t.li,null,"The maximum depth of a decision tree."),"\n"),"\n",l.createElement(t.p,null,"One crucial step in building an ML model is ",l.createElement(s.A,null,"hyperparameter tuning"),": systematically searching for hyperparameters that yield strong validation performance."),"\n",l.createElement(t.h2,{id:"loss-function-and-empirical-risk",style:{position:"relative"}},l.createElement(t.a,{href:"#loss-function-and-empirical-risk","aria-label":"loss function and empirical risk permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Loss function and empirical risk"),"\n",l.createElement(t.h3,{id:"understanding-the-loss-function",style:{position:"relative"}},l.createElement(t.a,{href:"#understanding-the-loss-function","aria-label":"understanding the loss function permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Understanding the loss function"),"\n",l.createElement(t.p,null,"A ",l.createElement(s.A,null,"loss function")," measures how far a model's predictions deviate from the actual target values. For a single data point ",l.createElement(o.A,{text:"\\( (x_i, y_i) \\)"}),", we might define a loss function ",l.createElement(o.A,{text:"\\(L(a(x_i), y_i)\\)"}),". A common example in regression is the squared error loss:"),"\n",l.createElement(o.A,{text:"\\[\nL(a(x_i), y_i) = \\left(a(x_i) - y_i\\right)^2.\n\\]"}),"\n",l.createElement(t.p,null,"Here, ",l.createElement(o.A,{text:"\\(y_i\\)"})," is the true label and ",l.createElement(o.A,{text:"\\(a(x_i)\\)"})," is the model's prediction. The larger the difference, the bigger the loss. For classification, a typical loss function is cross-entropy, which penalizes deviations in predicted probabilities of classes."),"\n",l.createElement(t.h3,{id:"empirical-risk-minimization",style:{position:"relative"}},l.createElement(t.a,{href:"#empirical-risk-minimization","aria-label":"empirical risk minimization permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Empirical risk minimization"),"\n",l.createElement(t.p,null,"For a dataset of ",l.createElement(o.A,{text:"\\(m\\)"})," samples, the average of the loss over all training examples is called the ",l.createElement(s.A,null,"empirical risk"),":"),"\n",l.createElement(o.A,{text:"\\[\nR_{\\text{emp}}(\\theta) = \\frac{1}{m} \\sum_{i=1}^m L(a_\\theta(x_i), y_i).\n\\]"}),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(o.A,{text:"\\( \\theta \\)"}),": Model parameters (the quantity we optimize)."),"\n",l.createElement(t.li,null,l.createElement(o.A,{text:"\\( L(\\cdot) \\)"}),": The per-sample loss function."),"\n",l.createElement(t.li,null,l.createElement(o.A,{text:"\\( m \\)"}),": Number of training samples."),"\n"),"\n",l.createElement(t.p,null,"In ",l.createElement(s.A,null,"empirical risk minimization"),", we choose the parameters ",l.createElement(o.A,{text:"\\( \\theta \\)"})," that minimize ",l.createElement(o.A,{text:"\\(R_{\\text{emp}}(\\theta)\\)"}),". This principle underlies many standard ML training algorithms, from linear regression to deep neural networks. For deeper theoretical treatments, see ",l.createElement(t.em,null,"Statistical Learning Theory")," (Vapnik, 2000) or ",l.createElement(t.em,null,"Mathematics for ML, Chapter 8")," (mml-book, 2019)."),"\n",l.createElement(t.h2,{id:"steps-of-any-ml-problem",style:{position:"relative"}},l.createElement(t.a,{href:"#steps-of-any-ml-problem","aria-label":"steps of any ml problem permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Steps of any ml problem"),"\n",l.createElement(t.h3,{id:"define-the-loss-function-defining-the-problem",style:{position:"relative"}},l.createElement(t.a,{href:"#define-the-loss-function-defining-the-problem","aria-label":"define the loss function defining the problem permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Define the loss function (defining the problem)"),"\n",l.createElement(t.p,null,"The first step in tackling an ML problem is to ",l.createElement(s.A,null,"frame the problem"),". We must decide what we are trying to predict or classify, and how mistakes are measured. This is typically embodied by the choice of a loss function. For instance, if you are predicting stock prices, you might care about the mean squared error (MSE). If you are classifying spam vs. non-spam emails, you might use cross-entropy to penalize incorrect probability estimates."),"\n",l.createElement(t.h3,{id:"get-data",style:{position:"relative"}},l.createElement(t.a,{href:"#get-data","aria-label":"get data permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Get data"),"\n",l.createElement(t.p,null,"Next, we gather and prepare data. This can involve:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"Collecting data from various sources (databases, public datasets, APIs)."),"\n",l.createElement(t.li,null,"Cleaning data (fixing missing values, removing duplicates)."),"\n",l.createElement(t.li,null,"Engineer additional features, if needed."),"\n",l.createElement(t.li,null,"Split into train, validation, and test sets (or use cross-validation strategies)."),"\n"),"\n",l.createElement(t.p,null,"Practitioners often say they spend 80% of the time ",l.createElement(s.A,null,"cleaning and wrangling data"),", illustrating that real-world data is rarely neat or fully representative."),"\n",l.createElement(t.h3,{id:"build-the-model-find-parameters-minimizing-empirical-risk",style:{position:"relative"}},l.createElement(t.a,{href:"#build-the-model-find-parameters-minimizing-empirical-risk","aria-label":"build the model find parameters minimizing empirical risk permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Build the model (find parameters minimizing empirical risk)"),"\n",l.createElement(t.p,null,"After the loss function is determined and data is prepared, we choose a model form, initialize the parameters, and train it by minimizing the empirical risk. Various optimization algorithms (like stochastic gradient descent) can be employed. After training multiple candidate models — each with possibly different hyperparameters — we select the best performer based on validation scores."),"\n",l.createElement(t.h2,{id:"validation-techniques",style:{position:"relative"}},l.createElement(t.a,{href:"#validation-techniques","aria-label":"validation techniques permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Validation techniques"),"\n",l.createElement(t.h3,{id:"why-validation-matters",style:{position:"relative"}},l.createElement(t.a,{href:"#why-validation-matters","aria-label":"why validation matters permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Why validation matters"),"\n",l.createElement(t.p,null,"Without an unbiased estimate of how a model will perform on unseen data, we can easily fool ourselves into believing the model is better than it actually is. Overfitting is a prime risk here. ",l.createElement(s.A,null,"Validation")," helps us:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"Compare different models or hyperparameter choices."),"\n",l.createElement(t.li,null,"Detect overfitting or data leakage."),"\n",l.createElement(t.li,null,"Guide further iterations of feature engineering."),"\n"),"\n",l.createElement(t.h3,{id:"cross-validation-basics",style:{position:"relative"}},l.createElement(t.a,{href:"#cross-validation-basics","aria-label":"cross validation basics permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Cross-validation basics"),"\n",l.createElement(t.p,null,l.createElement(s.A,null,"Cross-validation")," is a strategy for making better use of limited data and getting more reliable estimates of model performance. The fundamental idea is to systematically partition the data into multiple splits, train on some partitions, and validate on the remaining ones."),"\n",l.createElement(t.h3,{id:"k-fold-cross-validation",style:{position:"relative"}},l.createElement(t.a,{href:"#k-fold-cross-validation","aria-label":"k fold cross validation permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"K-fold cross-validation"),"\n",l.createElement(t.p,null,"In ",l.createElement(s.A,null,"K-fold cross-validation"),", we split the dataset into ",l.createElement(o.A,{text:"\\(K\\)"}),' roughly equal "folds":'),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"For each fold ",l.createElement(o.A,{text:"\\(k\\)"}),", use fold ",l.createElement(o.A,{text:"\\(k\\)"})," as the validation set and train on all other folds."),"\n",l.createElement(t.li,null,"Evaluate the performance metric for that fold."),"\n",l.createElement(t.li,null,"Average the performance metrics across all ",l.createElement(o.A,{text:"\\(K\\)"})," folds."),"\n"),"\n",l.createElement(t.p,null,"This method is widely used in research and practice because it provides stable performance estimates, especially for smaller datasets."),"\n",l.createElement(a,{alt:"Illustration of K-fold cross-validation",path:"",caption:"K-fold cross-validation partitions the dataset into multiple folds, systematically training and validating on distinct portions of the data.",zoom:"false"}),"\n",l.createElement(t.h3,{id:"shufflesplit",style:{position:"relative"}},l.createElement(t.a,{href:"#shufflesplit","aria-label":"shufflesplit permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Shufflesplit"),"\n",l.createElement(t.p,null,l.createElement(s.A,null,"ShuffleSplit")," repeatedly randomly partitions the dataset into train and validation sets. Unlike K-fold, which partitions the dataset into sequential folds, ShuffleSplit relies on random sampling for each iteration, giving multiple random splits. This can be helpful when data is large or when you suspect that simple partitioning might not thoroughly capture variability."),"\n",l.createElement(t.h3,{id:"stratified-approaches",style:{position:"relative"}},l.createElement(t.a,{href:"#stratified-approaches","aria-label":"stratified approaches permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Stratified approaches"),"\n",l.createElement(t.p,null,"When dealing with classification tasks where classes are imbalanced, stratification ensures that each split has approximately the same class proportions as the original dataset. ",l.createElement(s.A,null,"Stratified K-fold")," is a popular technique in this category: each fold maintains the overall class distribution, preventing folds from inadvertently containing only majority or only minority classes."),"\n",l.createElement(t.h3,{id:"group-based-splits",style:{position:"relative"}},l.createElement(t.a,{href:"#group-based-splits","aria-label":"group based splits permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Group-based splits"),"\n",l.createElement(t.p,null,"Sometimes, data is grouped in ways that must be respected. For example, if you have multiple images or measurements from the same person, you want to keep all data from that person together in either the training set or the validation set (to avoid overestimating real-world performance). ",l.createElement(s.A,null,"Group-based cross-validation")," handles this by ensuring that no group appears in both training and validation partitions."),"\n",l.createElement(t.h2,{id:"types-of-machine-learning",style:{position:"relative"}},l.createElement(t.a,{href:"#types-of-machine-learning","aria-label":"types of machine learning permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Types of machine learning"),"\n",l.createElement(t.h3,{id:"supervised-learning",style:{position:"relative"}},l.createElement(t.a,{href:"#supervised-learning","aria-label":"supervised learning permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Supervised learning"),"\n",l.createElement(t.p,null,l.createElement(s.A,null,"Supervised learning")," focuses on learning from labeled examples. Each training sample has an associated ground truth label. Typical tasks:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Classification"),": Predict a discrete category (spam or not spam)."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Regression"),": Predict a continuous quantity (house prices)."),"\n"),"\n",l.createElement(t.p,null,"Supervised learning is popular for many real-world applications, such as image recognition, fraud detection, and language translation."),"\n",l.createElement(t.h3,{id:"unsupervised-learning",style:{position:"relative"}},l.createElement(t.a,{href:"#unsupervised-learning","aria-label":"unsupervised learning permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Unsupervised learning"),"\n",l.createElement(t.p,null,l.createElement(s.A,null,"Unsupervised learning")," has no labeled data. It focuses on discovering structure in the data itself. Common tasks:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Clustering")," (e.g., K-means, DBSCAN): Group samples into meaningful clusters."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Dimensionality reduction")," (e.g., PCA): Compress data while preserving its principal structures."),"\n"),"\n",l.createElement(t.p,null,"Unsupervised techniques can reveal hidden patterns that might otherwise remain undetected."),"\n",l.createElement(t.h3,{id:"other-paradigms-reinforcement-semi-supervised-etc",style:{position:"relative"}},l.createElement(t.a,{href:"#other-paradigms-reinforcement-semi-supervised-etc","aria-label":"other paradigms reinforcement semi supervised etc permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Other paradigms (reinforcement, semi-supervised, etc.)"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(s.A,null,"Reinforcement Learning")," (RL): An agent interacts with an environment and learns behaviors that maximize cumulative rewards. RL is behind recent breakthroughs in robotics and game-playing AI (e.g., AlphaGo)."),"\n",l.createElement(t.li,null,l.createElement(s.A,null,"Semi-supervised learning"),": Some data is labeled, most is unlabeled. The algorithm exploits both labeled and unlabeled data."),"\n",l.createElement(t.li,null,l.createElement(s.A,null,"Self-supervised learning"),": Generates labels from the data itself (e.g., next-word prediction tasks in large language models)."),"\n"),"\n",l.createElement(t.p,null,"In industry, semi-supervised or self-supervised methods are especially appealing when acquiring labeled data is expensive or time-consuming, but unlabeled data is abundant."),"\n",l.createElement(t.h2,{id:"deep-learning-in-brief",style:{position:"relative"}},l.createElement(t.a,{href:"#deep-learning-in-brief","aria-label":"deep learning in brief permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Deep learning in brief"),"\n",l.createElement(t.h3,{id:"what-is-deep-learning",style:{position:"relative"}},l.createElement(t.a,{href:"#what-is-deep-learning","aria-label":"what is deep learning permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"What is deep learning"),"\n",l.createElement(t.p,null,l.createElement(s.A,null,"Deep learning")," refers to ML methods — primarily neural networks — that use multiple layers of transformations (often dozens or even hundreds of layers). These stacked layers of neurons can learn more abstract features at higher layers, which has proven extremely effective in tasks like image recognition, natural language processing, and speech recognition (LeCun, Bengio, and Hinton, 2015). Modern success stories include GPT-based language models, large-scale vision models, and real-time generative models."),"\n",l.createElement(t.h3,{id:"relationship-between-ml-and-deep-learning",style:{position:"relative"}},l.createElement(t.a,{href:"#relationship-between-ml-and-deep-learning","aria-label":"relationship between ml and deep learning permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Relationship between ml and deep learning"),"\n",l.createElement(t.p,null,"Deep learning is not the entirety of machine learning, but rather a powerful subset. Traditional ML often relies on ",l.createElement(s.A,null,"hand-crafted feature engineering"),", while deep learning attempts to discover useful features automatically through layered representations. Despite deep networks' impressive performance and popularity, they are not always the best solution — particularly when data is limited or interpretability is paramount."),"\n",l.createElement(t.h2,{id:"conclusion-and-next-steps",style:{position:"relative"}},l.createElement(t.a,{href:"#conclusion-and-next-steps","aria-label":"conclusion and next steps permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Conclusion and next steps"),"\n",l.createElement(t.p,null,"This introduction is meant to set the stage for deeper dives into the technical aspects of machine learning throughout the rest of the course. We have covered:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"The fundamentals of ML and why data underpins everything."),"\n",l.createElement(t.li,null,"The notion of a model, how we measure its performance using loss functions, and how we validate it to ensure generalization."),"\n",l.createElement(t.li,null,"The standard taxonomy of supervised vs. unsupervised (plus hybrid paradigms like semi-supervised and reinforcement learning)."),"\n",l.createElement(t.li,null,"An overview of deep learning's place within the broader ML landscape."),"\n"),"\n",l.createElement(t.p,null,"From here, we will progress into more focused areas: linear and logistic regression, advanced regularization strategies, decision trees, and beyond. We will explore how the mathematics of linear algebra, calculus, and probability tie into the design, training, and evaluation of these models. Over time, we'll build a complete toolkit for constructing state-of-the-art predictive models and understanding their performance in depth."),"\n",l.createElement(t.p,null,"Stay motivated: machine learning can be difficult, but learning it thoroughly is one of the most exciting — and rewarding — skills in modern data science!"))}var m=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,i.RP)(),e.components);return t?l.createElement(t,e,l.createElement(c,e)):c(e)};var d=a(36710),h=a(58481),u=a.n(h),p=a(36310),g=a(87245),f=a(27042),v=a(59849),E=a(5591),y=a(61122),b=a(9219),w=a(33203),S=a(95751),x=a(94328),M=a(80791),z=a(78137);const L=e=>{let{toc:t}=e;if(!t||!t.items)return null;return l.createElement("nav",{className:M.R},l.createElement("ul",null,t.items.map(((e,t)=>l.createElement("li",{key:t},l.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const a=t.replace("#",""),n=document.getElementById(a);n&&n.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&l.createElement(L,{toc:{items:e.items}}))))))};function H(e){let{data:{mdx:t,allMdx:r,allPostImages:s},children:o}=e;const{frontmatter:c,body:m,tableOfContents:d}=t,h=c.index,v=c.slug.split("/")[1],M=r.nodes.filter((e=>e.frontmatter.slug.includes(`/${v}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),H=M.findIndex((e=>e.frontmatter.index===h)),C=M[H+1],k=M[H-1],T=c.slug.replace(/\/$/,""),N=/[^/]*$/.exec(T)[0],A=`posts/${v}/content/${N}/`,{0:I,1:_}=(0,l.useState)(c.flagWideLayoutByDefault),{0:j,1:V}=(0,l.useState)(!1);var B;(0,l.useEffect)((()=>{V(!0);const e=setTimeout((()=>V(!1)),340);return()=>clearTimeout(e)}),[I]),"adventures"===v?B=b.cb:"research"===v?B=b.Qh:"thoughts"===v&&(B=b.T6);const D=u()(m).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,P=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),a=e%60;return a<=30?`~${t}${a>0?".5":""} h`:`~${t+1} h`}(Math.ceil(D/B)+(c.extraReadTimeMin||0)),O=[{flag:c.flagDraft,component:()=>Promise.all([a.e(3231),a.e(8809)]).then(a.bind(a,28809))},{flag:c.flagMindfuckery,component:()=>Promise.all([a.e(3231),a.e(2471)]).then(a.bind(a,67709))},{flag:c.flagRewrite,component:()=>Promise.all([a.e(3231),a.e(6764)]).then(a.bind(a,62002))},{flag:c.flagOffensive,component:()=>Promise.all([a.e(3231),a.e(2443)]).then(a.bind(a,17681))},{flag:c.flagProfane,component:()=>Promise.all([a.e(3231),a.e(8048)]).then(a.bind(a,53286))},{flag:c.flagMultilingual,component:()=>Promise.all([a.e(3231),a.e(4069)]).then(a.bind(a,78831))},{flag:c.flagUnreliably,component:()=>Promise.all([a.e(3231),a.e(3417)]).then(a.bind(a,8179))},{flag:c.flagPolitical,component:()=>Promise.all([a.e(3231),a.e(5195)]).then(a.bind(a,30433))},{flag:c.flagCognitohazard,component:()=>Promise.all([a.e(3231),a.e(3175)]).then(a.bind(a,8413))},{flag:c.flagHidden,component:()=>Promise.all([a.e(3231),a.e(9556)]).then(a.bind(a,14794))}],{0:G,1:W}=(0,l.useState)([]);return(0,l.useEffect)((()=>{O.forEach((e=>{let{flag:t,component:a}=e;t&&a().then((e=>{W((t=>[].concat((0,n.A)(t),[e.default])))}))}))}),[]),l.createElement(f.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},l.createElement(E.A,{postNumber:c.index,date:c.date,updated:c.updated,readTime:P,difficulty:c.difficultyLevel,title:c.title,desc:c.desc,banner:c.banner,section:v,postKey:N,isMindfuckery:c.flagMindfuckery,mainTag:c.mainTag}),l.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},c.otherTags.map(((e,t)=>l.createElement("span",{key:t,className:`noselect ${z.MW}`,style:{margin:"0 5px 5px 0"}},e)))),l.createElement("div",{className:"postBody"},l.createElement(L,{toc:d})),l.createElement("br"),l.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},l.createElement(f.P.button,{className:`noselect ${x.pb}`,id:x.xG,onClick:()=>{_(!I)},whileTap:{scale:.93}},l.createElement(f.P.div,{className:S.DJ,key:I,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},I?"Switch to default layout":"Switch to wide layout"))),l.createElement("br"),l.createElement("div",{className:"postBody",style:{margin:I?"0 -14%":"",maxWidth:I?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},l.createElement("div",{className:`${x.P_} ${j?x.Xn:x.qG}`},G.map(((e,t)=>l.createElement(e,{key:t}))),c.indexCourse?l.createElement(w.A,{index:c.indexCourse,category:c.courseCategoryName}):"",l.createElement(p.Z.Provider,{value:{images:s.nodes,basePath:A.replace(/\/$/,"")+"/"}},l.createElement(i.xA,{components:{Image:g.A}},o)))),l.createElement(y.A,{nextPost:C,lastPost:k,keyCurrent:N,section:v}))}function C(e){return l.createElement(H,e,l.createElement(m,e))}function k(e){var t,a,n,i,r;let{data:s}=e;const{frontmatter:o}=s.mdx,c=o.titleSEO||o.title,m=o.titleOG||c,h=o.titleTwitter||c,u=o.descSEO||o.desc,p=o.descOG||u,g=o.descTwitter||u,f=o.schemaType||"BlogPosting",E=o.keywordsSEO,y=o.date,b=o.updated||y,w=o.imageOG||(null===(t=o.banner)||void 0===t||null===(a=t.childImageSharp)||void 0===a||null===(n=a.gatsbyImageData)||void 0===n||null===(i=n.images)||void 0===i||null===(r=i.fallback)||void 0===r?void 0:r.src),S=o.imageAltOG||p,x=o.imageTwitter||w,M=o.imageAltTwitter||g,z=o.canonicalURL,L=o.flagHidden||!1,H=o.mainTag||"Posts",C=o.slug.split("/")[1]||"posts",{siteUrl:k}=(0,d.Q)(),T={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:k},{"@type":"ListItem",position:2,name:H,item:`${k}/${o.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${k}${o.slug}`}]};return l.createElement(v.A,{title:c+" - avrtt.blog",titleOG:m,titleTwitter:h,description:u,descriptionOG:p,descriptionTwitter:g,schemaType:f,keywords:E,datePublished:y,dateModified:b,imageOG:w,imageAltOG:S,imageTwitter:x,imageAltTwitter:M,canonicalUrl:z,flagHidden:L,mainTag:H,section:C,type:"article"},l.createElement("script",{type:"application/ld+json"},JSON.stringify(T)))}},66501:function(e,t,a){a.d(t,{A:function(){return r}});var n=a(96540),i=a(3962),l="styles-module--tooltiptext--a263b";var r=e=>{let{text:t,isBadge:a=!1}=e;const{0:r,1:s}=(0,n.useState)(!1),o=(0,n.useRef)(null);return(0,n.useEffect)((()=>{function e(e){o.current&&!o.current.contains(e.target)&&s(!1)}return document.addEventListener("click",e),()=>{document.removeEventListener("click",e)}}),[]),n.createElement("span",{className:"styles-module--tooltipWrapper--75ebf",ref:o},n.createElement("img",{id:a?"styles-module--infoBadge--e3d66":"styles-module--info--26c1f",src:i.A,alt:"info",onClick:e=>{e.stopPropagation(),s((e=>!e))}}),n.createElement("span",{className:r?`${l} styles-module--visible--c063c`:l},t))}},96098:function(e,t,a){var n=a(96540),i=a(7978);t.A=e=>{let{text:t}=e;return n.createElement(i.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-js-content-file-path-src-pages-posts-research-introduction-to-machine-learning-mdx-879912da4c3b957a2712.js.map