"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[7354],{3962:function(e,t){t.A="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iaXNvLTg4NTktMSI/Pgo8IS0tIEdlbmVyYXRvcjogQWRvYmUgSWxsdXN0cmF0b3IgMTYuMC4wLCBTVkcgRXhwb3J0IFBsdWctSW4gLiBTVkcgVmVyc2lvbjogNi4wMCBCdWlsZCAwKSAgLS0+CjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+CjxzdmcgdmVyc2lvbj0iMS4xIiBpZD0iQ2FwYV8xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB4PSIwcHgiIHk9IjBweCIKCSB3aWR0aD0iNDE2Ljk3OXB4IiBoZWlnaHQ9IjQxNi45NzlweCIgdmlld0JveD0iMCAwIDQxNi45NzkgNDE2Ljk3OSIgc3R5bGU9ImVuYWJsZS1iYWNrZ3JvdW5kOm5ldyAwIDAgNDE2Ljk3OSA0MTYuOTc5OyIKCSB4bWw6c3BhY2U9InByZXNlcnZlIj4KPGc+Cgk8cGF0aCBkPSJNMzU2LjAwNCw2MS4xNTZjLTgxLjM3LTgxLjQ3LTIxMy4zNzctODEuNTUxLTI5NC44NDgtMC4xODJjLTgxLjQ3LDgxLjM3MS04MS41NTIsMjEzLjM3OS0wLjE4MSwyOTQuODUKCQljODEuMzY5LDgxLjQ3LDIxMy4zNzgsODEuNTUxLDI5NC44NDksMC4xODFDNDM3LjI5MywyNzQuNjM2LDQzNy4zNzUsMTQyLjYyNiwzNTYuMDA0LDYxLjE1NnogTTIzNy42LDM0MC43ODYKCQljMCwzLjIxNy0yLjYwNyw1LjgyMi01LjgyMiw1LjgyMmgtNDYuNTc2Yy0zLjIxNSwwLTUuODIyLTIuNjA1LTUuODIyLTUuODIyVjE2Ny44ODVjMC0zLjIxNywyLjYwNy01LjgyMiw1LjgyMi01LjgyMmg0Ni41NzYKCQljMy4yMTUsMCw1LjgyMiwyLjYwNCw1LjgyMiw1LjgyMlYzNDAuNzg2eiBNMjA4LjQ5LDEzNy45MDFjLTE4LjYxOCwwLTMzLjc2Ni0xNS4xNDYtMzMuNzY2LTMzLjc2NQoJCWMwLTE4LjYxNywxNS4xNDctMzMuNzY2LDMzLjc2Ni0zMy43NjZjMTguNjE5LDAsMzMuNzY2LDE1LjE0OCwzMy43NjYsMzMuNzY2QzI0Mi4yNTYsMTIyLjc1NSwyMjcuMTA3LDEzNy45MDEsMjA4LjQ5LDEzNy45MDF6Ii8+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPC9zdmc+Cg=="},54106:function(e,t,n){n.r(t),n.d(t,{Head:function(){return N},PostTemplate:function(){return C},default:function(){return L}});var a=n(54506),i=n(28453),l=n(96540),r=n(66501),s=n(16886),o=n(46295),c=n(96098);function m(e){const t=Object.assign({p:"p",em:"em",h3:"h3",a:"a",span:"span",ul:"ul",li:"li",h2:"h2",strong:"strong",br:"br",ol:"ol"},(0,i.RP)(),e.components),{Image:n}=t;return n||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),l.createElement(l.Fragment,null,l.createElement(t.p,null,l.createElement(t.em,null,"According to statistics, people who have pets are happier than those who have had a blood clot break off.")),"\n",l.createElement("br"),"\n","\n","\n",l.createElement(t.p,null,"Statistics is the field dedicated to collecting, analyzing, interpreting, and presenting data with the goal of making informed decisions or uncovering underlying patterns. At its core, it provides frameworks and methods to deal with uncertainty and variability inherent in real-world phenomena. In ",l.createElement(s.A,null,"data science")," and ",l.createElement(s.A,null,"machine learning"),", statistics underpins everything from exploratory data analysis to hypothesis testing, model validation, and beyond. For instance, key statistical concepts ensure that our predictive models are robust, unbiased, and generalize well to unseen data."),"\n",l.createElement(t.p,null,'Beyond "number crunching," statistics often involves formulating scientific or business questions in a way that can be tested with data. In this sense, a ',l.createElement(s.A,null,"statistic")," can be seen as a function that maps sample data to some numerical summary. For example, the sample mean is a statistic that summarizes central tendency, and the sample variance is a statistic that measures variability. These statistics are the building blocks of deeper statistical inference that helps us navigate a world dominated by incomplete information."),"\n",l.createElement(t.h3,{id:"why-we-care-about-randomness-and-variability",style:{position:"relative"}},l.createElement(t.a,{href:"#why-we-care-about-randomness-and-variability","aria-label":"why we care about randomness and variability permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Why we care about randomness and variability"),"\n",l.createElement(t.p,null,"One fundamental reason statistics is so crucial lies in how it helps us handle and make sense of randomness. In everyday life, many events occur with some degree of uncertainty — rolling dice, predicting the weather, or even modeling fluctuations in the stock market. Statistics provides the tools for drawing conclusions from incomplete or noisy data. For example:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"A meteorologist analyzing years of historical temperature data to predict future weather patterns."),"\n",l.createElement(t.li,null,"A healthcare professional determining the effectiveness of a new medication based on clinical trial results."),"\n",l.createElement(t.li,null,"A machine learning engineer tuning models based on performance metrics across different datasets."),"\n"),"\n",l.createElement(t.p,null,"In all these cases, randomness and variability are at play, and statistics offers a systematic way to measure, model, and reduce uncertainties in our conclusions."),"\n",l.createElement(t.h2,{id:"fundamentals-of-probability-theory",style:{position:"relative"}},l.createElement(t.a,{href:"#fundamentals-of-probability-theory","aria-label":"fundamentals of probability theory permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Fundamentals of probability theory"),"\n",l.createElement(t.p,null,"While statistics uses data to infer properties about populations, probability theory provides the mathematical language to describe how data might be generated in the first place. Together, these fields form the foundation for most methods in data science and machine learning. Modern ML models rely heavily on probabilistic thinking, from understanding how likely an event is to Bayesian updating of model parameters."),"\n",l.createElement(t.h3,{id:"basic-definitions-and-set-operations",style:{position:"relative"}},l.createElement(t.a,{href:"#basic-definitions-and-set-operations","aria-label":"basic definitions and set operations permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Basic definitions and set operations"),"\n",l.createElement(t.p,null,"At the core of probability theory are events, outcomes, and the sample space:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Sample space (",l.createElement(c.A,{text:"\\(\\Omega\\)"}),")"),": The set of all possible outcomes of an experiment."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Event (A)"),": A subset of outcomes in the sample space."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Probability (P(A))"),": A value between 0 and 1 that quantifies how likely it is that event ",l.createElement(c.A,{text:"\\(A\\)"})," occurs."),"\n"),"\n",l.createElement(t.p,null,"Often, Venn diagrams help illustrate how events intersect (",l.createElement(c.A,{text:"\\(A \\cap B\\)"}),"), unite (",l.createElement(c.A,{text:"\\(A \\cup B\\)"}),"), or complement (e.g., ",l.createElement(c.A,{text:"\\(A^c\\)"}),' is "not ',l.createElement(c.A,{text:"\\(A\\)"}),'"). For example, if ',l.createElement(c.A,{text:"\\(A\\)"}),' is the event "roll an even number on a six-sided die," and ',l.createElement(c.A,{text:"\\(B\\)"}),' is the event "roll a number greater than 3," then:'),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(c.A,{text:"\\(A \\cap B\\)"}),' is the event "roll a number that is both even and greater than 3," i.e., ',l.createElement(c.A,{text:"\\(\\{4,6\\}\\)"}),"."),"\n",l.createElement(t.li,null,l.createElement(c.A,{text:"\\(A \\cup B\\)"}),' is "roll an even number or a number greater than 3," i.e., ',l.createElement(c.A,{text:"\\(\\{2,4,5,6\\}\\)"}),"."),"\n"),"\n",l.createElement(t.h3,{id:"expected-value-and-the-law-of-large-numbers",style:{position:"relative"}},l.createElement(t.a,{href:"#expected-value-and-the-law-of-large-numbers","aria-label":"expected value and the law of large numbers permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Expected value and the law of large numbers"),"\n",l.createElement(t.p,null,"A ",l.createElement(s.A,null,"random variable")," is a variable whose possible values are numerical outcomes of a random phenomenon. The ",l.createElement(s.A,null,"expected value")," (or mean) of a random variable ",l.createElement(c.A,{text:"\\(X\\)"})," is the long-run average outcome we'd expect if we could repeat the underlying process infinitely many times. Formally, for a discrete random variable ",l.createElement(c.A,{text:"\\(X\\)"})," that takes values ",l.createElement(c.A,{text:"\\(x_i\\)"})," with probability ",l.createElement(c.A,{text:"\\(p_i\\)"}),":"),"\n",l.createElement(c.A,{text:"\\[\nE[X] = \\sum_i x_i p_i,\n\\]"}),"\n",l.createElement(t.p,null,"where ",l.createElement(c.A,{text:"\\(E[X]\\)"})," denotes the expected value of ",l.createElement(c.A,{text:"\\(X\\)"}),". For a continuous random variable, the sum becomes an integral of ",l.createElement(c.A,{text:"\\(x\\)"})," against its probability density function."),"\n",l.createElement(t.p,null,"The ",l.createElement(s.A,null,"law of large numbers")," tells us that as the sample size increases, the average of the sample outcomes converges to the expected value of the population. For example, if you repeatedly roll a fair six-sided die, the average of the observed values will approach 3.5 as the number of rolls grows larger."),"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Illustration"),":",l.createElement(t.br),"\n","Imagine rolling two dice 20 times each, then plotting the running average of the sums after each roll. You'll see the running average fluctuate initially, but it will tend to settle around the theoretical mean of 7 as the number of rolls gets large. This visual demonstration can be done with a short Python script:"),"\n",l.createElement(o.A,{text:'\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\nnum_rolls = 500\nroll_sums = []\n\ncurrent_sum = 0\nfor i in range(num_rolls):\n    dice_sum = np.random.randint(1, 7) + np.random.randint(1, 7)\n    current_sum += dice_sum\n    roll_sums.append(current_sum / (i + 1))\n\nplt.plot(roll_sums, label="Running Average of Dice Sums")\nplt.axhline(y=7, color=\'r\', linestyle=\'--\', label="Theoretical Mean (7)")\nplt.xlabel("Number of Rolls")\nplt.ylabel("Average Sum")\nplt.legend()\nplt.show()\n'}),"\n",l.createElement(t.h3,{id:"dependent-and-independent-events",style:{position:"relative"}},l.createElement(t.a,{href:"#dependent-and-independent-events","aria-label":"dependent and independent events permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Dependent and independent events"),"\n",l.createElement(t.p,null,"Two events ",l.createElement(c.A,{text:"\\(A\\)"})," and ",l.createElement(c.A,{text:"\\(B\\)"})," are said to be ",l.createElement(s.A,null,"independent")," if knowing that ",l.createElement(c.A,{text:"\\(A\\)"})," has occurred provides no information about whether ",l.createElement(c.A,{text:"\\(B\\)"})," occurs. Formally:"),"\n",l.createElement(c.A,{text:"\\( P(A \\cap B) = P(A) \\, P(B). \\)"}),"\n",l.createElement(t.p,null,"If the above condition is not satisfied, the events are ",l.createElement(s.A,null,"dependent"),". Dependence and independence matter greatly in modeling and inference. Many machine learning algorithms assume independence across data points or features for simplification, even though in reality, features can be correlated."),"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Example"),":"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"Rolling a fair die and flipping a fair coin are independent events. The outcome of the die (1 through 6) in no way affects the coin toss (Heads or Tails)."),"\n",l.createElement(t.li,null,'On the other hand, the event "It is raining" and the event "The ground is wet" are dependent. Knowing that it has rained changes the likelihood that the ground is wet.'),"\n"),"\n",l.createElement(t.h3,{id:"bayes-theorem",style:{position:"relative"}},l.createElement(t.a,{href:"#bayes-theorem","aria-label":"bayes theorem permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Bayes' theorem"),"\n",l.createElement(t.p,null,l.createElement(s.A,null,"Bayes' theorem")," provides a way to update our beliefs (probabilities) after observing new data. It is often written as:"),"\n",l.createElement(c.A,{text:"\\( P(A|B) = \\frac{P(B|A)\\,P(A)}{P(B)}. \\)"}),"\n",l.createElement(t.p,null,"Here, ",l.createElement(c.A,{text:"\\(P(A|B)\\)"})," is called the ",l.createElement(r.A,{text:"Posterior probability: the probability of event A occurring after we observe event B."}),". ",l.createElement(c.A,{text:"\\(P(A)\\)"})," is the ",l.createElement(r.A,{text:"Prior probability: our original belief (before observing B)."}),", and ",l.createElement(c.A,{text:"\\(P(B|A)\\)"})," is the likelihood of observing ",l.createElement(c.A,{text:"\\(B\\)"})," given ",l.createElement(c.A,{text:"\\(A\\)"}),". This formula is central to Bayesian statistics and is used extensively in many modern machine learning approaches, such as Bayesian neural networks (e.g., Smith and gang, NeurIPS 2022)."),"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Example"),":",l.createElement(t.br),"\n","A classic illustration is medical testing. Suppose ",l.createElement(c.A,{text:"\\(A\\)"}),' is the event "Person has a certain disease," and ',l.createElement(c.A,{text:"\\(B\\)"}),' is the event "Test is positive." Even if a test is 99% accurate, if the disease prevalence in the population is very low, a positive test result might still not mean a high probability of actually having the disease. Bayes\' theorem helps calculate the updated (posterior) probability.'),"\n",l.createElement(t.h2,{id:"probability-distributions",style:{position:"relative"}},l.createElement(t.a,{href:"#probability-distributions","aria-label":"probability distributions permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Probability distributions"),"\n",l.createElement(t.p,null,"A ",l.createElement(s.A,null,"probability distribution")," specifies how probable each possible value (or range of values) of a random variable is. Distributions are frequently described by key parameters (e.g., mean, variance) that help characterize their shape and spread."),"\n",l.createElement(t.h3,{id:"discrete-distributions",style:{position:"relative"}},l.createElement(t.a,{href:"#discrete-distributions","aria-label":"discrete distributions permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Discrete distributions"),"\n",l.createElement(t.p,null,l.createElement(s.A,null,"Discrete distributions")," describe random variables that can take on a countable number of possible outcomes (e.g., ",l.createElement(c.A,{text:"\\(\\{0, 1, 2, \\dots\\}\\)"}),"). Examples include the Bernoulli, Binomial, and Poisson distributions. For discrete random variables ",l.createElement(c.A,{text:"\\(X\\)"}),", the function that describes the probability of each outcome ",l.createElement(c.A,{text:"\\(x_i\\)"})," is called the ",l.createElement(s.A,null,"probability mass function")," (pmf):"),"\n",l.createElement(c.A,{text:"\\( p_X(x_i) = P(X = x_i). \\)"}),"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Example – Bernoulli Distribution"),":",l.createElement(t.br),"\n","A single coin flip can be modeled as a Bernoulli random variable ",l.createElement(c.A,{text:"\\(X\\)"}),", where ",l.createElement(c.A,{text:"\\(X = 1\\)"})," if the coin lands heads, and ",l.createElement(c.A,{text:"\\(X = 0\\)"})," otherwise. The parameter ",l.createElement(c.A,{text:"\\(p\\)"})," is the probability of heads. Therefore:"),"\n",l.createElement(c.A,{text:"\\[\nP(X = 1) = p, \\quad P(X = 0) = 1 - p.\n\\]"}),"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Example – Binomial Distribution"),":",l.createElement(t.br),"\n","If you flip a coin ",l.createElement(c.A,{text:"\\(n\\)"})," times, each flip being a Bernoulli trial with probability ",l.createElement(c.A,{text:"\\(p\\)"})," of heads, the total count of heads among those ",l.createElement(c.A,{text:"\\(n\\)"})," flips follows a Binomial(",l.createElement(c.A,{text:"\\(n, p\\)"}),") distribution."),"\n",l.createElement(t.h3,{id:"continuous-distributions",style:{position:"relative"}},l.createElement(t.a,{href:"#continuous-distributions","aria-label":"continuous distributions permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Continuous distributions"),"\n",l.createElement(t.p,null,l.createElement(s.A,null,"Continuous distributions")," describe random variables that take values from continuous intervals (e.g., all real numbers). Common examples include the Uniform distribution, Normal (Gaussian) distribution, and Exponential distribution. For continuous random variables, the probability is described by a ",l.createElement(s.A,null,"probability density function")," (pdf) ",l.createElement(c.A,{text:"\\(f_X(x)\\)"}),". To find probabilities over intervals, you integrate the pdf:"),"\n",l.createElement(c.A,{text:"\\( P(a \\le X \\le b) = \\int_a^b f_X(x)\\, dx. \\)"}),"\n",l.createElement("br"),"\n",l.createElement("br"),"\n",l.createElement(t.h3,{id:"pmf-pdf-and-cdf",style:{position:"relative"}},l.createElement(t.a,{href:"#pmf-pdf-and-cdf","aria-label":"pmf pdf and cdf permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"pmf, pdf, and cdf"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(s.A,null,"pmf (Probability Mass Function)")," applies to discrete variables and gives the probability of each possible outcome."),"\n"),"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(s.A,null,"pdf (Probability Density Function)")," applies to continuous variables; it does not directly give a probability for each point, but the area under the curve between two points gives the probability that the variable lies within that interval."),"\n"),"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(s.A,null,"cdf (Cumulative Distribution Function)")," is defined for both discrete and continuous cases. For a random variable ",l.createElement(c.A,{text:"\\(X\\)"}),", the cdf ",l.createElement(c.A,{text:"\\(F_X(x)\\)"})," gives the probability that ",l.createElement(c.A,{text:"\\(X\\)"})," is less than or equal to ",l.createElement(c.A,{text:"\\(x\\)"}),":"),"\n",l.createElement(c.A,{text:"\\( F_X(x) = P(X \\le x). \\)"}),"\n"),"\n"),"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Illustration"),":",l.createElement(t.br),"\n","If ",l.createElement(c.A,{text:"\\(X\\)"})," is the number of heads after flipping a fair coin 3 times, the pmf would be:"),"\n",l.createElement(c.A,{text:"\\[\nP(X = 0) = \\frac{1}{8}, \\quad\nP(X = 1) = \\frac{3}{8}, \\quad\nP(X = 2) = \\frac{3}{8}, \\quad\nP(X = 3) = \\frac{1}{8}.\n\\]"}),"\n",l.createElement(t.p,null,"The cdf ",l.createElement(c.A,{text:"\\(F_X(x)\\)"})," at ",l.createElement(c.A,{text:"\\(x=2\\)"})," would be ",l.createElement(c.A,{text:"\\(P(X \\le 2) = P(X=0) + P(X=1) + P(X=2) = \\frac{1}{8} + \\frac{3}{8} + \\frac{3}{8} = \\frac{7}{8}.\\)"})),"\n",l.createElement(t.h2,{id:"characteristics-of-distributions",style:{position:"relative"}},l.createElement(t.a,{href:"#characteristics-of-distributions","aria-label":"characteristics of distributions permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Characteristics of distributions"),"\n",l.createElement(t.p,null,"Descriptive statistics aim to summarize and describe important characteristics of a distribution. They provide a simpler set of metrics to understand the underlying data."),"\n",l.createElement(t.h3,{id:"mean-mode-and-median",style:{position:"relative"}},l.createElement(t.a,{href:"#mean-mode-and-median","aria-label":"mean mode and median permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Mean, mode, and median"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(s.A,null,"Mean")," (arithmetic average): For a sample of values ",l.createElement(c.A,{text:"\\(\\{x_1, x_2, \\dots, x_n\\}\\)"}),", the mean is:"),"\n",l.createElement(c.A,{text:"\\( \\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i. \\)"}),"\n"),"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(s.A,null,"Mode"),": The most frequently occurring value in the dataset (for discrete or categorical variables) or the value at which the pdf attains its maximum (for continuous variables)."),"\n"),"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(s.A,null,"Median"),": The middle value when the data are sorted. It splits the distribution into two halves of equal probability."),"\n"),"\n"),"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Example"),":",l.createElement(t.br),"\n","Consider exam scores in a class of 20 students. Suppose the scores (out of 10) are:"),"\n",l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">8, 9, 4, 6, 3, 7, 3, 5, 5, 5, \n3, 3, 4, 7, 8, 5, 10, 3, 8, 7</code></pre></div>'}}),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"Sorting these yields:","\n",l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">3, 3, 3, 3, 3, 4, 4, 5, 5, 5, \n5, 6, 7, 7, 7, 8, 8, 8, 9, 10</code></pre></div>'}}),"\n"),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Mean"),": Add them up and divide by 20 (or weigh each score by its frequency) to get ~5.65."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Mode"),": The most frequent score is 3."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Median"),": The average of the 10th and 11th scores in the sorted list is ",l.createElement(c.A,{text:"\\((5 + 5)/2 = 5\\)"}),"."),"\n"),"\n",l.createElement(t.h3,{id:"variance-and-standard-deviation",style:{position:"relative"}},l.createElement(t.a,{href:"#variance-and-standard-deviation","aria-label":"variance and standard deviation permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Variance and standard deviation"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(s.A,null,"Variance")," of a random variable ",l.createElement(c.A,{text:"\\(X\\)"})," measures the expected squared deviation from its mean. In sample form, the variance of ",l.createElement(c.A,{text:"\\(\\{x_1, \\dots, x_n\\}\\)"})," (assuming the population mean is unknown) often uses Bessel's correction:"),"\n",l.createElement(c.A,{text:"\\( s^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2. \\)"}),"\n"),"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(s.A,null,"Standard deviation")," is the square root of the variance and measures the spread in the same units as the data."),"\n"),"\n"),"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Illustration"),":",l.createElement(t.br),"\n","If the exam scores mentioned above have a sample mean of 5.65, you'd calculate each ",l.createElement(c.A,{text:"\\((x_i - 5.65)\\)"}),", square it, sum all squares, then divide by ",l.createElement(c.A,{text:"\\(n-1\\)"})," (i.e., 19) to get ",l.createElement(c.A,{text:"\\(s^2\\)"}),". Taking the square root gives ",l.createElement(c.A,{text:"\\(s\\)"}),", the sample standard deviation."),"\n",l.createElement(t.h3,{id:"population-variance-and-sample-variance",style:{position:"relative"}},l.createElement(t.a,{href:"#population-variance-and-sample-variance","aria-label":"population variance and sample variance permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Population variance and sample variance"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(s.A,null,"Population variance")," (",l.createElement(c.A,{text:"\\(\\sigma^2\\)"}),") is used when you have data for the entire population; it divides by ",l.createElement(c.A,{text:"\\(n\\)"}),"."),"\n",l.createElement(t.li,null,l.createElement(s.A,null,"Sample variance")," (",l.createElement(c.A,{text:"\\(s^2\\)"}),") is used when data come from a sample of a larger population; it divides by ",l.createElement(c.A,{text:"\\(n-1\\)"}),". This correction helps reduce bias in estimating the true population variance."),"\n"),"\n",l.createElement(t.h3,{id:"variational-series",style:{position:"relative"}},l.createElement(t.a,{href:"#variational-series","aria-label":"variational series permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Variational series"),"\n",l.createElement(t.p,null,"A ",l.createElement(s.A,null,"variational series")," is a sorted (or ordered) list of sample observations. It's often used in descriptive statistics to analyze how data points lie relative to one another. For example, box plots and percentile-based analyses rely on the sorted nature of the data. In the exam score illustration, turning the unsorted list of scores into a sorted sequence helps compute the median or see the distribution at a glance."),"\n",l.createElement(t.h3,{id:"skewness-and-kurtosis",style:{position:"relative"}},l.createElement(t.a,{href:"#skewness-and-kurtosis","aria-label":"skewness and kurtosis permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Skewness and kurtosis"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(s.A,null,"Skewness")," measures the asymmetry of the distribution. A positive skew (right-skew) indicates a distribution with a longer right tail, while a negative skew (left-skew) indicates a longer left tail."),"\n",l.createElement(t.li,null,l.createElement(s.A,null,"Kurtosis"),' measures the "tailedness" of the distribution. Distributions with high kurtosis tend to have heavier tails and more extreme values (outliers).'),"\n"),"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Example"),":"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"Test scores that cluster around the high end with a few very low outliers might show negative skewness (left-skew)."),"\n",l.createElement(t.li,null,"A distribution of daily stock returns could have heavy tails (high kurtosis), meaning outliers are more probable than in a normal distribution."),"\n"),"\n",l.createElement(t.h2,{id:"normal-and-uniform-distributions",style:{position:"relative"}},l.createElement(t.a,{href:"#normal-and-uniform-distributions","aria-label":"normal and uniform distributions permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Normal and uniform distributions"),"\n",l.createElement(t.h3,{id:"normal-gaussian-distribution",style:{position:"relative"}},l.createElement(t.a,{href:"#normal-gaussian-distribution","aria-label":"normal gaussian distribution permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Normal (Gaussian) distribution"),"\n",l.createElement(t.p,null,"The ",l.createElement(s.A,null,"Normal distribution")," is perhaps the most important distribution in statistics. It is fully characterized by its mean ",l.createElement(c.A,{text:"\\(\\mu\\)"})," and variance ",l.createElement(c.A,{text:"\\(\\sigma^2\\)"}),". The pdf is:"),"\n",l.createElement(c.A,{text:"\\[\nf(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right),\n\\]"}),"\n",l.createElement(t.p,null,"where:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(c.A,{text:"\\(\\mu\\)"})," is the location (center)."),"\n",l.createElement(t.li,null,l.createElement(c.A,{text:"\\(\\sigma\\)"})," is the standard deviation."),"\n",l.createElement(t.li,null,l.createElement(c.A,{text:"\\(\\exp\\)"})," denotes the exponential function."),"\n"),"\n",l.createElement(n,{alt:"A normal distribution bell-curve",path:"",caption:"The bell-curve shape of a normal distribution.",zoom:"false"}),"\n",l.createElement(t.p,null,"Its symmetrical bell-curve shape and mathematically tractable properties make it extremely useful in analysis and inference. Many real-world phenomena (heights, test scores, measurement errors) are approximately normally distributed, justifying the wide use of normal-based statistical methods."),"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Rule of thumb"),":"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"About 68% of values drawn from a normal distribution lie within 1 standard deviation of the mean."),"\n",l.createElement(t.li,null,"About 95% are within 2 standard deviations."),"\n",l.createElement(t.li,null,"About 99.7% are within 3 standard deviations."),"\n"),"\n",l.createElement(t.p,null,'These facts underlie the popular "68–95–99.7 rule" used in everyday statistical practice.'),"\n",l.createElement(t.h3,{id:"uniform-distribution",style:{position:"relative"}},l.createElement(t.a,{href:"#uniform-distribution","aria-label":"uniform distribution permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Uniform distribution"),"\n",l.createElement(t.p,null,"The ",l.createElement(s.A,null,"Uniform distribution")," describes a random variable that is equally likely to occur anywhere in a specified interval ",l.createElement(c.A,{text:"\\([a, b]\\)"}),". Its pdf is:"),"\n",l.createElement(c.A,{text:"\\[\nf(x) = \n\\begin{cases}\n\\frac{1}{b - a}, & a \\le x \\le b \\\\\n0, & \\text{otherwise.}\n\\end{cases}\n\\]"}),"\n",l.createElement(t.p,null,"This distribution is often used as a baseline or reference, especially in simulations or to model processes that have constant probability across a finite interval."),"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Example"),":",l.createElement(t.br),"\n","Picking a random real number between 0 and 1 with equal likelihood follows the Uniform(",l.createElement(c.A,{text:"\\(0,1\\)"}),") distribution. In simulations, it's common to use random draws from ",l.createElement(c.A,{text:"\\(\\text{Uniform}(0,1)\\)"})," to generate other distributions using transformations."),"\n",l.createElement(t.h2,{id:"empirical-vs-theoretical-distributions-and-histograms",style:{position:"relative"}},l.createElement(t.a,{href:"#empirical-vs-theoretical-distributions-and-histograms","aria-label":"empirical vs theoretical distributions and histograms permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Empirical vs. theoretical distributions and histograms"),"\n",l.createElement(t.p,null,"In practice, we rarely know the true distribution of data; we only observe samples. The empirical distribution is formed directly from observed data, while the theoretical distribution is a mathematical model (like Normal or Binomial) that approximates the data-generating process. Comparing empirical data with a theoretical distribution can help us:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"Check how well a chosen model fits the data."),"\n",l.createElement(t.li,null,"Explore departures from assumptions (e.g., normality tests)."),"\n",l.createElement(t.li,null,"Develop new models if the existing ones do not fit well."),"\n"),"\n",l.createElement(t.p,null,"A ",l.createElement(s.A,null,"histogram")," is one of the most common ways to visualize an empirical distribution. It segments the observed data into bins and counts how many data points fall into each bin. Below is a simple Python snippet to generate random data, plot a histogram, and compare it with the theoretical normal curve:"),"\n",l.createElement(o.A,{text:'\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Generate random normal data\ndata = np.random.normal(loc=0, scale=1, size=10000)\n\n# Plot the histogram\nplt.hist(data, bins=50, density=True, alpha=0.5, color=\'blue\')\n\n# Generate points for the theoretical normal curve\nx = np.linspace(-4, 4, 1000)\npdf = norm.pdf(x, loc=0, scale=1)\nplt.plot(x, pdf, \'r\', linewidth=2)\n\nplt.title("Empirical vs. Theoretical Normal Distribution")\nplt.xlabel("Value")\nplt.ylabel("Density")\nplt.show()\n'}),"\n",l.createElement(n,{alt:"Histogram of generated normal data with theoretical pdf",path:"",caption:"A histogram overlaid with the theoretical normal pdf.",zoom:"false"}),"\n",l.createElement(t.p,null,"This visual approach helps us see how well our empirical data approximates the bell shape of a theoretical normal distribution."),"\n",l.createElement(t.h2,{id:"the-central-limit-theorem",style:{position:"relative"}},l.createElement(t.a,{href:"#the-central-limit-theorem","aria-label":"the central limit theorem permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"The central limit theorem"),"\n",l.createElement(t.p,null,"The ",l.createElement(s.A,null,"central limit theorem (CLT)")," states that the sum (or average) of a large number of i.i.d. random variables (with finite mean and variance) will approximate a normal distribution regardless of the original distribution of those variables. Formally, for i.i.d. random variables ",l.createElement(c.A,{text:"\\(X_1, X_2, \\ldots, X_n\\)"})," with mean ",l.createElement(c.A,{text:"\\(\\mu\\)"})," and variance ",l.createElement(c.A,{text:"\\(\\sigma^2\\)"}),":"),"\n",l.createElement(c.A,{text:"\\[\n\\frac{\\sum_{i=1}^n X_i - n\\mu}{\\sigma\\sqrt{n}} \\xrightarrow{d} \\mathcal{N}(0, 1),\n\\]"}),"\n",l.createElement(t.p,null,"where ",l.createElement(c.A,{text:"\\(\\xrightarrow{d}\\)"})," denotes convergence in distribution, and ",l.createElement(c.A,{text:"\\(\\mathcal{N}(0,1)\\)"})," is the standard normal distribution. This theorem justifies why many phenomena seem to follow (or nearly follow) a normal distribution and is a cornerstone of statistical inference (e.g., confidence intervals, hypothesis testing)."),"\n",l.createElement(t.p,null,"The CLT is behind countless practical techniques, such as building approximate confidence intervals for sample means. Even if the original data are not normally distributed, the distribution of sample means tends to become more and more normal as the sample size grows. This insight is particularly valuable in machine learning when dealing with averages of large samples or sums of errors."),"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Illustration"),":"),"\n",l.createElement(t.ol,null,"\n",l.createElement(t.li,null,"Start with a distribution that is ",l.createElement(t.em,null,"not")," normal (e.g., uniform or skewed)."),"\n",l.createElement(t.li,null,"Draw a large number of samples of a fixed size ",l.createElement(c.A,{text:"\\(n\\)"}),"."),"\n",l.createElement(t.li,null,"Compute the mean of each sample."),"\n",l.createElement(t.li,null,"Plot a histogram of these means. As ",l.createElement(c.A,{text:"\\(n\\)"})," grows, the histogram of sample means will start to resemble a normal distribution."),"\n"),"\n",l.createElement(t.h2,{id:"frequentist-vs-bayesian-approaches",style:{position:"relative"}},l.createElement(t.a,{href:"#frequentist-vs-bayesian-approaches","aria-label":"frequentist vs bayesian approaches permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Frequentist vs. Bayesian approaches"),"\n",l.createElement(t.p,null,"There are two main frameworks for statistical inference:"),"\n",l.createElement(t.ol,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Frequentist statistics"),": Probabilities are interpreted as long-run frequencies. Inference involves constructing confidence intervals and performing hypothesis tests with p-values (we'll discuss it a moment)."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Bayesian statistics"),": Probabilities are interpreted as degrees of belief. Bayes' theorem is used to update prior distributions to posterior distributions after observing data."),"\n"),"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Example"),":"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,'Frequentist approach to a coin-toss experiment might say: "If we tossed this coin many times and repeated the experiment, 95% of our calculated confidence intervals would contain the true probability ',l.createElement(c.A,{text:"\\(p\\)"}),' of heads."'),"\n",l.createElement(t.li,null,'Bayesian approach: "Given a prior belief about ',l.createElement(c.A,{text:"\\(p\\)"}),", after observing some heads and tails, we update our belief and get a posterior distribution for ",l.createElement(c.A,{text:"\\(p\\)"}),'."'),"\n"),"\n",l.createElement(t.h2,{id:"conclusion-and-further-directions",style:{position:"relative"}},l.createElement(t.a,{href:"#conclusion-and-further-directions","aria-label":"conclusion and further directions permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Conclusion and further directions"),"\n",l.createElement(t.p,null,"In this first part of our ",l.createElement(s.A,null,"Introduction to statistics")," module, we have laid the foundations of probability theory and begun exploring how real-world variability is captured mathematically. These concepts — random variables, distributions, measures of central tendency and spread — are essential stepping stones to more advanced topics in estimation, hypothesis testing, and beyond. Throughout the rest of the course, we will continually build on these ideas, applying them to data science workflows and sophisticated machine learning algorithms."),"\n",l.createElement(t.p,null,"Moving forward, you might explore:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Regression and classification techniques")," (e.g., linear regression, logistic regression)."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Advanced Bayesian methods"),", such as Bayesian hierarchical models."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Nonparametric statistics")," for scenarios where strict parametric assumptions do not hold."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Resampling techniques")," (e.g., bootstrapping, permutation tests) for building robust estimates without heavy distributional assumptions."),"\n"))}var d=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,i.RP)(),e.components);return t?l.createElement(t,e,l.createElement(m,e)):m(e)};var u=n(36710),h=n(58481),p=n.n(h),f=n(36310),g=n(87245),v=n(27042),b=n(59849),E=n(5591),y=n(61122),w=n(9219),x=n(33203),A=n(95751),S=n(94328),M=n(80791),T=n(78137);const z=e=>{let{toc:t}=e;if(!t||!t.items)return null;return l.createElement("nav",{className:M.R},l.createElement("ul",null,t.items.map(((e,t)=>l.createElement("li",{key:t},l.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const n=t.replace("#",""),a=document.getElementById(n);a&&a.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&l.createElement(z,{toc:{items:e.items}}))))))};function C(e){let{data:{mdx:t,allMdx:r,allPostImages:s},children:o}=e;const{frontmatter:c,body:m,tableOfContents:d}=t,u=c.index,h=c.slug.split("/")[1],b=r.nodes.filter((e=>e.frontmatter.slug.includes(`/${h}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),M=b.findIndex((e=>e.frontmatter.index===u)),C=b[M+1],L=b[M-1],N=c.slug.replace(/\/$/,""),I=/[^/]*$/.exec(N)[0],H=`posts/${h}/content/${I}/`,{0:k,1:B}=(0,l.useState)(c.flagWideLayoutByDefault),{0:_,1:P}=(0,l.useState)(!1);var j;(0,l.useEffect)((()=>{P(!0);const e=setTimeout((()=>P(!1)),340);return()=>clearTimeout(e)}),[k]),"adventures"===h?j=w.cb:"research"===h?j=w.Qh:"thoughts"===h&&(j=w.T6);const D=p()(m).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,V=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),n=e%60;return n<=30?`~${t}${n>0?".5":""} h`:`~${t+1} h`}(Math.ceil(D/j)+(c.extraReadTimeMin||0)),O=[{flag:c.flagDraft,component:()=>Promise.all([n.e(3231),n.e(8809)]).then(n.bind(n,28809))},{flag:c.flagMindfuckery,component:()=>Promise.all([n.e(3231),n.e(2471)]).then(n.bind(n,67709))},{flag:c.flagRewrite,component:()=>Promise.all([n.e(3231),n.e(6764)]).then(n.bind(n,62002))},{flag:c.flagOffensive,component:()=>Promise.all([n.e(3231),n.e(2443)]).then(n.bind(n,17681))},{flag:c.flagProfane,component:()=>Promise.all([n.e(3231),n.e(8048)]).then(n.bind(n,53286))},{flag:c.flagMultilingual,component:()=>Promise.all([n.e(3231),n.e(4069)]).then(n.bind(n,78831))},{flag:c.flagUnreliably,component:()=>Promise.all([n.e(3231),n.e(3417)]).then(n.bind(n,8179))},{flag:c.flagPolitical,component:()=>Promise.all([n.e(3231),n.e(5195)]).then(n.bind(n,30433))},{flag:c.flagCognitohazard,component:()=>Promise.all([n.e(3231),n.e(3175)]).then(n.bind(n,8413))},{flag:c.flagHidden,component:()=>Promise.all([n.e(3231),n.e(9556)]).then(n.bind(n,14794))}],{0:X,1:F}=(0,l.useState)([]);return(0,l.useEffect)((()=>{O.forEach((e=>{let{flag:t,component:n}=e;t&&n().then((e=>{F((t=>[].concat((0,a.A)(t),[e.default])))}))}))}),[]),l.createElement(v.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},l.createElement(E.A,{postNumber:c.index,date:c.date,updated:c.updated,readTime:V,difficulty:c.difficultyLevel,title:c.title,desc:c.desc,banner:c.banner,section:h,postKey:I,isMindfuckery:c.flagMindfuckery,mainTag:c.mainTag}),l.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},c.otherTags.map(((e,t)=>l.createElement("span",{key:t,className:`noselect ${T.MW}`,style:{margin:"0 5px 5px 0"}},e)))),l.createElement("div",{className:"postBody"},l.createElement(z,{toc:d})),l.createElement("br"),l.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},l.createElement(v.P.button,{className:`noselect ${S.pb}`,id:S.xG,onClick:()=>{B(!k)},whileTap:{scale:.93}},l.createElement(v.P.div,{className:A.DJ,key:k,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},k?"Switch to default layout":"Switch to wide layout"))),l.createElement("br"),l.createElement("div",{className:"postBody",style:{margin:k?"0 -14%":"",maxWidth:k?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},l.createElement("div",{className:`${S.P_} ${_?S.Xn:S.qG}`},X.map(((e,t)=>l.createElement(e,{key:t}))),c.indexCourse?l.createElement(x.A,{index:c.indexCourse,category:c.courseCategoryName}):"",l.createElement(f.Z.Provider,{value:{images:s.nodes,basePath:H.replace(/\/$/,"")+"/"}},l.createElement(i.xA,{components:{Image:g.A}},o)))),l.createElement(y.A,{nextPost:C,lastPost:L,keyCurrent:I,section:h}))}function L(e){return l.createElement(C,e,l.createElement(d,e))}function N(e){var t,n,a,i,r;let{data:s}=e;const{frontmatter:o}=s.mdx,c=o.titleSEO||o.title,m=o.titleOG||c,d=o.titleTwitter||c,h=o.descSEO||o.desc,p=o.descOG||h,f=o.descTwitter||h,g=o.schemaType||"BlogPosting",v=o.keywordsSEO,E=o.date,y=o.updated||E,w=o.imageOG||(null===(t=o.banner)||void 0===t||null===(n=t.childImageSharp)||void 0===n||null===(a=n.gatsbyImageData)||void 0===a||null===(i=a.images)||void 0===i||null===(r=i.fallback)||void 0===r?void 0:r.src),x=o.imageAltOG||p,A=o.imageTwitter||w,S=o.imageAltTwitter||f,M=o.canonicalURL,T=o.flagHidden||!1,z=o.mainTag||"Posts",C=o.slug.split("/")[1]||"posts",{siteUrl:L}=(0,u.Q)(),N={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:L},{"@type":"ListItem",position:2,name:z,item:`${L}/${o.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${L}${o.slug}`}]};return l.createElement(b.A,{title:c+" - avrtt.blog",titleOG:m,titleTwitter:d,description:h,descriptionOG:p,descriptionTwitter:f,schemaType:g,keywords:v,datePublished:E,dateModified:y,imageOG:w,imageAltOG:x,imageTwitter:A,imageAltTwitter:S,canonicalUrl:M,flagHidden:T,mainTag:z,section:C,type:"article"},l.createElement("script",{type:"application/ld+json"},JSON.stringify(N)))}},66501:function(e,t,n){n.d(t,{A:function(){return r}});var a=n(96540),i=n(3962),l="styles-module--tooltiptext--a263b";var r=e=>{let{text:t,isBadge:n=!1}=e;const{0:r,1:s}=(0,a.useState)(!1),o=(0,a.useRef)(null);return(0,a.useEffect)((()=>{function e(e){o.current&&!o.current.contains(e.target)&&s(!1)}return document.addEventListener("click",e),()=>{document.removeEventListener("click",e)}}),[]),a.createElement("span",{className:"styles-module--tooltipWrapper--75ebf",ref:o},a.createElement("img",{id:n?"styles-module--infoBadge--e3d66":"styles-module--info--26c1f",src:i.A,alt:"info",onClick:e=>{e.stopPropagation(),s((e=>!e))}}),a.createElement("span",{className:r?`${l} styles-module--visible--c063c`:l},t))}},96098:function(e,t,n){var a=n(96540),i=n(7978);t.A=e=>{let{text:t}=e;return a.createElement(i.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-js-content-file-path-src-pages-posts-research-non-boring-intro-to-statistics-mdx-88e45bef50690060acfa.js.map