"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[8477],{3962:function(e,t){t.A="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iaXNvLTg4NTktMSI/Pgo8IS0tIEdlbmVyYXRvcjogQWRvYmUgSWxsdXN0cmF0b3IgMTYuMC4wLCBTVkcgRXhwb3J0IFBsdWctSW4gLiBTVkcgVmVyc2lvbjogNi4wMCBCdWlsZCAwKSAgLS0+CjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+CjxzdmcgdmVyc2lvbj0iMS4xIiBpZD0iQ2FwYV8xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB4PSIwcHgiIHk9IjBweCIKCSB3aWR0aD0iNDE2Ljk3OXB4IiBoZWlnaHQ9IjQxNi45NzlweCIgdmlld0JveD0iMCAwIDQxNi45NzkgNDE2Ljk3OSIgc3R5bGU9ImVuYWJsZS1iYWNrZ3JvdW5kOm5ldyAwIDAgNDE2Ljk3OSA0MTYuOTc5OyIKCSB4bWw6c3BhY2U9InByZXNlcnZlIj4KPGc+Cgk8cGF0aCBkPSJNMzU2LjAwNCw2MS4xNTZjLTgxLjM3LTgxLjQ3LTIxMy4zNzctODEuNTUxLTI5NC44NDgtMC4xODJjLTgxLjQ3LDgxLjM3MS04MS41NTIsMjEzLjM3OS0wLjE4MSwyOTQuODUKCQljODEuMzY5LDgxLjQ3LDIxMy4zNzgsODEuNTUxLDI5NC44NDksMC4xODFDNDM3LjI5MywyNzQuNjM2LDQzNy4zNzUsMTQyLjYyNiwzNTYuMDA0LDYxLjE1NnogTTIzNy42LDM0MC43ODYKCQljMCwzLjIxNy0yLjYwNyw1LjgyMi01LjgyMiw1LjgyMmgtNDYuNTc2Yy0zLjIxNSwwLTUuODIyLTIuNjA1LTUuODIyLTUuODIyVjE2Ny44ODVjMC0zLjIxNywyLjYwNy01LjgyMiw1LjgyMi01LjgyMmg0Ni41NzYKCQljMy4yMTUsMCw1LjgyMiwyLjYwNCw1LjgyMiw1LjgyMlYzNDAuNzg2eiBNMjA4LjQ5LDEzNy45MDFjLTE4LjYxOCwwLTMzLjc2Ni0xNS4xNDYtMzMuNzY2LTMzLjc2NQoJCWMwLTE4LjYxNywxNS4xNDctMzMuNzY2LDMzLjc2Ni0zMy43NjZjMTguNjE5LDAsMzMuNzY2LDE1LjE0OCwzMy43NjYsMzMuNzY2QzI0Mi4yNTYsMTIyLjc1NSwyMjcuMTA3LDEzNy45MDEsMjA4LjQ5LDEzNy45MDF6Ii8+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPC9zdmc+Cg=="},9360:function(e,t,n){n.d(t,{A:function(){return i}});var a=n(96540),r=n(3962),l="styles-module--tooltiptext--a263b";var i=e=>{let{text:t,isBadge:n=!1}=e;const{0:i,1:o}=(0,a.useState)(!1),s=(0,a.useRef)(null);return(0,a.useEffect)((()=>{function e(e){s.current&&e.target instanceof Node&&!s.current.contains(e.target)&&o(!1)}return document.addEventListener("click",e),()=>{document.removeEventListener("click",e)}}),[]),a.createElement("span",{className:"styles-module--tooltipWrapper--75ebf",ref:s},a.createElement("img",{id:n?"styles-module--infoBadge--e3d66":"styles-module--info--26c1f",src:r.A,alt:"info",onClick:e=>{e.stopPropagation(),o((e=>!e))}}),a.createElement("span",{className:i?`${l} styles-module--visible--c063c`:l},t))}},49940:function(e,t,n){n.r(t),n.d(t,{Head:function(){return H},PostTemplate:function(){return L},default:function(){return C}});var a=n(28453),r=n(96540),l=n(9360),i=n(61992),o=n(62087),s=n(90548);function c(e){const t=Object.assign({p:"p",h3:"h3",a:"a",span:"span",ul:"ul",li:"li",h2:"h2",strong:"strong",ol:"ol",hr:"hr"},(0,a.RP)(),e.components),{Image:n}=t;return n||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),r.createElement(r.Fragment,null,"\n",r.createElement("br"),"\n","\n",r.createElement(t.p,null,"Linear regression is one of the foundational methods in machine learning and statistics, used widely in both academic research and practical applications. It provides a systematic way to model the relationship between one or more explanatory variables (also called features or predictors) and a continuous target variable. Despite being one of the oldest and perhaps simplest forms of regression, linear regression remains a cornerstone for understanding model building, interpretability, and optimization in data science."),"\n",r.createElement(t.h3,{id:"motivation-and-overview-of-linear-regression-in-machine-learning",style:{position:"relative"}},r.createElement(t.a,{href:"#motivation-and-overview-of-linear-regression-in-machine-learning","aria-label":"motivation and overview of linear regression in machine learning permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Motivation and overview of linear regression in machine learning"),"\n",r.createElement(t.p,null,'Motivation for linear regression stems from its ability to capture a linear (or linearly transformed) relationship between inputs and a continuous output. In other words, it tries to find a hyperplane in the feature space that best fits the observed data. This "best fit" is typically defined by minimizing some form of error measure, most commonly the sum of squared errors.'),"\n",r.createElement(t.p,null,"Many real-world phenomena — such as forecasting housing prices, predicting the lifespan of an engineering component, or relating health factors to overall well-being — can often be approximated with a linear model if we limit the scope or cleverly design features. Even in modern deep learning systems, linear components appear in the final layers for tasks like regression or classification (logistic regression being the linear model for classification)."),"\n",r.createElement(t.h3,{id:"historical-background-and-practical-applications",style:{position:"relative"}},r.createElement(t.a,{href:"#historical-background-and-practical-applications","aria-label":"historical background and practical applications permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Historical background and practical applications"),"\n",r.createElement(t.p,null,"Linear regression can be traced back to the 19th century, where it was studied in the context of astronomical observations and social statistics. Pioneers like Adrien-Marie Legendre and Carl Friedrich Gauss formalized the method of least squares, the mathematical backbone of linear regression."),"\n",r.createElement(t.p,null,"From this historical standpoint, linear regression has grown into a ubiquitous tool across disciplines:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(i.A,null,"Economics"),": to predict economic indicators (e.g., GDP growth, inflation rates)."),"\n",r.createElement(t.li,null,r.createElement(i.A,null,"Healthcare"),": to model risk factors against patient outcomes like blood pressure or insurance claim amounts."),"\n",r.createElement(t.li,null,r.createElement(i.A,null,"Marketing and sales"),": to understand relationships between advertising spend and sales revenue."),"\n",r.createElement(t.li,null,r.createElement(i.A,null,"Engineering"),": to estimate how factors like stress, temperature, or load affect a system's performance."),"\n"),"\n",r.createElement(t.p,null,"Its popularity persists largely because of its interpretability: each coefficient has a meaningful explanation relating a specific feature to the outcome."),"\n",r.createElement(t.h2,{id:"the-linear-model-in-a-regression-problem",style:{position:"relative"}},r.createElement(t.a,{href:"#the-linear-model-in-a-regression-problem","aria-label":"the linear model in a regression problem permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"The linear model in a regression problem"),"\n",r.createElement(t.p,null,"A regression problem differs from classification primarily in that the target variable ",r.createElement(l.A,{text:"Label or response variable"})," is continuous rather than discrete. A linear regression model posits that the target ",r.createElement(s.A,{text:"\\(y\\)"})," can be described (or approximated) by a weighted sum of input features ",r.createElement(s.A,{text:"\\(x_1, x_2, \\dots, x_p\\)"})," plus an intercept (often called bias in machine learning contexts)."),"\n",r.createElement(t.p,null,"Formally, for a single feature ",r.createElement(s.A,{text:"\\(x\\)"}),":"),"\n",r.createElement(s.A,{text:"\\[\ny = a + b x\n\\]"}),"\n",r.createElement(t.p,null,"where ",r.createElement(s.A,{text:"\\(a\\)"})," is the intercept and ",r.createElement(s.A,{text:"\\(b\\)"})," the slope. We can write this in vector form for multiple features:"),"\n",r.createElement(s.A,{text:"\\[\n\\hat{y} = w_0 + w_1 x_1 + \\ldots + w_p x_p\n\\]"}),"\n",r.createElement(t.p,null,"When using a more compact notation, we incorporate ",r.createElement(s.A,{text:"\\(w_0\\)"})," (the intercept) into the weight vector by extending each feature vector with a constant 1:"),"\n",r.createElement(s.A,{text:"\\[\n\\hat{y} = \\mathbf{w}^\\top \\mathbf{x} = w_0 \\times 1 + w_1 x_1 + \\dots + w_p x_p.\n\\]"}),"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"Basic assumptions")," of linear regression commonly include:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Linearity"),": The relationship between inputs and the output is linear in parameters."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Independence"),": Observations are assumed to be independent."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Homoscedasticity"),": The variance of the error terms is constant."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Normality of residuals"),": The error terms are often assumed (though not strictly required) to be normally distributed for small-sample inference."),"\n"),"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"Interpretation of coefficients"),": Each coefficient ",r.createElement(s.A,{text:"\\(w_i\\)"})," indicates how much the predicted value changes with respect to a unit change in feature ",r.createElement(s.A,{text:"\\(x_i\\)"}),", holding other features constant. This interpretability is a major advantage of linear models."),"\n",r.createElement(t.h2,{id:"cost-function-and-error-metrics",style:{position:"relative"}},r.createElement(t.a,{href:"#cost-function-and-error-metrics","aria-label":"cost function and error metrics permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Cost function and error metrics"),"\n",r.createElement(t.p,null,"A central theme in regression modeling is how to measure the discrepancy between model predictions and actual observed values. This measurement is crucial for both training (where we minimize the cost) and evaluation (where we assess performance). Below are the most frequently used metrics, each with its own implications:"),"\n",r.createElement(t.h3,{id:"mean-squared-error-mse",style:{position:"relative"}},r.createElement(t.a,{href:"#mean-squared-error-mse","aria-label":"mean squared error mse permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Mean squared error (MSE)"),"\n",r.createElement(s.A,{text:"\\[\n\\text{MSE}(\\hat{y}, y) = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n\\]"}),"\n",r.createElement(t.p,null,"Here, ",r.createElement(s.A,{text:"\\(y_i\\)"})," is the true target, ",r.createElement(s.A,{text:"\\(\\hat{y}_i\\)"})," is the predicted value, and ",r.createElement(s.A,{text:"\\(n\\)"})," is the number of observations. It is also commonly used as the cost function to optimize in linear regression via least squares. The squaring of the errors penalizes large deviations more heavily, making MSE sensitive to outliers. Variables in the formula:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(s.A,{text:"\\(y_i\\)"}),": Ground truth label for sample ",r.createElement(s.A,{text:"\\(i\\)"}),"."),"\n",r.createElement(t.li,null,r.createElement(s.A,{text:"\\(\\hat{y}_i\\)"}),": Predicted label for sample ",r.createElement(s.A,{text:"\\(i\\)"}),"."),"\n",r.createElement(t.li,null,r.createElement(s.A,{text:"\\(n\\)"}),": Total number of samples."),"\n"),"\n",r.createElement(t.h3,{id:"mean-absolute-error-mae",style:{position:"relative"}},r.createElement(t.a,{href:"#mean-absolute-error-mae","aria-label":"mean absolute error mae permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Mean absolute error (MAE)"),"\n",r.createElement(s.A,{text:"\\[\n\\text{MAE}(\\hat{y}, y) = \\frac{1}{n}\\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n\\]"}),"\n",r.createElement(t.p,null,"MAE measures the average magnitude of the errors without squaring. Therefore, unlike MSE, it penalizes all residuals in a more uniform way and is less sensitive to outliers. However, it is not differentiable at zero, which can complicate the analytic solutions or certain gradient-based optimizations (though subgradient methods do exist)."),"\n",r.createElement(t.h3,{id:"root-mean-squared-error-rmse",style:{position:"relative"}},r.createElement(t.a,{href:"#root-mean-squared-error-rmse","aria-label":"root mean squared error rmse permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Root mean squared error (RMSE)"),"\n",r.createElement(s.A,{text:"\\[\n\\text{RMSE}(\\hat{y}, y) = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n\\]"}),"\n",r.createElement(t.p,null,"RMSE is simply the square root of MSE, bringing the error metric back to the same units as the target variable. This makes RMSE often easier to interpret in many practical scenarios."),"\n",r.createElement(t.h3,{id:"mean-absolute-percentage-error-mape-and-symmetric-mape-smape",style:{position:"relative"}},r.createElement(t.a,{href:"#mean-absolute-percentage-error-mape-and-symmetric-mape-smape","aria-label":"mean absolute percentage error mape and symmetric mape smape permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Mean absolute percentage error (MAPE) and symmetric MAPE (SMAPE)"),"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"MAPE")," gauges the relative size of the errors by dividing by the actual target values:"),"\n",r.createElement(s.A,{text:"\\[\n\\text{MAPE}(\\hat{y}, y) = \\frac{100\\%}{n} \\sum_{i=1}^{n} \\left|\\frac{y_i - \\hat{y}_i}{y_i}\\right|.\n\\]"}),"\n",r.createElement(t.p,null,"This is particularly useful if you want to measure the error in percentage terms (e.g., if an error of 10k on a 1 million-dollar house is less serious than a 10k error on a 100k-dollar house)."),"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"SMAPE")," modifies MAPE to account for both predicted and actual values in the denominator:"),"\n",r.createElement(s.A,{text:"\\[\n\\text{SMAPE}(\\hat{y}, y) = \\frac{100\\%}{n} \\sum_{i=1}^{n} \\frac{|y_i - \\hat{y}_i|}{(|y_i| + |\\hat{y}_i|)/2}.\n\\]"}),"\n",r.createElement(t.p,null,"This helps control skew issues when ",r.createElement(s.A,{text:"\\(y_i\\)"})," is very large or very small."),"\n",r.createElement(t.h3,{id:"r-squared-coefficient-of-determination",style:{position:"relative"}},r.createElement(t.a,{href:"#r-squared-coefficient-of-determination","aria-label":"r squared coefficient of determination permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"R-squared (coefficient of determination)"),"\n",r.createElement(s.A,{text:"\\[\nR^2 = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}\n\\]"}),"\n",r.createElement(t.p,null,"Here, ",r.createElement(s.A,{text:"\\(\\bar{y}\\)"})," denotes the mean of the observed data. ",r.createElement(i.A,null,"R-squared")," measures how much better (or worse) your regression model is compared to a simple baseline that predicts the mean of ",r.createElement(s.A,{text:"\\(y\\)"})," for all observations. An ",r.createElement(s.A,{text:"\\(R^2\\)"})," value of 1 indicates a perfect fit, and 0 indicates that your model does no better than the naive mean-based approach."),"\n",r.createElement(t.h3,{id:"other-potential-metrics",style:{position:"relative"}},r.createElement(t.a,{href:"#other-potential-metrics","aria-label":"other potential metrics permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Other potential metrics"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Adjusted R-squared"),": Adjusts for the number of features, preventing artificially high ",r.createElement(s.A,{text:"\\(R^2\\)"})," due to adding irrelevant predictors."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"AIC and BIC"),": Information criteria used for model selection (these go beyond measuring pure predictive error, incorporating complexity penalties)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Explained variance score"),": Indicates how much variance is explained by the model vs. total variance in the data."),"\n"),"\n",r.createElement(t.h2,{id:"analytical-approach-to-linear-regression",style:{position:"relative"}},r.createElement(t.a,{href:"#analytical-approach-to-linear-regression","aria-label":"analytical approach to linear regression permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Analytical approach to linear regression"),"\n",r.createElement(t.h3,{id:"derivation-of-the-normal-equation",style:{position:"relative"}},r.createElement(t.a,{href:"#derivation-of-the-normal-equation","aria-label":"derivation of the normal equation permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Derivation of the normal equation"),"\n",r.createElement(t.p,null,"When using the ",r.createElement(t.strong,null,"least squares")," approach and MSE as the cost function, one can solve for the optimal weights ",r.createElement(s.A,{text:"(\\mathbf{w})"})," in closed form. Suppose you have a dataset ",r.createElement(s.A,{text:"(\\mathbf{X}, \\mathbf{y})"}),", where ",r.createElement(s.A,{text:"\\(\\mathbf{X}\\)"})," is an ",r.createElement(s.A,{text:"\\(n \\times (p+1)\\)"})," matrix of features (including a column of ones for the intercept) and ",r.createElement(s.A,{text:"\\(\\mathbf{y}\\)"})," is an ",r.createElement(s.A,{text:"\\(n\\)"}),"-dimensional vector of targets. The cost function in matrix form is:"),"\n",r.createElement(s.A,{text:"\\[\nJ(\\mathbf{w}) = \\frac{1}{2}(\\mathbf{X}\\mathbf{w} - \\mathbf{y})^\\top(\\mathbf{X}\\mathbf{w} - \\mathbf{y}),\n\\]"}),"\n",r.createElement(t.p,null,"where the factor of ",r.createElement(s.A,{text:"\\(1/2\\)"})," is simply a convenience for cleaner derivatives. Differentiating and setting to zero yields:"),"\n",r.createElement(s.A,{text:"\\[\n\\nabla_{\\mathbf{w}}J(\\mathbf{w}) = \\mathbf{X}^\\top(\\mathbf{X}\\mathbf{w} - \\mathbf{y}) = 0.\n\\]"}),"\n",r.createElement(t.p,null,"If ",r.createElement(s.A,{text:"\\(\\mathbf{X}^\\top\\mathbf{X}\\)"})," is invertible (non-singular), the ",r.createElement(t.strong,null,"normal equation")," becomes:"),"\n",r.createElement(s.A,{text:"\\[\n\\mathbf{w}^* = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{y}.\n\\]"}),"\n",r.createElement(t.h3,{id:"advantages-and-limitations-of-the-analytical-solution",style:{position:"relative"}},r.createElement(t.a,{href:"#advantages-and-limitations-of-the-analytical-solution","aria-label":"advantages and limitations of the analytical solution permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Advantages and limitations of the analytical solution"),"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"Advantages"),":"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"Direct formula, no need for iterative methods if the dimensionality is manageable."),"\n",r.createElement(t.li,null,"Conceptual simplicity, easy to understand from a linear algebra perspective."),"\n"),"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"Limitations"),":"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(i.A,null,"Computational cost"),": Inverting a ",r.createElement(s.A,{text:"\\(p+1\\)"})," by ",r.createElement(s.A,{text:"\\(p+1\\)"})," matrix can be expensive and numerically unstable for large ",r.createElement(s.A,{text:"\\(p\\)"}),"."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Ill-conditioning"),": If features are collinear or nearly collinear, ",r.createElement(s.A,{text:"(\\mathbf{X}^\\top\\mathbf{X})"})," can become singular or poorly conditioned, leading to unstable solutions."),"\n"),"\n",r.createElement(t.h3,{id:"computational-considerations-for-large-scale-problems",style:{position:"relative"}},r.createElement(t.a,{href:"#computational-considerations-for-large-scale-problems","aria-label":"computational considerations for large scale problems permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Computational considerations for large-scale problems"),"\n",r.createElement(t.p,null,"For high-dimensional scenarios (large ",r.createElement(s.A,{text:"\\(p\\)"}),"), direct matrix inversion is impractical. Instead, methods such as ",r.createElement(i.A,null,"gradient descent"),", ",r.createElement(i.A,null,"stochastic gradient descent"),", or advanced linear algebra techniques (e.g., Singular Value Decomposition, QR decomposition) are typically used. These methods can handle very large datasets where the matrix ",r.createElement(s.A,{text:"(\\mathbf{X}^\\top\\mathbf{X})"})," wouldn't even fit in memory for direct computation."),"\n",r.createElement(t.h2,{id:"multiple-linear-regression",style:{position:"relative"}},r.createElement(t.a,{href:"#multiple-linear-regression","aria-label":"multiple linear regression permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Multiple linear regression"),"\n",r.createElement(t.h3,{id:"extending-from-simple-to-multiple-predictors",style:{position:"relative"}},r.createElement(t.a,{href:"#extending-from-simple-to-multiple-predictors","aria-label":"extending from simple to multiple predictors permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Extending from simple to multiple predictors"),"\n",r.createElement(t.p,null,"In multiple linear regression, we allow more features. Conceptually, the line becomes a plane (for two features) or a hyperplane (for higher dimensions). Our model is:"),"\n",r.createElement(s.A,{text:"\\[\n\\hat{y} = w_0 + w_1 x_1 + w_2 x_2 + \\cdots + w_p x_p.\n\\]"}),"\n",r.createElement(t.h3,{id:"geometric-interpretation-hyperplanes",style:{position:"relative"}},r.createElement(t.a,{href:"#geometric-interpretation-hyperplanes","aria-label":"geometric interpretation hyperplanes permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Geometric interpretation (hyperplanes)"),"\n",r.createElement(t.p,null,"Each data point in ",r.createElement(s.A,{text:"\\(p\\)"}),"-dimensional space is projected onto a hyperplane defined by the weights ",r.createElement(s.A,{text:"(\\mathbf{w}\\)"}),". The objective is to find the hyperplane that minimizes the sum of squared distances to the observed data points in the target dimension."),"\n",r.createElement(t.h3,{id:"common-pitfalls-and-multicollinearity",style:{position:"relative"}},r.createElement(t.a,{href:"#common-pitfalls-and-multicollinearity","aria-label":"common pitfalls and multicollinearity permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Common pitfalls and multicollinearity"),"\n",r.createElement(t.p,null,"When two or more features are almost linearly dependent, it causes ",r.createElement(t.strong,null,"multicollinearity"),". This can lead to large swings in the values of the estimated coefficients. Methods like Ridge Regression or Lasso (discussed in a later chapter on regularization) introduce penalties that help mitigate these issues by shrinking coefficients."),"\n",r.createElement(t.h2,{id:"polynomial-features",style:{position:"relative"}},r.createElement(t.a,{href:"#polynomial-features","aria-label":"polynomial features permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Polynomial features"),"\n",r.createElement(t.h3,{id:"motivation-for-non-linear-patterns",style:{position:"relative"}},r.createElement(t.a,{href:"#motivation-for-non-linear-patterns","aria-label":"motivation for non linear patterns permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Motivation for non-linear patterns"),"\n",r.createElement(t.p,null,"A purely linear model might be insufficient for certain phenomena that exhibit curvature or more intricate relationships. ",r.createElement(i.A,null,"Polynomial features")," allow us to handle such cases without leaving the conceptual framework of linear regression."),"\n",r.createElement(t.h3,{id:"generating-polynomial-terms",style:{position:"relative"}},r.createElement(t.a,{href:"#generating-polynomial-terms","aria-label":"generating polynomial terms permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Generating polynomial terms"),"\n",r.createElement(t.p,null,"For a single feature ",r.createElement(s.A,{text:"\\(x\\)"}),", generating polynomial features up to degree ",r.createElement(s.A,{text:"\\(d\\)"})," means you include ",r.createElement(s.A,{text:"\\(x, x^2, \\ldots, x^d\\)"})," as if they were separate input features in a linear model. The resulting hypothesis remains linear in terms of these extended features but can fit non-linear patterns in ",r.createElement(s.A,{text:"\\(x\\)"}),"."),"\n",r.createElement(s.A,{text:"\\[\n\\hat{y} = w_0 + w_1 x + w_2 x^2 + \\dots + w_d x^d.\n\\]"}),"\n",r.createElement(t.h3,{id:"balancing-complexity-and-overfitting-risks",style:{position:"relative"}},r.createElement(t.a,{href:"#balancing-complexity-and-overfitting-risks","aria-label":"balancing complexity and overfitting risks permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Balancing complexity and overfitting risks"),"\n",r.createElement(t.p,null,"Higher-degree polynomials can capture complex trends, yet they are prone to overfitting. A polynomial model of very high degree could track noise in the data rather than the underlying relationship. Regularization strategies (e.g., Ridge, Lasso, or ElasticNet) help penalize large coefficients, reducing variance and improving generalization."),"\n",r.createElement(t.h2,{id:"practical-implementation-hints",style:{position:"relative"}},r.createElement(t.a,{href:"#practical-implementation-hints","aria-label":"practical implementation hints permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Practical implementation hints"),"\n",r.createElement(t.p,null,"In applying linear regression, several practical considerations can make the difference between a robust model and a misleading one:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"Data preprocessing"),":"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"Handle missing values appropriately (imputation or removal if justified)."),"\n",r.createElement(t.li,null,"Address outliers if they are suspected to distort the fit."),"\n",r.createElement(t.li,null,"Use feature scaling (standardization or min-max normalization) especially when using gradient-based optimizers."),"\n"),"\n"),"\n",r.createElement(t.li,null,"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"Choosing the right metric"),":"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"If outliers are critical to capture, MSE might be preferable."),"\n",r.createElement(t.li,null,"If you care about overall consistency in magnitude of errors, MAE might be a better choice."),"\n",r.createElement(t.li,null,"If relative performance is important (e.g., 10k difference means different things for small vs. large values), consider MAPE or SMAPE."),"\n"),"\n"),"\n",r.createElement(t.li,null,"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"Best practices for model evaluation"),":"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"Use cross-validation to obtain a more reliable estimate of performance."),"\n",r.createElement(t.li,null,"Examine residual plots to detect patterns in errors (non-linearity, heteroskedasticity, etc.)."),"\n",r.createElement(t.li,null,"Compare the model against baseline approaches (e.g., predicting the mean, or a simpler model) using ",r.createElement(s.A,{text:"\\(R^2\\)"})," or other relevant statistics."),"\n"),"\n"),"\n",r.createElement(t.li,null,"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"Handling multicollinearity"),":"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"If features are highly correlated, consider dimension reduction techniques (e.g., PCA, introduced later in the course) or apply regularization."),"\n",r.createElement(t.li,null,"Sometimes removing redundant features or combining them in a more meaningful way is sufficient."),"\n"),"\n"),"\n",r.createElement(t.li,null,"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"Large-scale data"),":"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"Instead of the closed-form normal equation, rely on gradient descent or its variants. Libraries like ",r.createElement(i.A,null,"scikit-learn")," often default to more numerically stable decompositions (like SVD)."),"\n",r.createElement(t.li,null,"For extremely large datasets, consider stochastic gradient methods and frameworks with automatic differentiation (e.g., TensorFlow, PyTorch)."),"\n"),"\n"),"\n"),"\n",r.createElement(t.h3,{id:"example-implementations-in-python",style:{position:"relative"}},r.createElement(t.a,{href:"#example-implementations-in-python","aria-label":"example implementations in python permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Example implementations in Python"),"\n",r.createElement(t.p,null,"Below is a simple demonstration of using Python with ",r.createElement(i.A,null,"scikit-learn"),". This example uses a single feature for simplicity, but it easily extends to multiple features."),"\n",r.createElement(o.A,{text:'\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Hypothetical dataset\nX = np.array([1, 2, 3, 4, 5]).reshape(-1,1)\ny = np.array([2.3, 2.9, 3.6, 4.5, 5.1])\n\n# Fit the model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Predictions\ny_pred = model.predict(X)\n\n# Evaluate\nmse = mean_squared_error(y, y_pred)\nprint("MSE:", mse)\nprint("Weights:", model.coef_, "Intercept:", model.intercept_)\n\n# Visualization\nplt.scatter(X, y, label="Data")\nplt.plot(X, y_pred, color="orange", label="Fitted line")\nplt.legend()\nplt.show()\n'}),"\n",r.createElement(t.p,null,"By adding polynomial features:"),"\n",r.createElement(o.A,{text:'\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly_transform = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly_transform.fit_transform(X)\nmodel_poly = LinearRegression()\nmodel_poly.fit(X_poly, y)\n\ny_pred_poly = model_poly.predict(X_poly)\nmse_poly = mean_squared_error(y, y_pred_poly)\nprint("Polynomial MSE:", mse_poly)\n'}),"\n",r.createElement(t.p,null,"If you have more extensive feature sets or large data volumes, consider gradient descent-based methods such as ",r.createElement(i.A,null,"SGDRegressor")," in scikit-learn or frameworks like TensorFlow and PyTorch for automatic differentiation and more advanced optimization strategies."),"\n",r.createElement(n,{alt:"Linear regression line illustration",path:"",caption:"Straight-line fit on a synthetic dataset",zoom:"false"}),"\n",r.createElement(t.p,null,"For multicollinearity, or ill-conditioned feature matrices, you can switch to ",r.createElement(i.A,null,"Ridge")," or ",r.createElement(i.A,null,"Lasso"),":"),"\n",r.createElement(o.A,{text:"\nfrom sklearn.linear_model import Ridge\n\nridge_model = Ridge(alpha=1.0)\nridge_model.fit(X, y)\n"}),"\n",r.createElement(t.p,null,"Larger ",r.createElement(s.A,{text:"(\\alpha\\)"})," in Ridge means more shrinkage on coefficients, mitigating large coefficient blow-ups in near-singular systems."),"\n",r.createElement(t.hr),"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"Key takeaways"),":"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"Linear regression, despite its apparent simplicity, provides a critical stepping stone for more advanced machine learning methods."),"\n",r.createElement(t.li,null,"Proper understanding of cost functions and error metrics ensures consistent optimization and model evaluation."),"\n",r.createElement(t.li,null,"Analytical solutions can be derived neatly, but in practice, we often rely on numerical methods for large-scale or ill-posed problems."),"\n",r.createElement(t.li,null,"Polynomial features allow linear regression to capture non-linearities, albeit with caution regarding overfitting."),"\n",r.createElement(t.li,null,"Thorough data preprocessing, metric selection, and the right blend of regularization remain essential for robust performance."),"\n"),"\n",r.createElement(t.p,null,"This chapter sets the stage for deeper discussions on regularization, model interpretability, and advanced optimization methods. Mastering the fundamentals of linear regression is essential before moving on to more complex machine learning algorithms."))}var m=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,a.RP)(),e.components);return t?r.createElement(t,e,r.createElement(c,e)):c(e)};var h=n(54506),d=n(88864),u=n(58481),p=n.n(u),f=n(5984),g=n(43672),v=n(27042),y=n(72031),E=n(81817),b=n(27105),w=n(17265),x=n(2043),S=n(95751),M=n(94328),z=n(80791),A=n(78137);const _=e=>{let{toc:t}=e;if(!t||!t.items)return null;return r.createElement("nav",{className:z.R},r.createElement("ul",null,t.items.map(((e,t)=>r.createElement("li",{key:t},r.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const n=t.replace("#",""),a=document.getElementById(n);a&&a.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&r.createElement(_,{toc:{items:e.items}}))))))};function L(e){let{data:{mdx:t,allMdx:l,allPostImages:i},children:o}=e;const{frontmatter:s,body:c,tableOfContents:m}=t,d=s.index,u=s.slug.split("/")[1],y=l.nodes.filter((e=>e.frontmatter.slug.includes(`/${u}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),z=y.findIndex((e=>e.frontmatter.index===d)),L=y[z+1],C=y[z-1],H=s.slug.replace(/\/$/,""),N=/[^/]*$/.exec(H)[0],T=`posts/${u}/content/${N}/`,{0:I,1:k}=(0,r.useState)(s.flagWideLayoutByDefault),{0:j,1:P}=(0,r.useState)(!1);var V;(0,r.useEffect)((()=>{P(!0);const e=setTimeout((()=>P(!1)),340);return()=>clearTimeout(e)}),[I]),"adventures"===u?V=w.cb:"research"===u?V=w.Qh:"thoughts"===u&&(V=w.T6);const D=p()(c).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,B=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),n=e%60;return n<=30?`~${t}${n>0?".5":""} h`:`~${t+1} h`}(Math.ceil(D/V)+(s.extraReadTimeMin||0)),R=[{flag:s.flagDraft,component:()=>Promise.all([n.e(5850),n.e(9833)]).then(n.bind(n,49833))},{flag:s.flagMindfuckery,component:()=>Promise.all([n.e(5850),n.e(7805)]).then(n.bind(n,27805))},{flag:s.flagRewrite,component:()=>Promise.all([n.e(5850),n.e(8916)]).then(n.bind(n,78916))},{flag:s.flagOffensive,component:()=>Promise.all([n.e(5850),n.e(6731)]).then(n.bind(n,49112))},{flag:s.flagProfane,component:()=>Promise.all([n.e(5850),n.e(3336)]).then(n.bind(n,83336))},{flag:s.flagMultilingual,component:()=>Promise.all([n.e(5850),n.e(2343)]).then(n.bind(n,62343))},{flag:s.flagUnreliably,component:()=>Promise.all([n.e(5850),n.e(6865)]).then(n.bind(n,11627))},{flag:s.flagPolitical,component:()=>Promise.all([n.e(5850),n.e(4417)]).then(n.bind(n,24417))},{flag:s.flagCognitohazard,component:()=>Promise.all([n.e(5850),n.e(8669)]).then(n.bind(n,18669))},{flag:s.flagHidden,component:()=>Promise.all([n.e(5850),n.e(8124)]).then(n.bind(n,48124))}],{0:O,1:q}=(0,r.useState)([]);return(0,r.useEffect)((()=>{R.forEach((e=>{let{flag:t,component:n}=e;t&&n().then((e=>{q((t=>[].concat((0,h.A)(t),[e.default])))}))}))}),[]),r.createElement(v.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},r.createElement(E.A,{postNumber:s.index,date:s.date,updated:s.updated,readTime:B,difficulty:s.difficultyLevel,title:s.title,desc:s.desc,banner:s.banner,section:u,postKey:N,isMindfuckery:s.flagMindfuckery,mainTag:s.mainTag}),r.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},s.otherTags.map(((e,t)=>r.createElement("span",{key:t,className:`noselect ${A.MW}`,style:{margin:"0 5px 5px 0"}},e)))),r.createElement("div",{className:"postBody"},r.createElement(_,{toc:m})),r.createElement("br",null),r.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},r.createElement(v.P.button,{className:`noselect ${M.pb}`,id:M.xG,onClick:()=>{k(!I)},whileTap:{scale:.93}},r.createElement(v.P.div,{className:S.DJ,key:I,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},I?"Switch to default layout":"Switch to wide layout"))),r.createElement("br",null),r.createElement("div",{className:"postBody",style:{margin:I?"0 -14%":"",maxWidth:I?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},r.createElement("div",{className:`${M.P_} ${j?M.Xn:M.qG}`},O.map(((e,t)=>r.createElement(e,{key:t}))),s.indexCourse?r.createElement(x.A,{index:s.indexCourse,category:s.courseCategoryName}):"",r.createElement(f.Z.Provider,{value:{images:i.nodes,basePath:T.replace(/\/$/,"")+"/"}},r.createElement(a.xA,{components:{Image:g.A}},o)))),r.createElement(b.A,{nextPost:L,lastPost:C,keyCurrent:N,section:u}))}function C(e){return r.createElement(L,e,r.createElement(m,e))}function H(e){var t,n,a,l,i;let{data:o}=e;const{frontmatter:s}=o.mdx,c=s.titleSEO||s.title,m=s.titleOG||c,h=s.titleTwitter||c,u=s.descSEO||s.desc,p=s.descOG||u,f=s.descTwitter||u,g=s.schemaType||"BlogPosting",v=s.keywordsSEO,E=s.date,b=s.updated||E,w=s.imageOG||(null===(t=s.banner)||void 0===t||null===(n=t.childImageSharp)||void 0===n||null===(a=n.gatsbyImageData)||void 0===a||null===(l=a.images)||void 0===l||null===(i=l.fallback)||void 0===i?void 0:i.src),x=s.imageAltOG||p,S=s.imageTwitter||w,M=s.imageAltTwitter||f,z=s.canonicalURL,A=s.flagHidden||!1,_=s.mainTag||"Posts",L=s.slug.split("/")[1]||"posts",{siteUrl:C}=(0,d.Q)(),H={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:C},{"@type":"ListItem",position:2,name:_,item:`${C}/${s.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${C}${s.slug}`}]};return r.createElement(y.A,{title:c+" - avrtt.blog",titleOG:m,titleTwitter:h,description:u,descriptionOG:p,descriptionTwitter:f,schemaType:g,keywords:v,datePublished:E,dateModified:b,imageOG:w,imageAltOG:x,imageTwitter:S,imageAltTwitter:M,canonicalUrl:z,flagHidden:A,mainTag:_,section:L,type:"article"},r.createElement("script",{type:"application/ld+json"},JSON.stringify(H)))}},90548:function(e,t,n){var a=n(96540),r=n(7978);t.A=e=>{let{text:t}=e;return a.createElement(r.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-tsx-content-file-path-src-pages-posts-research-linear-regression-mdx-0a6cbdb3c28e0eff2842.js.map