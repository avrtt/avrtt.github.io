"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[2595],{36874:function(e,t,n){n.r(t),n.d(t,{Head:function(){return _},PostTemplate:function(){return C},default:function(){return z}});var a=n(28453),i=n(96540),r=n(61992),s=n(62087),o=n(90548);function l(e){const t=Object.assign({p:"p",ul:"ul",li:"li",strong:"strong",h3:"h3",a:"a",span:"span",h2:"h2",h4:"h4"},(0,a.RP)(),e.components),{Image:n}=t;return n||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),i.createElement(i.Fragment,null,"\n",i.createElement("br"),"\n","\n",i.createElement(t.p,null,"Deep learning has revolutionized the way we process and interpret complex data, especially in domains like computer vision, natural language processing, and reinforcement learning. However, when it comes to geometric data — such as 3D models, point clouds, meshes, graphs, and voxel representations — there are numerous unique challenges that traditional deep learning architectures are not fully optimized to handle. In this article, I aim to dive deeply into the nature of geometric data, the types of deep learning approaches that can effectively extract geometric features, and how these methods compare with classical geometry processing. This is not a superficial overview: I intend to give you an in-depth look at modern solutions for geometry estimation and shape understanding, including the methodological underpinnings, the relevant mathematics, and practical applications."),"\n",i.createElement(t.p,null,"Before diving into the specialized methods, it's crucial to understand what geometric data looks like, why it is inherently more complicated than regular grids, and how these complications drive the development of specialized learning architectures. The term \"",i.createElement(r.A,null,"geometric data"),'" can refer to multiple representations:'),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Point clouds"),": Collections of points in 3D space (sometimes 2D, but usually 3D in these discussions). Each point may have features such as ",i.createElement(o.A,{text:"\\(x, y, z\\)"})," coordinates, RGB values, surface normals, reflectance, or other attributes."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Meshes"),": A more structured representation that describes surfaces using vertices (points in 3D), edges (connections between vertices), and faces (polygons that define enclosed regions). The most common faces for computational geometry are triangles, but you may also encounter quadrilaterals or more complex polygons."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Graphs"),": Abstract data structures consisting of nodes (vertices) and edges that define the connectivity among them. A mesh can be represented as a graph, but graphs are far more general: you can incorporate edges to represent relationships that are not purely spatial, or to encode multi-scale connectivity in geometric data."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Voxel grids"),": A volumetric representation that partitions 3D space into small, regular 3D cubes, or volumetric pixels (voxels). Each voxel may contain a binary occupancy value, color information, or other feature data."),"\n"),"\n",i.createElement(t.h3,{id:"why-deep-learning-excels-at-extracting-complex-geometric-features",style:{position:"relative"}},i.createElement(t.a,{href:"#why-deep-learning-excels-at-extracting-complex-geometric-features","aria-label":"why deep learning excels at extracting complex geometric features permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Why deep learning excels at extracting complex geometric features"),"\n",i.createElement(t.p,null,"Classical geometry processing techniques traditionally relied on handcrafted features, curvature estimations, or manually defined descriptors. For instance, you might have heard of curvature-based descriptors (Gaussian curvature, mean curvature) or local shape signatures like spin images. While these techniques are extremely valuable and still in use, they typically require substantial domain expertise, do not always generalize well to unseen data, and can struggle with noisy or partial observations."),"\n",i.createElement(t.p,null,"Deep learning models, on the other hand, autonomously learn hierarchical representations of input data. By stacking layers of linear or convolutional filters with nonlinearities, these models can learn features that capture both local details (e.g., local curvature, small-scale geometry) and global structure (e.g., overall shape, topology). This results in a more robust representation that can adapt to new variations, including differences in scale, occlusion, or missing data."),"\n",i.createElement(t.p,null,"Furthermore, deep learning techniques often offer end-to-end learning pipelines. Instead of piecing together multiple handcrafted processing steps, you can train a single model to handle feature extraction, classification or regression tasks, and possibly even generation or reconstruction — all at once."),"\n",i.createElement(t.h3,{id:"comparison-of-classical-and-deep-learning-based-geometric-processing",style:{position:"relative"}},i.createElement(t.a,{href:"#comparison-of-classical-and-deep-learning-based-geometric-processing","aria-label":"comparison of classical and deep learning based geometric processing permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Comparison of classical and deep learning-based geometric processing"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Classical methods")," often revolve around geometry-theoretic principles, differential geometry, partial differential equations for shape smoothing, or spectral geometry for shape analysis. They tend to require domain-specific knowledge and careful parameter tuning (e.g., scale parameters for curvature estimation, thresholds for edge detection)."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Deep learning methods")," aim to reduce the necessity for handcrafted features by training large parametric models. They do, however, rely on large annotated datasets or advanced self-supervised techniques in order to discover relevant geometry features. They also bring new challenges like GPU memory constraints (especially for large 3D grids or giant point clouds) and the need for special architectures that respect geometric properties such as rotational or translation invariance."),"\n"),"\n",i.createElement(t.h3,{id:"outline-of-primary-neural-architectures-used-in-geometric-tasks",style:{position:"relative"}},i.createElement(t.a,{href:"#outline-of-primary-neural-architectures-used-in-geometric-tasks","aria-label":"outline of primary neural architectures used in geometric tasks permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Outline of primary neural architectures used in geometric tasks"),"\n",i.createElement(t.p,null,"There is a plethora of deep neural architectures designed to handle 3D shapes and geometry. Let's briefly enumerate them to orient the rest of this article:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Voxel-based CNNs"),": Extending 2D convolutional layers into 3D."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Sparse CNNs"),": Efficient processing of large 3D volumes with fewer active voxels."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Mesh-based networks"),": Specialized architectures that define convolution-like operations on mesh edges or faces."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Graph neural networks (GNNs)"),": Processing general graph representations of shapes; these might be derived from meshes or other connectivity-based structures."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Point-based networks"),": Architectures (e.g., PointNet, PointNet++) specifically designed for unordered point cloud data, focusing on local neighborhoods or hierarchical groupings."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Transformer-based models"),": Attention-driven methods that can operate on tokenized 3D points, patches, or graph nodes."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Generative models")," for shape synthesis: Autoencoders, variational autoencoders, generative adversarial networks, and neural radiance fields for geometry."),"\n"),"\n",i.createElement(t.p,null,"In the sections that follow, we will explore each of these neural approaches in extensive detail. My intention is to show you how to apply them in practice, as well as to illuminate key theoretical aspects of how they function, what makes them effective, and where challenges remain."),"\n",i.createElement("br"),"\n",i.createElement(t.h2,{id:"convolutional-neural-networks-cnns-for-geometric-data",style:{position:"relative"}},i.createElement(t.a,{href:"#convolutional-neural-networks-cnns-for-geometric-data","aria-label":"convolutional neural networks cnns for geometric data permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Convolutional neural networks (cnnS) for geometric data"),"\n",i.createElement(t.p,null,"Convolutional neural networks are typically associated with image data, employing 2D convolutions to learn from pixel arrays. However, CNNs can also be extended to handle 3D data. This includes the direct extension of 2D convolutions to 3D volumetric convolutions on voxel grids, as well as using 2D CNNs on multiple image-based projections of 3D objects. In this chapter, I will provide a thorough exploration of how CNN-based methods can be leveraged for geometric tasks."),"\n",i.createElement(t.h3,{id:"using-cnns-on-structured-grids-and-voxel-based-representations",style:{position:"relative"}},i.createElement(t.a,{href:"#using-cnns-on-structured-grids-and-voxel-based-representations","aria-label":"using cnns on structured grids and voxel based representations permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Using CNNs on structured grids and voxel-based representations"),"\n",i.createElement(t.p,null,'A straightforward way to bring 3D shapes into the CNN framework is to discretize the 3D space into a volumetric grid. This grid is often referred to as a "voxel grid" — a 3D array of discrete cells, each storing occupancy or some feature vector. Then, 3D convolutions are applied in a manner analogous to 2D convolutions for images:'),"\n",i.createElement(o.A,{text:"\\[\n\\text{feature\\_map}[x, y, z] = \\sum_{i, j, k} W[i, j, k] \\cdot \\text{input}[x+i, y+j, z+k] \n\\]"}),"\n",i.createElement(t.p,null,"Here, ",i.createElement(o.A,{text:"\\(W[i,j,k]\\)"})," represents the learned convolutional filters in three dimensions, and ",i.createElement(o.A,{text:"\\(x, y, z\\)"})," are the voxel coordinates in the feature map. It's straightforward but suffers from large memory consumption: a modest resolution like ",i.createElement(o.A,{text:"\\(64^3\\)"})," can already be quite large in memory, and many shapes need much higher resolutions to capture detail."),"\n",i.createElement(t.p,null,"Despite these challenges, volumetric 3D CNNs have been used with success in tasks like 3D object classification or basic shape completion — especially when the resolution and memory constraints are manageable. Research such as Wu and gang (ShapeNets) has demonstrated that 3D CNNs can be trained on large-scale shape datasets to learn useful volumetric descriptors."),"\n",i.createElement(t.h3,{id:"2d-cnns-for-image-based-projections-of-3d-data",style:{position:"relative"}},i.createElement(t.a,{href:"#2d-cnns-for-image-based-projections-of-3d-data","aria-label":"2d cnns for image based projections of 3d data permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2D CNNs for image-based projections of 3D data"),"\n",i.createElement(t.p,null,"Another approach is to capture a shape by rendering it from multiple viewpoints and then applying standard 2D CNNs to these rendered images. The final representations or scores can be aggregated by pooling or a fully connected layer. This approach has an advantage: 2D CNNs are extremely well-developed, and the memory overhead is typically less severe than storing full 3D grids. However, it can lose some structural information because the 2D images are only partial representations of the 3D object, and occlusions or missing views may degrade performance."),"\n",i.createElement(t.p,null,"Nevertheless, multi-view CNN approaches have a strong track record in 3D shape classification and retrieval. By capturing enough viewpoints, you can create a representation that is quite robust and leverages the power of 2D convolution. Su and gang, for instance, showed that combining multiple projected views with standard CNN architectures yields a powerful shape recognition system."),"\n",i.createElement(t.h3,{id:"3d-cnns-on-voxel-grids",style:{position:"relative"}},i.createElement(t.a,{href:"#3d-cnns-on-voxel-grids","aria-label":"3d cnns on voxel grids permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3D CNNs on voxel grids"),"\n",i.createElement(t.p,null,"In many robotics and CAD applications, the direct 3D volumetric representation is extremely natural. You can feed voxel grids to a 3D CNN, and the output might be a class label, a 3D bounding box, or some reconstruction. The major issues are again memory usage and computational cost. For example, a naive 3D convolution kernel has more parameters than a 2D kernel of the same size (because it extends the kernel in an additional dimension)."),"\n",i.createElement(t.p,null,"There are some mitigations: careful architectures like 3D U-Nets or 3D ResNets can be employed to process volumes in a hierarchical manner (downsampling in the first half, upsampling in the second). This makes tasks like segmentation or reconstruction feasible. However, for truly high-resolution data, specialized methods are still necessary to avoid an explosion in memory usage."),"\n",i.createElement(t.h3,{id:"sparse-convolutions",style:{position:"relative"}},i.createElement(t.a,{href:"#sparse-convolutions","aria-label":"sparse convolutions permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Sparse convolutions"),"\n",i.createElement(t.p,null,'To address the inefficiency of dense voxel grids (where large regions of space might be empty), sparse convolutions have been introduced. Sparse convolutional layers only process the "active" voxels (i.e., voxels that contain data, such as surface points or features above some occupancy threshold), thereby drastically reducing computational requirements. Libraries such as MinkowskiEngine by Choy and gang implement these sparse convolutional operations and have become popular in 3D object detection and segmentation tasks, especially for LiDAR data in autonomous driving.'),"\n",i.createElement(t.p,null,"Sparse convolution-based pipelines can handle huge environments without resorting to extremely coarse voxel resolution or heavy subsampling. This combination of convolutional structure and sparsity has proven indispensable in many large-scale applications — for instance, mapping entire city blocks with fine detail in real time."),"\n",i.createElement("br"),"\n",i.createElement(t.h2,{id:"geometric-specific-architectures-for-cnns",style:{position:"relative"}},i.createElement(t.a,{href:"#geometric-specific-architectures-for-cnns","aria-label":"geometric specific architectures for cnns permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Geometric-specific architectures for cnnS"),"\n",i.createElement(t.p,null,"While voxel-based CNNs or multi-view CNNs have proven effective, they do not always preserve the intrinsic geometry of surfaces or manifold structures. Spherical CNNs and MeshCNNs aim to handle the curved nature of shapes more directly, enabling better use of local geometry and a more faithful representation of surfaces. Let's dissect these specialized variants."),"\n",i.createElement(t.h3,{id:"spherical-cnns",style:{position:"relative"}},i.createElement(t.a,{href:"#spherical-cnns","aria-label":"spherical cnns permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"spherical CNNs"),"\n",i.createElement(t.p,null,"Spherical CNNs are motivated by the desire to handle data that has a natural spherical parameterization, such as omnidirectional cameras or certain classes of shape. They define convolutional kernels on the sphere by parameterizing the sphere with spherical coordinates (",i.createElement(o.A,{text:"(\\theta, \\phi)"}),"). One advantage of spherical parameterizations is rotational equivariance: a rotation on the sphere can be interpreted as a shift in spherical coordinates, which a spherical convolution can handle more gracefully than standard 2D or 3D convolutions."),"\n",i.createElement(t.p,null,"A classical example is the S2CNN approach (Esteves and gang), which leverages group convolutions on the rotation group ",i.createElement(o.A,{text:"\\(\\mathrm{SO}(3)\\)"})," to achieve rotational equivariance. This is particularly relevant for tasks involving panoramic images, environmental mapping, or spherical range data from LiDAR."),"\n",i.createElement(t.p,null,"While spherical CNNs can elegantly handle full 360° coverage of a scene, there are practical challenges. The parameterization can introduce distortions, and implementing convolutions on the sphere involves specialized algorithms and data structures. Nevertheless, if your geometric data or sensor domain fits well into a spherical representation, spherical CNNs can bring significant benefits in capturing global shape context."),"\n",i.createElement(t.h3,{id:"meshcnns",style:{position:"relative"}},i.createElement(t.a,{href:"#meshcnns","aria-label":"meshcnns permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"meshCNNs"),"\n",i.createElement(t.p,null,"Meshes are frequently used for representing surfaces in computer graphics, CAD, and certain robotics tasks. A triangle mesh, for instance, may have many thousands or millions of faces. Unlike images, where pixels lie on a regular grid, the connectivity in a mesh is defined by edges that connect vertices in an irregular pattern. To adapt the concept of convolution to this irregular domain, researchers have explored ways to define local operations that respect the mesh's connectivity and geometry."),"\n",i.createElement(t.p,null,"A typical approach is to define an operation on each mesh edge (or face) that aggregates features from neighboring edges (or faces) according to some kernel. For example, in MeshCNN (Hanocka and gang), the authors define a series of mesh operations that reduce or pool edges, while also applying local filters that gather information from connected edges. This approach respects the underlying geometry and connectivity, enabling the network to learn shape features that would be challenging to express in voxel or point-based representations."),"\n",i.createElement(t.p,null,"Mesh-based networks often incorporate notions of edge collapse or edge pooling to downsample the mesh similarly to pooling in classical CNNs. One concept is to unify geometry features (like edge length, dihedral angles) with learned features at each mesh entity. The result is a hierarchical, convolution-like process that can capture curvature patterns, part structures, or other shape-specific phenomena."),"\n",i.createElement(t.h3,{id:"spectral-methods-on-meshes",style:{position:"relative"}},i.createElement(t.a,{href:"#spectral-methods-on-meshes","aria-label":"spectral methods on meshes permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"spectral methods on meshes"),"\n",i.createElement(t.p,null,'Another category of mesh-based deep learning leverages the graph Laplacian, an operator that generalizes frequency analysis from signals on grids to signals on graphs. For a mesh, the Laplacian can be constructed from its adjacency relationships. This approach is often referred to as "spectral graph convolution" because it uses the eigenbasis of the Laplacian to define convolutions in the frequency domain.'),"\n",i.createElement(t.p,null,"Graph-based Fourier transforms allow the decomposition of a function defined on the vertices of the mesh into orthonormal basis functions. Convolutions can then be expressed as element-wise multiplications in this spectral domain:"),"\n",i.createElement(o.A,{text:"\\[\n\\text{Conv}(f, g) = U \\Big( \\hat{g} \\odot (U^\\top f) \\Big),\n\\]"}),"\n",i.createElement(t.p,null,"where ",i.createElement(o.A,{text:"\\(U\\)"})," is the matrix of eigenvectors of the graph Laplacian, ",i.createElement(o.A,{text:"\\(f\\)"})," is a signal defined on the vertices, and ",i.createElement(o.A,{text:"\\( \\hat{g}\\)"})," is a filter in the spectral domain. Although elegant, this technique can face challenges with scalability (computing eigen decompositions for large meshes can be expensive) and with generalizing across different mesh topologies. A mesh with a different connectivity pattern will yield a different Laplacian basis, complicating direct application to new shapes."),"\n",i.createElement(t.p,null,"However, spectral methods remain a foundational concept in many advanced approaches to geometric deep learning, and they provide an insightful theoretical framework for understanding geometry on irregular domains. They have also inspired subsequent work in localized spectral filters and wavelet-based generalizations that are more robust to changes in mesh topology."),"\n",i.createElement("br"),"\n",i.createElement(t.h2,{id:"graph-neural-networks-gnns",style:{position:"relative"}},i.createElement(t.a,{href:"#graph-neural-networks-gnns","aria-label":"graph neural networks gnns permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Graph neural networks (gnns)"),"\n",i.createElement(t.p,null,'Geometric data is fundamentally about relationships, whether it\'s the relationship between mesh vertices, the connectivity in a 3D scene, or the adjacency structure of shapes. Graph neural networks, or GNNs, provide a powerful framework for learning from these relational structures by performing "message passing" among connected nodes. In a GNN, each node updates its representation by combining messages from its neighbors, enabling the network to capture both local geometry and global topology.'),"\n",i.createElement(t.h3,{id:"message-passing-and-graph-convolutions",style:{position:"relative"}},i.createElement(t.a,{href:"#message-passing-and-graph-convolutions","aria-label":"message passing and graph convolutions permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Message passing and graph convolutions"),"\n",i.createElement(t.p,null,"At the core of GNNs is the idea that each node's next-layer embedding is a function of its current embedding and the aggregated embeddings of its neighbors. A general message-passing step can be written as:"),"\n",i.createElement(o.A,{text:"\\[\nh_i^{(l+1)} = \\sigma \\Big( W_1^{(l)} \\cdot h_i^{(l)} + W_2^{(l)} \\cdot \\sum_{j \\in \\mathcal{N}(i)} M\\big(h_i^{(l)}, h_j^{(l)}, e_{ij}\\big) \\Big),\n\\]"}),"\n",i.createElement(t.p,null,"where ",i.createElement(o.A,{text:"\\(h_i^{(l)}\\)"})," is the embedding of node ",i.createElement(o.A,{text:"\\(i\\)"})," at layer ",i.createElement(o.A,{text:"\\(l\\)"}),", ",i.createElement(o.A,{text:"\\(\\mathcal{N}(i)\\)"})," denotes the neighbors of ",i.createElement(o.A,{text:"\\(i\\)"}),", ",i.createElement(o.A,{text:"\\(e_{ij}\\)"})," is the edge feature between nodes ",i.createElement(o.A,{text:"\\(i\\)"})," and ",i.createElement(o.A,{text:"\\(j\\)"})," (if available), and ",i.createElement(o.A,{text:"\\(\\sigma\\)"})," is a nonlinear activation. ",i.createElement(o.A,{text:"\\(W_1^{(l)}, W_2^{(l)}\\)"})," are trainable weight matrices. ",i.createElement(o.A,{text:"\\(M\\)"})," represents a message function that might simply be an addition or concatenation of node features, or could be more sophisticated. The aggregated neighbor messages are then combined with the node's own features to produce the next embedding."),"\n",i.createElement(t.p,null,"These general steps can be tailored for geometric data by including positional or geometric features in ",i.createElement(o.A,{text:"\\(e_{ij}\\)"}),", such as distances, angles, or curvature measures, ensuring that the GNN's message passing respects the underlying geometry."),"\n",i.createElement(t.h3,{id:"node-embeddings-preserving-geometric-relationships",style:{position:"relative"}},i.createElement(t.a,{href:"#node-embeddings-preserving-geometric-relationships","aria-label":"node embeddings preserving geometric relationships permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Node embeddings: preserving geometric relationships"),"\n",i.createElement(t.p,null,"One of the biggest advantages of GNNs is their ability to incorporate geometric relationships naturally. For example, if your graph is derived from a mesh, each edge might carry information about the local curvature, the normal vectors of the adjacent faces, or the distance between vertices. Through iterative message passing, the network can learn to represent large-scale shape properties (e.g., overall topology or shape class) while preserving local details (e.g., small holes, sharp edges)."),"\n",i.createElement(t.p,null,"Researchers have used GNNs for tasks like:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Shape classification"),": Assigning labels to entire meshes or shapes (chair, table, car, etc.)."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Part segmentation"),": Predicting a label for each mesh vertex or face (e.g., seat vs. back vs. legs of a chair)."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Mesh correspondence"),": Matching vertices of one shape to corresponding vertices of another shape, which is a significant challenge in classical geometry processing."),"\n"),"\n",i.createElement(t.p,null,"In each scenario, GNNs shine by offering flexibility in the form of the adjacency structure and by being able to learn from both local and global connectivity."),"\n",i.createElement(t.h3,{id:"applications-shape-classification-part-segmentation-mesh-correspondence",style:{position:"relative"}},i.createElement(t.a,{href:"#applications-shape-classification-part-segmentation-mesh-correspondence","aria-label":"applications shape classification part segmentation mesh correspondence permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Applications: shape classification, part segmentation, mesh correspondence"),"\n",i.createElement(t.p,null,"GNN-based models for shape classification can outperform classical descriptors by learning to combine local geometry information in an end-to-end fashion. For part segmentation, the iterative propagation of geometric context helps ensure consistency in labeling, even when local geometry might be ambiguous."),"\n",i.createElement(t.p,null,"Mesh correspondence is a more advanced application of GNNs. It seeks to find a bijection or near-bijection mapping between vertices of two shapes, usually with the assumption that the shapes are topologically similar. Classical methods for correspondence might rely on hand-crafted features or geodesic distances, but GNNs can learn how to match correspondences from data, enabling more robust and generalizable solutions — especially when shapes exhibit moderate variations or partial deformations."),"\n",i.createElement("br"),"\n",i.createElement(t.h2,{id:"point-based-learning-frameworks",style:{position:"relative"}},i.createElement(t.a,{href:"#point-based-learning-frameworks","aria-label":"point based learning frameworks permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"point-based learning frameworks"),"\n",i.createElement(t.p,null,"Point-based neural networks represent one of the most significant breakthroughs in learning directly from raw 3D point clouds. In industrial and research contexts, point clouds often arise from LiDARs, 3D scanners, or depth cameras. Unlike images or voxel grids, point clouds don't have a regular structure; they're just sets of points in ",i.createElement(o.A,{text:"\\(\\mathbb{R}^3\\)"}),". This poses unique challenges for deep learning."),"\n",i.createElement(t.h3,{id:"pointnet-and-pointnet",style:{position:"relative"}},i.createElement(t.a,{href:"#pointnet-and-pointnet","aria-label":"pointnet and pointnet permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"PointNet and PointNet++"),"\n",i.createElement(t.p,null,"PointNet (Qi and gang, 2017) was a trailblazer in learning directly from raw, unordered point sets. The original PointNet architecture processes each point individually with a series of shared MLPs (multilayer perceptrons), and then aggregates point features using a global max pooling. The key insight is that the max pooling operation is symmetric with respect to the ordering of input points, which ensures permutation invariance. Formally:"),"\n",i.createElement(o.A,{text:"\\[\nh = \\max_{i=1,\\dots,N} \\big( f_{\\theta}(x_i) \\big),\n\\]"}),"\n",i.createElement(t.p,null,"where ",i.createElement(o.A,{text:"\\(x_i\\)"})," is a point coordinate (possibly extended with color or other features), ",i.createElement(o.A,{text:"\\(f_{\\theta}\\)"})," represents the shared MLP, and the max is taken elementwise across all points. This yields a global feature vector ",i.createElement(o.A,{text:"\\(h\\)"})," describing the entire point cloud. PointNet can then use ",i.createElement(o.A,{text:"\\(h\\)"})," for classification, or combine it with intermediate per-point features for segmentation."),"\n",i.createElement(t.p,null,"A limitation of PointNet is that it doesn't explicitly capture local structures and is somewhat reliant on the network to learn these implicitly. PointNet++ extends the original idea by using a hierarchical approach: it partitions the point cloud into local regions, applies a mini-PointNet to each region, and progressively samples points at different scales. This helps the network learn hierarchical features that mirror the spatial structure of the point cloud."),"\n",i.createElement(t.h4,{id:"code-snippet-a-simple-pointnet-like-forward-pass-in-pytorch",style:{position:"relative"}},i.createElement(t.a,{href:"#code-snippet-a-simple-pointnet-like-forward-pass-in-pytorch","aria-label":"code snippet a simple pointnet like forward pass in pytorch permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Code snippet: a simple PointNet-like forward pass in PyTorch"),"\n",i.createElement(t.p,null,"Below is a simplified illustration of how you might implement a forward pass in a PointNet-like model. Obviously, in practice, you would handle many more details (batch normalization, advanced pooling, local transformations, etc.):"),"\n",i.createElement(s.A,{text:"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SimplePointNet(nn.Module):\n    def __init__(self, in_channels, hidden_dim, out_channels):\n        super(SimplePointNet, self).__init__()\n        self.mlp1 = nn.Linear(in_channels, hidden_dim)\n        self.mlp2 = nn.Linear(hidden_dim, hidden_dim)\n        self.mlp3 = nn.Linear(hidden_dim, out_channels)\n\n    def forward(self, x):\n        # x is shape (batch_size, num_points, in_channels)\n        # Apply MLPs pointwise\n        x = F.relu(self.mlp1(x))\n        x = F.relu(self.mlp2(x))\n        \n        # Global max pooling over points (dim=1)\n        x, _ = torch.max(x, dim=1)  \n        \n        # Final classification or embedding\n        x = self.mlp3(x)\n        return x\n"}),"\n",i.createElement(t.p,null,"PointNet++ adds another layer of complexity by sampling points and grouping them into local neighborhoods before applying the MLP. The local features are then pooled to produce a hierarchy of increasingly higher-level features. This structure helps the model adapt to scenes with widely varying densities, noise, and partial occlusions."),"\n",i.createElement(t.h3,{id:"hierarchical-feature-extraction-and-spatial-attention-mechanisms",style:{position:"relative"}},i.createElement(t.a,{href:"#hierarchical-feature-extraction-and-spatial-attention-mechanisms","aria-label":"hierarchical feature extraction and spatial attention mechanisms permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Hierarchical feature extraction and spatial attention mechanisms"),"\n",i.createElement(t.p,null,"Beyond simple local grouping, more sophisticated networks incorporate dynamic graph neighborhoods, attention layers, or advanced feature pooling. These methods look at subsets of points in local patches, compute local descriptors, and then pass the data up a hierarchy. Some solutions incorporate attention to emphasize key points or to weigh neighbor contributions differently. This can be extremely beneficial in tasks like instance segmentation (detecting individual objects), where certain points might be more relevant for boundary delineation."),"\n",i.createElement(t.h3,{id:"applications-object-recognition-scene-segmentation-shape-reconstruction",style:{position:"relative"}},i.createElement(t.a,{href:"#applications-object-recognition-scene-segmentation-shape-reconstruction","aria-label":"applications object recognition scene segmentation shape reconstruction permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Applications: object recognition, scene segmentation, shape reconstruction"),"\n",i.createElement(t.p,null,"Point-based methods are widely used in many 3D perception tasks:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Object recognition"),": Detecting and classifying objects like cars, pedestrians, or furniture directly from point clouds."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Scene segmentation"),": Labeling each point in a large-scale LiDAR scan (e.g., city blocks, indoor environments)."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Shape reconstruction"),": Working with partial point clouds (common in scanning tasks) and reconstructing a dense mesh or volumetric representation."),"\n"),"\n",i.createElement(t.p,null,"When dealing with large-scale environments, point-based networks must manage huge input sizes (hundreds of thousands or millions of points). Common strategies include random sampling, iterative downsampling, or using voxelization to produce superpoints. Despite these challenges, point-based frameworks remain some of the most direct and elegant ways to handle complex 3D data without imposing rigid grid structures."),"\n",i.createElement("br"),"\n",i.createElement(t.h2,{id:"transformer-based-models-for-geometric-data",style:{position:"relative"}},i.createElement(t.a,{href:"#transformer-based-models-for-geometric-data","aria-label":"transformer based models for geometric data permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"transformer-based models for geometric data"),"\n",i.createElement(t.p,null,"The transformer architecture, which originated in natural language processing (NLP), has proven remarkably versatile. Central to this versatility is the multi-head self-attention mechanism, which enables the model to learn relationships between tokens in a sequence without relying on a strict convolutional or recurrent structure. Recent research has adapted transformer architectures to various geometric data types, particularly point clouds and graphs."),"\n",i.createElement(t.h3,{id:"adapting-attention-based-architectures-to-3d-structures",style:{position:"relative"}},i.createElement(t.a,{href:"#adapting-attention-based-architectures-to-3d-structures","aria-label":"adapting attention based architectures to 3d structures permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Adapting attention-based architectures to 3D structures"),"\n",i.createElement(t.p,null,"The standard transformer processes inputs as a sequence of tokens. To apply it to geometric data, each point (or node in a graph) is considered a token, potentially with positional information or additional attributes. The self-attention mechanism allows each token to attend to other tokens, capturing both local and global patterns in a single layer."),"\n",i.createElement(t.p,null,"For instance, you can embed 3D point coordinates ",i.createElement(o.A,{text:"(x_i, y_i, z_i)"})," into higher-dimensional feature vectors (possibly including color, intensity, etc.), add positional encodings that reflect the geometry, and then feed these into a transformer block:"),"\n",i.createElement(o.A,{text:"\\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\Big(\\frac{Q K^\\top}{\\sqrt{d}}\\Big) V,\n\\]"}),"\n",i.createElement(t.p,null,"where ",i.createElement(o.A,{text:"\\(Q, K, V\\)"})," are the query, key, and value matrices derived from the input embeddings, and ",i.createElement(o.A,{text:"\\(d\\)"})," is the dimensionality. By stacking multiple such attention layers, the network can capture complex relationships across the entire shape."),"\n",i.createElement(t.h3,{id:"positional-encodings-for-capturing-spatial-relationships",style:{position:"relative"}},i.createElement(t.a,{href:"#positional-encodings-for-capturing-spatial-relationships","aria-label":"positional encodings for capturing spatial relationships permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Positional encodings for capturing spatial relationships"),"\n",i.createElement(t.p,null,"In NLP, positional encodings signal the position of tokens in a sentence. For 3D point data, you might define continuous encodings that incorporate the Cartesian coordinates directly, possibly transformed by sinusoidal functions or learned linear layers. Some architectures even compute pairwise distances or angles and incorporate them as edge attributes in a graph-like approach. The goal is to ensure the model understands geometric proximity and orientation, which can be crucial for shape-based tasks."),"\n",i.createElement(t.h3,{id:"advantages-of-self-attention-over-convolution-based-models",style:{position:"relative"}},i.createElement(t.a,{href:"#advantages-of-self-attention-over-convolution-based-models","aria-label":"advantages of self attention over convolution based models permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Advantages of self-attention over convolution-based models"),"\n",i.createElement(t.p,null,"Self-attention's primary advantage is that it can capture long-range dependencies without iterative or hierarchical pooling. In a point cloud with thousands of points, the model could, in principle, learn direct interactions between distant points in a single attention layer. This can be highly beneficial when global context is essential (e.g., understanding the overall shape)."),"\n",i.createElement(t.p,null,"Additionally, transformers can handle variable input sizes more gracefully, as the sequence length can adapt to the number of tokens (points or nodes). However, the computational cost of self-attention scales quadratically with the sequence length, making it challenging to apply naïve transformers to very large point clouds. Techniques like sparse attention or local attention have been developed to mitigate this cost."),"\n",i.createElement(t.h3,{id:"potential-benefits-long-range-dependencies-flexible-representation-interpretability",style:{position:"relative"}},i.createElement(t.a,{href:"#potential-benefits-long-range-dependencies-flexible-representation-interpretability","aria-label":"potential benefits long range dependencies flexible representation interpretability permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Potential benefits: long-range dependencies, flexible representation, interpretability"),"\n",i.createElement(t.p,null,"Transformers often come with better interpretability, thanks to attention maps that can highlight which points or regions are most relevant to a given feature extraction step or classification decision. This can be immensely helpful in diagnosing the network's behavior and refining your model to better focus on salient geometric structures."),"\n",i.createElement("br"),"\n",i.createElement(t.h2,{id:"self-supervised-learning-for-geometric-features",style:{position:"relative"}},i.createElement(t.a,{href:"#self-supervised-learning-for-geometric-features","aria-label":"self supervised learning for geometric features permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"self-supervised learning for geometric features"),"\n",i.createElement(t.p,null,"Many 3D datasets lack comprehensive labels for all tasks of interest, making self-supervised learning appealing. Self-supervised methods create proxy tasks that do not require manual annotation; instead, they exploit intrinsic properties of the data for learning. This approach can produce robust, generalizable representations of shape."),"\n",i.createElement(t.h3,{id:"autoencoders-and-generative-models-for-shape-learning",style:{position:"relative"}},i.createElement(t.a,{href:"#autoencoders-and-generative-models-for-shape-learning","aria-label":"autoencoders and generative models for shape learning permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Autoencoders and generative models for shape learning"),"\n",i.createElement(t.p,null,"An ",i.createElement(t.strong,null,"autoencoder")," tries to reconstruct its own input through a bottleneck layer. By doing so, the encoder learns a latent embedding that must capture essential information about the shape. For 3D data, you might apply an autoencoder to voxel grids, point clouds, or meshes."),"\n",i.createElement(t.p,null,"A typical architecture might have an encoder that reduces the point cloud or voxel grid to a latent code ",i.createElement(o.A,{text:"\\(z\\)"})," and a decoder that attempts to reconstruct the shape from ",i.createElement(o.A,{text:"\\(z\\)"}),". Formally:"),"\n",i.createElement(o.A,{text:"\\[\nz = f_{\\theta}(X), \\quad \\hat{X} = g_{\\phi}(z),\n\\]"}),"\n",i.createElement(t.p,null,"where ",i.createElement(o.A,{text:"\\(X\\)"})," is the shape representation, ",i.createElement(o.A,{text:"\\(f_{\\theta}\\)"})," is the encoder, and ",i.createElement(o.A,{text:"\\(g_{\\phi}\\)"})," is the decoder. The network is trained to minimize a reconstruction loss ",i.createElement(o.A,{text:"\\(L(\\hat{X}, X)\\)"})," (e.g., Chamfer distance for point clouds). This compresses shape information into a compact code that can then be used for downstream tasks like classification, segmentation, or shape editing."),"\n",i.createElement(t.p,null,"Generative adversarial networks (GANs) for 3D shapes also exist, enabling unsupervised or self-supervised training of shape generators. A popular line of work (e.g., 3D-GAN by Wu and gang) focuses on voxel representations, while others attempt point-based or implicit surfaces."),"\n",i.createElement(t.h3,{id:"contrastive-learning-with-geometric-transformations",style:{position:"relative"}},i.createElement(t.a,{href:"#contrastive-learning-with-geometric-transformations","aria-label":"contrastive learning with geometric transformations permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Contrastive learning with geometric transformations"),"\n",i.createElement(t.p,null,"Contrastive learning encourages embeddings of augmented versions of the same shape to be similar, while embeddings of different shapes are pushed apart. You can define geometric augmentations such as random rotations, translations, or partial occlusions to create positive pairs. These augmentations serve as a pretext task, training a network to be invariant (or robust) to those transformations."),"\n",i.createElement(t.p,null,"Methods in 3D contrastive learning might define a contrastive loss function such as:"),"\n",i.createElement(o.A,{text:"\\[\nL = - \\sum_{i=1}^N \\log \\frac{\\exp(\\text{sim}(z_i, z_i^+) / \\tau)}{\\sum_{j=1}^N \\exp(\\text{sim}(z_i, z_j) / \\tau)},\n\\]"}),"\n",i.createElement(t.p,null,"where ",i.createElement(o.A,{text:"\\(z_i\\)"})," is the latent embedding for shape ",i.createElement(o.A,{text:"\\(i\\)"}),", ",i.createElement(o.A,{text:"\\(z_i^+\\)"})," is the embedding of an augmented version of shape ",i.createElement(o.A,{text:"\\(i\\)"}),", and ",i.createElement(o.A,{text:"\\(\\text{sim}\\)"})," is a similarity measure (often cosine similarity). ",i.createElement(o.A,{text:"\\(\\tau\\)"})," is a temperature parameter. This approach can substantially improve downstream performance, even with limited labels."),"\n",i.createElement(t.h3,{id:"shape-completion-and-inpainting-as-self-supervised-tasks",style:{position:"relative"}},i.createElement(t.a,{href:"#shape-completion-and-inpainting-as-self-supervised-tasks","aria-label":"shape completion and inpainting as self supervised tasks permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Shape completion and inpainting as self-supervised tasks"),"\n",i.createElement(t.p,null,"Another self-supervised strategy is shape completion or inpainting. The network is given a partial shape and must reconstruct the full shape. By artificially masking or removing points, the network learns to fill in the missing geometry. This is conceptually similar to image inpainting but extended into 3D. The local and global cues learned in this process are often generalizable and beneficial for tasks like classification, part segmentation, or normal estimation."),"\n",i.createElement("br"),"\n",i.createElement(t.h2,{id:"hybrid-models-combining-symbolic-and-neural-approaches",style:{position:"relative"}},i.createElement(t.a,{href:"#hybrid-models-combining-symbolic-and-neural-approaches","aria-label":"hybrid models combining symbolic and neural approaches permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"hybrid models combining symbolic and neural approaches"),"\n",i.createElement(t.p,null,"Purely data-driven deep learning has its strengths but can struggle with certain forms of domain knowledge — especially explicit geometric rules, topological constraints, or logical relationships. Hybrid methods aim to fuse the flexibility of neural networks with the rigor of symbolic or rule-based systems."),"\n",i.createElement(t.h3,{id:"combining-rule-based-geometric-constraints-with-deep-learning",style:{position:"relative"}},i.createElement(t.a,{href:"#combining-rule-based-geometric-constraints-with-deep-learning","aria-label":"combining rule based geometric constraints with deep learning permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"combining rule-based geometric constraints with deep learning"),"\n",i.createElement(t.p,null,"In some CAD or architectural design scenarios, geometric constraints might be well-defined by domain experts: angles must sum to certain values, parts must remain aligned, or shapes must obey certain structural constraints. A hybrid system can incorporate these constraints directly, either by post-processing neural predictions or by building them into the architecture through special layers or losses."),"\n",i.createElement(t.h3,{id:"neural-networks-as-approximate-solvers",style:{position:"relative"}},i.createElement(t.a,{href:"#neural-networks-as-approximate-solvers","aria-label":"neural networks as approximate solvers permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"neural networks as approximate solvers"),"\n",i.createElement(t.p,null,"In tasks such as inverse kinematics or motion planning for robotics, classical methods might rely heavily on geometry-based solvers. However, you can approximate these solvers with a neural network that's faster to run at inference time. The network can be trained using simulation data or real-world data, effectively learning to approximate the symbolic solver's mapping from configurations to feasible poses."),"\n",i.createElement(t.h3,{id:"symbolic-representations-of-geometric-relations-in-neural-networks",style:{position:"relative"}},i.createElement(t.a,{href:"#symbolic-representations-of-geometric-relations-in-neural-networks","aria-label":"symbolic representations of geometric relations in neural networks permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"symbolic representations of geometric relations in neural networks"),"\n",i.createElement(t.p,null,"An emerging area of research focuses on designing layers or modules that explicitly store symbolic constraints. For instance, you might encode the knowledge that two faces must remain parallel or that a certain angle must remain constant. The neural network can then correct its predictions to respect these constraints, or at least incorporate them as a differentiable prior."),"\n",i.createElement(t.p,null,"This approach might be employed in advanced 3D modeling tools that allow partial manual constraints (e.g., parallel lines, tangential arcs) while letting the neural network fill in the rest. The interplay between symbolic and learned representations promises improved accuracy, interpretability, and user control."),"\n",i.createElement(t.h3,{id:"use-cases-robotics-motion-planning-cad-model-generation-structural-optimization",style:{position:"relative"}},i.createElement(t.a,{href:"#use-cases-robotics-motion-planning-cad-model-generation-structural-optimization","aria-label":"use cases robotics motion planning cad model generation structural optimization permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"use cases: robotics motion planning, CAD model generation, structural optimization"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Robotics motion planning"),": Incorporating geometry constraints (obstacle avoidance, kinematic feasibility) within a neural policy that can handle uncertain or dynamic environments."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"CAD model generation"),": Respecting design rules and parametric constraints while generating new part geometries or entire assemblies."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Structural optimization"),": Coupling finite element analysis (a symbolic or numeric approach) with deep learning to explore large design spaces efficiently."),"\n"),"\n",i.createElement("br"),"\n",i.createElement(t.h2,{id:"3d-vision-and-perception",style:{position:"relative"}},i.createElement(t.a,{href:"#3d-vision-and-perception","aria-label":"3d vision and perception permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3D vision and perception"),"\n",i.createElement(t.p,null,"3D vision spans a broad range of tasks, from estimating depth from images to building complete 3D reconstructions of a scene. Deep learning has significantly advanced the state of the art by enabling robust features for depth estimation, semantic segmentation, and 3D object detection."),"\n",i.createElement(t.h3,{id:"depth-estimation",style:{position:"relative"}},i.createElement(t.a,{href:"#depth-estimation","aria-label":"depth estimation permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Depth estimation"),"\n",i.createElement(t.p,null,"Depth estimation can be approached via stereo vision, structure-from-motion (SfM), or even single-image depth prediction:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Stereo vision")," uses two or more calibrated cameras to infer depth by triangulating the disparities between corresponding pixels in different camera views. Deep learning networks (e.g., GA-Net) can help by providing robust matching cost functions."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Structure-from-motion")," uses multiple views from a moving camera to reconstruct a scene's 3D points, typically relying on classical multi-view geometry to solve for camera poses and 3D points. However, deep features can significantly improve the reliability of keypoint matching and outlier rejection."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Monocular cues")," rely on a single image, learning depth from large datasets of paired RGB and depth images. A convolutional or transformer-based model estimates a dense depth map, often supplemented by geometric constraints or smoothness priors."),"\n"),"\n",i.createElement(t.h3,{id:"multi-view-geometry",style:{position:"relative"}},i.createElement(t.a,{href:"#multi-view-geometry","aria-label":"multi view geometry permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Multi-view geometry"),"\n",i.createElement(t.p,null,"Multi-view geometry is fundamental to 3D perception. It describes how 3D points project into 2D images, the constraints linking camera poses, and the geometry of epipolar lines. Epipolar geometry is crucial in stereo setups, ensuring that a point in one image can only lie on a corresponding epipolar line in the other image."),"\n",i.createElement(t.p,null,"Camera calibration (intrinsic and extrinsic parameters) is another cornerstone. Knowing the focal length, principal point, and the camera orientation relative to a world coordinate system is essential for accurate 3D reconstruction or scene understanding. Neural networks increasingly assist in both calibrations (learning how to refine or estimate camera intrinsics) and pose estimation (learning robust correspondences or direct pose regression)."),"\n",i.createElement(t.h3,{id:"semantic-scene-understanding",style:{position:"relative"}},i.createElement(t.a,{href:"#semantic-scene-understanding","aria-label":"semantic scene understanding permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Semantic scene understanding"),"\n",i.createElement(t.p,null,"Scene segmentation in 3D, object detection, and instance segmentation are cornerstones in 3D perception. Methods like ",i.createElement(t.strong,null,"Mask R-CNN")," have been extended to 3D data, or multi-view approaches can fuse 2D detections to produce 3D bounding boxes. Modern pipelines often combine classical geometry (like SLAM) with advanced deep learning for object detection, enabling robots or autonomous vehicles to build semantically rich maps of the environment."),"\n",i.createElement("br"),"\n",i.createElement(t.h2,{id:"scene-rendering-and-view-synthesis",style:{position:"relative"}},i.createElement(t.a,{href:"#scene-rendering-and-view-synthesis","aria-label":"scene rendering and view synthesis permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"scene rendering and view synthesis"),"\n",i.createElement(t.p,null,"Another critical direction in geometric deep learning is scene rendering and view synthesis: using learned representations to generate realistic 2D images from 3D content or to create novel views from limited input data. This has exciting implications for VR/AR, robotics, and any application where we need to visualize or interpret 3D structures."),"\n",i.createElement(t.h3,{id:"differentiable-rendering",style:{position:"relative"}},i.createElement(t.a,{href:"#differentiable-rendering","aria-label":"differentiable rendering permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"differentiable rendering"),"\n",i.createElement(t.p,null,"Traditional rendering pipelines in computer graphics rely on rasterization or ray tracing. Differentiable rendering introduces rendering operators that are differentiable with respect to scene parameters, such as mesh vertex positions or material properties. This allows gradients to flow from pixel-level losses back into geometry or texture representations, enabling end-to-end learning of geometry and appearance."),"\n",i.createElement(t.h3,{id:"neural-radiance-fields-nerfs",style:{position:"relative"}},i.createElement(t.a,{href:"#neural-radiance-fields-nerfs","aria-label":"neural radiance fields nerfs permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"neural radiance fields (nerfs)"),"\n",i.createElement(t.p,null,"NeRFs (Mildenhall and gang) represent a scene as a continuous function — typically parameterized by a multi-layer perceptron (MLP) — that maps 3D coordinates and viewing directions to color and density values. By rendering scenes using volume rendering, NeRFs can generate novel views that match real training images surprisingly well. Formally, the rendering equation for NeRFs integrates sampled colors and densities along a ray:"),"\n",i.createElement(o.A,{text:"\\[\nC(r) = \\int_{t_n}^{t_f} T(t) \\sigma(\\mathbf{x}(t)) \\mathbf{c}(\\mathbf{x}(t), \\mathbf{d}) \\, dt,\n\\]"}),"\n",i.createElement(t.p,null,"where ",i.createElement(o.A,{text:"\\(C(r)\\)"})," is the rendered color along ray ",i.createElement(o.A,{text:"\\(r\\)"}),", ",i.createElement(o.A,{text:"\\(\\mathbf{x}(t)\\)"})," is a point along the ray at depth ",i.createElement(o.A,{text:"\\(t\\)"}),", ",i.createElement(o.A,{text:"\\(\\sigma(\\mathbf{x}(t))\\)"})," is the density, and ",i.createElement(o.A,{text:"\\(\\mathbf{c}(\\mathbf{x}(t), \\mathbf{d})\\)"})," is the color. ",i.createElement(o.A,{text:"\\(T(t) = \\exp\\Big(-\\int_{t_n}^t \\sigma(\\mathbf{x}(s)) ds\\Big)\\)"})," is the transmittance. NeRFs use a neural network to approximate ",i.createElement(o.A,{text:"\\(\\sigma\\)"})," and ",i.createElement(o.A,{text:"\\(\\mathbf{c}\\)"}),", learning a photorealistic representation of the scene."),"\n",i.createElement(t.h3,{id:"generative-query-networks-gqn",style:{position:"relative"}},i.createElement(t.a,{href:"#generative-query-networks-gqn","aria-label":"generative query networks gqn permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"generative query networks (gqn)"),"\n",i.createElement(t.p,null,"GQNs can learn an internal representation of a scene such that they can render that scene from arbitrary viewpoints. By training on multiple views of synthetic or real environments, a GQN learns to generate consistent new views that reflect the underlying 3D structure. While not as explicitly geometric as NeRFs, GQNs also highlight the power of latent scene representations in deep learning."),"\n",i.createElement(t.h3,{id:"applications-in-vrar",style:{position:"relative"}},i.createElement(t.a,{href:"#applications-in-vrar","aria-label":"applications in vrar permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"applications in VR/AR"),"\n",i.createElement(t.p,null,"View synthesis is extremely important for VR/AR, where you need to render realistic scenes from user viewpoints that change in real time. Real-time neural rendering remains a challenge, but advances in hardware acceleration and optimized neural networks are bridging this gap. Eventually, we can expect interactive AR experiences that are powered by neural scene representations, enabling robust occlusion handling, real-time lighting, and dynamic object insertion."),"\n",i.createElement("br"),"\n",i.createElement(t.h2,{id:"scene-reconstruction",style:{position:"relative"}},i.createElement(t.a,{href:"#scene-reconstruction","aria-label":"scene reconstruction permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"scene reconstruction"),"\n",i.createElement(t.p,null,"Scene reconstruction synthesizes many of the previously discussed technologies, combining 3D perception with modeling and often with semantic information. In robotics, scene reconstruction is frequently paired with ",i.createElement(t.strong,null,"Simultaneous Localization and Mapping (SLAM)")," so that a robot or drone can navigate and reconstruct the environment on the fly."),"\n",i.createElement(t.h3,{id:"simultaneous-localization-and-mapping-slam",style:{position:"relative"}},i.createElement(t.a,{href:"#simultaneous-localization-and-mapping-slam","aria-label":"simultaneous localization and mapping slam permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"simultaneous localization and mapping (slam)"),"\n",i.createElement(t.p,null,"SLAM solutions fuse sensor data (camera images, LiDAR, IMUs) to both locate the sensor in the environment and to build a map of the environment itself. Deep learning has improved SLAM by providing robust feature extraction (for keypoints and descriptors), loop closure detection, or semantic labeling. Some approaches even integrate a learned depth or flow network for more accurate motion estimation."),"\n",i.createElement(t.h3,{id:"multi-sensor-fusion",style:{position:"relative"}},i.createElement(t.a,{href:"#multi-sensor-fusion","aria-label":"multi sensor fusion permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"multi-sensor fusion"),"\n",i.createElement(t.p,null,"Modern reconstruction pipelines may incorporate LiDAR scans for large-scale outdoor environments, RGB-D cameras for detailed indoor maps, and inertial sensors for additional motion cues. Fusing these data streams can yield reconstructions with high fidelity and robustness to sensor-specific noise. Deep networks can weigh or align these streams based on learned features, ensuring that the final map or model is both geometrically and semantically consistent."),"\n",i.createElement(t.h3,{id:"advanced-reconstruction-techniques",style:{position:"relative"}},i.createElement(t.a,{href:"#advanced-reconstruction-techniques","aria-label":"advanced reconstruction techniques permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"advanced reconstruction techniques"),"\n",i.createElement(t.p,null,"Beyond producing a bare-bones point cloud or mesh, advanced scene reconstruction might:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Texture map")," the resulting mesh with high-quality textures."),"\n",i.createElement(t.li,null,"Apply ",i.createElement(t.strong,null,"semantic labeling")," so that different regions are identified by class or instance."),"\n",i.createElement(t.li,null,"Perform ",i.createElement(t.strong,null,"mesh refinement"),", smoothing surfaces or ensuring manifold properties."),"\n",i.createElement(t.li,null,"Employ ",i.createElement(t.strong,null,"neural-based completion")," to fill gaps in partial scans or to infer unseen geometry."),"\n"),"\n",i.createElement("br"),"\n",i.createElement(t.h2,{id:"novel-view-synthesis",style:{position:"relative"}},i.createElement(t.a,{href:"#novel-view-synthesis","aria-label":"novel view synthesis permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"novel view synthesis"),"\n",i.createElement(t.p,null,"Though closely related to scene rendering, novel view synthesis has become an extensive domain of research on its own, with multiple specialized approaches. The goal: generate new perspectives of a scene or object based on limited input views. NeRF-like approaches or GQNs are prime examples, but many other specialized methods exist, each with unique trade-offs."),"\n",i.createElement(t.h3,{id:"learning-based-approaches-for-generating-new-perspectives",style:{position:"relative"}},i.createElement(t.a,{href:"#learning-based-approaches-for-generating-new-perspectives","aria-label":"learning based approaches for generating new perspectives permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"learning-based approaches for generating new perspectives"),"\n",i.createElement(t.p,null,"Machine learning-based view synthesis can use 2D CNNs on stacks of images or rely on 3D representations like voxel grids or point clouds with learned texture. The potential is immense, from single-view to multi-view approaches. Single-view approaches rely heavily on learned priors to hallucinate unseen parts of a scene, while multi-view approaches can more accurately reconstruct geometry and texture if enough viewpoints are available."),"\n",i.createElement(t.h3,{id:"nerf-based-view-synthesis",style:{position:"relative"}},i.createElement(t.a,{href:"#nerf-based-view-synthesis","aria-label":"nerf based view synthesis permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"NeRF-based view synthesis"),"\n",i.createElement(t.p,null,"NeRFs are currently among the state-of-the-art methods for photorealistic novel view synthesis. By optimizing a neural network to reproduce the color intensities of training views, a NeRF effectively inverts the rendering process to learn a volumetric representation. This representation is then used to render new views by tracing rays through the learned volume. Despite their impressive results, standard NeRFs have drawbacks, including lengthy per-scene training times and a reliance on accurate camera pose estimates."),"\n",i.createElement(t.h3,{id:"real-world-applications",style:{position:"relative"}},i.createElement(t.a,{href:"#real-world-applications","aria-label":"real world applications permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"real-world applications"),"\n",i.createElement(t.p,null,"Novel view synthesis is used in AR/VR to let users seamlessly move around a virtual environment. It's also used to generate training data for other machine learning models. For instance, you might photograph an object from a handful of angles, generate extra synthetic views, and use them to train a robust classifier or object detector. The synergy between geometry learning and synthetic data generation is particularly exciting, as it allows AI to learn effectively even when real-world data is scarce."),"\n",i.createElement("br"),"\n",i.createElement(t.h2,{id:"scalability-and-large-scale-geometric-data",style:{position:"relative"}},i.createElement(t.a,{href:"#scalability-and-large-scale-geometric-data","aria-label":"scalability and large scale geometric data permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"scalability and large-scale geometric data"),"\n",i.createElement(t.p,null,"Many real-world geometric applications involve massive datasets. Whether you're scanning entire cities, creating high-fidelity 3D maps for autonomous driving, or analyzing million-point point clouds from industrial metrology scans, you need algorithms that scale in terms of both memory and computation."),"\n",i.createElement(t.h3,{id:"handling-massive-3d-datasets",style:{position:"relative"}},i.createElement(t.a,{href:"#handling-massive-3d-datasets","aria-label":"handling massive 3d datasets permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"handling massive 3D datasets"),"\n",i.createElement(t.p,null,"One of the biggest challenges is memory consumption. High-density scans might produce billions of points. Even if you decimate or downsample, you can still be dealing with tens of millions of points. Parallel processing is often essential, leveraging GPUs or distributed systems to handle the data in chunks or to run specialized 3D deep learning frameworks. Cloud computing platforms offer on-demand scalability, but data transfer costs and hardware provisioning complexities can become bottlenecks."),"\n",i.createElement(t.h3,{id:"out-of-core-algorithms",style:{position:"relative"}},i.createElement(t.a,{href:"#out-of-core-algorithms","aria-label":"out of core algorithms permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"out-of-core algorithms"),"\n",i.createElement(t.p,null,'"Out-of-core" methods process data that does not fit entirely in GPU or even system memory by streaming data chunks from disk or network. This technique requires specialized data structures to efficiently load relevant portions of the dataset and to store intermediate results. Some deep learning libraries are beginning to adopt out-of-core or streaming-based approaches, but it\'s still a relatively nascent area compared to 2D image processing pipelines.'),"\n",i.createElement(t.h3,{id:"real-world-deployment",style:{position:"relative"}},i.createElement(t.a,{href:"#real-world-deployment","aria-label":"real world deployment permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"real-world deployment"),"\n",i.createElement(t.p,null,"In real-world settings, you might have to balance speed, accuracy, and resource constraints. For instance, an autonomous vehicle cannot spend minutes processing a LiDAR scan; it needs quick turnarounds. Sparse representations, efficient kernel approximations, and specialized hardware acceleration (like dedicated 3D convolution modules) are all active areas of research and engineering to help meet these stringent demands."),"\n",i.createElement("br"),"\n",i.createElement(t.h2,{id:"future-directions-and-open-challenges",style:{position:"relative"}},i.createElement(t.a,{href:"#future-directions-and-open-challenges","aria-label":"future directions and open challenges permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"future directions and open challenges"),"\n",i.createElement(t.p,null,"Geometric deep learning is a rapidly evolving field. New architectures and techniques continue to push the boundaries of what is possible in 3D understanding, reconstruction, and synthesis. Yet many challenges remain."),"\n",i.createElement(t.h3,{id:"temporal-geometry-learning-from-dynamic-4d-datasets",style:{position:"relative"}},i.createElement(t.a,{href:"#temporal-geometry-learning-from-dynamic-4d-datasets","aria-label":"temporal geometry learning from dynamic 4d datasets permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"temporal geometry: learning from dynamic 4D datasets"),"\n",i.createElement(t.p,null,'Most of what we\'ve covered focuses on static 3D geometry. However, time-variant geometry, or "4D" data, arises in a variety of contexts: human motion capture, fluid simulations, dynamic environmental scans, or any scenario where objects move and deform. Methods that can effectively handle 4D data — capturing changes over time — are still in their infancy. The ability to combine spatial and temporal features at scale could enable more realistic simulations, better motion analysis, and robust predictive models for dynamic scenes.'),"\n",i.createElement(t.h3,{id:"explainable-geometric-models",style:{position:"relative"}},i.createElement(t.a,{href:"#explainable-geometric-models","aria-label":"explainable geometric models permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"explainable geometric models"),"\n",i.createElement(t.p,null,"The interpretability of deep models for 3D data lags behind 2D vision models. In geometric contexts, it's often critical to understand why a system made a particular reconstruction or classification, especially for safety-critical domains like robotics or autonomous driving. Techniques that provide local or global explanations — e.g., identifying which regions of a shape triggered a certain prediction — are increasingly important."),"\n",i.createElement(t.h3,{id:"advances-in-sensors-and-hardware",style:{position:"relative"}},i.createElement(t.a,{href:"#advances-in-sensors-and-hardware","aria-label":"advances in sensors and hardware permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"advances in sensors and hardware"),"\n",i.createElement(t.p,null,"On the hardware side, LiDAR sensors are becoming cheaper and more accurate, consumer-grade depth cameras are improving, and specialized sensors like event cameras offer new paradigms for data collection. Neuromorphic computing, with event-based data streams, may inspire new neural architectures that process spatiotemporal information more efficiently. The synergy between new sensor technologies and advanced geometric learning algorithms could open entirely new frontiers in 3D perception and modeling."),"\n",i.createElement("br"),"\n",i.createElement(t.h2,{id:"data-representation-challenges",style:{position:"relative"}},i.createElement(t.a,{href:"#data-representation-challenges","aria-label":"data representation challenges permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"data representation challenges"),"\n",i.createElement(t.p,null,"No discussion of geometric learning would be complete without highlighting the inherent representation challenges that come with 3D data:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Irregularity of geometric data"),": 3D shapes often have non-uniform densities, holes, or partial visibility, making them more difficult to handle than regular pixel grids."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Scalability"),": As resolution or scene size increases, so does memory usage, computational cost, and complexity."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Quantization trade-offs"),": Voxel grids are easy to handle but lose detail if the resolution is too low. Point-based methods must deal with partial coverage, missing data, and noise. Mesh-based methods can capture surfaces well but require consistent connectivity."),"\n"),"\n",i.createElement(t.p,null,"Balancing these trade-offs often requires domain-specific choices. Autonomous driving might rely on sparse 3D point-based methods for speed, whereas film or gaming might use high-resolution mesh or volumetric methods for visual fidelity. Researchers continue to innovate with hybrid or adaptive representations that combine the best of each approach."),"\n",i.createElement("br"),"\n",i.createElement(t.h2,{id:"additional-perspectives-and-expansions",style:{position:"relative"}},i.createElement(t.a,{href:"#additional-perspectives-and-expansions","aria-label":"additional perspectives and expansions permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Additional perspectives and expansions"),"\n",i.createElement(t.p,null,"Given the scope of modern geometry estimation and the depth of ongoing research, there are other subtopics that, while briefly touched upon, deserve deeper consideration. I will add a few extra insights below for completeness, ensuring you see some potential directions for further study."),"\n",i.createElement(t.h3,{id:"implicit-neural-representations",style:{position:"relative"}},i.createElement(t.a,{href:"#implicit-neural-representations","aria-label":"implicit neural representations permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"implicit neural representations"),"\n",i.createElement(t.p,null,"We touched on NeRFs as an implicit representation of geometry and appearance. More broadly, implicit neural representations represent 3D shapes as level sets of an MLP ",i.createElement(o.A,{text:"\\(f_{\\theta}(x, y, z)\\)"}),". For a surface, you could define:"),"\n",i.createElement(o.A,{text:"\\[\nf_{\\theta}(x, y, z) = 0 \\implies \\text{surface}\n\\]"}),"\n",i.createElement(t.p,null,"These representations circumvent the need for explicit discretization (like voxels) or explicit connectivity (like meshes) and can yield high-resolution surfaces from small latent codes. They can also incorporate other attributes, like textures or material properties."),"\n",i.createElement(t.h3,{id:"multi-modal-geometric-learning",style:{position:"relative"}},i.createElement(t.a,{href:"#multi-modal-geometric-learning","aria-label":"multi modal geometric learning permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"multi-modal geometric learning"),"\n",i.createElement(t.p,null,"Many applications combine 3D geometry with additional modalities: color images, text, or even audio. Multi-modal networks might fuse data from multiple sensors (RGB, depth, IR, LiDAR) or incorporate textual instructions (in robotics tasks). Integrating geometry with these modalities extends deep learning's capabilities, enabling tasks like natural language manipulation of 3D objects or multi-sensor scene analysis. It also introduces new complexities in network design and training."),"\n",i.createElement(t.h3,{id:"domain-adaptation-and-transfer-learning",style:{position:"relative"}},i.createElement(t.a,{href:"#domain-adaptation-and-transfer-learning","aria-label":"domain adaptation and transfer learning permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"domain adaptation and transfer learning"),"\n",i.createElement(t.p,null,"3D datasets can differ significantly across domains. For instance, synthetic CAD models are typically clean, manifold, and complete, whereas real-world scans are noisy and incomplete. Domain adaptation techniques attempt to bridge the gap between these distributions. Self-supervised pretraining on synthetic data followed by fine-tuning on real scans is one strategy, but more sophisticated methods explicitly align distributions or correct for domain-specific artifacts."),"\n",i.createElement(t.h3,{id:"real-time-geometric-deep-learning",style:{position:"relative"}},i.createElement(t.a,{href:"#real-time-geometric-deep-learning","aria-label":"real time geometric deep learning permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"real-time geometric deep learning"),"\n",i.createElement(t.p,null,"Robotics, AR/VR, and autonomous driving applications often require real-time or near real-time performance. Achieving this demands not only efficient architectures (like sparse CNNs, lightweight GNNs, or pruned transformers) but also hardware-specific optimizations. As GPUs and specialized accelerators improve, we might see more real-time 3D perception tasks that once were considered computationally prohibitive."),"\n",i.createElement("br"),"\n",i.createElement(t.h2,{id:"potential-code-example-partial-shape-completion",style:{position:"relative"}},i.createElement(t.a,{href:"#potential-code-example-partial-shape-completion","aria-label":"potential code example partial shape completion permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Potential code example: partial shape completion"),"\n",i.createElement(t.p,null,"To illustrate a practical scenario, consider a partial shape completion pipeline using a point-based autoencoder approach. Suppose you have partial point clouds from a 3D scanner and wish to complete them to full shapes."),"\n",i.createElement(s.A,{text:"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass PointCloudAutoencoder(nn.Module):\n    def __init__(self, input_dim=3, bottleneck=128):\n        super(PointCloudAutoencoder, self).__init__()\n        # Encoder\n        self.enc1 = nn.Linear(input_dim, 64)\n        self.enc2 = nn.Linear(64, 128)\n        self.enc3 = nn.Linear(128, bottleneck)\n        \n        # Decoder\n        self.dec1 = nn.Linear(bottleneck, 128)\n        self.dec2 = nn.Linear(128, 64)\n        # We'll output 3D coordinates for each point, \n        # assuming we want a fixed number of points\n        self.dec3 = nn.Linear(64, input_dim)\n\n    def forward(self, x):\n        # x: (batch_size, num_points, 3)\n        bsz, num_pts, _ = x.size()\n        x = x.view(bsz * num_pts, -1)\n        \n        # Encoder\n        x = F.relu(self.enc1(x))\n        x = F.relu(self.enc2(x))\n        x = self.enc3(x)  # bottleneck features\n        # Global max pooling across all points\n        x = x.view(bsz, num_pts, -1)\n        x, _ = torch.max(x, dim=1)\n        \n        # Decoder\n        x = F.relu(self.dec1(x))\n        x = F.relu(self.dec2(x))\n        x = self.dec3(x)\n        \n        # Expand back to num_points\n        # In a typical approach, you'd have a final layer \n        # output that has (num_points*3) to reorder, \n        # or a different scheme for sampling\n        x = x.unsqueeze(1).repeat(1, num_pts, 1)\n        return x\n"}),"\n",i.createElement(t.p,null,"This simplified example demonstrates the core idea but omits multiple complexities. In practice, you might:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"Use a more sophisticated architecture (e.g., PointNet++ style hierarchical grouping)."),"\n",i.createElement(t.li,null,"Incorporate skip connections to retain fine local detail."),"\n",i.createElement(t.li,null,"Output varying numbers of points or an implicit surface representation."),"\n"),"\n",i.createElement("br"),"\n",i.createElement(t.h2,{id:"conclusion",style:{position:"relative"}},i.createElement(t.a,{href:"#conclusion","aria-label":"conclusion permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"conclusion"),"\n",i.createElement(t.p,null,"Deep geometric learning is a vast and rapidly evolving field that lies at the intersection of computer vision, graphics, and classical geometry processing. The techniques explored here — from voxel-based CNNs and mesh-specific networks to GNNs, point-based frameworks, transformers, and self-supervised models — offer a remarkable toolbox for tackling modern 3D challenges. In addition, the growing interest in bridging symbolic and neural approaches provides new avenues to imbue models with explicit domain knowledge and constraints."),"\n",i.createElement(t.p,null,"Through advanced rendering and novel view synthesis methods, it has become possible to generate photorealistic or highly structured 3D scenes from sparse or noisy data. The potential applications extend into robotics (where online mapping and navigation are critical), AR/VR (where real-time, high-fidelity rendering is essential), engineering design (where geometric constraints must be satisfied), entertainment (where realistic 3D assets are in high demand), and countless other areas."),"\n",i.createElement(t.p,null,"Many challenges remain, such as achieving truly real-time performance for large-scale or dynamic geometry, dealing with incomplete or noisy data at scale, or ensuring explainability in safety-critical applications. Yet the trajectory of research is extremely promising, with new architectures and techniques continually improving the representational power and efficiency of geometric deep learning. As sensor technology advances and novel computational approaches come online, I foresee a future in which 3D and 4D geometric deep learning become ubiquitous — forming a core capability for intelligent machines and data-driven applications in the real world."),"\n",i.createElement("br"),"\n",i.createElement(t.h2,{id:"references-and-further-reading",style:{position:"relative"}},i.createElement(t.a,{href:"#references-and-further-reading","aria-label":"references and further reading permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"references and further reading"),"\n",i.createElement(t.p,null,"Below are selected references to prominent work in geometric deep learning. Many are from conferences like NeurIPS, ICML, and CVPR:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"PointNet & PointNet++"),': Qi and gang, "PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation," CVPR 2017; Qi and gang, "PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space," NeurIPS 2017.'),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"MeshCNN"),': Hanocka and gang, "MeshCNN: A Network with an Edge," SIGGRAPH 2019.'),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"S2CNN"),': Esteves and gang, "Learning SO(3) Equivariant Representations with Spherical CNNs," ECCV 2018.'),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Neural Radiance Fields (NeRF)"),': Mildenhall and gang, "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis," ECCV 2020.'),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"3D-GAN"),': Wu and gang, "Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling," NeurIPS 2016.'),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Sparse Convolutions"),': Choy and gang, "4D Spatio-Temporal Convolutional Networks: Minkowski Convolutional Neural Networks," CVPR 2019.'),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Spectral Graph Convolutions"),': Bruna and gang, "Spectral Networks and Locally Connected Networks on Graphs," ICLR 2014; Defferrard and gang, "Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering," NeurIPS 2016.'),"\n"),"\n",i.createElement(t.p,null,"These works and others have laid the groundwork for the current wave of deep geometric learning innovations. As you progress through your own research or applications, I encourage you to explore not only the architectures themselves but also the underlying geometry and domain considerations that make them function effectively in practice."),"\n",i.createElement("br"),"\n",i.createElement(n,{alt:"3D point cloud example",path:"",caption:"An illustrative point cloud, where each colored dot represents a point in 3D space. High density of points forms surfaces corresponding to scanned objects or environments.",zoom:"false"}),"\n",i.createElement(n,{alt:"Voxel grid illustration",path:"",caption:"A voxel representation, showing how 3D space can be discretized into small volumetric cells for convolutional processing.",zoom:"false"}),"\n",i.createElement(n,{alt:"Mesh connectivity example",path:"",caption:"A polygonal mesh of a bunny, illustrating the irregular connectivity that mesh-based neural networks must handle.",zoom:"false"}),"\n",i.createElement(n,{alt:"Transformer attention map in 3D",path:"",caption:"A conceptual diagram indicating how self-attention weights might highlight particular regions of a 3D shape when processing point tokens.",zoom:"false"}))}var c=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,a.RP)(),e.components);return t?i.createElement(t,e,i.createElement(l,e)):l(e)};var h=n(54506),d=n(88864),m=n(58481),p=n.n(m),u=n(5984),g=n(43672),f=n(27042),v=n(72031),b=n(81817),y=n(27105),w=n(17265),E=n(2043),S=n(95751),x=n(94328),k=n(80791),H=n(78137);const N=e=>{let{toc:t}=e;if(!t||!t.items)return null;return i.createElement("nav",{className:k.R},i.createElement("ul",null,t.items.map(((e,t)=>i.createElement("li",{key:t},i.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const n=t.replace("#",""),a=document.getElementById(n);a&&a.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&i.createElement(N,{toc:{items:e.items}}))))))};function C(e){let{data:{mdx:t,allMdx:r,allPostImages:s},children:o}=e;const{frontmatter:l,body:c,tableOfContents:d}=t,m=l.index,v=l.slug.split("/")[1],k=r.nodes.filter((e=>e.frontmatter.slug.includes(`/${v}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),C=k.findIndex((e=>e.frontmatter.index===m)),z=k[C+1],_=k[C-1],M=l.slug.replace(/\/$/,""),T=/[^/]*$/.exec(M)[0],D=`posts/${v}/content/${T}/`,{0:A,1:V}=(0,i.useState)(l.flagWideLayoutByDefault),{0:L,1:I}=(0,i.useState)(!1);var B;(0,i.useEffect)((()=>{I(!0);const e=setTimeout((()=>I(!1)),340);return()=>clearTimeout(e)}),[A]),"adventures"===v?B=w.cb:"research"===v?B=w.Qh:"thoughts"===v&&(B=w.T6);const P=p()(c).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,R=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),n=e%60;return n<=30?`~${t}${n>0?".5":""} h`:`~${t+1} h`}(Math.ceil(P/B)+(l.extraReadTimeMin||0)),q=[{flag:l.flagDraft,component:()=>Promise.all([n.e(5850),n.e(9833)]).then(n.bind(n,49833))},{flag:l.flagMindfuckery,component:()=>Promise.all([n.e(5850),n.e(7805)]).then(n.bind(n,27805))},{flag:l.flagRewrite,component:()=>Promise.all([n.e(5850),n.e(8916)]).then(n.bind(n,78916))},{flag:l.flagOffensive,component:()=>Promise.all([n.e(5850),n.e(6731)]).then(n.bind(n,49112))},{flag:l.flagProfane,component:()=>Promise.all([n.e(5850),n.e(3336)]).then(n.bind(n,83336))},{flag:l.flagMultilingual,component:()=>Promise.all([n.e(5850),n.e(2343)]).then(n.bind(n,62343))},{flag:l.flagUnreliably,component:()=>Promise.all([n.e(5850),n.e(6865)]).then(n.bind(n,11627))},{flag:l.flagPolitical,component:()=>Promise.all([n.e(5850),n.e(4417)]).then(n.bind(n,24417))},{flag:l.flagCognitohazard,component:()=>Promise.all([n.e(5850),n.e(8669)]).then(n.bind(n,18669))},{flag:l.flagHidden,component:()=>Promise.all([n.e(5850),n.e(8124)]).then(n.bind(n,48124))}],{0:G,1:j}=(0,i.useState)([]);return(0,i.useEffect)((()=>{q.forEach((e=>{let{flag:t,component:n}=e;t&&n().then((e=>{j((t=>[].concat((0,h.A)(t),[e.default])))}))}))}),[]),i.createElement(f.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},i.createElement(b.A,{postNumber:l.index,date:l.date,updated:l.updated,readTime:R,difficulty:l.difficultyLevel,title:l.title,desc:l.desc,banner:l.banner,section:v,postKey:T,isMindfuckery:l.flagMindfuckery,mainTag:l.mainTag}),i.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},l.otherTags.map(((e,t)=>i.createElement("span",{key:t,className:`noselect ${H.MW}`,style:{margin:"0 5px 5px 0"}},e)))),i.createElement("div",{className:"postBody"},i.createElement(N,{toc:d})),i.createElement("br",null),i.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},i.createElement(f.P.button,{className:`noselect ${x.pb}`,id:x.xG,onClick:()=>{V(!A)},whileTap:{scale:.93}},i.createElement(f.P.div,{className:S.DJ,key:A,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},A?"Switch to default layout":"Switch to wide layout"))),i.createElement("br",null),i.createElement("div",{className:"postBody",style:{margin:A?"0 -14%":"",maxWidth:A?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},i.createElement("div",{className:`${x.P_} ${L?x.Xn:x.qG}`},G.map(((e,t)=>i.createElement(e,{key:t}))),l.indexCourse?i.createElement(E.A,{index:l.indexCourse,category:l.courseCategoryName}):"",i.createElement(u.Z.Provider,{value:{images:s.nodes,basePath:D.replace(/\/$/,"")+"/"}},i.createElement(a.xA,{components:{Image:g.A}},o)))),i.createElement(y.A,{nextPost:z,lastPost:_,keyCurrent:T,section:v}))}function z(e){return i.createElement(C,e,i.createElement(c,e))}function _(e){var t,n,a,r,s;let{data:o}=e;const{frontmatter:l}=o.mdx,c=l.titleSEO||l.title,h=l.titleOG||c,m=l.titleTwitter||c,p=l.descSEO||l.desc,u=l.descOG||p,g=l.descTwitter||p,f=l.schemaType||"BlogPosting",b=l.keywordsSEO,y=l.date,w=l.updated||y,E=l.imageOG||(null===(t=l.banner)||void 0===t||null===(n=t.childImageSharp)||void 0===n||null===(a=n.gatsbyImageData)||void 0===a||null===(r=a.images)||void 0===r||null===(s=r.fallback)||void 0===s?void 0:s.src),S=l.imageAltOG||u,x=l.imageTwitter||E,k=l.imageAltTwitter||g,H=l.canonicalURL,N=l.flagHidden||!1,C=l.mainTag||"Posts",z=l.slug.split("/")[1]||"posts",{siteUrl:_}=(0,d.Q)(),M={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:_},{"@type":"ListItem",position:2,name:C,item:`${_}/${l.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${_}${l.slug}`}]};return i.createElement(v.A,{title:c+" - avrtt.blog",titleOG:h,titleTwitter:m,description:p,descriptionOG:u,descriptionTwitter:g,schemaType:f,keywords:b,datePublished:y,dateModified:w,imageOG:E,imageAltOG:S,imageTwitter:x,imageAltTwitter:k,canonicalUrl:H,flagHidden:N,mainTag:C,section:z,type:"article"},i.createElement("script",{type:"application/ld+json"},JSON.stringify(M)))}},90548:function(e,t,n){var a=n(96540),i=n(7978);t.A=e=>{let{text:t}=e;return a.createElement(i.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-tsx-content-file-path-src-pages-posts-research-geometry-estimation-2-mdx-f61da558f6c347d00d24.js.map