{"version":3,"file":"component---src-templates-post-js-content-file-path-src-pages-posts-research-gradient-optimization-mdx-04fa24efac4ae2e11202.js","mappings":"8RAmHA,SAASA,EAAkBC,GACzB,MAAMC,EAAcC,OAAOC,OAAO,CAChCC,EAAG,IACHC,GAAI,KACJC,EAAG,IACHC,KAAM,OACNC,GAAI,KACJC,GAAI,KACJC,GAAI,KACJC,GAAI,KACJC,OAAQ,SACRC,GAAI,OACHC,EAAAA,EAAAA,MAAsBd,EAAMe,aAAa,MAACC,GAASf,EAEtD,OADKe,GAudP,SAA8BC,EAAIC,GAChC,MAAM,IAAIC,MAAM,aAAeD,EAAY,YAAc,UAAY,KAAOD,EAAK,qEACnF,CAzdcG,CAAqB,SAAS,GACnCC,EAAAA,cAAoBA,EAAAA,SAAgB,KAAM,KAAMA,EAAAA,cAAoB,MAAO,KAAM,KAAM,KAAM,KAAMA,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,+pBAAgqB,KAAMiB,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,2cAA4c,KAAMiB,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,8sBAAgtB,KAAMiB,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,uVAAwV,KAAMiB,EAAAA,cAAoBpB,EAAYI,GAAI,CAC59EY,GAAI,6CACJK,MAAO,CACLC,SAAU,aAEXF,EAAAA,cAAoBpB,EAAYK,EAAG,CACpCkB,KAAM,8CACN,aAAc,uDACdC,UAAW,iBACVJ,EAAAA,cAAoBpB,EAAYM,KAAM,CACvCmB,wBAAyB,CACvBC,OAAQ,meAEP,+CAAgD,KAAMN,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,iPAAkPiB,EAAAA,cAAoBO,EAAAA,EAAO,CAC/WC,KAAM,8DACJ,UAAWR,EAAAA,cAAoBO,EAAAA,EAAO,CACxCC,KAAM,cACJ,iCAAkCR,EAAAA,cAAoBO,EAAAA,EAAO,CAC/DC,KAAM,YACJ,qBAAsBR,EAAAA,cAAoBO,EAAAA,EAAO,CACnDC,KAAM,cACJ,kFAAmF,KAAMR,EAAAA,cAAoBpB,EAAYO,GAAI,CAC/HS,GAAI,+CACJK,MAAO,CACLC,SAAU,aAEXF,EAAAA,cAAoBpB,EAAYK,EAAG,CACpCkB,KAAM,gDACN,aAAc,yDACdC,UAAW,iBACVJ,EAAAA,cAAoBpB,EAAYM,KAAM,CACvCmB,wBAAyB,CACvBC,OAAQ,meAEP,kDAAmD,KAAMN,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,sRAAuR,KAAMiB,EAAAA,cAAoBO,EAAAA,EAAO,CAC7ZC,KAAM,2GACJ,KAAMR,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,SAAUiB,EAAAA,cAAoBO,EAAAA,EAAO,CACtFC,KAAM,oCACJ,sCAAuCR,EAAAA,cAAoBO,EAAAA,EAAO,CACpEC,KAAM,YACJ,uBAAwBR,EAAAA,cAAoBO,EAAAA,EAAO,CACrDC,KAAM,sBACJ,0JAA2J,KAAMR,EAAAA,cAAoBpB,EAAYO,GAAI,CACvMS,GAAI,2CACJK,MAAO,CACLC,SAAU,aAEXF,EAAAA,cAAoBpB,EAAYK,EAAG,CACpCkB,KAAM,4CACN,aAAc,qDACdC,UAAW,iBACVJ,EAAAA,cAAoBpB,EAAYM,KAAM,CACvCmB,wBAAyB,CACvBC,OAAQ,meAEP,4CAA6C,KAAMN,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,4RAA6R,KAAMiB,EAAAA,cAAoBpB,EAAYO,GAAI,CACtaS,GAAI,4CACJK,MAAO,CACLC,SAAU,aAEXF,EAAAA,cAAoBpB,EAAYK,EAAG,CACpCkB,KAAM,6CACN,aAAc,sDACdC,UAAW,iBACVJ,EAAAA,cAAoBpB,EAAYM,KAAM,CACvCmB,wBAAyB,CACvBC,OAAQ,meAEP,6CAA8C,KAAMN,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,yRAA0RiB,EAAAA,cAAoBO,EAAAA,EAAO,CACrZC,KAAM,yBACJ,YAAaR,EAAAA,cAAoBO,EAAAA,EAAO,CAC1CC,KAAM,YACJ,OAAQR,EAAAA,cAAoBO,EAAAA,EAAO,CACrCC,KAAM,uCACJ,kCAAmC,KAAMR,EAAAA,cAAoBO,EAAAA,EAAO,CACtEC,KAAM,4FACJ,KAAMR,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,SAAUiB,EAAAA,cAAoBO,EAAAA,EAAO,CACtFC,KAAM,gBACJ,2PAA4P,KAAMR,EAAAA,cAAoBpB,EAAYI,GAAI,CACxSY,GAAI,qBACJK,MAAO,CACLC,SAAU,aAEXF,EAAAA,cAAoBpB,EAAYK,EAAG,CACpCkB,KAAM,sBACN,aAAc,+BACdC,UAAW,iBACVJ,EAAAA,cAAoBpB,EAAYM,KAAM,CACvCmB,wBAAyB,CACvBC,OAAQ,meAEP,uBAAwB,KAAMN,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,0IAA2I,KAAMiB,EAAAA,cAAoBpB,EAAYQ,GAAI,KAAM,KAAMY,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,iFAAkF,KAAMW,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,qGAAsG,KAAMW,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,0GAA2G,KAAMW,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,gFAAiF,MAAO,KAAMW,EAAAA,cAAoBpB,EAAYG,EAAG,KAAMiB,EAAAA,cAAoBS,EAAAA,EAAW,KAAM,aAAc,qBAAsB,KAAMT,EAAAA,cAAoBpB,EAAYU,GAAI,KAAM,KAAMU,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,iCAAkC,mEAAoE,KAAMS,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,YAAa,4FAA6F,KAAMS,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,eAAgB,+GAAgH,KAAMS,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,iBAAkB,uBAAwBS,EAAAA,cAAoBO,EAAAA,EAAO,CACrvDC,KAAM,gBACJ,mFAAoF,MAAO,KAAMR,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,wWAAyW,KAAMiB,EAAAA,cAAoBpB,EAAYI,GAAI,CAC/hBY,GAAI,iCACJK,MAAO,CACLC,SAAU,aAEXF,EAAAA,cAAoBpB,EAAYK,EAAG,CACpCkB,KAAM,kCACN,aAAc,2CACdC,UAAW,iBACVJ,EAAAA,cAAoBpB,EAAYM,KAAM,CACvCmB,wBAAyB,CACvBC,OAAQ,meAEP,mCAAoC,KAAMN,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,uUAAwU,KAAMiB,EAAAA,cAAoBpB,EAAYO,GAAI,CACxcS,GAAI,2CACJK,MAAO,CACLC,SAAU,aAEXF,EAAAA,cAAoBpB,EAAYK,EAAG,CACpCkB,KAAM,4CACN,aAAc,qDACdC,UAAW,iBACVJ,EAAAA,cAAoBpB,EAAYM,KAAM,CACvCmB,wBAAyB,CACvBC,OAAQ,meAEP,6CAA8C,KAAMN,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,mCAAoCiB,EAAAA,cAAoBO,EAAAA,EAAO,CAC/JC,KAAM,yBACJ,kBAAmBR,EAAAA,cAAoBO,EAAAA,EAAO,CAChDC,KAAM,iCACJ,0CAA2C,KAAMR,EAAAA,cAAoBO,EAAAA,EAAO,CAC9EC,KAAM,qPACJ,KAAMR,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,kCAAmCiB,EAAAA,cAAoBO,EAAAA,EAAO,CAC/GC,KAAM,iCACJ,4EAA6ER,EAAAA,cAAoBO,EAAAA,EAAO,CAC1GC,KAAM,cACJ,4LAA6L,KAAMR,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,2EAA4E,KAAMiB,EAAAA,cAAoBO,EAAAA,EAAO,CAC3VC,KAAM,+FACJ,KAAMR,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,SAAUiB,EAAAA,cAAoBO,EAAAA,EAAO,CACtFC,KAAM,4BACJ,yCAA0CR,EAAAA,cAAoBO,EAAAA,EAAO,CACvEC,KAAM,YACJ,SAAUR,EAAAA,cAAoBO,EAAAA,EAAO,CACvCC,KAAM,gBACJ,0BAA2B,KAAMR,EAAAA,cAAoBpB,EAAYO,GAAI,CACvES,GAAI,0DACJK,MAAO,CACLC,SAAU,aAEXF,EAAAA,cAAoBpB,EAAYK,EAAG,CACpCkB,KAAM,2DACN,aAAc,oEACdC,UAAW,iBACVJ,EAAAA,cAAoBpB,EAAYM,KAAM,CACvCmB,wBAAyB,CACvBC,OAAQ,meAEP,4DAA6D,KAAMN,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,sPAAuP,KAAMiB,EAAAA,cAAoBO,EAAAA,EAAO,CACvYC,KAAM,2GACJ,KAAMR,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,SAAUiB,EAAAA,cAAoBO,EAAAA,EAAO,CACtFC,KAAM,sBACJ,4CAA6CR,EAAAA,cAAoBO,EAAAA,EAAO,CAC1EC,KAAM,sBACJ,wCAAyCR,EAAAA,cAAoBO,EAAAA,EAAO,CACtEC,KAAM,YACJ,uCAAwC,KAAMR,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,6PAA8P,KAAMiB,EAAAA,cAAoBpB,EAAYO,GAAI,CACjYS,GAAI,0BACJK,MAAO,CACLC,SAAU,aAEXF,EAAAA,cAAoBpB,EAAYK,EAAG,CACpCkB,KAAM,2BACN,aAAc,oCACdC,UAAW,iBACVJ,EAAAA,cAAoBpB,EAAYM,KAAM,CACvCmB,wBAAyB,CACvBC,OAAQ,meAEP,4BAA6B,KAAMN,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,iJAAoJ,KAAMiB,EAAAA,cAAoBpB,EAAYU,GAAI,KAAM,KAAMU,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,oBAAqB,8BAA+BS,EAAAA,cAAoBO,EAAAA,EAAO,CAChcC,KAAM,yDACJ,kCAAmC,KAAMR,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,sBAAuB,eAAgBS,EAAAA,cAAoBO,EAAAA,EAAO,CACrMC,KAAM,6CACJ,wBAAyB,KAAMR,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,sBAAuB,wDAAyD,KAAMS,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,qBAAsB,0KAA2K,MAAO,KAAMS,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,gJAAiJ,KAAMiB,EAAAA,cAAoBpB,EAAYO,GAAI,CACztBS,GAAI,kCACJK,MAAO,CACLC,SAAU,aAEXF,EAAAA,cAAoBpB,EAAYK,EAAG,CACpCkB,KAAM,mCACN,aAAc,4CACdC,UAAW,iBACVJ,EAAAA,cAAoBpB,EAAYM,KAAM,CACvCmB,wBAAyB,CACvBC,OAAQ,meAEP,oCAAqC,KAAMN,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,qBAAsBiB,EAAAA,cAAoBO,EAAAA,EAAO,CACxIC,KAAM,gBACJ,qMAAsM,KAAMR,EAAAA,cAAoBpB,EAAYU,GAAI,KAAM,KAAMU,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,aAAcS,EAAAA,cAAoBO,EAAAA,EAAO,CAC/XC,KAAM,iBACH,uJAAwJ,KAAMR,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,aAAcS,EAAAA,cAAoBO,EAAAA,EAAO,CAClSC,KAAM,iBACH,oGAAqG,KAAMR,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,0BAA2B,kFAAqFS,EAAAA,cAAoBO,EAAAA,EAAO,CACjVC,KAAM,gBACJ,uGAAwGR,EAAAA,cAAoBO,EAAAA,EAAO,CACrIC,KAAM,gDACJ,kCAAmCR,EAAAA,cAAoBO,EAAAA,EAAO,CAChEC,KAAM,iDACJ,WAAYR,EAAAA,cAAoBO,EAAAA,EAAO,CACzCC,KAAM,YACJ,OAAQR,EAAAA,cAAoBO,EAAAA,EAAO,CACrCC,KAAM,kBACJ,+CAAgD,MAAO,KAAMR,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,oSAAqS,KAAMiB,EAAAA,cAAoBpB,EAAYI,GAAI,CACvbY,GAAI,mCACJK,MAAO,CACLC,SAAU,aAEXF,EAAAA,cAAoBpB,EAAYK,EAAG,CACpCkB,KAAM,oCACN,aAAc,6CACdC,UAAW,iBACVJ,EAAAA,cAAoBpB,EAAYM,KAAM,CACvCmB,wBAAyB,CACvBC,OAAQ,meAEP,qCAAsC,KAAMN,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,uOAAwO,KAAMiB,EAAAA,cAAoBpB,EAAYQ,GAAI,KAAM,KAAMY,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,gCAAiC,wEAAyE,KAAMS,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,qCAAsC,uFAAwF,KAAMS,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,sCAAuC,+FAAgG,MAAO,KAAMS,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,4IAA6I,KAAMiB,EAAAA,cAAoBpB,EAAYO,GAAI,CACtuCS,GAAI,4BACJK,MAAO,CACLC,SAAU,aAEXF,EAAAA,cAAoBpB,EAAYK,EAAG,CACpCkB,KAAM,6BACN,aAAc,sCACdC,UAAW,iBACVJ,EAAAA,cAAoBpB,EAAYM,KAAM,CACvCmB,wBAAyB,CACvBC,OAAQ,meAEP,8BAA+B,KAAMN,EAAAA,cAAoBpB,EAAYG,EAAG,KAAMiB,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,0BAA2B,8IAA+I,KAAMS,EAAAA,cAAoBO,EAAAA,EAAO,CAC1UC,KAAM,8FACJ,KAAMR,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,SAAUiB,EAAAA,cAAoBO,EAAAA,EAAO,CACtFC,KAAM,2BACJ,sCAAuCR,EAAAA,cAAoBO,EAAAA,EAAO,CACpEC,KAAM,YACJ,oBAAqBR,EAAAA,cAAoBO,EAAAA,EAAO,CAClDC,KAAM,YACJ,8CAA+C,KAAMR,EAAAA,cAAoBpB,EAAYG,EAAG,KAAMiB,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,QAAS,KAAM,KAAMS,EAAAA,cAAoBpB,EAAYU,GAAI,KAAM,KAAMU,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,2HAA4H,KAAMW,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,gDAAiD,MAAO,KAAMW,EAAAA,cAAoBpB,EAAYG,EAAG,KAAMiB,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,QAAS,KAAM,KAAMS,EAAAA,cAAoBpB,EAAYU,GAAI,KAAM,KAAMU,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,4FAA6F,KAAMW,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,6FAA8F,MAAO,KAAMW,EAAAA,cAAoBpB,EAAYG,EAAG,KAAMiB,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,aAAc,KAAM,KAAMS,EAAAA,cAAoBpB,EAAYU,GAAI,KAAM,KAAMU,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,yHAA0H,KAAMW,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,qEAAsE,MAAO,KAAMW,EAAAA,cAAoBpB,EAAYO,GAAI,CACj5CS,GAAI,iCACJK,MAAO,CACLC,SAAU,aAEXF,EAAAA,cAAoBpB,EAAYK,EAAG,CACpCkB,KAAM,kCACN,aAAc,2CACdC,UAAW,iBACVJ,EAAAA,cAAoBpB,EAAYM,KAAM,CACvCmB,wBAAyB,CACvBC,OAAQ,meAEP,mCAAoC,KAAMN,EAAAA,cAAoBpB,EAAYG,EAAG,KAAMiB,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,qCAAsC,wIAAyIS,EAAAA,cAAoBO,EAAAA,EAAO,CAC9UC,KAAM,YACJ,4BAA6BR,EAAAA,cAAoBO,EAAAA,EAAO,CAC1DC,KAAM,iCACJ,WAAY,KAAMR,EAAAA,cAAoBO,EAAAA,EAAO,CAC/CC,KAAM,wEACJ,KAAMR,EAAAA,cAAoBpB,EAAYG,EAAG,KAAMiB,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,QAAS,KAAM,KAAMS,EAAAA,cAAoBpB,EAAYU,GAAI,KAAM,KAAMU,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,2GAA4G,KAAMW,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,kFAAqF,KAAMW,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,0EAA2E,MAAO,KAAMW,EAAAA,cAAoBpB,EAAYG,EAAG,KAAMiB,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,QAAS,KAAM,KAAMS,EAAAA,cAAoBpB,EAAYU,GAAI,KAAM,KAAMU,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,uEAAwE,KAAMW,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,mFAAoF,KAAMW,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,wGAAyG,MAAO,KAAMW,EAAAA,cAAoBpB,EAAYG,EAAG,KAAMiB,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,aAAc,KAAM,KAAMS,EAAAA,cAAoBpB,EAAYU,GAAI,KAAM,KAAMU,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,0CAA2C,KAAMW,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,sFAAuF,KAAMW,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,kDAAmD,MAAO,KAAMW,EAAAA,cAAoBpB,EAAYO,GAAI,CAChpDS,GAAI,iCACJK,MAAO,CACLC,SAAU,aAEXF,EAAAA,cAAoBpB,EAAYK,EAAG,CACpCkB,KAAM,kCACN,aAAc,2CACdC,UAAW,iBACVJ,EAAAA,cAAoBpB,EAAYM,KAAM,CACvCmB,wBAAyB,CACvBC,OAAQ,meAEP,mCAAoC,KAAMN,EAAAA,cAAoBpB,EAAYG,EAAG,KAAMiB,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,+BAAgC,2HAA4HS,EAAAA,cAAoBO,EAAAA,EAAO,CAC3TC,KAAM,YACJ,iCAAkC,KAAMR,EAAAA,cAAoBO,EAAAA,EAAO,CACrEC,KAAM,kHACJ,KAAMR,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,gFAAmFiB,EAAAA,cAAoBO,EAAAA,EAAO,CAC/JC,KAAM,qBACJ,0FAA2F,KAAMR,EAAAA,cAAoBpB,EAAYG,EAAG,KAAMiB,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,QAAS,KAAM,KAAMS,EAAAA,cAAoBpB,EAAYU,GAAI,KAAM,KAAMU,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,yGAA0G,KAAMW,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,oGAAqG,KAAMW,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,uFAAwF,MAAO,KAAMW,EAAAA,cAAoBpB,EAAYG,EAAG,KAAMiB,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,QAAS,KAAM,KAAMS,EAAAA,cAAoBpB,EAAYU,GAAI,KAAM,KAAMU,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,sHAAuH,KAAMW,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,wEAAyE,MAAO,KAAMW,EAAAA,cAAoBpB,EAAYG,EAAG,KAAMiB,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,aAAc,KAAM,KAAMS,EAAAA,cAAoBpB,EAAYU,GAAI,KAAM,KAAMU,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,yHAA0H,KAAMW,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,2CAA4C,KAAMW,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,iFAAkF,MAAO,KAAMW,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,oRAAqR,KAAMiB,EAAAA,cAAoBpB,EAAYI,GAAI,CACxhEY,GAAI,4CACJK,MAAO,CACLC,SAAU,aAEXF,EAAAA,cAAoBpB,EAAYK,EAAG,CACpCkB,KAAM,6CACN,aAAc,sDACdC,UAAW,iBACVJ,EAAAA,cAAoBpB,EAAYM,KAAM,CACvCmB,wBAAyB,CACvBC,OAAQ,meAEP,8CAA+C,KAAMN,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,sJAAuJ,KAAMiB,EAAAA,cAAoBpB,EAAYQ,GAAI,KAAM,KAAMY,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,KAAMW,EAAAA,cAAoBpB,EAAYG,EAAG,KAAMiB,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,sBAAuB,KAAM,KAAMS,EAAAA,cAAoBpB,EAAYU,GAAI,KAAM,KAAMU,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,oCAAqC,+HAAgI,KAAMS,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,aAAc,iIAAkI,MAAO,MAAO,KAAMS,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,KAAMW,EAAAA,cAAoBpB,EAAYG,EAAG,KAAMiB,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,qDAAsD,KAAM,KAAMS,EAAAA,cAAoBpB,EAAYU,GAAI,KAAM,KAAMU,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,0EAA2E,KAAMW,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,kFAAmF,MAAO,MAAO,KAAMW,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,KAAMW,EAAAA,cAAoBpB,EAAYG,EAAG,KAAMiB,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,yBAA0B,KAAM,KAAMS,EAAAA,cAAoBpB,EAAYU,GAAI,KAAM,KAAMU,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,iBAAkB,0EAA2E,KAAMS,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,cAAe,yGAA0G,KAAMS,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,kBAAmB,kBAAmBS,EAAAA,cAAoBO,EAAAA,EAAO,CACpxEC,KAAM,cACJ,eAAgBR,EAAAA,cAAoBO,EAAAA,EAAO,CAC7CC,KAAM,cACJ,2HAA4H,MAAO,MAAO,KAAMR,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,KAAMW,EAAAA,cAAoBpB,EAAYG,EAAG,KAAMiB,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,iCAAkC,KAAM,KAAMS,EAAAA,cAAoBpB,EAAYU,GAAI,KAAM,KAAMU,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,gTAAiT,MAAO,MAAO,KAAMW,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,KAAMW,EAAAA,cAAoBpB,EAAYG,EAAG,KAAMiB,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,yBAA0B,KAAM,KAAMS,EAAAA,cAAoBpB,EAAYU,GAAI,KAAM,KAAMU,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,6GAA8G,KAAMW,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,mFAAoF,MAAO,MAAO,KAAMW,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,KAAMW,EAAAA,cAAoBpB,EAAYG,EAAG,KAAMiB,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,0BAA2B,KAAM,KAAMS,EAAAA,cAAoBpB,EAAYU,GAAI,KAAM,KAAMU,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,iHAAkH,KAAMW,EAAAA,cAAoBpB,EAAYS,GAAI,KAAM,6IAA8I,MAAO,MAAO,MAAO,KAAMW,EAAAA,cAAoBpB,EAAYI,GAAI,CAC12DY,GAAI,oBACJK,MAAO,CACLC,SAAU,aAEXF,EAAAA,cAAoBpB,EAAYK,EAAG,CACpCkB,KAAM,qBACN,aAAc,8BACdC,UAAW,iBACVJ,EAAAA,cAAoBpB,EAAYM,KAAM,CACvCmB,wBAAyB,CACvBC,OAAQ,meAEP,sBAAuB,KAAMN,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,gPAAiPiB,EAAAA,cAAoBS,EAAAA,EAAW,KAAM,iBAAkB,8BAA+B,KAAMT,EAAAA,cAAoBpB,EAAYY,IAAK,KAAMQ,EAAAA,cAAoBpB,EAAYO,GAAI,CACreS,GAAI,gEACJK,MAAO,CACLC,SAAU,aAEXF,EAAAA,cAAoBpB,EAAYK,EAAG,CACpCkB,KAAM,iEACN,aAAc,0EACdC,UAAW,iBACVJ,EAAAA,cAAoBpB,EAAYM,KAAM,CACvCmB,wBAAyB,CACvBC,OAAQ,meAEP,qEAAsE,KAAMN,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,yCAA0CiB,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,0BAA2B,iGAAkGS,EAAAA,cAAoBO,EAAAA,EAAO,CACxWC,KAAM,sBACJ,eAAgBR,EAAAA,cAAoBO,EAAAA,EAAO,CAC7CC,KAAM,sBACJ,yBAA0BR,EAAAA,cAAoBO,EAAAA,EAAO,CACvDC,KAAM,sBACJ,iBAAkBR,EAAAA,cAAoBO,EAAAA,EAAO,CAC/CC,KAAM,YACJ,KAAM,KAAMR,EAAAA,cAAoBpB,EAAYM,KAAM,CACpDmB,wBAAyB,CACvBC,OAAQ,4wCAER,KAAMN,EAAAA,cAAoBpB,EAAYU,GAAI,KAAM,KAAMU,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,kBAAmB,yCAA0CS,EAAAA,cAAoBO,EAAAA,EAAO,CACxOC,KAAM,YACJ,uCAAwC,KAAMR,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,iBAAkB,gBAAiBS,EAAAA,cAAoBO,EAAAA,EAAO,CACtMC,KAAM,eACJ,KAAM,KAAMR,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,wBAAyB,wDAAyD,KAAMS,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,eAAgB,KAAMS,EAAAA,cAAoBO,EAAAA,EAAO,CACvUC,KAAM,oDACJ,KAAM,KAAMR,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,UAAW,wCAAyC,MAAO,KAAMS,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,qNAAsN,KAAMiB,EAAAA,cAAoBpB,EAAYO,GAAI,CAChdS,GAAI,qEACJK,MAAO,CACLC,SAAU,aAEXF,EAAAA,cAAoBpB,EAAYK,EAAG,CACpCkB,KAAM,sEACN,aAAc,+EACdC,UAAW,iBACVJ,EAAAA,cAAoBpB,EAAYM,KAAM,CACvCmB,wBAAyB,CACvBC,OAAQ,meAEP,0EAA2E,KAAMN,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,0CAA2CiB,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,+BAAgC,iJAAkJ,KAAMS,EAAAA,cAAoBpB,EAAYM,KAAM,CACpbmB,wBAAyB,CACvBC,OAAQ,gyCAER,KAAMN,EAAAA,cAAoBpB,EAAYU,GAAI,KAAM,KAAMU,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,SAAU,+BAAgCS,EAAAA,cAAoBO,EAAAA,EAAO,CACrNC,KAAM,YACJ,sBAAuB,KAAMR,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,WAAY,gBAAiBS,EAAAA,cAAoBO,EAAAA,EAAO,CAC/KC,KAAM,YACJ,QAASR,EAAAA,cAAoBO,EAAAA,EAAO,CACtCC,KAAM,YACJ,kDAAmD,KAAMR,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,qBAAsB,qDAAsDS,EAAAA,cAAoBO,EAAAA,EAAO,CAC1PC,KAAM,YACJ,MAAO,MAAO,KAAMR,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,oCAAqCiB,EAAAA,cAAoBO,EAAAA,EAAO,CAC/HC,KAAM,YACJ,qLAAsL,KAAMR,EAAAA,cAAoBpB,EAAYO,GAAI,CAClOS,GAAI,iCACJK,MAAO,CACLC,SAAU,aAEXF,EAAAA,cAAoBpB,EAAYK,EAAG,CACpCkB,KAAM,kCACN,aAAc,2CACdC,UAAW,iBACVJ,EAAAA,cAAoBpB,EAAYM,KAAM,CACvCmB,wBAAyB,CACvBC,OAAQ,meAEP,mCAAoC,KAAMN,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,KAAMiB,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,uBAAwB,gSAAiSS,EAAAA,cAAoBO,EAAAA,EAAO,CAC9dC,KAAM,YACJ,KAAM,KAAMR,EAAAA,cAAoBpB,EAAYM,KAAM,CACpDmB,wBAAyB,CACvBC,OAAQ,k7CAER,KAAMN,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,4JAA6JiB,EAAAA,cAAoBO,EAAAA,EAAO,CACzOC,KAAM,YACJ,MAAO,KAAMR,EAAAA,cAAoBpB,EAAYO,GAAI,CACnDS,GAAI,+CACJK,MAAO,CACLC,SAAU,aAEXF,EAAAA,cAAoBpB,EAAYK,EAAG,CACpCkB,KAAM,gDACN,aAAc,yDACdC,UAAW,iBACVJ,EAAAA,cAAoBpB,EAAYM,KAAM,CACvCmB,wBAAyB,CACvBC,OAAQ,meAEP,mDAAoD,KAAMN,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,2FAA4FiB,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,gBAAiB,MAAOS,EAAAA,cAAoBS,EAAAA,EAAW,KAAM,iBAAkB,iCAAkC,KAAMT,EAAAA,cAAoBpB,EAAYM,KAAM,CAC7YmB,wBAAyB,CACvBC,OAAQ,g/BAER,KAAMN,EAAAA,cAAoBpB,EAAYU,GAAI,KAAM,KAAMU,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,QAAS,iBAAkBS,EAAAA,cAAoBpB,EAAYM,KAAM,CACjNmB,wBAAyB,CACvBC,OAAQ,kDAER,yDAA0D,KAAMN,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,WAAY,4BAA6BS,EAAAA,cAAoBpB,EAAYM,KAAM,CACzOmB,wBAAyB,CACvBC,OAAQ,+CAER,KAAMN,EAAAA,cAAoBpB,EAAYM,KAAM,CAC9CmB,wBAAyB,CACvBC,OAAQ,+CAER,QAASN,EAAAA,cAAoBpB,EAAYM,KAAM,CACjDmB,wBAAyB,CACvBC,OAAQ,uDAER,MAAO,KAAMN,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,SAAU,wCAAyC,KAAMS,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,iBAAkB,KAAMS,EAAAA,cAAoBpB,EAAYM,KAAM,CACtTmB,wBAAyB,CACvBC,OAAQ,oDAER,2EAA4E,KAAMN,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,WAAY,gEAAiE,KAAMS,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,gBAAiB,mCAAoC,MAAO,KAAMS,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,WAAYiB,EAAAA,cAAoBS,EAAAA,EAAW,KAAM,iBAAkB,6JAA8J,KAAMT,EAAAA,cAAoBpB,EAAYI,GAAI,CAC7sBY,GAAI,oCACJK,MAAO,CACLC,SAAU,aAEXF,EAAAA,cAAoBpB,EAAYK,EAAG,CACpCkB,KAAM,qCACN,aAAc,8CACdC,UAAW,iBACVJ,EAAAA,cAAoBpB,EAAYM,KAAM,CACvCmB,wBAAyB,CACvBC,OAAQ,meAEP,uCAAwC,KAAMN,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,iKAAkK,KAAMiB,EAAAA,cAAoBL,EAAO,CAC7Re,IAAK,qDACLC,KAAM,GACNC,QAAS,uHACTC,KAAM,UACJ,KAAMb,EAAAA,cAAoBL,EAAO,CACnCe,IAAK,qCACLC,KAAM,GACNC,QAAS,sHACTC,KAAM,UACJ,KAAMb,EAAAA,cAAoBL,EAAO,CACnCe,IAAK,8CACLC,KAAM,GACNC,QAAS,0IACTC,KAAM,UACJ,KAAMb,EAAAA,cAAoBpB,EAAYI,GAAI,CAC5CY,GAAI,0BACJK,MAAO,CACLC,SAAU,aAEXF,EAAAA,cAAoBpB,EAAYK,EAAG,CACpCkB,KAAM,2BACN,aAAc,oCACdC,UAAW,iBACVJ,EAAAA,cAAoBpB,EAAYM,KAAM,CACvCmB,wBAAyB,CACvBC,OAAQ,meAEP,2BAA4B,KAAMN,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,sSAAuS,KAAMiB,EAAAA,cAAoBpB,EAAYU,GAAI,KAAM,KAAMU,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,qBAAsB,2RAA4R,KAAMS,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,iBAAkB,0HAA2H,KAAMS,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,0BAA2B,iIAAkI,KAAMS,EAAAA,cAAoBpB,EAAYS,GAAI,KAAMW,EAAAA,cAAoBpB,EAAYW,OAAQ,KAAM,wBAAyB,kMAAmM,MAAO,KAAMS,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,6ZAA8Z,KAAMiB,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,4aAA6a,KAAMiB,EAAAA,cAAoBpB,EAAYG,EAAG,KAAM,kYACxjF,CAKA,MAJA,SAAoBJ,QAAK,IAALA,IAAAA,EAAQ,CAAC,GAC3B,MAAOmC,QAASC,GAAalC,OAAOC,OAAO,CAAC,GAAGW,EAAAA,EAAAA,MAAsBd,EAAMe,YAC3E,OAAOqB,EAAYf,EAAAA,cAAoBe,EAAWpC,EAAOqB,EAAAA,cAAoBtB,EAAmBC,IAAUD,EAAkBC,EAC9H,E,qKCrjBA,MAAMqC,EAAkBC,IAAW,IAAV,IAACC,GAAID,EAC5B,IAAKC,IAAQA,EAAIC,MAAO,OAAO,KAY/B,OAAOnB,EAAAA,cAAoB,MAAO,CAChCI,UAAWgB,EAAAA,GACVpB,EAAAA,cAAoB,KAAM,KAAMkB,EAAIC,MAAME,KAAI,CAACC,EAAMC,IAAUvB,EAAAA,cAAoB,KAAM,CAC1FwB,IAAKD,GACJvB,EAAAA,cAAoB,IAAK,CAC1BG,KAAMmB,EAAKG,IACXC,QAASC,GAjBSC,EAACD,EAAGF,KACtBE,EAAEE,iBACF,MAAMC,EAAWL,EAAIM,QAAQ,IAAK,IAC5BC,EAAgBC,SAASC,eAAeJ,GAC1CE,GACFA,EAAcG,eAAe,CAC3BC,SAAU,SACVC,MAAO,SAEX,EAQcT,CAAYD,EAAGL,EAAKG,MACjCH,EAAKgB,OAAQhB,EAAKH,OAASnB,EAAAA,cAAoBgB,EAAiB,CACjEE,IAAK,CACHC,MAAOG,EAAKH,aAEV,EAED,SAASoB,EAAYC,GAAiD,IAA/CC,MAAM,IAACC,EAAG,OAAEC,EAAM,cAAEC,GAAc,SAAEC,GAASL,EACzE,MAAM,YAACM,EAAW,KAAEC,EAAI,gBAAEC,GAAmBN,EACvCnB,EAAQuB,EAAYvB,MAEpB0B,EADOH,EAAYI,KACJC,MAAM,KAAK,GAE1BC,EADQT,EAAOU,MAAMC,QAAOC,GAAQA,EAAKT,YAAYI,KAAKM,SAAS,IAAIP,QACnDQ,MAAK,CAACxE,EAAGyE,IAAMzE,EAAE6D,YAAYvB,MAAQmC,EAAEZ,YAAYvB,QACvEoC,EAAeP,EAAYQ,WAAUL,GAAQA,EAAKT,YAAYvB,QAAUA,IACxEsC,EAAWT,EAAYO,EAAe,GACtCG,EAAWV,EAAYO,EAAe,GACtCI,EAAcjB,EAAYI,KAAKnB,QAAQ,MAAO,IAC9CiC,EAAc,SAAUC,KAAKF,GAAa,GAC1CG,EAAW,SAASjB,aAAmBe,MACvC,EAACG,EAAY,EAAEC,IAAmBC,EAAAA,EAAAA,UAASvB,EAAYwB,0BACvD,EAACC,EAAW,EAAEC,IAAkBH,EAAAA,EAAAA,WAAS,GAS/C,IAAII,GALJC,EAAAA,EAAAA,YAAU,KACRF,GAAe,GACf,MAAMG,EAAQC,YAAW,IAAMJ,GAAe,IAAQ,KACtD,MAAO,IAAMK,aAAaF,EAAM,GAC/B,CAACR,IAEY,eAAZlB,EACFwB,EAAiBK,EAAAA,GACI,aAAZ7B,EACTwB,EAAiBM,EAAAA,GACI,aAAZ9B,IACTwB,EAAiBO,EAAAA,IAEnB,MACMC,EADgBC,IAAenC,GAAMhB,QAAQ,wBAAyB,IAAIA,QAAQ,SAAU,IAAIA,QAAQ,wBAAyB,IAAIoD,OAC3GhC,MAAM,OAAOiC,OAIvCC,EA5ER,SAAwBC,GACtB,GAAIA,GAAW,GAAI,MAAO,UAC1B,GAAIA,GAAW,GAAI,MAAO,UAC1B,GAAIA,GAAW,GAAI,MAAO,UAC1B,GAAIA,GAAW,GAAI,MAAO,UAC1B,GAAIA,GAAW,GAAI,MAAO,UAC1B,GAAIA,GAAW,GAAI,MAAO,OAC1B,MAAMC,EAAQC,KAAKC,MAAMH,EAAU,IAC7BI,EAAYJ,EAAU,GAC5B,OAAII,GAAa,GACR,IAAIH,IAAQG,EAAY,EAAI,KAAO,OAErC,IAAIH,EAAQ,KACrB,CA+DmBI,CAHWH,KAAKI,KAAKX,EAAYR,IAChC3B,EAAY+C,kBAAoB,IAG5CC,EAAU,CAAC,CACfC,KAAMjD,EAAYkD,UAClBnG,UAAWA,IAAM,0DAChB,CACDkG,KAAMjD,EAAYmD,gBAClBpG,UAAWA,IAAM,0DAChB,CACDkG,KAAMjD,EAAYoD,YAClBrG,UAAWA,IAAM,0DAChB,CACDkG,KAAMjD,EAAYqD,cAClBtG,UAAWA,IAAM,0DAChB,CACDkG,KAAMjD,EAAYsD,YAClBvG,UAAWA,IAAM,0DAChB,CACDkG,KAAMjD,EAAYuD,iBAClBxG,UAAWA,IAAM,0DAChB,CACDkG,KAAMjD,EAAYwD,eAClBzG,UAAWA,IAAM,yDAChB,CACDkG,KAAMjD,EAAYyD,cAClB1G,UAAWA,IAAM,0DAChB,CACDkG,KAAMjD,EAAY0D,kBAClB3G,UAAWA,IAAM,yDAChB,CACDkG,KAAMjD,EAAY2D,WAClB5G,UAAWA,IAAM,4DAEb,EAAC6G,EAAa,EAAEC,IAAoBtC,EAAAA,EAAAA,UAAS,IAUnD,OATAK,EAAAA,EAAAA,YAAU,KACRoB,EAAQc,SAAQC,IAAuB,IAAtB,KAACd,EAAI,UAAElG,GAAUgH,EAC5Bd,GACFlG,IAAYiH,MAAKC,IACfJ,GAAiBK,GAAQ,GAAJC,QAAAC,EAAAA,EAAAA,GAAQF,GAAI,CAAED,EAAOI,WAAS,GAEvD,GACA,GACD,IACInH,EAAAA,cAAoBoH,EAAAA,EAAOC,IAAK,CACrCC,QAAS,CACPC,QAAS,GAEXC,QAAS,CACPD,QAAS,GAEXE,KAAM,CACJF,QAAS,GAEXG,WAAY,CACVC,SAAU,MAEX3H,EAAAA,cAAoB4H,EAAAA,EAAY,CACjCC,WAAY/E,EAAYvB,MACxBuG,KAAMhF,EAAYgF,KAClBC,QAASjF,EAAYiF,QACrB1C,SAAUA,EACV2C,WAAYlF,EAAYmF,gBACxB3F,MAAOQ,EAAYR,MACnB4F,KAAMpF,EAAYoF,KAClBC,OAAQrF,EAAYqF,OACpBlF,QAASA,EACTmF,QAASpE,EACTqE,cAAevF,EAAYmD,gBAC3BqC,QAASxF,EAAYwF,UACnBtI,EAAAA,cAAoB,MAAO,CAC7BC,MAAO,CACLsI,QAAS,OACTC,eAAgB,WAChBC,SAAU,OACVC,SAAU,MACVC,WAAY,OACZC,aAAc,MACdC,UAAW,OACXC,aAAc,QAEfhG,EAAYiG,UAAU1H,KAAI,CAAC2H,EAAKzH,IAAUvB,EAAAA,cAAoB,OAAQ,CACvEwB,IAAKD,EACLnB,UAAW,YAAY6I,EAAAA,KACvBhJ,MAAO,CACLiJ,OAAQ,gBAETF,MAAQhJ,EAAAA,cAAoB,MAAO,CACpCmJ,MAAO,YACNnJ,EAAAA,cAAoBgB,EAAiB,CACtCE,IAAK8B,KACFhD,EAAAA,cAAoB,MAAOA,EAAAA,cAAoB,MAAO,CACzDC,MAAO,CACLiJ,OAAQ,iBACRE,UAAW,UAEZpJ,EAAAA,cAAoBoH,EAAAA,EAAOiC,OAAQ,CACpCF,MAAO,WACP/I,UAAWkJ,EAAAA,GACX1J,GAAI0J,EAAAA,GACJ5H,QAvHmB6H,KACnBnF,GAAiBD,EAAa,EAuH9BqF,SAAU,CACRC,MAAO,MAERzJ,EAAAA,cAAoBoH,EAAAA,EAAOC,IAAK,CACjCjH,UAAWsJ,EAAAA,GACXlI,IAAK2C,EACLmD,QAAS,CACPC,QAAS,GAEXC,QAAS,CACPD,QAAS,GAEXE,KAAM,CACJF,QAAS,GAEXG,WAAY,CACVC,SAAU,GACVgC,KAAM,cAEPxF,EAAe,2BAA6B,2BAA4BnE,EAAAA,cAAoB,MAAOA,EAAAA,cAAoB,MAAO,CAC/HmJ,MAAO,WACPlJ,MAAO,CACLiJ,OAAQ/E,EAAe,SAAW,GAClCuE,SAAUvE,EAAe,OAAS,GAClCuD,WAAY,uDAEb1H,EAAAA,cAAoB,MAAO,CAC5BI,UAAW,GAAGkJ,EAAAA,MAAuC/E,EAAc+E,EAAAA,GAAkCA,EAAAA,MACpG5C,EAAcrF,KAAI,CAACuI,EAAiBrI,IAAUvB,EAAAA,cAAoB4J,EAAiB,CACpFpI,IAAKD,MACFuB,EAAY+G,YAAc7J,EAAAA,cAAoB8J,EAAAA,EAAoB,CACrEvI,MAAOuB,EAAY+G,YACnBE,SAAUjH,EAAYkH,qBACnB,GAAIhK,EAAAA,cAAoBiK,EAAAA,EAAaC,SAAU,CAClDC,MAAO,CACLC,OAAQxH,EAAcS,MACtBa,SAAUA,EAASnC,QAAQ,MAAO,IAAM,MAEzC/B,EAAAA,cAAoBqK,EAAAA,GAAa,CAClC3K,WAAY,CACVC,MAAKA,EAAAA,IAENkD,MAAc7C,EAAAA,cAAoBsK,EAAAA,EAAY,CAC/CzG,SAAUA,EACVC,SAAUA,EACVE,WAAYA,EACZf,QAASA,IAEb,CAEe,SAASsH,EAAiB5L,GACvC,OAAOqB,EAAAA,cAAoBuC,EAAc5D,EAAOqB,EAAAA,cAAoBwK,EAAqB7L,GAC3F,CACO,SAAS8L,EAAIC,GAAS,IAAAC,EAAAC,EAAAC,EAAAC,EAAAC,EAAA,IAAR,KAACtI,GAAKiI,EACzB,MAAM,YAAC5H,GAAeL,EAAKC,IACrBJ,EAAQQ,EAAYkI,UAAYlI,EAAYR,MAC5C2I,EAAUnI,EAAYmI,SAAW3I,EACjC4I,EAAepI,EAAYoI,cAAgB5I,EAC3C6I,EAAcrI,EAAYsI,SAAWtI,EAAYoF,KACjDmD,EAAgBvI,EAAYwI,QAAUH,EACtCI,EAAqBzI,EAAY0I,aAAeL,EAChDM,EAAa3I,EAAY2I,YAAc,cACvCC,EAAW5I,EAAY6I,YACvBC,EAAgB9I,EAAYgF,KAC5B+D,EAAe/I,EAAYiF,SAAW6D,EACtCE,EAAUhJ,EAAYgJ,UAA6B,QAAtBnB,EAAI7H,EAAYqF,cAAM,IAAAwC,GAAiB,QAAjBC,EAAlBD,EAAoBoB,uBAAe,IAAAnB,GAAiB,QAAjBC,EAAnCD,EAAqCoB,uBAAe,IAAAnB,GAAQ,QAARC,EAApDD,EAAsDT,cAAM,IAAAU,GAAU,QAAVC,EAA5DD,EAA8DmB,gBAAQ,IAAAlB,OAApD,EAAlBA,EAAwEmB,KACzGC,EAAarJ,EAAYqJ,YAAcd,EACvCe,EAAetJ,EAAYsJ,cAAgBN,EAC3CO,EAAkBvJ,EAAYuJ,iBAAmBd,EACjDe,EAAexJ,EAAYyJ,aAC3B9F,EAAa3D,EAAY2D,aAAc,EACvC6B,EAAUxF,EAAYwF,SAAW,QACjCrF,EAAUH,EAAYI,KAAKC,MAAM,KAAK,IAAM,SAE5C,QAACqJ,IAAWC,EAAAA,EAAAA,KACZC,EAAiB,CACrB,WAAY,qBACZ,QAAS,iBACT,gBAAmB,CAAC,CAClB,QAAS,WACT,SAAY,EACZ,KAAQ,OACR,KAAQF,GACP,CACD,QAAS,WACT,SAAY,EACZ,KAAQlE,EACR,KAAQ,GAAGkE,KAAW1J,EAAYI,KAAKC,MAAM,KAAK,MACjD,CACD,QAAS,WACT,SAAY,EACZ,KAAQb,EACR,KAAQ,GAAGkK,IAAU1J,EAAYI,UAGrC,OAAOlD,EAAAA,cAAoB2M,EAAAA,EAAK,CAC9BrK,MAAOA,EAAQ,gBACf2I,QAASA,EACTC,aAAcA,EACdC,YAAaA,EACbE,cAAeA,EACfE,mBAAoBA,EACpBE,WAAYA,EACZC,SAAUA,EACVE,cAAeA,EACfC,aAAcA,EACdC,QAASA,EACTK,WAAYA,EACZC,aAAcA,EACdC,gBAAiBA,EACjBC,aAAcA,EACd7F,WAAYA,EACZ6B,QAASA,EACTrF,QAASA,EACT2J,KAzCW,WA0CV5M,EAAAA,cAAoB,SAAU,CAC/B4M,KAAM,uBACLC,KAAKC,UAAUJ,IACpB,C,iDC9SA,IALUzL,IAAe,IAAd,KAAET,GAAMS,EACjB,OACEjB,EAAAA,cAACO,EAAAA,EAAK,KAAEC,EAAa,C","sources":["webpack://avrtt.blog/./src/pages/posts/research/gradient_optimization.mdx","webpack://avrtt.blog/./src/templates/post.js","webpack://avrtt.blog/./src/components/Latex/index.js"],"sourcesContent":["/*@jsxRuntime classic @jsx React.createElement @jsxFrag React.Fragment*/\n/**(intro: a quote, catchphrase, joke, etc.)**/\n/*\n\n[https://youtu.be/sDv4f4s2SB8](https://youtu.be/sDv4f4s2SB8)\n[https://www.youtube.com/watch?v=xOB10eTjoQ8](https://www.youtube.com/watch?v=xOB10eTjoQ8)\n[https://github.com/lenferdetroud/jupyter-notebooks/blob/master/sgd.ipynb](https://github.com/lenferdetroud/jupyter-notebooks/blob/master/sgd.ipynb)\n[https://www.youtube.com/watch?v=wPk8Z3aOBsg&list=PLA0M1Bcd0w8zxDIDOTQHsX68MCDOAJDtj&index=8](https://www.youtube.com/watch?v=wPk8Z3aOBsg&list=PLA0M1Bcd0w8zxDIDOTQHsX68MCDOAJDtj&index=8)\n[https://www.youtube.com/watch?v=gPC2B--Sza4&list=PLA0M1Bcd0w8zxDIDOTQHsX68MCDOAJDtj&index=9](https://www.youtube.com/watch?v=gPC2B--Sza4&list=PLA0M1Bcd0w8zxDIDOTQHsX68MCDOAJDtj&index=9)\nMathematics for ML: глава 7 (7.1)\n[https://mml-book.github.io/book/mml-book.pdf](https://mml-book.github.io/book/mml-book.pdf)\n[https://ml-handbook.ru/chapters/optimization/intro](https://ml-handbook.ru/chapters/optimization/intro)\n[https://ml-handbook.ru/chapters/optimization/sgd_convergence](https://ml-handbook.ru/chapters/optimization/sgd_convergence)\n[https://youtu.be/YovTqTY-PYY](https://youtu.be/YovTqTY-PYY)\n[https://youtu.be/GtSf2T6Co80](https://youtu.be/GtSf2T6Co80)\nВариации градиентного спуска:\n[https://youtu.be/W9iWNJNFzQI](https://youtu.be/W9iWNJNFzQI)\n[https://youtu.be/l4lSUAcvHFs](https://youtu.be/l4lSUAcvHFs)\n[https://youtu.be/G97ZtT8mKXk](https://youtu.be/G97ZtT8mKXk)\nВидимо, это ссылки из удалённого плейлиста старого курса Andrew Ng. Замена ссылки:\nhttps://www.youtube.com/watch?v=ed4whd9B-xw\n[https://youtu.be/vMh0zPT0tLI](https://youtu.be/vMh0zPT0tLI)\nСтохастический градиентный спуск\nhttps://neerc.ifmo.ru/wiki/index.php?title=%D0%A1%D1%82%D0%BE%D1%85%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9_%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B9_%D1%81%D0%BF%D1%83%D1%81%D0%BA\n\n*/\n/*\n\n# Batch Gradient Descent\nВ заметке про линейную регрессию мы вывели аналитическое решение, но из-за обращения матрицы оно слишком затратно: вычислительная сложность обращения - O(k^3-n*k^3), где k - количество признаков, n - размер выборки. Это ведет нас к итеративным оптимизационным методам, которые используются повсеместно в машинном обучении.\nGradient descent is one of the most popular optimization methods. It is a generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea is to tweak parameters iteratively in order to minimize a cost function.\nДля начала выбираем случайные начальные параметры и вычисляем градиент. Далее будет произведен градиентый шаг, размер которого мы задаём сами. Минимизация теперь заключается в том, чтобы делать шаги против градиента.\nДопустим, у нас есть линейная регрессия от нескольких переменных и cost-функция:\nТогда алгоритм выглядит следующим образом:\n...\nAlpha is the step size or learning rate. Он умножается на градиент J.\nВ матричной форме градиент для MSE считается так:\ndeltaJ(thetta) = -2*X^T*Y + 2*X^T*X*thetta = 2*X^T*(X*thetta-Y)\nIn this case the complexity is only O(nk).\nПовторяя шаги, мы получим дорогу, по которой наш алгоритм спускается к минимуму. В случае функции двух переменных это могло бы выглядеть так:\n![[img/Untitled 3 23.png|Untitled 3 23.png]]\nАлгоритм делает cost function всё меньше и меньше, пока не подберет лучшие параметры, то есть не найдет минимум. Так выглядит ситауция, когда алгоритм сходится к минимуму:\n![[img/Untitled 4 21.png|Untitled 4 21.png]]\nIt's important to choose the right step size because if it's too small, the algorithm will work slowly, and if it's too big - the risk of overshooting the minimum increases:\n![[img/Untitled 5 19.png|Untitled 5 19.png]]\nIt would be a good idea to find this parameter with a grid search.\nFinally, not all cost functions look like nice, regular bowls. There may be holes, ridges, plateaus, and all sorts of irregular terrains, making convergence to the minimum difficult. There is also the problem that the algorithm can converge to a local minimum.\nТакже важно, чтобы features были одного масштаба. Иначе алгоритм будет сходиться по вытянутой плоской поверхности, что займет много времени или, наоборот, перескочит минимум.\n![[img/Untitled 6 17.png|Untitled 6 17.png]]\nКонтурный график в случае сильного несоответствия масштабов будет слишком тонким, а значит градиентый спуск будет проходить сильными рывками.\nThis algorithm involves calculations over the full training set X, at each step. This is why it's called batch gradient descent: it uses the whole batch of training data at every step. As a result it's terribly slow on very large training sets - вспомним сложность O(nk). However, gradient descent scales well with the number of features; training a linear regression model when there are hundreds of thousands of features is much faster using gradient descent than using the normal equation or SVD decomposition.\n# Stochastic Gradient Descent\n\nBatch gradient descent uses the whole training set to compute the gradients at every step, which makes it very slow when the training set is large.\nAt the opposite extreme, Stochastic Gradient Descent picks a random instance in the training set at every step and computes the gradients based only on that single instance. Obviously, working on a single instance at a time makes the algorithm much faster because it has very little data to manipulate at every iteration. It also makes it possible to train on huge training sets, since only one instance needs to be in memory at each iteration.\nOn the other hand, due to its stochastic (i.e., random) nature, this algorithm is much less regular than Batch Gradient Descent: instead of gently decreasing until it reaches the minimum, the cost function will bounce up and down, decreasing only on average. Over time it will end up very close to the minimum, but once it gets there it will continue to bounce around, never settling down.\n![[img/Untitled 7 16.png|Untitled 7 16.png]]\nThis can actually help the algorithm jump out of local minima, so stochastic gradient descent has a better chance of finding the global minimum.\nTherefore, randomness is good to escape from local optima, but bad because it means that the algorithm can never settle at the minimum.\nOne solution to this dilemma is to gradually reduce the learning rate. The steps start out large (which helps make quick progress and escape local minima), then get smaller and\nsmaller, allowing the algorithm to settle at the global minimum. This process is akin to simulated annealing, an algorithm inspired from the process in metallurgy of annealing, where molten metal is slowly cooled down. The function that determines the learning rate at each iteration is called the learning schedule. If the learning rate is reduced too quickly, you may get stuck in a local\nminimum, or even end up frozen halfway to the minimum. If the learning rate is reduced too slowly, you may jump around the minimum for a long time and end up with a suboptimal solution if you halt training too early.\n\nBy convention we iterate by rounds of m iterations; each round is called an epoch. While the Batch Gradient Descent code iterated 1,000 times through the whole training set, this code goes through the training set only 50 times and reaches a pretty good solution.\n\nWarning: when using Stochastic gradient descent, the training instances must be i.i.d. to ensure that the parameters get pulled toward the global optimum, on average.\n\n# Mini-batch Gradient Descent\nA mixture of both algorithms. Иногда его называют просто стохастическим градиентным спуском, вместо предыдущего алгоритма. Если вы слышите SGD, то скорее всего это именно mini-batch вариация. Эту разновидность применяют в большинстве случаев, т.к. датасеты на практике большие.\nAt each step, instead of computing the gradients based on the full training set (as in Batch GD) or based on just one instance (as in Stochastic GD), Mini-batch GD computes the gradients on small random sets of instances called mini-batches.\nThe algorithm's progress in parameter space is less erratic than with Stochastic\nGD, especially with fairly large mini-batches. As a result, Mini-batch GD will\nend up walking around a bit closer to the minimum than Stochastic GD — but it\nmay be harder for it to escape from local minima (in the case of problems that\nsuffer from local minima, unlike Linear Regression).\n![[img/Untitled 10 12.png|Untitled 10 12.png]]\n\nОдна итерация обрабатывает один мини-батч, например 10% датасета. Затем алгоритм переходит к другой части датасета и так обрабатывает весь. Перебор всего датасета мини-батчами называется epoch.\nРазмер батча задается исследователем и зависит от задачи и доступных CPU/GPU. Нужна точность и скорость сходимости - увеличиваем батчи (на одном батче нужно минимальное число итераций для сходимости), нужны ресурсы оперативной памяти - уменьшаем.\n\nКак связаны размер батча и learning rate? Что нужно сделать с learning rate при уменьшении размера батча? (см. ML-канал)\n\nОпишите суть градиентного спуска, особенности SGD, Mini-batch GD и в каких ситуациях следует использовать каждый (см. ML-канал)\n\n*/\n/*\n\n1. Introduction\nImportance of gradient optimization in machine learning.\n2. Revisiting the basics of ML optimization\nThe objective function and cost/loss functions. Parameter space and searching for minima. Significance of gradients in optimization.\n3. Gradient descent\nDefinition and core idea of gradient descent. The iterative update rule and parameter adjustments. Common terms: cost function, gradient, and convergence. Role of gradient descent in broader ML pipelines.\n4. Math behind gradient descent\nWith examples: gradient calculations for common cost functions.\n4.1 Deriving the gradient descent formula\n4.2 Calculating partial derivatives and their importance\n4.3 Convergence criteria\nWhen to stop updating.\n4.4. Learning rate and its impact\nDefinition of the learning rate. Consequences of large vs. small learning rates. Learning rate decay and scheduling (excluding advanced adaptive methods).\n5. Main types of gradient descent\n5.1 Batch gradient descent\n5.2 Stochastic gradient descent\n5.3 Mini-batch gradient descent\n6. Practical implementation considerations\nData preprocessing and shuffling strategies. Monitoring convergence and validation performance. Tuning the learning rate and other hyperparameters. Common pitfalls (e.g., exploding/vanishing gradients). Practical tips for efficient computation.\n7. Implementations\n\n*/\nimport {useMDXComponents as _provideComponents} from \"@mdx-js/react\";\nimport React from \"react\";\nimport Highlight from \"../../../components/Highlight\";\nimport Code from \"../../../components/Code\";\nimport Latex from \"../../../components/Latex\";\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    h2: \"h2\",\n    a: \"a\",\n    span: \"span\",\n    h3: \"h3\",\n    ol: \"ol\",\n    li: \"li\",\n    ul: \"ul\",\n    strong: \"strong\",\n    hr: \"hr\"\n  }, _provideComponents(), props.components), {Image} = _components;\n  if (!Image) _missingMdxReference(\"Image\", true);\n  return React.createElement(React.Fragment, null, \"\\n\", React.createElement(\"br\"), \"\\n\", \"\\n\", \"\\n\", \"\\n\", React.createElement(_components.p, null, \"Gradient optimization lies at the heart of nearly all modern machine learning (ML) methods, powering everything from classical linear regression models to massive deep neural networks. When we train a model — be it a simple regression model or a state-of-the-art Transformer — our objective is to adjust the model's internal parameters in a way that minimizes some loss or cost function. This minimization is rarely performed analytically, as exact solutions often do not exist for complex models or might be computationally intractable. Instead, we rely on iterative optimization procedures that exploit the gradient of the loss with respect to the model parameters.\"), \"\\n\", React.createElement(_components.p, null, \"A major reason gradient optimization has become so ubiquitous is that it scales relatively well to high-dimensional parameter spaces and large datasets. This is particularly crucial in today's world of deep learning, where models may consist of hundreds of millions (or even billions) of parameters. As a result, expertise in gradient-based optimization is essential for designing, training, and understanding a broad range of machine learning algorithms.\"), \"\\n\", React.createElement(_components.p, null, \"In this article, we will dive into the fundamentals of gradient optimization, revisit the basics of loss minimization in ML, explore gradient descent (the most widely used gradient optimization algorithm), and then examine its three main variants: batch gradient descent, stochastic gradient descent, and mini-batch gradient descent. Along the way, we will study both theoretical underpinnings and practical considerations. Our focus here is on the \\\"classical\\\" versions of gradient-based optimization. In a subsequent article, we will discuss advanced modifications, including adaptive learning rate methods (e.g., Adam, RMSProp, Adagrad) and second-order approaches (e.g., Newton's method, quasi-Newton methods).\"), \"\\n\", React.createElement(_components.p, null, \"Whether you come from a background in machine learning, data science, or a related field, understanding the core mechanics and nuances of gradient optimization is foundational. By the end of this piece, you should have a deeper grasp of how gradient-based optimizers work, why they are used, and how to implement and tune them in practice.\"), \"\\n\", React.createElement(_components.h2, {\n    id: \"2-revisiting-the-basics-of-ml-optimization\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#2-revisiting-the-basics-of-ml-optimization\",\n    \"aria-label\": \"2 revisiting the basics of ml optimization permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"2. Revisiting the basics of ml optimization\"), \"\\n\", React.createElement(_components.p, null, \"In machine learning, a model is typically defined by a set of parameters (weights, biases, coefficients, or any other internal variables) that we wish to learn from data. For a supervised learning problem, we often have a labeled dataset \", React.createElement(Latex, {\n    text: \"\\\\( \\\\{(x_1, y_1), (x_2, y_2), \\\\ldots, (x_m, y_m)\\\\} \\\\)\"\n  }), \". Each \", React.createElement(Latex, {\n    text: \"\\\\(x_i\\\\)\"\n  }), \" denotes the features for the \", React.createElement(Latex, {\n    text: \"\\\\(i\\\\)\"\n  }), \"-th instance, and \", React.createElement(Latex, {\n    text: \"\\\\(y_i\\\\)\"\n  }), \" denotes the corresponding target (e.g., a class label or a continuous value).\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"the-objective-function-loss-or-cost-function\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#the-objective-function-loss-or-cost-function\",\n    \"aria-label\": \"the objective function loss or cost function permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"The objective function (loss or cost function)\"), \"\\n\", React.createElement(_components.p, null, \"A core piece of machine learning is defining how we measure the performance of a model's parameters on the given data. This is typically done via a loss (or cost) function. For example, in linear regression with mean squared error (MSE) as the loss, our goal is to minimize:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\nL(\\\\mathbf{w}) = \\\\frac{1}{m} \\\\sum_{i=1}^{m} \\\\bigl(y_i - \\\\hat{y}_i(\\\\mathbf{w})\\\\bigr)^2,\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"where \", React.createElement(Latex, {\n    text: \"\\\\( \\\\hat{y}_i(\\\\mathbf{w}) \\\\)\"\n  }), \" is the model's prediction for the \", React.createElement(Latex, {\n    text: \"\\\\(i\\\\)\"\n  }), \"-th data point, and \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathbf{w}\\\\)\"\n  }), \" represents the model parameters (weights). In classification tasks, one might use a cross-entropy loss, hinge loss, or another appropriate objective.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"parameter-space-and-searching-for-minima\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#parameter-space-and-searching-for-minima\",\n    \"aria-label\": \"parameter space and searching for minima permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Parameter space and searching for minima\"), \"\\n\", React.createElement(_components.p, null, \"For many models, the parameter space can be extremely large (sometimes millions of dimensions). Directly solving for the global optimum can be very difficult or impossible in closed form. Instead, we rely on numerical optimization methods that iteratively refine an initial guess.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"significance-of-gradients-in-optimization\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#significance-of-gradients-in-optimization\",\n    \"aria-label\": \"significance of gradients in optimization permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Significance of gradients in optimization\"), \"\\n\", React.createElement(_components.p, null, \"The gradient of the loss function with respect to the parameters tells us the local direction of steepest ascent in the loss landscape. To minimize the loss, we want to step in the opposite direction of that gradient. This insight underpins gradient descent: if the gradient of \", React.createElement(Latex, {\n    text: \"\\\\(L(\\\\mathbf{w})\\\\)\"\n  }), \" at step \", React.createElement(Latex, {\n    text: \"\\\\(t\\\\)\"\n  }), \" is \", React.createElement(Latex, {\n    text: \"\\\\(\\\\nabla L(\\\\mathbf{w}^{(t)})\\\\)\"\n  }), \", we update our parameters as:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\( \\\\mathbf{w}^{(t+1)} = \\\\mathbf{w}^{(t)} - \\\\eta \\\\,\\\\nabla L(\\\\mathbf{w}^{(t)}) \\\\)\"\n  }), \"\\n\", React.createElement(_components.p, null, \"where \", React.createElement(Latex, {\n    text: \"\\\\(\\\\eta\\\\)\"\n  }), \" is the learning rate. While conceptually simple, properly tuning the gradient descent procedure can be an art in itself. Learning rate choice, initialization strategies, and iteration scheduling all factor into the final performance of our model.\"), \"\\n\", React.createElement(_components.h2, {\n    id: \"3-gradient-descent\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#3-gradient-descent\",\n    \"aria-label\": \"3 gradient descent permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"3. Gradient descent\"), \"\\n\", React.createElement(_components.p, null, \"Gradient descent (GD) is one of the most fundamental and well-known optimization algorithms in machine learning. Its basic premise is:\"), \"\\n\", React.createElement(_components.ol, null, \"\\n\", React.createElement(_components.li, null, \"Start with an initial guess for the parameters (e.g., random initialization).\"), \"\\n\", React.createElement(_components.li, null, \"Compute the gradient of the objective function (the cost or loss) with respect to the parameters.\"), \"\\n\", React.createElement(_components.li, null, \"Update the parameters by moving a small step against the gradient (the direction of steepest descent).\"), \"\\n\", React.createElement(_components.li, null, \"Repeat steps 2 and 3 until convergence or until a stopping criterion is met.\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, React.createElement(Highlight, null, \"Key Terms\"), \" to keep in mind:\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Cost function (Loss function)\"), \": The function you want to minimize (e.g., MSE, cross-entropy).\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Gradient\"), \": The vector of partial derivatives of the cost function with respect to all parameters.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Convergence\"), \": A state where the parameters are no longer changing meaningfully, or a defined stopping criterion is met.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Learning rate\"), \": A hyperparameter (\", React.createElement(Latex, {\n    text: \"\\\\(\\\\eta\\\\)\"\n  }), \") that controls the size of the step in the direction of the negative gradient.\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, \"Gradient descent's concept is straightforward, yet the practical details — especially regarding convergence speed and numerical stability — are crucial to making it work effectively. For example, setting the learning rate too high can cause the parameters to oscillate wildly or diverge, while a too-small learning rate can lead to painfully slow training.\"), \"\\n\", React.createElement(_components.h2, {\n    id: \"4-math-behind-gradient-descent\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#4-math-behind-gradient-descent\",\n    \"aria-label\": \"4 math behind gradient descent permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"4. Math behind gradient descent\"), \"\\n\", React.createElement(_components.p, null, \"The mathematics of gradient descent can be understood by looking at how we compute partial derivatives of a chosen loss function and update the model parameters accordingly. In machine learning, these partial derivatives often correspond to how each weight in a neural network or a linear model influences the overall cost.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"41-deriving-the-gradient-descent-formula\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#41-deriving-the-gradient-descent-formula\",\n    \"aria-label\": \"41 deriving the gradient descent formula permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"4.1 deriving the gradient descent formula\"), \"\\n\", React.createElement(_components.p, null, \"Suppose we have a cost function \", React.createElement(Latex, {\n    text: \"\\\\(L(\\\\mathbf{w})\\\\)\"\n  }), \". The gradient \", React.createElement(Latex, {\n    text: \"\\\\(\\\\nabla L(\\\\mathbf{w})\\\\)\"\n  }), \" is the vector of partial derivatives:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\nabla L(\\\\mathbf{w}) =\\n\\\\begin{bmatrix}\\n\\\\frac{\\\\partial L(\\\\mathbf{w})}{\\\\partial w_1} \\\\\\\\\\n\\\\frac{\\\\partial L(\\\\mathbf{w})}{\\\\partial w_2} \\\\\\\\\\n\\\\vdots \\\\\\\\\\n\\\\frac{\\\\partial L(\\\\mathbf{w})}{\\\\partial w_n}\\n\\\\end{bmatrix}.\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"Intuitively, each component of \", React.createElement(Latex, {\n    text: \"\\\\(\\\\nabla L(\\\\mathbf{w})\\\\)\"\n  }), \" measures how sensitive the loss is to changes in a particular parameter \", React.createElement(Latex, {\n    text: \"\\\\(w_j\\\\)\"\n  }), \". Moving against the gradient — i.e., subtracting a small multiple of the gradient from the current parameter vector — lowers the value of the cost function, at least in a local sense.\"), \"\\n\", React.createElement(_components.p, null, \"Mathematically, the update rule for gradient descent can be written as:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\mathbf{w}^{(t+1)} = \\\\mathbf{w}^{(t)} - \\\\eta \\\\,\\\\nabla L(\\\\mathbf{w}^{(t)}),\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"where \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathbf{w}^{(t)}\\\\)\"\n  }), \" is the parameter vector at iteration \", React.createElement(Latex, {\n    text: \"\\\\(t\\\\)\"\n  }), \", and \", React.createElement(Latex, {\n    text: \"\\\\(\\\\eta\\\\)\"\n  }), \" is the learning rate.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"42-calculating-partial-derivatives-and-their-importance\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#42-calculating-partial-derivatives-and-their-importance\",\n    \"aria-label\": \"42 calculating partial derivatives and their importance permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"4.2 calculating partial derivatives and their importance\"), \"\\n\", React.createElement(_components.p, null, \"For many ML models — like linear or logistic regression — the partial derivatives of the loss function can be computed analytically. For instance, in linear regression with mean squared error, one can derive a closed-form gradient expression:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\nabla L(\\\\mathbf{w}) = \\\\frac{2}{m} \\\\mathbf{X}^T (\\\\mathbf{X} \\\\mathbf{w} - \\\\mathbf{y}),\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"where \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathbf{X}\\\\)\"\n  }), \" is the design matrix of input features, \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathbf{y}\\\\)\"\n  }), \" is the vector of target values, and \", React.createElement(Latex, {\n    text: \"\\\\(m\\\\)\"\n  }), \" is the number of training samples.\"), \"\\n\", React.createElement(_components.p, null, \"In other models, especially neural networks, we often use backpropagation (the chain rule) to compute partial derivatives. Regardless of the model, calculating partial derivatives accurately and efficiently is essential to applying gradient descent.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"43-convergence-criteria\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#43-convergence-criteria\",\n    \"aria-label\": \"43 convergence criteria permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"4.3 convergence criteria\"), \"\\n\", React.createElement(_components.p, null, \"Determining when gradient descent \\\"converges\\\" can be somewhat subjective or dependent on application-specific needs. Common criteria include:\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Parameter change\"), \": Stop when the difference \", React.createElement(Latex, {\n    text: \"\\\\( \\\\|\\\\mathbf{w}^{(t+1)} - \\\\mathbf{w}^{(t)}\\\\|\\\\)\"\n  }), \" is below a certain threshold.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Gradient magnitude\"), \": Stop when \", React.createElement(Latex, {\n    text: \"\\\\(\\\\|\\\\nabla L(\\\\mathbf{w}^{(t)})\\\\|\\\\)\"\n  }), \" becomes very small.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Maximum iterations\"), \": Stop after a fixed number of iterations or epochs.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Validation metric\"), \": In practice, we often monitor a validation set's performance. If it stops improving (or worsens), we may reduce the learning rate or halt entirely (early stopping).\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, \"In many real-world ML applications, early stopping based on validation performance is especially common, since it helps prevent overfitting.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"44-learning-rate-and-its-impact\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#44-learning-rate-and-its-impact\",\n    \"aria-label\": \"44 learning rate and its impact permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"4.4 learning rate and its impact\"), \"\\n\", React.createElement(_components.p, null, \"The learning rate \", React.createElement(Latex, {\n    text: \"\\\\(\\\\eta\\\\)\"\n  }), \" is arguably the single most important hyperparameter in gradient descent. Its role is to determine how big a step we take in the negative gradient direction each time we update the parameters.\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Too large \", React.createElement(Latex, {\n    text: \"\\\\(\\\\eta\\\\)\"\n  })), \": The parameter updates might overshoot the minimum. Loss values can explode or fluctuate drastically, resulting in divergence or chaotic behavior.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Too small \", React.createElement(Latex, {\n    text: \"\\\\(\\\\eta\\\\)\"\n  })), \": Convergence becomes very slow, potentially requiring an impractically large number of updates.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Variable learning rate\"), \": Many training regimens feature learning rate \\\"decay\\\" or scheduling, in which \", React.createElement(Latex, {\n    text: \"\\\\(\\\\eta\\\\)\"\n  }), \" is reduced over time to refine the convergence process. A common schedule might be a simple decay: \", React.createElement(Latex, {\n    text: \"\\\\(\\\\eta^{(t)} = \\\\frac{\\\\eta_0}{1 + kt}\\\\)\"\n  }), \", or an exponential decay like \", React.createElement(Latex, {\n    text: \"\\\\(\\\\eta^{(t)} = \\\\eta_0 \\\\cdot \\\\alpha^t\\\\)\"\n  }), \", where \", React.createElement(Latex, {\n    text: \"\\\\(k\\\\)\"\n  }), \" or \", React.createElement(Latex, {\n    text: \"\\\\(\\\\alpha\\\\)\"\n  }), \" is some constant chosen by the researcher.\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, \"In practice, the learning rate can be tuned by trial and error, grid search, random search, or more sophisticated automated hyperparameter optimization. The perfect setting is highly model- and dataset-dependent, making this a critical point of experimentation in real-world ML pipelines.\"), \"\\n\", React.createElement(_components.h2, {\n    id: \"5-main-types-of-gradient-descent\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#5-main-types-of-gradient-descent\",\n    \"aria-label\": \"5 main types of gradient descent permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"5. main types of gradient descent\"), \"\\n\", React.createElement(_components.p, null, \"While the classical idea of gradient descent updates is straightforward, the exact mechanism by which we compute the gradient (and the portion of the dataset used) can vary significantly. This gives rise to three main variants:\"), \"\\n\", React.createElement(_components.ol, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Batch gradient descent (BGD)\"), \": Uses the entire training set to compute the gradient at each step.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Stochastic gradient descent (SGD)\"), \": Uses one training example (or sometimes a very small subset) per gradient update.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Mini-batch gradient descent (MBGD)\"), \": Uses a small batch of training examples (e.g., 32, 64, or 256 samples) at each iteration.\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, \"Each approach has advantages and drawbacks, influencing how it handles large datasets, converges to minima, and generalizes to new data.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"51-batch-gradient-descent\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#51-batch-gradient-descent\",\n    \"aria-label\": \"51 batch gradient descent permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"5.1 batch gradient descent\"), \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Batch gradient descent\"), \" computes the gradient of the cost function by summing or averaging over all training examples before performing a single update. That is:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\nabla L(\\\\mathbf{w}) = \\\\frac{1}{m} \\\\sum_{i=1}^{m} \\\\nabla L_i(\\\\mathbf{w}),\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"where \", React.createElement(Latex, {\n    text: \"\\\\(L_i(\\\\mathbf{w})\\\\)\"\n  }), \" is the loss contribution from the \", React.createElement(Latex, {\n    text: \"\\\\(i\\\\)\"\n  }), \"-th example, and \", React.createElement(Latex, {\n    text: \"\\\\(m\\\\)\"\n  }), \" is the total number of training examples.\"), \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Pros\"), \":\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, \"The gradient computation is exact for the current set of parameters (assuming no sampling, the entire dataset is used).\"), \"\\n\", React.createElement(_components.li, null, \"Often yields stable and predictable updates.\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Cons\"), \":\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, \"Very slow for large datasets, as each update requires a pass over all the training data.\"), \"\\n\", React.createElement(_components.li, null, \"Consumes significant memory if the dataset does not fit comfortably in RAM or GPU memory.\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Use cases\"), \":\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, \"Datasets that are moderately sized (where computing the gradient over the entire set is not prohibitively expensive).\"), \"\\n\", React.createElement(_components.li, null, \"Problems where stable, more deterministic updates are preferable.\"), \"\\n\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"52-stochastic-gradient-descent\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#52-stochastic-gradient-descent\",\n    \"aria-label\": \"52 stochastic gradient descent permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"5.2 stochastic gradient descent\"), \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Stochastic gradient descent (SGD)\"), \" picks one training example (or sometimes a single random subset) at each iteration to compute an approximate gradient. Formally, if \", React.createElement(Latex, {\n    text: \"\\\\(i\\\\)\"\n  }), \" is chosen randomly from \", React.createElement(Latex, {\n    text: \"\\\\(\\\\{1, 2, \\\\ldots, m\\\\}\\\\)\"\n  }), \", then:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\nabla L(\\\\mathbf{w}) \\\\approx \\\\nabla L_i(\\\\mathbf{w}).\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Pros\"), \":\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, \"Can be extremely fast and memory efficient, as it processes one example at a time (in the purest form).\"), \"\\n\", React.createElement(_components.li, null, \"Potentially escapes local minima more easily, because the gradient is \\\"noisy.\\\"\"), \"\\n\", React.createElement(_components.li, null, \"Scales well to massive datasets (common in online learning scenarios).\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Cons\"), \":\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, \"High variance in updates can result in an erratic convergence path.\"), \"\\n\", React.createElement(_components.li, null, \"Requires more careful tuning of the learning rate, often with a decay schedule.\"), \"\\n\", React.createElement(_components.li, null, \"The objective function does not necessarily decrease at every iteration (due to the sampling noise).\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Use cases\"), \":\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, \"Very large datasets or streaming data.\"), \"\\n\", React.createElement(_components.li, null, \"Online or real-time machine learning applications where data arrives continuously.\"), \"\\n\", React.createElement(_components.li, null, \"Situations where memory resources are limited.\"), \"\\n\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"53-mini-batch-gradient-descent\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#53-mini-batch-gradient-descent\",\n    \"aria-label\": \"53 mini batch gradient descent permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"5.3 mini-batch gradient descent\"), \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Mini-batch gradient descent\"), \" is in many ways a middle ground between batch and stochastic methods. It processes a small batch of training data (say \", React.createElement(Latex, {\n    text: \"\\\\(b\\\\)\"\n  }), \" examples) at each iteration:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\nabla L(\\\\mathbf{w}) \\\\approx \\\\frac{1}{b} \\\\sum_{i \\\\in \\\\text{batch}} \\\\nabla L_i(\\\\mathbf{w}),\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"where \\\"batch\\\" is a small random subset of the training set. For instance, if \", React.createElement(Latex, {\n    text: \"\\\\(m = 60,000\\\\)\"\n  }), \" and the chosen mini-batch size is 64, each update uses 64 examples out of the 60,000.\"), \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Pros\"), \":\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, \"Less computationally heavy than full batch descent, since it only processes a small subset at a time.\"), \"\\n\", React.createElement(_components.li, null, \"Reduces variance compared to pure SGD, since multiple examples smooth out the gradient estimate.\"), \"\\n\", React.createElement(_components.li, null, \"Efficiently vectorizable on modern hardware (GPUs often favor certain batch sizes).\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Cons\"), \":\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, \"Slightly more memory-intensive than pure SGD if the mini-batch is large, but typically not as large as full batch.\"), \"\\n\", React.createElement(_components.li, null, \"Still introduces some noise in the gradient, although less than SGD.\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Use cases\"), \":\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, \"The most common approach in modern deep learning, as it balances computational efficiency with convergence stability.\"), \"\\n\", React.createElement(_components.li, null, \"Works well with GPU-based acceleration.\"), \"\\n\", React.createElement(_components.li, null, \"Generally recommended for medium to large datasets in practical ML scenarios.\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, \"Most deep learning frameworks default to mini-batch training. Researchers often tune the mini-batch size based on hardware constraints (e.g., GPU VRAM) and find that certain batch sizes can lead to better (or faster) convergence, depending on the problem and architecture.\"), \"\\n\", React.createElement(_components.h2, {\n    id: \"6-practical-implementation-considerations\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#6-practical-implementation-considerations\",\n    \"aria-label\": \"6 practical implementation considerations permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"6. practical implementation considerations\"), \"\\n\", React.createElement(_components.p, null, \"The success of gradient-based methods depends on several practical details that go beyond the core update rule. Here are a few key considerations:\"), \"\\n\", React.createElement(_components.ol, null, \"\\n\", React.createElement(_components.li, null, \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Data preprocessing\"), \":\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Normalization or standardization\"), \": Bringing all features to a similar scale speeds up convergence by avoiding extremely elongated or skewed loss landscapes.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Shuffling\"), \": It is typically beneficial to shuffle the training data (or shuffle it in mini-batches) to avoid unwanted ordering effects.\"), \"\\n\"), \"\\n\"), \"\\n\", React.createElement(_components.li, null, \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Monitoring convergence and validation performance\"), \":\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, \"Track the training loss and validation loss over iterations or epochs.\"), \"\\n\", React.createElement(_components.li, null, \"Consider using early stopping when the validation performance stops improving.\"), \"\\n\"), \"\\n\"), \"\\n\", React.createElement(_components.li, null, \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Hyperparameter tuning\"), \":\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Learning rate\"), \": Often the single most important parameter; must be chosen carefully.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Batch size\"), \": For mini-batch methods, the size of each batch can significantly affect both speed and performance.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Regularization\"), \": Methods like \", React.createElement(Latex, {\n    text: \"\\\\(L_2\\\\)\"\n  }), \" (ridge) or \", React.createElement(Latex, {\n    text: \"\\\\(L_1\\\\)\"\n  }), \" (lasso) can be integrated with gradient descent simply by adjusting the loss function to include regularization terms.\"), \"\\n\"), \"\\n\"), \"\\n\", React.createElement(_components.li, null, \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Exploding/vanishing gradients\"), \":\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, \"In deep neural networks, the gradient can sometimes become extremely large or extremely small, causing numerical issues and hampering learning. Various techniques (e.g., gradient clipping, careful initialization, batch normalization, or sophisticated architectures) are used to mitigate these issues.\"), \"\\n\"), \"\\n\"), \"\\n\", React.createElement(_components.li, null, \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Efficient computation\"), \":\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, \"Tools like vectorization (in NumPy, PyTorch, TensorFlow) can dramatically speed up gradient computations.\"), \"\\n\", React.createElement(_components.li, null, \"GPUs or TPUs excel at mini-batch gradient computations for deep learning tasks.\"), \"\\n\"), \"\\n\"), \"\\n\", React.createElement(_components.li, null, \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Heuristic improvements\"), \":\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, \"Using momentum-based methods (to be discussed in detail in subsequent articles) can smooth out noisy updates.\"), \"\\n\", React.createElement(_components.li, null, \"Advanced learning rate schedules (e.g., warm restarts, cyclical learning rates) can sometimes yield better results than static schedules.\"), \"\\n\"), \"\\n\"), \"\\n\"), \"\\n\", React.createElement(_components.h2, {\n    id: \"7-implementations\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#7-implementations\",\n    \"aria-label\": \"7 implementations permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"7. implementations\"), \"\\n\", React.createElement(_components.p, null, \"Below are two sample implementations that demonstrate how gradient descent might be coded from scratch in Python, focusing on a simple linear regression scenario. Following that, we'll briefly illustrate how one might use scikit-learn's \", React.createElement(Highlight, null, \"SGDClassifier\"), \" for classification tasks.\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.h3, {\n    id: \"71-example-batch-gradient-descent-for-linear-regression-numpy\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#71-example-batch-gradient-descent-for-linear-regression-numpy\",\n    \"aria-label\": \"71 example batch gradient descent for linear regression numpy permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"7.1 example: batch gradient descent for linear regression (numpy)\"), \"\\n\", React.createElement(_components.p, null, \"Here, we demonstrate how to implement \", React.createElement(_components.strong, null, \"batch gradient descent\"), \" to learn the parameters of a simple linear regression model. Suppose we have a design matrix \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathbf{X}\\\\)\"\n  }), \" (dimension \", React.createElement(Latex, {\n    text: \"\\\\(m \\\\times n\\\\)\"\n  }), \") and a target vector \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathbf{y}\\\\)\"\n  }), \" of dimension \", React.createElement(Latex, {\n    text: \"\\\\(m\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<div class=\\\"gatsby-highlight\\\" data-language=\\\"text\\\"><pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">&lt;Code text={`\\nimport numpy as np\\n\\ndef batch_gradient_descent(X, y, learning_rate=0.01, n_iterations=1000):\\n    \\\"\\\"\\\"\\n    Perform batch gradient descent for a linear regression model.\\n    X is an m x n matrix of features.\\n    y is an m-dimensional vector of targets.\\n    \\\"\\\"\\\"\\n    m, n = X.shape\\n    # Initialize parameters (weights) randomly or with zeros\\n    w = np.zeros(n)\\n    \\n    # Optionally, you can add a column of 1s to X externally for the intercept\\n    # or manage it separately. We'll assume X is already preprocessed.\\n    \\n    for iteration in range(n_iterations):\\n        # Predictions\\n        y_pred = X.dot(w)\\n        \\n        # Compute the gradient of the MSE cost function\\n        # L(w) = (1/m) * sum((y_pred - y)^2)\\n        # Gradient: (2/m) * X.T.dot(y_pred - y)\\n        \\n        gradient = (2.0 / m) * X.T.dot(y_pred - y)\\n        \\n        # Update rule\\n        w = w - learning_rate * gradient\\n        \\n        # (Optional) Monitor the loss\\n        if iteration % 100 == 0:\\n            loss = np.mean((y_pred - y) ** 2)\\n            print(f\\\"Iteration {iteration}, Loss: {loss:.5f}\\\")\\n    \\n    return w\\n`}/></code></pre></div>\"\n    }\n  }), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Initialization\"), \": We set the initial parameter vector \", React.createElement(Latex, {\n    text: \"\\\\(w\\\\)\"\n  }), \" to zeros (or random small values).\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Learning rate\"), \": Default is \", React.createElement(Latex, {\n    text: \"\\\\(0.01\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Gradient computation\"), \": Uses the entire dataset on each update (batch GD).\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Update step\"), \": \", React.createElement(Latex, {\n    text: \"\\\\( w \\\\leftarrow w - \\\\eta \\\\, \\\\nabla L(w)\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Output\"), \": Returns the final learned weights.\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, \"In practice, you might add advanced features like dynamic learning rate schedules or early stopping. You would also preprocess the input data (e.g., normalization, mean-centering) before calling this function.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"72-example-stochastic-gradient-descent-for-linear-regression-numpy\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#72-example-stochastic-gradient-descent-for-linear-regression-numpy\",\n    \"aria-label\": \"72 example stochastic gradient descent for linear regression numpy permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"7.2 example: stochastic gradient descent for linear regression (numpy)\"), \"\\n\", React.createElement(_components.p, null, \"Below is a simplistic code snippet for \", React.createElement(_components.strong, null, \"stochastic gradient descent\"), \". Each iteration uses exactly one randomly chosen data point to update the weights. This can converge quickly in practice but is often noisy.\"), \"\\n\", React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<div class=\\\"gatsby-highlight\\\" data-language=\\\"text\\\"><pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">&lt;Code text={`\\nimport numpy as np\\n\\ndef stochastic_gradient_descent(X, y, learning_rate=0.01, n_epochs=5):\\n    \\\"\\\"\\\"\\n    Perform stochastic gradient descent for a linear regression model.\\n    X is an m x n matrix of features.\\n    y is an m-dimensional vector of targets.\\n    \\\"\\\"\\\"\\n    m, n = X.shape\\n    w = np.zeros(n)\\n    \\n    for epoch in range(n_epochs):\\n        # Shuffle the data to avoid cycles\\n        indices = np.random.permutation(m)\\n        X_shuffled = X[indices]\\n        y_shuffled = y[indices]\\n        \\n        for i in range(m):\\n            # Pick one example\\n            xi = X_shuffled[i, :].reshape(1, -1)\\n            yi = y_shuffled[i]\\n            \\n            # Predict\\n            y_pred = xi.dot(w)\\n            \\n            # Compute the gradient for this single example\\n            gradient = 2.0 * xi.T.dot(y_pred - yi)\\n            \\n            # Update\\n            w = w - learning_rate * gradient.flatten()\\n        \\n        # (Optional) Monitoring the overall loss at the end of each epoch\\n        total_loss = np.mean((X.dot(w) - y) ** 2)\\n        print(f\\\"Epoch {epoch+1}, Loss: {total_loss:.5f}\\\")\\n    \\n    return w\\n`}/></code></pre></div>\"\n    }\n  }), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Epoch\"), \": A single pass through all \", React.createElement(Latex, {\n    text: \"\\\\(m\\\\)\"\n  }), \" training samples.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Shuffle\"), \": We shuffle \", React.createElement(Latex, {\n    text: \"\\\\(X\\\\)\"\n  }), \" and \", React.createElement(Latex, {\n    text: \"\\\\(y\\\\)\"\n  }), \" each epoch for better convergence properties.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"One-sample update\"), \": The gradient is computed using just one sample (\", React.createElement(Latex, {\n    text: \"\\\\(i\\\\)\"\n  }), \").\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, \"Here, the parameters get updated \", React.createElement(Latex, {\n    text: \"\\\\(m\\\\)\"\n  }), \" times per epoch, one per training sample. This often allows faster initial progress but can be quite noisy, necessitating learning rate schedules or other smoothing techniques.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"73-mini-batch-gradient-descent\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#73-mini-batch-gradient-descent\",\n    \"aria-label\": \"73 mini batch gradient descent permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"7.3 mini-batch gradient descent\"), \"\\n\", React.createElement(_components.p, null, \"A \", React.createElement(_components.strong, null, \"mini-batch approach\"), \" is usually more efficient on modern hardware (particularly GPUs), especially for deep learning tasks. Conceptually, it is halfway between the above two approaches, so the implementation is similar; the main difference is that we pick (say) 32 or 64 samples at a time rather than 1 or \", React.createElement(Latex, {\n    text: \"\\\\(m\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<div class=\\\"gatsby-highlight\\\" data-language=\\\"text\\\"><pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">&lt;Code text={`\\nimport numpy as np\\n\\ndef mini_batch_gradient_descent(X, y, learning_rate=0.01, n_epochs=5, batch_size=32):\\n    \\\"\\\"\\\"\\n    Perform mini-batch gradient descent for a linear regression model.\\n    X is an m x n matrix of features.\\n    y is an m-dimensional vector of targets.\\n    \\\"\\\"\\\"\\n    m, n = X.shape\\n    w = np.zeros(n)\\n    n_batches_per_epoch = m // batch_size\\n    \\n    for epoch in range(n_epochs):\\n        # Shuffle the data\\n        indices = np.random.permutation(m)\\n        X_shuffled = X[indices]\\n        y_shuffled = y[indices]\\n        \\n        for b in range(n_batches_per_epoch):\\n            start = b * batch_size\\n            end = start + batch_size\\n            \\n            X_batch = X_shuffled[start:end, :]\\n            y_batch = y_shuffled[start:end]\\n            \\n            # Predictions\\n            y_pred = X_batch.dot(w)\\n            \\n            # Compute gradient on the mini-batch\\n            gradient = (2.0 / batch_size) * X_batch.T.dot(y_pred - y_batch)\\n            \\n            # Update\\n            w = w - learning_rate * gradient\\n        \\n        # At the end of each epoch, you could measure the global training loss\\n        total_loss = np.mean((X.dot(w) - y) ** 2)\\n        print(f\\\"Epoch {epoch+1}, Loss: {total_loss:.5f}\\\")\\n    \\n    return w\\n`}/></code></pre></div>\"\n    }\n  }), \"\\n\", React.createElement(_components.p, null, \"This approach typically converges more smoothly than pure stochastic gradient descent and more quickly than batch gradient descent (especially for large \", React.createElement(Latex, {\n    text: \"\\\\(m\\\\)\"\n  }), \").\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"74-example-using-scikit-learns-sgdclassifier\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#74-example-using-scikit-learns-sgdclassifier\",\n    \"aria-label\": \"74 example using scikit learns sgdclassifier permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"7.4 example: using scikit-learn's sgdclassifier\"), \"\\n\", React.createElement(_components.p, null, \"Many frameworks provide ready-made SGD-based estimators. Below is a short example using \", React.createElement(_components.strong, null, \"scikit-learn\"), \"'s \", React.createElement(Highlight, null, \"SGDClassifier\"), \" on the classic Iris dataset:\"), \"\\n\", React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<div class=\\\"gatsby-highlight\\\" data-language=\\\"text\\\"><pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">&lt;Code text={`\\nfrom sklearn.linear_model import SGDClassifier\\nfrom sklearn import datasets\\nfrom sklearn.model_selection import train_test_split\\n\\n# Load the iris dataset\\niris = datasets.load_iris()\\nX = iris.data  # shape (150, 4)\\ny = iris.target  # shape (150,)\\n\\n# Split into train/test sets\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X, y, test_size=0.3, random_state=42\\n)\\n\\n# Create and fit an SGD classifier\\nclf = SGDClassifier(\\n    loss='hinge',      # The 'hinge' loss gives a linear SVM\\n    penalty='l2',      # L2 regularization\\n    alpha=0.0001,      # Regularization parameter\\n    learning_rate='optimal',\\n    max_iter=1000,\\n    shuffle=True,\\n    random_state=42\\n)\\n\\nclf.fit(X_train, y_train)\\n\\n# Evaluate\\naccuracy = clf.score(X_test, y_test)\\nprint(f\\\"Test set accuracy with SGDClassifier: {accuracy * 100:.2f}%\\\")\\n`}/></code></pre></div>\"\n    }\n  }), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"loss\"), \": By default, \", React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<code class=\\\"language-text\\\">'hinge'</code>\"\n    }\n  }), \" is used, which corresponds to a linear SVM approach.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"penalty\"), \": Regularization method (\", React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<code class=\\\"language-text\\\">'l2'</code>\"\n    }\n  }), \", \", React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<code class=\\\"language-text\\\">'l1'</code>\"\n    }\n  }), \", or \", React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<code class=\\\"language-text\\\">'elasticnet'</code>\"\n    }\n  }), \").\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"alpha\"), \": The coefficient of regularization.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"learning_rate\"), \": \", React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<code class=\\\"language-text\\\">'optimal'</code>\"\n    }\n  }), \" is an adaptive method that scikit-learn uses to help with convergence.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"shuffle\"), \": We usually shuffle data each epoch to improve performance.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"random_state\"), \": Ensures reproducible results.\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, \"Because \", React.createElement(Highlight, null, \"SGDClassifier\"), \" is integrated into scikit-learn, it can handle the details of iteration, convergence detection, and even partial fitting on streaming data if necessary.\"), \"\\n\", React.createElement(_components.h2, {\n    id: \"optional-additional-illustrations\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#optional-additional-illustrations\",\n    \"aria-label\": \"optional additional illustrations permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"(optional) additional illustrations\"), \"\\n\", React.createElement(_components.p, null, \"Below are a few conceptual image placeholders that can be very helpful for visualizing gradient descent, especially for those who are more visually inclined:\"), \"\\n\", React.createElement(Image, {\n    alt: \"Illustration of gradient descent on a contour plot\",\n    path: \"\",\n    caption: \"A contour plot of a cost function in 2D parameter space, showing iterative steps moving downhill toward the minimum.\",\n    zoom: \"false\"\n  }), \"\\n\", React.createElement(Image, {\n    alt: \"Effect of different learning rates\",\n    path: \"\",\n    caption: \"Depiction of how a small vs. large learning rate can either converge slowly or overshoot the minimum, respectively.\",\n    zoom: \"false\"\n  }), \"\\n\", React.createElement(Image, {\n    alt: \"Batch vs. Stochastic vs. Mini-batch updates\",\n    path: \"\",\n    caption: \"Comparison of the three approaches: batch (large stable updates), stochastic (small noisy updates), and mini-batch (balanced approach).\",\n    zoom: \"false\"\n  }), \"\\n\", React.createElement(_components.h2, {\n    id: \"putting-it-all-together\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#putting-it-all-together\",\n    \"aria-label\": \"putting it all together permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"putting it all together\"), \"\\n\", React.createElement(_components.p, null, \"Gradient descent in its many forms is an indispensable tool in the modern machine learning toolbox. While simple on the surface, mastering the intricacies of gradient-based optimization can elevate the performance and stability of your models in real-world contexts. Key points to remember:\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Choice of variant\"), \": Decide which gradient descent approach best suits your dataset size, memory constraints, and hardware. Mini-batch is the de facto standard in deep learning, whereas batch or even pure stochastic gradient descent might be more fitting in smaller-scale or streaming applications.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Learning rate\"), \": Tune it carefully. Experiment with different schedules and watch for signs of divergence or overly slow convergence.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Implementation details\"), \": Proper data preprocessing, random shuffling, regularization, and gradient checks can drastically improve training outcomes.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Further enhancements\"), \": Momentum, adaptive gradient methods (e.g., Adam, RMSProp), and second-order approaches can lead to faster or more robust convergence. We will discuss these methods in a subsequent article.\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, \"Modern ML practitioners often take these fundamentals for granted, but understanding precisely how gradients are computed and used for parameter updates gives you deeper insight into why certain methods and heuristics work as they do. It also opens the door to creative experimentation and innovation, whether you're tackling a standard classification task or pushing the boundaries of deep learning research.\"), \"\\n\", React.createElement(_components.p, null, \"Gradient optimization is a cornerstone — once you grasp it, you're better positioned to tackle everything from logistic regression to large-scale deep networks, from simple academic examples to real-time streaming data scenarios. Moreover, the same principles carry over when you move into advanced concepts: adaptive optimizers, large-batch training for supercomputing clusters, distributed gradient computations, and more.\"), \"\\n\", React.createElement(_components.p, null, \"Finally, keep in mind that while gradient descent is ubiquitous, it isn't always a panacea. Certain classes of problems may be amenable to specialized solvers or alternative optimization strategies. Nonetheless, for the vast majority of machine learning tasks, gradient-based optimization (in one form or another) is the proven workhorse driving model training from start to finish.\"));\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? React.createElement(MDXLayout, props, React.createElement(_createMdxContent, props)) : _createMdxContent(props);\n}\nexport default MDXContent;\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","import GATSBY_COMPILED_MDX from \"/home/avrtt/Repos/avrtt.github.io/src/pages/posts/research/gradient_optimization.mdx\";\nimport React, {useState, useEffect} from 'react';\nimport {useSiteMetadata} from \"../hooks/useSiteMetadata\";\nimport RemoveMarkdown from 'remove-markdown';\nimport {ImageContext} from '../context/ImageContext';\nimport {MDXProvider} from '@mdx-js/react';\nimport Image from '../components/PostImage';\nimport {motion} from 'framer-motion';\nimport SEO from \"../components/seo\";\nimport PostBanner from '../components/PostBanner';\nimport PostBottom from '../components/PostBottom';\nimport {wordsPerMinuteAdventures, wordsPerMinuteResearch, wordsPerMinuteThoughts} from '../data/commonVariables';\nimport {graphql} from 'gatsby';\nimport PartOfCourseNotice from \"../components/PartOfCourseNotice\";\nimport * as stylesButtonsCommon from \"../styles/buttons_common.module.scss\";\nimport * as stylesCustomPostLayouts from \"../styles/custom_post_layouts.module.scss\";\nimport * as stylesTableOfContents from \"../styles/table_of_contents.module.scss\";\nimport * as stylesTagBadges from \"../styles/tag_badges.module.scss\";\nfunction formatReadTime(minutes) {\n  if (minutes <= 10) return '~10 min';\n  if (minutes <= 20) return '~20 min';\n  if (minutes <= 30) return '~30 min';\n  if (minutes <= 40) return '~40 min';\n  if (minutes <= 50) return '~50 min';\n  if (minutes <= 60) return '~1 h';\n  const hours = Math.floor(minutes / 60);\n  const remainder = minutes % 60;\n  if (remainder <= 30) {\n    return `~${hours}${remainder > 0 ? '.5' : ''} h`;\n  }\n  return `~${hours + 1} h`;\n}\nconst TableOfContents = ({toc}) => {\n  if (!toc || !toc.items) return null;\n  const handleClick = (e, url) => {\n    e.preventDefault();\n    const targetId = url.replace('#', '');\n    const targetElement = document.getElementById(targetId);\n    if (targetElement) {\n      targetElement.scrollIntoView({\n        behavior: 'smooth',\n        block: 'start'\n      });\n    }\n  };\n  return React.createElement(\"nav\", {\n    className: stylesTableOfContents.toc\n  }, React.createElement(\"ul\", null, toc.items.map((item, index) => React.createElement(\"li\", {\n    key: index\n  }, React.createElement(\"a\", {\n    href: item.url,\n    onClick: e => handleClick(e, item.url)\n  }, item.title), item.items && React.createElement(TableOfContents, {\n    toc: {\n      items: item.items\n    }\n  })))));\n};\nexport function PostTemplate({data: {mdx, allMdx, allPostImages}, children}) {\n  const {frontmatter, body, tableOfContents} = mdx;\n  const index = frontmatter.index;\n  const slug = frontmatter.slug;\n  const section = slug.split('/')[1];\n  const posts = allMdx.nodes.filter(post => post.frontmatter.slug.includes(`/${section}/`));\n  const sortedPosts = posts.sort((a, b) => a.frontmatter.index - b.frontmatter.index);\n  const currentIndex = sortedPosts.findIndex(post => post.frontmatter.index === index);\n  const nextPost = sortedPosts[currentIndex + 1];\n  const lastPost = sortedPosts[currentIndex - 1];\n  const trimmedSlug = frontmatter.slug.replace(/\\/$/, '');\n  const keyCurrent = (/[^/]*$/).exec(trimmedSlug)[0];\n  const basePath = `posts/${section}/content/${keyCurrent}/`;\n  const [isWideLayout, setIsWideLayout] = useState(frontmatter.flagWideLayoutByDefault);\n  const [isAnimating, setIsAnimating] = useState(false);\n  const toggleLayout = () => {\n    setIsWideLayout(!isWideLayout);\n  };\n  useEffect(() => {\n    setIsAnimating(true);\n    const timer = setTimeout(() => setIsAnimating(false), 340);\n    return () => clearTimeout(timer);\n  }, [isWideLayout]);\n  var wordsPerMinute;\n  if (section === \"adventures\") {\n    wordsPerMinute = wordsPerMinuteAdventures;\n  } else if (section === \"research\") {\n    wordsPerMinute = wordsPerMinuteResearch;\n  } else if (section === \"thoughts\") {\n    wordsPerMinute = wordsPerMinuteThoughts;\n  }\n  const plainTextBody = RemoveMarkdown(body).replace(/import .*? from .*?;/g, '').replace(/<.*?>/g, '').replace(/\\{\\/\\*[\\s\\S]*?\\*\\/\\}/g, '').trim();\n  const wordCount = plainTextBody.split(/\\s+/).length;\n  const baseReadTimeMinutes = Math.ceil(wordCount / wordsPerMinute);\n  const extraTime = frontmatter.extraReadTimeMin || 0;\n  const totalReadTime = baseReadTimeMinutes + extraTime;\n  const readTime = formatReadTime(totalReadTime);\n  const notices = [{\n    flag: frontmatter.flagDraft,\n    component: () => import(\"../components/NotFinishedNotice\")\n  }, {\n    flag: frontmatter.flagMindfuckery,\n    component: () => import(\"../components/MindfuckeryNotice\")\n  }, {\n    flag: frontmatter.flagRewrite,\n    component: () => import(\"../components/RewriteNotice\")\n  }, {\n    flag: frontmatter.flagOffensive,\n    component: () => import(\"../components/OffensiveNotice\")\n  }, {\n    flag: frontmatter.flagProfane,\n    component: () => import(\"../components/ProfanityNotice\")\n  }, {\n    flag: frontmatter.flagMultilingual,\n    component: () => import(\"../components/MultilingualNotice\")\n  }, {\n    flag: frontmatter.flagUnreliably,\n    component: () => import(\"../components/UnreliablyNotice\")\n  }, {\n    flag: frontmatter.flagPolitical,\n    component: () => import(\"../components/PoliticsNotice\")\n  }, {\n    flag: frontmatter.flagCognitohazard,\n    component: () => import(\"../components/CognitohazardNotice\")\n  }, {\n    flag: frontmatter.flagHidden,\n    component: () => import(\"../components/HiddenNotice\")\n  }];\n  const [loadedNotices, setLoadedNotices] = useState([]);\n  useEffect(() => {\n    notices.forEach(({flag, component}) => {\n      if (flag) {\n        component().then(module => {\n          setLoadedNotices(prev => [...prev, module.default]);\n        });\n      }\n    });\n  }, []);\n  return React.createElement(motion.div, {\n    initial: {\n      opacity: 0\n    },\n    animate: {\n      opacity: 1\n    },\n    exit: {\n      opacity: 0\n    },\n    transition: {\n      duration: 0.15\n    }\n  }, React.createElement(PostBanner, {\n    postNumber: frontmatter.index,\n    date: frontmatter.date,\n    updated: frontmatter.updated,\n    readTime: readTime,\n    difficulty: frontmatter.difficultyLevel,\n    title: frontmatter.title,\n    desc: frontmatter.desc,\n    banner: frontmatter.banner,\n    section: section,\n    postKey: keyCurrent,\n    isMindfuckery: frontmatter.flagMindfuckery,\n    mainTag: frontmatter.mainTag\n  }), React.createElement(\"div\", {\n    style: {\n      display: \"flex\",\n      justifyContent: \"flex-end\",\n      flexWrap: \"wrap\",\n      maxWidth: \"75%\",\n      marginLeft: \"auto\",\n      paddingRight: \"1vw\",\n      marginTop: \"-6vh\",\n      marginBottom: \"4vh\"\n    }\n  }, frontmatter.otherTags.map((tag, index) => React.createElement(\"span\", {\n    key: index,\n    className: `noselect ${stylesTagBadges.tagPosts}`,\n    style: {\n      margin: \"0 5px 5px 0\"\n    }\n  }, tag))), React.createElement(\"div\", {\n    class: \"postBody\"\n  }, React.createElement(TableOfContents, {\n    toc: tableOfContents\n  })), React.createElement(\"br\"), React.createElement(\"div\", {\n    style: {\n      margin: \"0 10% -2vh 30%\",\n      textAlign: \"right\"\n    }\n  }, React.createElement(motion.button, {\n    class: \"noselect\",\n    className: stylesCustomPostLayouts.postButton,\n    id: stylesCustomPostLayouts.postLayoutSwitchButton,\n    onClick: toggleLayout,\n    whileTap: {\n      scale: 0.93\n    }\n  }, React.createElement(motion.div, {\n    className: stylesButtonsCommon.buttonTextWrapper,\n    key: isWideLayout,\n    initial: {\n      opacity: 0\n    },\n    animate: {\n      opacity: 1\n    },\n    exit: {\n      opacity: 0\n    },\n    transition: {\n      duration: 0.3,\n      ease: \"easeInOut\"\n    }\n  }, isWideLayout ? \"Switch to default layout\" : \"Switch to wide layout\"))), React.createElement(\"br\"), React.createElement(\"div\", {\n    class: \"postBody\",\n    style: {\n      margin: isWideLayout ? \"0 -14%\" : \"\",\n      maxWidth: isWideLayout ? \"200%\" : \"\",\n      transition: \"margin 1s ease, max-width 1s ease, padding 1s ease\"\n    }\n  }, React.createElement(\"div\", {\n    className: `${stylesCustomPostLayouts.textContent} ${isAnimating ? stylesCustomPostLayouts.fadeOut : stylesCustomPostLayouts.fadeIn}`\n  }, loadedNotices.map((NoticeComponent, index) => React.createElement(NoticeComponent, {\n    key: index\n  })), frontmatter.indexCourse ? React.createElement(PartOfCourseNotice, {\n    index: frontmatter.indexCourse,\n    category: frontmatter.courseCategoryName\n  }) : \"\", React.createElement(ImageContext.Provider, {\n    value: {\n      images: allPostImages.nodes,\n      basePath: basePath.replace(/\\/$/, '') + '/'\n    }\n  }, React.createElement(MDXProvider, {\n    components: {\n      Image\n    }\n  }, children)))), React.createElement(PostBottom, {\n    nextPost: nextPost,\n    lastPost: lastPost,\n    keyCurrent: keyCurrent,\n    section: section\n  }));\n}\nPostTemplate\nexport default function GatsbyMDXWrapper(props) {\n  return React.createElement(PostTemplate, props, React.createElement(GATSBY_COMPILED_MDX, props));\n}\nexport function Head({data}) {\n  const {frontmatter} = data.mdx;\n  const title = frontmatter.titleSEO || frontmatter.title;\n  const titleOG = frontmatter.titleOG || title;\n  const titleTwitter = frontmatter.titleTwitter || title;\n  const description = frontmatter.descSEO || frontmatter.desc;\n  const descriptionOG = frontmatter.descOG || description;\n  const descriptionTwitter = frontmatter.descTwitter || description;\n  const schemaType = frontmatter.schemaType || \"BlogPosting\";\n  const keywords = frontmatter.keywordsSEO;\n  const datePublished = frontmatter.date;\n  const dateModified = frontmatter.updated || datePublished;\n  const imageOG = frontmatter.imageOG || frontmatter.banner?.childImageSharp?.gatsbyImageData?.images?.fallback?.src;\n  const imageAltOG = frontmatter.imageAltOG || descriptionOG;\n  const imageTwitter = frontmatter.imageTwitter || imageOG;\n  const imageAltTwitter = frontmatter.imageAltTwitter || descriptionTwitter;\n  const canonicalUrl = frontmatter.canonicalURL;\n  const flagHidden = frontmatter.flagHidden || false;\n  const mainTag = frontmatter.mainTag || \"Posts\";\n  const section = frontmatter.slug.split('/')[1] || \"posts\";\n  const type = \"article\";\n  const {siteUrl} = useSiteMetadata();\n  const breadcrumbJSON = {\n    \"@context\": \"https://schema.org\",\n    \"@type\": \"BreadcrumbList\",\n    \"itemListElement\": [{\n      \"@type\": \"ListItem\",\n      \"position\": 1,\n      \"name\": \"Home\",\n      \"item\": siteUrl\n    }, {\n      \"@type\": \"ListItem\",\n      \"position\": 2,\n      \"name\": mainTag,\n      \"item\": `${siteUrl}/${frontmatter.slug.split('/')[1]}`\n    }, {\n      \"@type\": \"ListItem\",\n      \"position\": 3,\n      \"name\": title,\n      \"item\": `${siteUrl}${frontmatter.slug}`\n    }]\n  };\n  return React.createElement(SEO, {\n    title: title + \" - avrtt.blog\",\n    titleOG: titleOG,\n    titleTwitter: titleTwitter,\n    description: description,\n    descriptionOG: descriptionOG,\n    descriptionTwitter: descriptionTwitter,\n    schemaType: schemaType,\n    keywords: keywords,\n    datePublished: datePublished,\n    dateModified: dateModified,\n    imageOG: imageOG,\n    imageAltOG: imageAltOG,\n    imageTwitter: imageTwitter,\n    imageAltTwitter: imageAltTwitter,\n    canonicalUrl: canonicalUrl,\n    flagHidden: flagHidden,\n    mainTag: mainTag,\n    section: section,\n    type: type\n  }, React.createElement(\"script\", {\n    type: \"application/ld+json\"\n  }, JSON.stringify(breadcrumbJSON)));\n}\nexport const query = graphql`\n  query($id: String!, $postsFilterRegex: String!, $imagePathRegex: String!) {\n    mdx(id: { eq: $id }) {\n      frontmatter {\n        index\n        indexCourse\n        title\n        titleSEO\n        titleOG\n        titleTwitter\n        courseCategoryName\n        desc\n        descSEO\n        descOG\n        descTwitter\n        date\n        updated\n        extraReadTimeMin\n        difficultyLevel\n        flagDraft\n        flagMindfuckery\n        flagRewrite\n        flagOffensive\n        flagProfane\n        flagMultilingual\n        flagUnreliably\n        flagPolitical\n        flagCognitohazard\n        flagHidden\n        flagWideLayoutByDefault\n        schemaType\n        mainTag\n        otherTags\n        keywordsSEO\n        banner {\n          childImageSharp {\n            gatsbyImageData(\n\t\t\t\t\t\t\tformats: [JPG, WEBP], \n\t\t\t\t\t\t\tplaceholder: BLURRED, \n\t\t\t\t\t\t\tquality: 100\n\t\t\t\t\t\t)\n          }\n        }\n        imageOG\n        imageAltOG\n        imageTwitter\n        imageAltTwitter\n        canonicalURL\n        slug\n      }\n      body\n      tableOfContents(maxDepth: 3)\n    }\n    allMdx(filter: {frontmatter: {slug: {regex: $postsFilterRegex}}}) {\n      nodes {\n        frontmatter {\n          index\n          slug\n          banner {\n            childImageSharp {\n              gatsbyImageData(\n                formats: [JPG, WEBP],\n                placeholder: BLURRED,\n                quality: 100\n              )\n            }\n          }\n        }\n      }\n    }\n    allPostImages: allFile(\n      filter: { \n        sourceInstanceName: { eq: \"images\" },\n        relativePath: { regex: $imagePathRegex }\n      }\n    ) {\n      nodes {\n        relativePath\n        childImageSharp {\n          gatsbyImageData(\n            layout: CONSTRAINED\n            placeholder: DOMINANT_COLOR\n            quality: 100\n          )\n        }\n      }\n    }\n  }\n`;\n","import React from \"react\";\nimport Latex from 'react-latex-next';\nimport 'katex/dist/katex.min.css'; \n  \nconst L = ({ text }) => {\n  return (\n    <Latex>{text}</Latex>\n  );\n};\nexport default L;\n"],"names":["_createMdxContent","props","_components","Object","assign","p","h2","a","span","h3","ol","li","ul","strong","hr","_provideComponents","components","Image","id","component","Error","_missingMdxReference","React","style","position","href","className","dangerouslySetInnerHTML","__html","Latex","text","Highlight","alt","path","caption","zoom","wrapper","MDXLayout","TableOfContents","_ref","toc","items","stylesTableOfContents","map","item","index","key","url","onClick","e","handleClick","preventDefault","targetId","replace","targetElement","document","getElementById","scrollIntoView","behavior","block","title","PostTemplate","_ref2","data","mdx","allMdx","allPostImages","children","frontmatter","body","tableOfContents","section","slug","split","sortedPosts","nodes","filter","post","includes","sort","b","currentIndex","findIndex","nextPost","lastPost","trimmedSlug","keyCurrent","exec","basePath","isWideLayout","setIsWideLayout","useState","flagWideLayoutByDefault","isAnimating","setIsAnimating","wordsPerMinute","useEffect","timer","setTimeout","clearTimeout","wordsPerMinuteAdventures","wordsPerMinuteResearch","wordsPerMinuteThoughts","wordCount","RemoveMarkdown","trim","length","readTime","minutes","hours","Math","floor","remainder","formatReadTime","ceil","extraReadTimeMin","notices","flag","flagDraft","flagMindfuckery","flagRewrite","flagOffensive","flagProfane","flagMultilingual","flagUnreliably","flagPolitical","flagCognitohazard","flagHidden","loadedNotices","setLoadedNotices","forEach","_ref3","then","module","prev","concat","_toConsumableArray","default","motion","div","initial","opacity","animate","exit","transition","duration","PostBanner","postNumber","date","updated","difficulty","difficultyLevel","desc","banner","postKey","isMindfuckery","mainTag","display","justifyContent","flexWrap","maxWidth","marginLeft","paddingRight","marginTop","marginBottom","otherTags","tag","stylesTagBadges","margin","class","textAlign","button","stylesCustomPostLayouts","toggleLayout","whileTap","scale","stylesButtonsCommon","ease","NoticeComponent","indexCourse","PartOfCourseNotice","category","courseCategoryName","ImageContext","Provider","value","images","MDXProvider","PostBottom","GatsbyMDXWrapper","GATSBY_COMPILED_MDX","Head","_ref4","_frontmatter$banner","_frontmatter$banner$c","_frontmatter$banner$c2","_frontmatter$banner$c3","_frontmatter$banner$c4","titleSEO","titleOG","titleTwitter","description","descSEO","descriptionOG","descOG","descriptionTwitter","descTwitter","schemaType","keywords","keywordsSEO","datePublished","dateModified","imageOG","childImageSharp","gatsbyImageData","fallback","src","imageAltOG","imageTwitter","imageAltTwitter","canonicalUrl","canonicalURL","siteUrl","useSiteMetadata","breadcrumbJSON","SEO","type","JSON","stringify"],"sourceRoot":""}