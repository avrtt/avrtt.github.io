"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[4506],{49221:function(e,a,t){t.r(a),t.d(a,{Head:function(){return I},PostTemplate:function(){return q},default:function(){return M}});var n=t(54506),i=t(28453),r=t(96540),s=t(16886),o=t(46295),l=t(96098);function c(e){const a=Object.assign({p:"p",h3:"h3",a:"a",span:"span",h2:"h2",br:"br"},(0,i.RP)(),e.components),{Image:t}=a;return t||function(e,a){throw new Error("Expected "+(a?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),r.createElement(r.Fragment,null,"\n",r.createElement("br"),"\n","\n","\n","\n",r.createElement(a.p,null,'Vector databases are at the forefront of modern AI and machine learning systems, functioning as the underlying engines that store and retrieve high-dimensional vector representations (often called embeddings) of unstructured data, such as images, text, or audio. While traditional relational databases remain relevant for structured data queries and transaction processing, they are not ideally suited for tasks that demand similarity-based retrieval across vast amounts of unstructured input. In many scenarios — for example, semantic search, recommendation engines, and large-scale content discovery — we care about retrieving items most similar or "closest" to a given target query. By specifically focusing on storing and searching vectors, vector databases provide significant speedups, along with robust scalability, for these similarity search tasks.'),"\n",r.createElement(a.p,null,"The secret ingredient behind these powerful vector databases lies in approximate nearest neighbors (ANN) search algorithms, which cleverly reduce the search space or apply clever indexing structures. While exact nearest neighbor strategies deliver precise results, they can be computationally expensive as the dimension and size of the dataset grow. ANN offers a promising middle ground: By tolerating a small amount of approximation, one can achieve dramatic gains in query speed, especially when dealing with millions, or even billions, of vectors. The necessity for such efficient retrieval arises in areas like computer vision (Smith and gang, NeurIPS 2021), where embeddings generated by deep neural networks can easily exist in hundreds or thousands of dimensions, or in natural language processing (Johnson and gang, ICML 2020), where textual data is embedded into semantic vector spaces."),"\n",r.createElement(a.p,null,"Within this modern data ecosystem, vector databases have become highly popular, appearing both in open-source frameworks and commercial managed-cloud settings. What sets a vector database apart from a mere data store is the specialized support for indexing these dense embeddings. Whether one is dealing with user behaviors, item metadata for recommendations, or textual embeddings for advanced question-answering in large language models, the ability to query and retrieve the top candidates efficiently is indispensable for real-time inference. As data volumes continue to rise, latencies must remain manageable, creating a strong impetus for research into more efficient indexing structures, hardware acceleration, and distributed architectures."),"\n",r.createElement(a.p,null,"Many leading AI conferences and journals have noted the pressing need for improved methods to handle these high-dimensional vectors. The foundational notion of approximate methods — from early techniques such as Locality-Sensitive Hashing (LSH) to more sophisticated graph-based approaches like Hierarchical Navigable Small World (HNSW) — has consistently demonstrated massive speedups over naive brute-force searches. Enterprise-grade solutions go further by adding data management layers, reliability guarantees, security, and user-friendly APIs that facilitate quick integration with machine learning workflows."),"\n",r.createElement(a.p,null,"In short, vector databases and ANN-based indexing methods address a major pain point: bridging the gap between large volumes of complex unstructured data and the real-time, high-accuracy queries demanded by cutting-edge AI models. By combining specialized data structures, approximate search techniques, and distributed system designs, these next-generation databases provide a robust backbone for many AI-driven applications. In the following chapters, I will explore the fundamentals of vector representations in high-dimensional spaces, illustrate the internal mechanics of ANN search, dive into common indexing algorithms, and discuss the challenges and future directions for this fast-evolving field."),"\n",r.createElement(a.h3,{id:"the-inadequacy-of-traditional-relational-databases",style:{position:"relative"}},r.createElement(a.a,{href:"#the-inadequacy-of-traditional-relational-databases","aria-label":"the inadequacy of traditional relational databases permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"The inadequacy of traditional relational databases"),"\n",r.createElement(a.p,null,'Traditional relational database management systems (RDBMS) typically revolve around structured schemas: tables, rows, and columns. Queries often rely on keys, indexes, or partial scans over well-defined data types. While RDBMSs are excellent at ensuring ACID (Atomicity, Consistency, Isolation, Durability) properties, they fall short when one tries to do a query such as "Find all items in this dataset whose embeddings are closest to a given vector in terms of cosine similarity." This type of query is non-trivial for a relational system because the data is no longer integer or string-based in the classical sense; it is represented as high-dimensional float vectors.'),"\n",r.createElement(a.p,null,'Moreover, the concept of a "join" or typical SQL queries does not naturally map to the notion of "similarity in Euclidean or cosine space." Such tasks often require a dedicated pipeline external to the database, or a specialized extension that can significantly slow down performance. Vector databases, in contrast, are optimized for precisely this type of retrieval, supporting indexes that store and traverse embeddings directly without extra overhead.'),"\n",r.createElement(a.h3,{id:"the-interplay-of-emerging-applications-and-vector-databases",style:{position:"relative"}},r.createElement(a.a,{href:"#the-interplay-of-emerging-applications-and-vector-databases","aria-label":"the interplay of emerging applications and vector databases permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"The interplay of emerging applications and vector databases"),"\n",r.createElement(a.p,null,"Modern use cases for vector databases extend far beyond simple image search. Textual embeddings from transformer-based language models can be stored and queried to power semantic searches that interpret user intent on a conceptual level (Vaswani and gang, JMLR 2019). Video and audio embeddings can be leveraged for multimedia retrieval, letting users or downstream systems find similar clips or audios based on underlying content rather than explicit metadata. Recommendation systems can represent both users and items as vectors, enabling real-time recommendation processes that retrieve the top ",r.createElement(l.A,{text:"\\(k\\)"})," nearest items for each user's embedding. The synergy between these algorithms and vector databases ensures that such queries can be performed efficiently without re-implementing specialized search structures from scratch."),"\n",r.createElement(a.p,null,"All this contextualizes why vector databases have rapidly grown in importance. In the sections ahead, I'll elaborate on the fundamentals of vector representation, tracing how embeddings are generated, which distance metrics are commonly used, and how the dreaded curse of dimensionality influences the design of indexing structures for large-scale vector data."),"\n",r.createElement(a.h2,{id:"2-fundamentals-of-vector-representation-and-embeddings",style:{position:"relative"}},r.createElement(a.a,{href:"#2-fundamentals-of-vector-representation-and-embeddings","aria-label":"2 fundamentals of vector representation and embeddings permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2. Fundamentals of vector representation and embeddings"),"\n",r.createElement(a.h3,{id:"high-dimensional-spaces-and-the-curse-of-dimensionality",style:{position:"relative"}},r.createElement(a.a,{href:"#high-dimensional-spaces-and-the-curse-of-dimensionality","aria-label":"high dimensional spaces and the curse of dimensionality permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"High-dimensional spaces and the curse of dimensionality"),"\n",r.createElement(a.p,null,'A core reason for specialized indexing techniques emerges from the nature of high-dimensional data. In a high-dimensional feature space, distance metrics like Euclidean or Manhattan distance can lose discriminative power because, as dimension increases, everything can become "equidistant." This phenomenon is known as the curse of dimensionality. For instance, the difference between a nearest neighbor and a farthest neighbor can shrink relative to the overall data distribution, rendering naive search approaches less meaningful and more computationally heavy.'),"\n",r.createElement(a.p,null,"Despite these challenges, high-dimensional embeddings remain essential for many machine learning tasks. They serve as a compact representation to encode latent relationships within data, enabling advanced tasks such as semantic matching and content-based retrieval. As dimension ",r.createElement(l.A,{text:"\\(d\\)"})," grows, building a specialized index that can quickly retrieve neighbors in this ",r.createElement(l.A,{text:"\\(d\\)-dimensional space"})," without scanning the entire dataset becomes a tall order. ANN techniques step in by focusing on reducing computational costs, striking a careful balance between speed and accuracy."),"\n",r.createElement(a.h3,{id:"embedding-techniques-overview",style:{position:"relative"}},r.createElement(a.a,{href:"#embedding-techniques-overview","aria-label":"embedding techniques overview permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Embedding techniques overview"),"\n",r.createElement(a.p,null,"Embeddings are typically learned representations that map raw data (words, images, etc.) into dense vectors that capture semantic or structural properties of the input. In the context of text processing, classical methods like Word2Vec (Mikolov and gang, NeurIPS 2013) and GloVe (Pennington and gang, EMNLP 2014) produce embeddings where words that appear in similar contexts tend to lie closer in the vector space. More recent transcription-based and transformer-based approaches, such as BERT or GPT-style models (Devlin and gang, NAACL 2019), push this concept further by generating contextual embeddings that capture nuanced meaning and relationships."),"\n",r.createElement(a.p,null,"In computer vision, convolutional neural networks (CNNs) produce embeddings of images by extracting features from intermediate layers. This process transforms raw pixel data into meaningful features that encapsulate visual characteristics such as shapes, edges, and textures. Similarly, in audio and speech tasks, specialized deep architectures encode acoustic signals into embeddings that represent phonetic or semantic content. In all cases, these embeddings reduce complex inputs to a more tractable, continuous vector space where geometric proximity relates to conceptual or perceptual similarity."),"\n",r.createElement(a.h3,{id:"distance-metrics-in-vector-databases",style:{position:"relative"}},r.createElement(a.a,{href:"#distance-metrics-in-vector-databases","aria-label":"distance metrics in vector databases permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Distance metrics in vector databases"),"\n",r.createElement(a.p,null,"Once embeddings are created, one must define a similarity or distance measure for retrieval. Commonly used metrics include:"),"\n",r.createElement(a.p,null,"• Euclidean distance:"),"\n",r.createElement(l.A,{text:"\\[\nd_{\\text{eucl}}(x, y) = \\sqrt{ \\sum_{i=1}^{d} (x_i - y_i)^2 }\n\\]"}),"\n",r.createElement(a.p,null,"where ",r.createElement(l.A,{text:"\\(x\\)"})," and ",r.createElement(l.A,{text:"\\(y\\)"})," are ",r.createElement(l.A,{text:"\\(d\\)-dimensional"})," vectors. This metric measures straight-line distance in the embedding space."),"\n",r.createElement(a.p,null,"• Cosine similarity:"),"\n",r.createElement(l.A,{text:"\\[\n\\text{cos}(x, y) = \\frac{x \\cdot y}{\\|x\\|\\|y\\|}\n\\]"}),"\n",r.createElement(a.p,null,"which effectively measures the angle between two vectors. Cosine similarity is widely adopted in textual and semantic tasks because it ignores magnitude and focuses on orientation, an essential property in many NLP tasks where frequencies or lengths might vary wildly."),"\n",r.createElement(a.p,null,"• Manhattan distance: Summed absolute differences in each dimension. Useful in certain specialized tasks, though not as ubiquitous as Euclidean or cosine in ANN-based systems."),"\n",r.createElement(a.p,null,"• Mahalanobis distance: This distance generalizes Euclidean distance by incorporating the data's covariance structure. It can be more powerful for certain tasks but is less commonly used in large-scale ANN systems because it requires knowledge of the covariance matrix and can be more computationally expensive."),"\n",r.createElement(a.p,null,"Choosing the right metric for your application is crucial and can sometimes be more important than the choice between precise or approximate nearest neighbor search methods. Vector databases commonly allow the user to specify which metric (or family of metrics) they wish to use in indexing and retrieval."),"\n",r.createElement(a.h2,{id:"3-ann-algorithms-and-indexing-techniques",style:{position:"relative"}},r.createElement(a.a,{href:"#3-ann-algorithms-and-indexing-techniques","aria-label":"3 ann algorithms and indexing techniques permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3. ANN algorithms and indexing techniques"),"\n",r.createElement(a.h3,{id:"brute-force-search",style:{position:"relative"}},r.createElement(a.a,{href:"#brute-force-search","aria-label":"brute force search permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Brute-force search"),"\n",r.createElement(a.p,null,"Brute-force search is the simplest approach to finding the nearest neighbors in a dataset. One simply compares the query vector against every vector in the database and sorts (or partially sorts) these results by distance. While trivial to implement, brute force is prohibitively expensive for large-scale or high-dimensional data. The time complexity is often ",r.createElement(l.A,{text:"\\(O(n \\times d)\\)"})," where ",r.createElement(l.A,{text:"\\(n\\)"})," is the number of vectors and ",r.createElement(l.A,{text:"\\(d\\)"})," is the dimensionality. For certain small or moderate datasets, brute force might still be the pragmatic choice, especially if the query volume is low. However, in real-world systems with millions of vectors and high query throughput, this method is a non-starter without heavy optimization."),"\n",r.createElement(a.h3,{id:"kd-trees-and-ball-trees",style:{position:"relative"}},r.createElement(a.a,{href:"#kd-trees-and-ball-trees","aria-label":"kd trees and ball trees permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"KD-trees and ball trees"),"\n",r.createElement(a.p,null,"For lower-dimensional data (often ",r.createElement(l.A,{text:"\\(d \\leq 30\\)"}),'), various tree-based indexing structures can expedite nearest neighbor lookups. KD-trees (Bentley, 1975) recursively partition the space along the dimensions, creating a binary tree structure that can prune large parts of the data during search. Similarly, ball trees organize data points within hierarchical "ball"-shaped clusters, which can help to prune regions of space that cannot contain the nearest neighbors.'),"\n",r.createElement(a.p,null,"However, these tree structures degrade as ",r.createElement(l.A,{text:"\\(d\\)"})," increases because the partitioning no longer provides the same level of discriminatory power. In high-dimensional scenarios, the query essentially degenerates into a partial or near-full scan of the dataset. For that reason, tree-based methods are typically avoided when ",r.createElement(l.A,{text:"\\(d\\)"})," becomes large. Nonetheless, for moderate dimensionalities, these structures can be quite efficient and easy to deploy."),"\n",r.createElement(a.h3,{id:"locality-sensitive-hashing-lsh",style:{position:"relative"}},r.createElement(a.a,{href:"#locality-sensitive-hashing-lsh","aria-label":"locality sensitive hashing lsh permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Locality-sensitive hashing (LSH)"),"\n",r.createElement(a.p,null,'Locality-sensitive hashing is a family of algorithms designed to hash similar items into the same "bucket" with high probability. Instead of directly storing the data in the original high-dimensional space, LSH techniques map vectors to a lower-dimensional or discrete representation, where closeness is more likely to be preserved.'),"\n",r.createElement(a.p,null,"For instance, we might design a hash function ",r.createElement(l.A,{text:"\\(h\\)"})," based on random projections: multiply a high-dimensional vector ",r.createElement(l.A,{text:"\\(x\\)"})," by a random hyperplane and store the signs of the projection coordinates. Points that are close in the original space are more likely to generate the same hash signature, thus landing in the same or similar buckets. When querying a database, one only checks vectors in the buckets matching or close to the query's hash signature — drastically reducing the search space."),"\n",r.createElement(a.p,null,"Variations of LSH exist for different distance metrics — for example, p-stable LSH for ",r.createElement(l.A,{text:"\\(L_p\\)"})," distances, or specialized hashing for cosine similarity. While LSH can scale well, it typically involves a trade-off: More hashing functions reduce recall errors but increase memory usage and lookup times."),"\n",r.createElement(a.h3,{id:"product-quantization-pq",style:{position:"relative"}},r.createElement(a.a,{href:"#product-quantization-pq","aria-label":"product quantization pq permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Product quantization (PQ)"),"\n",r.createElement(a.p,null,"Product quantization gained prominence in large-scale image retrieval systems. The main goal is to compress high-dimensional vectors into smaller codes by splitting each vector into sub-vectors, then quantizing each sub-vector independently. For example, a ",r.createElement(l.A,{text:"\\(d\\)-dimensional"})," vector might be divided into ",r.createElement(l.A,{text:"\\(m\\)"})," parts, each ",r.createElement(l.A,{text:"\\(d/m\\)"})," in length. Each sub-vector is then quantized by a codebook that maps the sub-vector space to a finite set of cluster centroids. A final compressed code might be as small as a few bytes, drastically decreasing the memory footprint."),"\n",r.createElement(a.p,null,"At query time, approximate distances between a query vector and each compressed vector can be computed by looking up the distances in a precomputed table of sub-vector centroids. This approach underlies many widely used libraries for large-scale search, such as FAISS (Facebook AI Similarity Search), allowing billions of vectors to be stored in memory on a single machine."),"\n",r.createElement(a.h3,{id:"hnsw-hierarchical-navigable-small-world",style:{position:"relative"}},r.createElement(a.a,{href:"#hnsw-hierarchical-navigable-small-world","aria-label":"hnsw hierarchical navigable small world permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"HNSW (hierarchical navigable small world)"),"\n",r.createElement(a.p,null,"HNSW is a graph-based ANN approach that has become popular due to its balance of speed and memory efficiency (Malkov & Yashunin, IEEE Trans. Big Data 2020). It constructs a hierarchical network in which each layer is a navigable small world graph — essentially a proximity graph that captures local neighborhoods in the vector space. Search starts at the top layer (which is small and navigable), gradually descending into lower layers that have more connections, eventually restricting the search to a local neighborhood around the query. By following likely candidates, HNSW quickly converges on nearest neighbors with low computational overhead."),"\n",r.createElement(a.p,null,"HNSW can achieve high recall with relatively compact indexes compared to older methods, though it still requires substantial memory for storing multiple layers and edges. Its popularity in state-of-the-art vector databases stems from its empirical speed and scalability on real-world tasks."),"\n",r.createElement(a.h3,{id:"ivf-inverted-file-index",style:{position:"relative"}},r.createElement(a.a,{href:"#ivf-inverted-file-index","aria-label":"ivf inverted file index permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"IVF (inverted file index)"),"\n",r.createElement(a.p,null,'Inverted file indexes partition the vector space into multiple "cells" or clusters. Each cell is represented by a centroid (e.g., learned via k-means). During query time, one first locates the closest centroids to the query and searches only within vectors assigned to those centroid cells. This approach is often combined with product quantization or other compression schemes to reduce memory usage and accelerate lookups.'),"\n",r.createElement(a.p,null,"IVF is well-suited for large-scale embeddings: it balances memory usage and speed by pruning vast portions of the dataset that are far from the query, thus avoiding exhaustive checks. The number of clusters (and the granularity of partitioning) can be tuned to control trade-offs between recall and retrieval speed."),"\n",r.createElement(a.h2,{id:"4-optimization-techniques-for-ann-search",style:{position:"relative"}},r.createElement(a.a,{href:"#4-optimization-techniques-for-ann-search","aria-label":"4 optimization techniques for ann search permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4. Optimization techniques for ANN search"),"\n",r.createElement(a.h3,{id:"approximation-and-accuracy-trade-offs",style:{position:"relative"}},r.createElement(a.a,{href:"#approximation-and-accuracy-trade-offs","aria-label":"approximation and accuracy trade offs permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Approximation and accuracy trade-offs"),"\n",r.createElement(a.p,null,"The main draw of ANN indexing stems from the trade-off between accuracy and query speed. By accepting that sometimes the retrieved neighbors are not perfectly exact, one reaps enormous time savings over brute-force methods. Each algorithm (LSH, HNSW, IVF, etc.) has hyperparameters that tune this trade-off: for instance, the number of probes in IVF or the maximum search breadth in HNSW. In real-world deployments, a small drop in recall (like retrieving 95% correct neighbors instead of 100%) can be acceptable if it cuts query times from seconds down to milliseconds."),"\n",r.createElement(a.h3,{id:"techniques-for-fast-query-processing",style:{position:"relative"}},r.createElement(a.a,{href:"#techniques-for-fast-query-processing","aria-label":"techniques for fast query processing permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Techniques for fast query processing"),"\n",r.createElement(a.p,null,"To reduce query latency further, many systems adopt a multi-pronged approach:\n• Early termination: Some graph-based or tree-based algorithms can stop once they are confident that better candidates cannot be found deeper in the structure.",r.createElement(a.br),"\n","• Pruning: By exploiting distances to cluster centroids, or by bounding distances in tree-based structures, an index might prune entire sub-trees or sub-graphs that cannot contain nearer neighbors.",r.createElement(a.br),"\n","• Cache-aware data layouts: Storing the data in memory-friendly layouts ensures that retrieval from DRAM or CPU caches is minimized, leading to faster memory access patterns.",r.createElement(a.br),"\n","• Parallel search: Many vector databases leverage multi-threading or GPU acceleration to handle sub-queries or expansions in parallel, drastically speeding up the per-query time."),"\n",r.createElement(a.h3,{id:"memory-usage-and-compression",style:{position:"relative"}},r.createElement(a.a,{href:"#memory-usage-and-compression","aria-label":"memory usage and compression permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Memory usage and compression"),"\n",r.createElement(a.p,null,"High-dimensional embeddings are typically 32-bit floating-point arrays, which can become extremely large if we store billions of vectors. Hence, compression strategies are paramount. Product quantization, binary hashing, or lower bit-depth representations (like 8-bit or 16-bit float) can reduce memory footprints. However, each compression technique might come at a cost in terms of precision."),"\n",r.createElement(a.p,null,"Libraries like FAISS provide many composite indexing strategies: You can combine an inverted index (IVF) with product quantization (PQ) to drastically reduce memory usage, or add a re-ranking step with a more accurate distance measure once a shortlist of candidates is found. Fine-tuning these components is often a matter of extensive experimentation, balancing speed, memory, and accuracy to align with specific application constraints."),"\n",r.createElement(a.h3,{id:"hybrid-approaches-and-hyperparameter-tuning",style:{position:"relative"}},r.createElement(a.a,{href:"#hybrid-approaches-and-hyperparameter-tuning","aria-label":"hybrid approaches and hyperparameter tuning permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Hybrid approaches and hyperparameter tuning"),"\n",r.createElement(a.p,null,"It is not uncommon to combine multiple indexing or search techniques. A pipeline might first use LSH to drastically narrow down the candidate set, then apply a more accurate but expensive approach — like a refined HNSW search or per-candidate re-ranking — on the reduced set. Each part of the pipeline has hyperparameters that can be optimized for either speed or accuracy."),"\n",r.createElement(a.p,null,"Key hyperparameters include:\n• Number of hash tables in LSH.",r.createElement(a.br),"\n","• Number of centroids in IVF or K-means.",r.createElement(a.br),"\n","• Number of layers or neighbors in HNSW.",r.createElement(a.br),"\n","• Depth of the search or the beam width in graph traversal."),"\n",r.createElement(a.p,null,"Empirically tuning these hyperparameters typically calls for a validation set on which one can measure recall and latency. Some advanced systems automate this process, using Bayesian optimization to find Pareto-efficient solutions that strike the best balance for a specific dataset and hardware environment."),"\n",r.createElement(a.h2,{id:"5-vector-databases-architectures-and-implementations",style:{position:"relative"}},r.createElement(a.a,{href:"#5-vector-databases-architectures-and-implementations","aria-label":"5 vector databases architectures and implementations permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5. Vector databases: architectures and implementations"),"\n",r.createElement(a.p,null,"Vector databases specialize in storing and querying vector data. While the indexing structures are crucial, equally important are the system-level functionalities (scalability, consistency, fault tolerance, advanced filtering, real-time insertion) that set them apart from a mere library or local indexing tool."),"\n",r.createElement(a.p,null,"In recent years, multiple open-source and commercial solutions have arisen, each with unique trade-offs. Several noteworthy examples are:"),"\n",r.createElement(a.h3,{id:"pinecone",style:{position:"relative"}},r.createElement(a.a,{href:"#pinecone","aria-label":"pinecone permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Pinecone"),"\n",r.createElement(a.p,null,"Pinecone is a managed vector database designed to provide real-time similarity search at scale. It abstracts away all infrastructure concerns, so developers can integrate with high-level APIs. Pinecone's architecture is heavily optimized for low-latency lookups, distributing data across multiple nodes with specialized ANN indexes. It automatically scales based on demand, ensuring that even under high query loads, latency remains manageable. Pinecone is especially attractive for production scenarios where teams prefer not to manage their own distributed systems."),"\n",r.createElement(a.h3,{id:"weaviate",style:{position:"relative"}},r.createElement(a.a,{href:"#weaviate","aria-label":"weaviate permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Weaviate"),"\n",r.createElement(a.p,null,"Weaviate is open-source and features a schema-based approach to storing vectors, letting you combine structured data with unstructured embeddings. It implements HNSW for efficient vector searches, supports hosting data on multiple backends, and offers modules for specific data types. Weaviate can also integrate external data sources and knowledge graph structures, blending classical database functionality with modern vector-based retrieval. It's frequently used in semantic search and question-answering applications."),"\n",r.createElement(a.h3,{id:"faiss",style:{position:"relative"}},r.createElement(a.a,{href:"#faiss","aria-label":"faiss permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"FAISS"),"\n",r.createElement(a.p,null,"Originally developed by Facebook AI (now Meta AI), FAISS is a library (rather than a full-fledged database) that provides state-of-the-art indexing and search functionalities. While smaller in scope (it doesn't inherently define a system for distribution or replication), it offers a robust foundation for large-scale vector search tasks. FAISS supports IVF, PQ, and HNSW, often combining them in nested indexes. Developers can build custom solutions around FAISS by layering a database, distribution engine, or caching logic. Because of its speed and variety of indexing methods, FAISS is a go-to choice for many research and production environments where tight control over indexing parameters is needed."),"\n",r.createElement(a.h3,{id:"chroma",style:{position:"relative"}},r.createElement(a.a,{href:"#chroma","aria-label":"chroma permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Chroma"),"\n",r.createElement(a.p,null,'Chroma is a newer open-source vector database that places emphasis on being "AI-native": it integrates closely with embedding-generation frameworks and aims to simplify end-to-end workflows, from creating embeddings to searching them. It focuses on handling large-scale embeddings and semantic vectors in real-time or near-real-time scenarios. Under the hood, Chroma uses specialized data structures and indexing algorithms to enable rapid similarity search, and it strives to be user-friendly, reducing the overhead associated with building indexing pipelines from scratch.'),"\n",r.createElement(a.h3,{id:"lancedb",style:{position:"relative"}},r.createElement(a.a,{href:"#lancedb","aria-label":"lancedb permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"LanceDB"),"\n",r.createElement(a.p,null,"LanceDB is built to handle large-scale vector embeddings while having easy integration into ML workflows. It leverages specialized indexing data structures and focuses on speed, allowing it to manage potentially billions of vectors for real-time or near-real-time queries. LanceDB can be a compelling option for recommendation systems or semantic search applications, with an emphasis on seamless synergy with model production pipelines."),"\n",r.createElement(a.h3,{id:"qdrant",style:{position:"relative"}},r.createElement(a.a,{href:"#qdrant","aria-label":"qdrant permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Qdrant"),"\n",r.createElement(a.p,null,"Qdrant is another open-source solution geared toward quick, accurate vector similarity search. It focuses on real-time updates, making it well-suited to applications where new embeddings arrive continuously (e.g., streaming data, user interactions). It has a built-in filtering layer, letting users combine structured queries (filters on metadata) with vector similarity searches. Qdrant's architecture is distributed and scalable, ensuring that even expansions to multi-billion vector datasets remain responsive and manageable."),"\n",r.createElement(a.h3,{id:"supabase-vector",style:{position:"relative"}},r.createElement(a.a,{href:"#supabase-vector","aria-label":"supabase vector permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Supabase vector"),"\n",r.createElement(a.p,null,"Supabase has evolved from a simple open-source alternative to Firebase into a platform offering vector capabilities. With the pgvector extension, PostgreSQL can store embeddings as vectors and perform similarity queries. This approach appeals to those who want to retain the relational power of PostgreSQL, while also dealing with vector-based data in a single, unified system. While it might not be as specialized or optimized as systems built purely for vector search, it's a pragmatic choice when you need a strong relational foundation coupled with moderate ANN search for lower-scale or specialized tasks."),"\n",r.createElement(a.h3,{id:"mongodb-atlas-vector-search",style:{position:"relative"}},r.createElement(a.a,{href:"#mongodb-atlas-vector-search","aria-label":"mongodb atlas vector search permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"MongoDB Atlas vector search"),"\n",r.createElement(a.p,null,"MongoDB has added vector search capabilities to its Atlas service, letting you embed unstructured data within the flexible document model that MongoDB is known for. By supporting indexing and searching of vectors in Atlas, MongoDB extends existing capabilities for hybrid (structured + unstructured) queries. Atlas automatically manages distribution and scaling on behalf of users, combining the convenience of document-based storage with specialized indexes for embeddings. This can be especially appealing to teams that already use MongoDB for other operational data."),"\n",r.createElement(a.h3,{id:"distributed-architectures-and-horizontal-scalability",style:{position:"relative"}},r.createElement(a.a,{href:"#distributed-architectures-and-horizontal-scalability","aria-label":"distributed architectures and horizontal scalability permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Distributed architectures and horizontal scalability"),"\n",r.createElement(a.p,null,"Frequently, vector databases adopt distributed architectures to handle extremely large datasets. They may split the index into shards, each stored on a different node, or replicate indexes across nodes to handle high query throughput. Coordination can be managed by a master or orchestrator node, or be fully peer-to-peer. The distributed aspect is crucial for real-world deployment, as even the best single-machine solution can be overwhelmed by traffic or memory constraints once vectors climb into the billions. In these designs, queries are typically routed to the appropriate shards, partial results are gathered, and a final ranking merges everything into a single result set."),"\n",r.createElement(a.h3,{id:"integration-with-machine-learning-pipelines",style:{position:"relative"}},r.createElement(a.a,{href:"#integration-with-machine-learning-pipelines","aria-label":"integration with machine learning pipelines permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Integration with machine learning pipelines"),"\n",r.createElement(a.p,null,"A key hallmark of a mature vector database solution is how well it integrates with the rest of the ML pipeline. This includes:\n• Automated ingestion of new embeddings when a model processes fresh data.",r.createElement(a.br),"\n","• Real-time or batch-based re-indexing if embeddings change or new data arrives.",r.createElement(a.br),"\n","• Tools for exploring the embedding space, visualizing nearest neighbors, or diagnosing indexing quality.",r.createElement(a.br),"\n","• Connectors for popular frameworks (TensorFlow, PyTorch, scikit-learn) that enable seamless retrieval and analysis."),"\n",r.createElement(a.p,null,"Some vector databases even offer specialized features, such as on-the-fly model inference or integration with custom re-ranking logic. The overall objective is to make it easy for data scientists and engineers to incorporate the power of ANN into their applications without reinventing the wheel each time."),"\n",r.createElement(a.h2,{id:"6-applications-of-vector-databases-and-ann",style:{position:"relative"}},r.createElement(a.a,{href:"#6-applications-of-vector-databases-and-ann","aria-label":"6 applications of vector databases and ann permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6. Applications of vector databases and ANN"),"\n",r.createElement(a.h3,{id:"semantic-search-and-question-answering-in-nlp",style:{position:"relative"}},r.createElement(a.a,{href:"#semantic-search-and-question-answering-in-nlp","aria-label":"semantic search and question answering in nlp permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Semantic search and question answering in NLP"),"\n",r.createElement(a.p,null,"One of the earliest mainstream successes for vector-based retrieval was in semantic search (e.g., for large-scale question answering or document retrieval). By embedding text passages into vectors, we can retrieve passages semantically relevant to the query, even if they do not share many (or any) exact keywords. This property allows for more intelligent and context-aware results. Systems like dense passage retrieval (Karpukhin and gang, EMNLP 2020) rely heavily on ANN queries over text embeddings to quickly surface relevant documents."),"\n",r.createElement(a.p,null,'Large language models (LLMs) can also be integrated with vector databases in a pipeline called "retrieval-augmented generation," enabling the LLM to ground its responses in relevant external knowledge. In such workflows, the LLM queries the vector database with an embedding of the user\'s question, retrieves the top matches, and conditions its answer on these results.'),"\n",r.createElement(a.h3,{id:"image-retrieval-and-computer-vision",style:{position:"relative"}},r.createElement(a.a,{href:"#image-retrieval-and-computer-vision","aria-label":"image retrieval and computer vision permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Image retrieval and computer vision"),"\n",r.createElement(a.p,null,'In image retrieval scenarios, each catalog image is converted into a feature vector by a deep CNN or a specialized encoding model. Finding images that are visually similar to a given query image translates to a nearest neighbor lookup in the embedding space (Krizhevsky and gang, 2012). This is crucial for e-commerce applications where customers want to "search by image" or for content moderation tasks where near-duplicates must be identified quickly. Vector databases can handle these specialized embeddings at scale, ensuring that results are both fast to retrieve and robust in capturing visual similarity.'),"\n",r.createElement(a.h3,{id:"recommendation-systems",style:{position:"relative"}},r.createElement(a.a,{href:"#recommendation-systems","aria-label":"recommendation systems permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Recommendation systems"),"\n",r.createElement(a.p,null,"Recommendation engines often represent users and items in the same embedding space, then compute their proximity as a measure of how likely a user is to enjoy a particular item. For instance, in a collaborative filtering setup, user embeddings might be learned by matrix factorization or neural networks. By employing vector databases to store this entire user-item space, retrieving top candidate items for a user's nearest neighbors becomes straightforward. This approach is ubiquitous in social media, streaming services, and e-commerce. Low-latency ANN queries enable real-time personalized suggestions which can drastically enhance user engagement (Zhang and gang, WWW 2019)."),"\n",r.createElement(a.h3,{id:"biological-and-medical-data",style:{position:"relative"}},r.createElement(a.a,{href:"#biological-and-medical-data","aria-label":"biological and medical data permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Biological and medical data"),"\n",r.createElement(a.p,null,"In genomics or proteomics, large-scale databases store billions of biometric sequences or protein structures in high-dimensional embeddings, often derived from specialized deep learning models. Finding the nearest neighbors to a novel sequence or identifying similarly structured proteins is central to tasks like drug discovery or diagnosing genetic conditions. Vector databases facilitate these queries at unprecedented scales, offering speed while managing the complexity of biological data."),"\n",r.createElement(a.h3,{id:"other-domains",style:{position:"relative"}},r.createElement(a.a,{href:"#other-domains","aria-label":"other domains permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Other domains"),"\n",r.createElement(a.p,null,"Additional domains benefiting from vector-based retrieval include anomaly detection in cybersecurity (where embedding network traffic logs can help surface suspicious events), audio processing (finding similar sound clips from large datasets), or even multi-modal projects that embed text, images, and audio into a joint latent space to enable cross-modal interactions. As embeddings become a universal representation tool, vector databases become equally universal solutions for storing and querying them at scale."),"\n",r.createElement(a.h2,{id:"7-scalability-and-real-time-performance",style:{position:"relative"}},r.createElement(a.a,{href:"#7-scalability-and-real-time-performance","aria-label":"7 scalability and real time performance permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7. Scalability and real-time performance"),"\n",r.createElement(a.h3,{id:"handling-big-data-volumes",style:{position:"relative"}},r.createElement(a.a,{href:"#handling-big-data-volumes","aria-label":"handling big data volumes permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Handling big data volumes"),"\n",r.createElement(a.p,null,"Scaling to billions of vectors typically demands distributed architectures, parallel query processing, and advanced indexing. One strategy is to shard data horizontally, assigning each shard to handle a subset of the vectors. Such a system must keep track of which shard is responsible for which partition of the embedding space. Queries then fan out to relevant shards, combine partial results, and produce a final ranking. This pattern is conceptually simple but can become tricky if the cluster rebalances or if embeddings change frequently. Another approach is using a more dynamic partitioning strategy, expanding or contracting the number of partitions as data arrives."),"\n",r.createElement(a.h3,{id:"real-time-search-constraints",style:{position:"relative"}},r.createElement(a.a,{href:"#real-time-search-constraints","aria-label":"real time search constraints permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Real-time search constraints"),"\n",r.createElement(a.p,null,"In many consumer-facing applications, queries must return results within tens of milliseconds, making query latency a prime concern. Vector databases might employ asynchronous indexing for newly arrived data, ensuring that previous data partitions remain stable and queries can be routed with minimal overhead. Sub-millisecond latencies are sometimes achievable with specialized hardware, such as GPUs or custom accelerators, provided the indexes and data structures are carefully tuned."),"\n",r.createElement(a.h3,{id:"load-balancing-and-fault-tolerance",style:{position:"relative"}},r.createElement(a.a,{href:"#load-balancing-and-fault-tolerance","aria-label":"load balancing and fault tolerance permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Load balancing and fault tolerance"),"\n",r.createElement(a.p,null,"When dealing with large-scale or mission-critical deployments, load balancing ensures that no single node is overwhelmed by queries. Techniques like consistent hashing or dynamic load distribution can direct queries to underutilized nodes. This approach helps maintain predictable query times even under heavy load. Meanwhile, fault tolerance often entails replicating data or indexes across multiple nodes. Should one node fail, the system can reroute queries to a replica, preventing service interruptions or data unavailability."),"\n",r.createElement(a.h3,{id:"hardware-accelerations",style:{position:"relative"}},r.createElement(a.a,{href:"#hardware-accelerations","aria-label":"hardware accelerations permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Hardware accelerations"),"\n",r.createElement(a.p,null,"To meet real-time requirements, some organizations leverage GPUs to offload vector operations. Libraries optimized for GPU-based dot products or distance calculations can drastically reduce query times, especially for cosine similarity or matrix multiplications. Further research in hardware acceleration includes FPGAs (Field-Programmable Gate Arrays) or specialized vector processors that can handle large volumes of parallel operations. These solutions can be expensive and complex to maintain, but for large enterprises, the performance benefit can prove decisive."),"\n",r.createElement(a.h2,{id:"8-challenges-in-vector-database-management",style:{position:"relative"}},r.createElement(a.a,{href:"#8-challenges-in-vector-database-management","aria-label":"8 challenges in vector database management permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8. Challenges in vector database management"),"\n",r.createElement(a.h3,{id:"data-drift-and-model-drift",style:{position:"relative"}},r.createElement(a.a,{href:"#data-drift-and-model-drift","aria-label":"data drift and model drift permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Data drift and model drift"),"\n",r.createElement(a.p,null,"Even after a vector database is in production, models continually evolve. Embeddings generated by older versions of a model might become incompatible with new embeddings, leading to potential mismatches or a decline in retrieval performance. Similarly, real-world data can drift over time, changing the underlying distribution of embeddings. Maintaining consistency and retraining indexes can be costly in terms of both computational resources and system downtime."),"\n",r.createElement(a.p,null,"In large-scale systems, a common approach is to maintain multiple indexes side by side: one for the old embeddings and one for the new. Queries might be routed differently based on user segments or content versions. Over time, data is migrated or re-embedded so that the old index can be phased out. This approach demands careful orchestration, as partial migrations can lead to user experiences that mix old and new results."),"\n",r.createElement(a.h3,{id:"keeping-indexes-fresh",style:{position:"relative"}},r.createElement(a.a,{href:"#keeping-indexes-fresh","aria-label":"keeping indexes fresh permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Keeping indexes fresh"),"\n",r.createElement(a.p,null,"Many applications demand near real-time updates as new items, users, or data points arrive (e.g., social networks, recommendation systems). Indexing an incoming vector typically involves a more expensive operation than simply appending a row to a relational table. Some indexing data structures allow incremental updates more smoothly than others. For example, HNSW can insert new vectors without reconstructing the entire graph, though performance might degrade slightly over time. Systems that rely heavily on clustering or partitioning might need to revise cluster centroids or reorder data. Achieving a balance between fast ingest and high-quality indexing is still an active area of research."),"\n",r.createElement(a.h3,{id:"security-privacy-and-compliance",style:{position:"relative"}},r.createElement(a.a,{href:"#security-privacy-and-compliance","aria-label":"security privacy and compliance permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Security, privacy, and compliance"),"\n",r.createElement(a.p,null,"Storing embeddings of user data raises new questions around privacy. While embeddings are typically not as easily interpretable as raw data, they can still potentially leak sensitive information with sophisticated inference attacks (Fredrikson and gang, CCS 2015). Ensuring compliance with regulations, such as GDPR, may require the ability to remove or anonymize user data from the index upon request. Techniques like homomorphic encryption or secure enclaves might be employed in sensitive domains, though these can substantially affect ANN performance."),"\n",r.createElement(a.h3,{id:"performance-debugging",style:{position:"relative"}},r.createElement(a.a,{href:"#performance-debugging","aria-label":"performance debugging permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Performance debugging"),"\n",r.createElement(a.p,null,"Because many ANN approaches rely on approximate methods, diagnosing issues can be more complex than in a conventional database. A sudden drop in search accuracy might stem from an overzealous hyperparameter that prunes too aggressively, a data distribution shift, or resource contention in a distributed environment. Monitoring real-time metrics like recall@k, average query latency, or memory usage while correlating them with system-level metrics is essential for robust production deployments."),"\n",r.createElement(a.h2,{id:"9-future-directions-and-emerging-trends",style:{position:"relative"}},r.createElement(a.a,{href:"#9-future-directions-and-emerging-trends","aria-label":"9 future directions and emerging trends permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9. Future directions and emerging trends"),"\n",r.createElement(a.h3,{id:"deep-learning-integration",style:{position:"relative"}},r.createElement(a.a,{href:"#deep-learning-integration","aria-label":"deep learning integration permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Deep learning integration"),"\n",r.createElement(a.p,null,"One emerging direction is to create synergy between deep learning models and ANN indexes. For instance, certain neural architectures can produce embeddings specifically tailored to the indexing structures in use, optimizing the search performance. Conversely, there are models that integrate the indexing logic directly into the neural network's forward pass, effectively learning to route queries to their neighbors with minimal overhead (Jegou and gang, NeurIPS 2019). Such cross-pollination raises the possibility of end-to-end trainable systems, bridging the classical divide between embedding generation and retrieval."),"\n",r.createElement(a.h3,{id:"quantum-computing",style:{position:"relative"}},r.createElement(a.a,{href:"#quantum-computing","aria-label":"quantum computing permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Quantum computing"),"\n",r.createElement(a.p,null,"Though still in early developmental stages, quantum computing might offer new paradigms for high-dimensional similarity search. Research into quantum nearest neighbor algorithms suggests the potential for exponential speedups under certain conditions (Lloyd and gang, Science 2013). Practical quantum hardware remains limited in capacity and reliability, so it's largely a realm of theoretical exploration. Nonetheless, in the long term, these approaches could reshape the complexity boundaries of large-scale ANN."),"\n",r.createElement(a.h3,{id:"hybrid-knowledge-graphs",style:{position:"relative"}},r.createElement(a.a,{href:"#hybrid-knowledge-graphs","aria-label":"hybrid knowledge graphs permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Hybrid knowledge graphs"),"\n",r.createElement(a.p,null,"In advanced use cases, vector databases are integrated with knowledge graphs to produce hybrid retrieval engines. A knowledge graph captures structured relationships among entities, while the vector database organizes high-dimensional embeddings that capture semantic or unstructured relationships. An application might first retrieve likely candidates through vector similarity, then refine results through graph-based queries or constraints. This hybrid approach can yield more interpretable and context-aware retrieval systems, bridging the gap between raw unstructured embeddings and explicit semantic structures."),"\n",r.createElement(a.h3,{id:"next-generation-ann-algorithms",style:{position:"relative"}},r.createElement(a.a,{href:"#next-generation-ann-algorithms","aria-label":"next generation ann algorithms permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Next-generation ANN algorithms"),"\n",r.createElement(a.p,null,"While HNSW, LSH, IVF, and PQ have proven effective, research in graph-based methods and learned indexes is rapidly expanding. Learned indexes use machine learning models to predict the position or cluster assignment of vectors, sometimes achieving higher speed with lower memory overhead. Meanwhile, advanced similarity-preserving hashing and hierarchical structures continue to push the envelope, aiming for near-exact recall at sub-millisecond latencies, even on billions of vectors. Additionally, specialized GPU-based solutions that unify indexing, classification, and transformation may soon become standard for real-time AI inference platforms."),"\n",r.createElement(a.h2,{id:"10-conclusion",style:{position:"relative"}},r.createElement(a.a,{href:"#10-conclusion","aria-label":"10 conclusion permalink",className:"anchor before"},r.createElement(a.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"10. Conclusion"),"\n",r.createElement(a.p,null,"Vector databases occupy a pivotal position in the modern AI landscape, serving as the linchpin that enables large-scale, real-time nearest neighbor queries across extraordinarily high-dimensional embedding spaces. As models advance in their ability to produce semantically meaningful vectors — be they textual embeddings from transformer-based architectures, visual features from deep convolutional nets, or multi-modal embeddings combining various data types — the demands on backend systems to retrieve nearest neighbors efficiently escalate. Approximate nearest neighbor algorithms, from classical LSH to refined graph approaches like HNSW, underpin these solutions by granting orders-of-magnitude speedups over naive exhaustive search."),"\n",r.createElement(a.p,null,"The ecosystem of vector databases continues to grow rapidly, from open-source projects like Weaviate, Qdrant, Chroma, and FAISS to managed services such as Pinecone or MongoDB Atlas. Each solution strives to solve the trifecta of speed, scalability, and manageability, and they often incorporate advanced indexing methods that can be tuned for a wide range of data and query patterns. This rich and dynamic marketplace is indicative of the essential role that vector databases have assumed in tasks as varied as search, recommendation, anomaly detection, and biomedical research."),"\n",r.createElement(a.p,null,"Yet, challenges remain. Maintaining consistent embeddings in the face of model and data drift, dealing with massive streaming data, ensuring security, and gracefully handling approximate matches are all active areas of exploration. Looking ahead, continued integration of deep learning, new hardware accelerations, and the potential disruptions of quantum approaches promise even more powerful solutions."),"\n",r.createElement(a.p,null,"Ultimately, vector databases and approximate nearest neighbor search are here to stay — a testament to the ongoing march toward ever more robust, efficient, and intelligent systems that distill complex data into meaningful representations. By bridging the gap between unstructured, high-dimensional data and practical retrieval needs, these technologies sit at the heart of modern AI applications and promise to shape the future of how we store and interpret data."),"\n",r.createElement(t,{alt:"Conceptual diagram showing a high-dimensional vector space and ANN indexing",path:"",caption:"Illustration of vectors in a high-dimensional space, demonstrating how approximate nearest neighbors can be found using specialized indexing structures.",zoom:"false"}),"\n",r.createElement(t,{alt:"Diagram comparing different ANN data structures: LSH, IVFPQ, HNSW",path:"",caption:"Comparative representation of indexing approaches, including hashing-based (LSH), centroid-based (IVF, PQ), and graph-based (HNSW) structures.",zoom:"false"}),"\n",r.createElement(t,{alt:"Architecture of a distributed vector database cluster",path:"",caption:"High-level design of a distributed vector database with shards and replicas, coordinating queries across multiple nodes.",zoom:"false"}),"\n",r.createElement(a.p,null,"Here is a brief example in Python illustrating how one might use FAISS for a simple vector similarity search:"),"\n",r.createElement(o.A,{text:"\nimport numpy as np\nimport faiss\n\n# Suppose we have a dataset of 10,000 vectors (dimension 128)\nd = 128\nn_data = 10000\nnp.random.seed(0)\n\ndata_vectors = np.random.random((n_data, d)).astype('float32')\nquery_vector = np.random.random((d, )).astype('float32').reshape(1, -1)\n\n# Build an index. For demonstration we use a simple IndexFlatL2 (brute force)\nindex = faiss.IndexFlatL2(d)\nindex.add(data_vectors)\n\n# Perform a search for the 5 nearest neighbors\nk = 5\ndistances, indices = index.search(query_vector, k)\n\nprint(\"Nearest neighbors:\", indices)\nprint(\"Distances:\", distances)\n"}),"\n",r.createElement(a.p,null,"In this toy example, we constructed a brute-force index using ",r.createElement(s.A,null,"IndexFlatL2"),", which performs exact nearest neighbor search by Euclidean distance. For larger datasets, you would replace this with more sophisticated indexes — for instance, ",r.createElement(s.A,null,"IndexIVFPQ")," combined with various compression or clustering approaches. Regardless, the fundamental principle remains the same: mapping data into a high-dimensional vector space, indexing it, and retrieving candidates near a query vector."),"\n",r.createElement(a.p,null,"In production workflows, you'd incorporate a full-fledged vector database on top of these indexing primitives to handle scaling, distributed query scheduling, data ingestion pipelines, updates, and fault tolerance. By diving into the many design choices for vector indexing, from LSH to advanced graph-based structures like HNSW, and combining them with specialized hardware, one can build solutions that remain efficient and accurate at practically any dataset size or throughput requirement."))}var d=function(e){void 0===e&&(e={});const{wrapper:a}=Object.assign({},(0,i.RP)(),e.components);return a?r.createElement(a,e,r.createElement(c,e)):c(e)};var h=t(36710),m=t(58481),u=t.n(m),p=t(36310),g=t(87245),f=t(27042),v=t(59849),b=t(5591),y=t(61122),w=t(9219),S=t(33203),E=t(95751),x=t(94328),H=t(80791),k=t(78137);const z=e=>{let{toc:a}=e;if(!a||!a.items)return null;return r.createElement("nav",{className:H.R},r.createElement("ul",null,a.items.map(((e,a)=>r.createElement("li",{key:a},r.createElement("a",{href:e.url,onClick:a=>((e,a)=>{e.preventDefault();const t=a.replace("#",""),n=document.getElementById(t);n&&n.scrollIntoView({behavior:"smooth",block:"start"})})(a,e.url)},e.title),e.items&&r.createElement(z,{toc:{items:e.items}}))))))};function q(e){let{data:{mdx:a,allMdx:s,allPostImages:o},children:l}=e;const{frontmatter:c,body:d,tableOfContents:h}=a,m=c.index,v=c.slug.split("/")[1],H=s.nodes.filter((e=>e.frontmatter.slug.includes(`/${v}/`))).sort(((e,a)=>e.frontmatter.index-a.frontmatter.index)),q=H.findIndex((e=>e.frontmatter.index===m)),M=H[q+1],I=H[q-1],N=c.slug.replace(/\/$/,""),C=/[^/]*$/.exec(N)[0],V=`posts/${v}/content/${C}/`,{0:T,1:A}=(0,r.useState)(c.flagWideLayoutByDefault),{0:_,1:L}=(0,r.useState)(!1);var B;(0,r.useEffect)((()=>{L(!0);const e=setTimeout((()=>L(!1)),340);return()=>clearTimeout(e)}),[T]),"adventures"===v?B=w.cb:"research"===v?B=w.Qh:"thoughts"===v&&(B=w.T6);const P=u()(d).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,F=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const a=Math.floor(e/60),t=e%60;return t<=30?`~${a}${t>0?".5":""} h`:`~${a+1} h`}(Math.ceil(P/B)+(c.extraReadTimeMin||0)),W=[{flag:c.flagDraft,component:()=>Promise.all([t.e(3231),t.e(8809)]).then(t.bind(t,28809))},{flag:c.flagMindfuckery,component:()=>Promise.all([t.e(3231),t.e(2471)]).then(t.bind(t,67709))},{flag:c.flagRewrite,component:()=>Promise.all([t.e(3231),t.e(6764)]).then(t.bind(t,62002))},{flag:c.flagOffensive,component:()=>Promise.all([t.e(3231),t.e(2443)]).then(t.bind(t,17681))},{flag:c.flagProfane,component:()=>Promise.all([t.e(3231),t.e(8048)]).then(t.bind(t,53286))},{flag:c.flagMultilingual,component:()=>Promise.all([t.e(3231),t.e(4069)]).then(t.bind(t,78831))},{flag:c.flagUnreliably,component:()=>Promise.all([t.e(3231),t.e(3417)]).then(t.bind(t,8179))},{flag:c.flagPolitical,component:()=>Promise.all([t.e(3231),t.e(5195)]).then(t.bind(t,30433))},{flag:c.flagCognitohazard,component:()=>Promise.all([t.e(3231),t.e(3175)]).then(t.bind(t,8413))},{flag:c.flagHidden,component:()=>Promise.all([t.e(3231),t.e(9556)]).then(t.bind(t,14794))}],{0:D,1:O}=(0,r.useState)([]);return(0,r.useEffect)((()=>{W.forEach((e=>{let{flag:a,component:t}=e;a&&t().then((e=>{O((a=>[].concat((0,n.A)(a),[e.default])))}))}))}),[]),r.createElement(f.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},r.createElement(b.A,{postNumber:c.index,date:c.date,updated:c.updated,readTime:F,difficulty:c.difficultyLevel,title:c.title,desc:c.desc,banner:c.banner,section:v,postKey:C,isMindfuckery:c.flagMindfuckery,mainTag:c.mainTag}),r.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},c.otherTags.map(((e,a)=>r.createElement("span",{key:a,className:`noselect ${k.MW}`,style:{margin:"0 5px 5px 0"}},e)))),r.createElement("div",{className:"postBody"},r.createElement(z,{toc:h})),r.createElement("br"),r.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},r.createElement(f.P.button,{className:`noselect ${x.pb}`,id:x.xG,onClick:()=>{A(!T)},whileTap:{scale:.93}},r.createElement(f.P.div,{className:E.DJ,key:T,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},T?"Switch to default layout":"Switch to wide layout"))),r.createElement("br"),r.createElement("div",{className:"postBody",style:{margin:T?"0 -14%":"",maxWidth:T?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},r.createElement("div",{className:`${x.P_} ${_?x.Xn:x.qG}`},D.map(((e,a)=>r.createElement(e,{key:a}))),c.indexCourse?r.createElement(S.A,{index:c.indexCourse,category:c.courseCategoryName}):"",r.createElement(p.Z.Provider,{value:{images:o.nodes,basePath:V.replace(/\/$/,"")+"/"}},r.createElement(i.xA,{components:{Image:g.A}},l)))),r.createElement(y.A,{nextPost:M,lastPost:I,keyCurrent:C,section:v}))}function M(e){return r.createElement(q,e,r.createElement(d,e))}function I(e){var a,t,n,i,s;let{data:o}=e;const{frontmatter:l}=o.mdx,c=l.titleSEO||l.title,d=l.titleOG||c,m=l.titleTwitter||c,u=l.descSEO||l.desc,p=l.descOG||u,g=l.descTwitter||u,f=l.schemaType||"BlogPosting",b=l.keywordsSEO,y=l.date,w=l.updated||y,S=l.imageOG||(null===(a=l.banner)||void 0===a||null===(t=a.childImageSharp)||void 0===t||null===(n=t.gatsbyImageData)||void 0===n||null===(i=n.images)||void 0===i||null===(s=i.fallback)||void 0===s?void 0:s.src),E=l.imageAltOG||p,x=l.imageTwitter||S,H=l.imageAltTwitter||g,k=l.canonicalURL,z=l.flagHidden||!1,q=l.mainTag||"Posts",M=l.slug.split("/")[1]||"posts",{siteUrl:I}=(0,h.Q)(),N={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:I},{"@type":"ListItem",position:2,name:q,item:`${I}/${l.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${I}${l.slug}`}]};return r.createElement(v.A,{title:c+" - avrtt.blog",titleOG:d,titleTwitter:m,description:u,descriptionOG:p,descriptionTwitter:g,schemaType:f,keywords:b,datePublished:y,dateModified:w,imageOG:S,imageAltOG:E,imageTwitter:x,imageAltTwitter:H,canonicalUrl:k,flagHidden:z,mainTag:q,section:M,type:"article"},r.createElement("script",{type:"application/ld+json"},JSON.stringify(N)))}},96098:function(e,a,t){var n=t(96540),i=t(7978);a.A=e=>{let{text:a}=e;return n.createElement(i.A,null,a)}}}]);
//# sourceMappingURL=component---src-templates-post-js-content-file-path-src-pages-posts-research-vector-databases-and-ann-mdx-4f50e2f8f8fe5d7df871.js.map