"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[4295],{74347:function(e,t,a){a.r(t),a.d(t,{Head:function(){return M},PostTemplate:function(){return k},default:function(){return C}});var n=a(28453),i=a(96540),l=a(61992),r=(a(62087),a(90548));function s(e){const t=Object.assign({p:"p",ol:"ol",li:"li",h2:"h2",a:"a",span:"span",h3:"h3",ul:"ul",h4:"h4",strong:"strong"},(0,n.RP)(),e.components);return i.createElement(i.Fragment,null,"\n",i.createElement("br"),"\n","\n","\n",i.createElement(t.p,null,"Anomaly detection, often referred to as ",i.createElement(l.A,null,"outlier detection")," or ",i.createElement(l.A,null,"novelty detection"),", is a powerful and widely studied field within data science and machine learning. It encompasses a range of methods whose goal is to identify patterns, observations, or data points that deviate considerably from the rest of the dataset. These deviant observations are what we label as anomalies, outliers, or novelties, depending on the context and the underlying assumptions. In many real-world scenarios — such as fraud detection, system health monitoring, event detection in sensor networks, or performance analysis — anomalies represent significant, unusual, or potentially critical instances that may require investigation."),"\n",i.createElement(t.p,null,"Historically, anomaly detection has been grounded in the idea of extreme value analysis and robust statistics, often with specialized domain knowledge to manually set thresholds or domain-specific rules. Over time, the prevalence and complexity of data have grown, leading to more sophisticated approaches that leverage both classical statistical inference and machine learning. From purely statistical methods (for instance, fitting a distribution to the data and identifying points in the tails) to more advanced techniques (such as one-class SVMs, isolation forests, autoencoders, and generative models), practitioners have at their disposal a rich arsenal of tools."),"\n",i.createElement(t.p,null,'One of the earliest seminal works on outliers came from Hawkins (1980), who defined an outlier as "an observation which deviates so much from other observations as to arouse suspicions that it was generated by a different mechanism." Additional literature, such as Chandola, Banerjee, and Kumar (2009), provided comprehensive surveys on anomaly detection methods across different domains (e.g., intrusion detection, industrial production lines, and financial transactions). Building on these foundations, modern research includes methods tailored to high-dimensional data, streaming or online scenarios, deep representation learning, and more.'),"\n",i.createElement(t.p,null,"In many respects, anomaly detection overlaps with other fundamental tasks in machine learning, such as classification and clustering. For example, in classification, anomalies can show up as rare samples in a predominantly normal class, and in clustering-based methods, small or distant clusters might be flagged as outliers. Simultaneously, anomaly detection has unique challenges: extreme class imbalances, changing data distributions (in streaming setups), or a lack of labeled data for anomalous events, among many others."),"\n",i.createElement(t.p,null,"The chapters that follow dive into key concepts, classification systems for anomalies, relevant statistical approaches, modern machine learning methods, feature engineering considerations, evaluation strategies, and real-world applications. Though there are many textbooks, research papers, and software frameworks addressing the topic, the intent here is to provide a deep, coherent overview that organizes the major approaches in a single narrative and highlights both classic and cutting-edge techniques."),"\n",i.createElement(t.p,null,"In what follows, we will:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,"Define the principal types of anomalies and how they typically arise."),"\n",i.createElement(t.li,null,"Discuss the statistical foundations, including parametric and non-parametric strategies for anomaly detection."),"\n",i.createElement(t.li,null,"Present various machine learning-based methods, from supervised to deep learning approaches."),"\n",i.createElement(t.li,null,"Outline best practices in feature engineering, dimensionality reduction, and domain-specific transformations."),"\n",i.createElement(t.li,null,"Describe model evaluation metrics and validation techniques that address class imbalance and other pitfalls."),"\n",i.createElement(t.li,null,"Present real-world applications, usage scenarios, and available open-source frameworks."),"\n"),"\n",i.createElement(t.p,null,"We conclude with a high-level summary of key points that help unify the field, while also pointing to ongoing research challenges and emerging methods."),"\n",i.createElement(t.h2,{id:"types-of-anomalies",style:{position:"relative"}},i.createElement(t.a,{href:"#types-of-anomalies","aria-label":"types of anomalies permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Types of anomalies"),"\n",i.createElement(t.p,null,'Though the concept of an "anomaly" might seem straightforward, it actually contains nuanced categories that help in selecting the right detection approach. Depending on the data characteristics and the context, the following classification is especially common:'),"\n",i.createElement(t.h3,{id:"point-anomalies",style:{position:"relative"}},i.createElement(t.a,{href:"#point-anomalies","aria-label":"point anomalies permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Point anomalies"),"\n",i.createElement(t.p,null,i.createElement(l.A,null,"Point anomalies")," are the simplest and most frequently encountered anomalies. In this scenario, a single point in the dataset is considered deviant relative to the overall data distribution. For example, imagine monitoring the temperature in a server room that normally oscillates between ",i.createElement(r.A,{text:"\\(18\\)"})," and ",i.createElement(r.A,{text:"\\(25\\)"})," °C. If one sensor reading randomly shoots up to ",i.createElement(r.A,{text:"\\(90\\)"})," °C, it would quite clearly be a point anomaly."),"\n",i.createElement(t.p,null,"This is often the first type of anomaly encountered in practical contexts because it is straightforward to detect using distance- or density-based methods. Some examples include credit-card fraud detection, in which a single very large purchase might be anomalous, or a sensor-based system that flags a single suspicious reading."),"\n",i.createElement(t.h3,{id:"contextual-anomalies",style:{position:"relative"}},i.createElement(t.a,{href:"#contextual-anomalies","aria-label":"contextual anomalies permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Contextual anomalies"),"\n",i.createElement(t.p,null,i.createElement(l.A,null,"Contextual anomalies"),' (sometimes referred to as conditional anomalies) require contextual information to determine whether a data point is truly anomalous. In other words, the "outlierness" of a point depends on the situation or environment in which it appears.'),"\n",i.createElement(t.p,null,"A quintessential example is temperature readings across different seasons or times. A reading of ",i.createElement(r.A,{text:"\\(15\\)"})," °C in summer (in a region typically reaching ",i.createElement(r.A,{text:"\\(30\\)"})," °C) might be considered anomalous, but the same ",i.createElement(r.A,{text:"\\(15\\)"})," °C reading in winter might be perfectly normal. Similarly, a traffic spike on a website might be anomalous on a normal weekday, but not during a major promotional event or product launch."),"\n",i.createElement(t.h3,{id:"collective-anomalies",style:{position:"relative"}},i.createElement(t.a,{href:"#collective-anomalies","aria-label":"collective anomalies permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Collective anomalies"),"\n",i.createElement(t.p,null,i.createElement(l.A,null,"Collective anomalies")," (or group anomalies) occur when a set or sequence of data points as a whole is deviant, even though each individual element in that set may not be particularly anomalous in isolation. A classic example is a burst of events in a time series — for instance, multiple back-to-back requests that collectively look suspicious, or a contiguous range of sensor readings in a manufacturing process that deviate together from the standard pattern."),"\n",i.createElement(t.p,null,"In these scenarios, anomalies are best detected by modeling the temporal or structural dependence of consecutive data points rather than analyzing each point individually. Clusters of network requests or unusual patterns in event logs also fall under this category."),"\n",i.createElement(t.h2,{id:"statistical-foundations",style:{position:"relative"}},i.createElement(t.a,{href:"#statistical-foundations","aria-label":"statistical foundations permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Statistical foundations"),"\n",i.createElement(t.h3,{id:"motivation-and-approach",style:{position:"relative"}},i.createElement(t.a,{href:"#motivation-and-approach","aria-label":"motivation and approach permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Motivation and approach"),"\n",i.createElement(t.p,null,"Statistical methods for anomaly detection aim to characterize the typical distribution of data and then identify any observations that appear inconsistent with the assumed distribution. These methods can be either ",i.createElement(l.A,null,"parametric")," (where a specific distribution like the Gaussian or Poisson distribution is assumed) or ",i.createElement(l.A,null,"non-parametric")," (where we forego strict assumptions about distributions and rely on data-driven techniques)."),"\n",i.createElement(t.p,null,"Such approaches are historically one of the earliest ways to detect outliers. They remain relevant due to their interpretability and ease of implementation. Moreover, they are often computationally lighter than more advanced ML methods, making them suitable for smaller datasets or well-understood data domains."),"\n",i.createElement(t.h3,{id:"distribution-based-methods",style:{position:"relative"}},i.createElement(t.a,{href:"#distribution-based-methods","aria-label":"distribution based methods permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Distribution-based methods"),"\n",i.createElement(t.p,null,'In distribution-based anomaly detection, we assume that "normal" data follows a specific distribution (e.g., normal, exponential, gamma). We then fit the parameters of that distribution (mean, variance, etc.) on the data that is presumed to be normal. Any point that significantly deviates, based on a probabilistic threshold, is flagged as an anomaly.'),"\n",i.createElement(t.p,null,"For instance, let ",i.createElement(r.A,{text:"\\( X \\in \\mathbb{R}^d \\)"})," denote a random variable representing the data (with ",i.createElement(r.A,{text:"\\(d\\)"})," features). Assume we fit a multivariate Gaussian distribution with mean vector ",i.createElement(r.A,{text:"\\(\\mu\\)"})," and covariance matrix ",i.createElement(r.A,{text:"\\(\\Sigma\\)"}),". Under that assumption, the log-likelihood for a point ",i.createElement(r.A,{text:"\\(x\\)"})," is:"),"\n",i.createElement(r.A,{text:"\\[\n\\log p(x) = - \\frac{d}{2}\\log(2\\pi) - \\frac{1}{2}\\log(\\det \\Sigma) - \\frac{1}{2}(x-\\mu)^\\top \\Sigma^{-1} (x-\\mu).\n\\]"}),"\n",i.createElement(t.p,null,"A threshold ",i.createElement(r.A,{text:"\\( \\tau \\)"})," might be chosen (based on domain knowledge or statistical significance) such that if ",i.createElement(r.A,{text:"\\(\\log p(x) < \\tau\\)"}),", we label ",i.createElement(r.A,{text:"\\(x\\)"})," as an anomaly."),"\n",i.createElement(t.p,null,"Variables here:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(r.A,{text:"\\(x\\)"})," is the data point in ",i.createElement(r.A,{text:"\\(d\\)"}),"-dimensional space."),"\n",i.createElement(t.li,null,i.createElement(r.A,{text:"\\(\\mu\\)"})," is the estimated mean vector."),"\n",i.createElement(t.li,null,i.createElement(r.A,{text:"\\(\\Sigma\\)"})," is the estimated covariance matrix."),"\n",i.createElement(t.li,null,i.createElement(r.A,{text:"\\(\\det \\Sigma\\)"})," is the determinant of ",i.createElement(r.A,{text:"\\(\\Sigma\\)"}),"."),"\n",i.createElement(t.li,null,i.createElement(r.A,{text:"\\(\\Sigma^{-1}\\)"})," is the inverse of the covariance matrix."),"\n",i.createElement(t.li,null,i.createElement(r.A,{text:"\\( \\tau \\)"})," is a threshold chosen either by cross-validation or a significance level approach."),"\n"),"\n",i.createElement(t.p,null,"Alternatively, one might look at the ",i.createElement(l.A,null,"Mahalanobis distance"),", ",i.createElement(r.A,{text:"\\(D_M\\)"}),", defined as:"),"\n",i.createElement(r.A,{text:"\\[\nD_{M}(x) = \\sqrt{(x-\\mu)^\\top \\Sigma^{-1} (x-\\mu)},\n\\]"}),"\n",i.createElement(t.p,null,"and if ",i.createElement(r.A,{text:"\\(D_M(x)\\)"}),' is too large compared to what is "typical," ',i.createElement(r.A,{text:"\\(x\\)"})," can be treated as anomalous."),"\n",i.createElement(t.h3,{id:"z-score-and-thresholding-approaches",style:{position:"relative"}},i.createElement(t.a,{href:"#z-score-and-thresholding-approaches","aria-label":"z score and thresholding approaches permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Z-score and thresholding approaches"),"\n",i.createElement(t.p,null,"A simpler but still widely used approach is based on the ",i.createElement(l.A,null,"z-score"),", which applies primarily to univariate data. Here, we compute:"),"\n",i.createElement(r.A,{text:"\\[\nz_i = \\frac{x_i - \\bar{x}}{\\sigma},\n\\]"}),"\n",i.createElement(t.p,null,"where ",i.createElement(r.A,{text:"\\(x_i\\)"})," is the ",i.createElement(r.A,{text:"\\(i\\)"}),"-th observation, ",i.createElement(r.A,{text:"\\(\\bar{x}\\)"})," is the sample mean, and ",i.createElement(r.A,{text:"\\(\\sigma\\)"})," is the sample standard deviation. Any observation that satisfies ",i.createElement(r.A,{text:"\\(|z_i| > k\\)"})," for some threshold ",i.createElement(r.A,{text:"\\(k\\)"})," (commonly 3 or 4) might be flagged as an outlier."),"\n",i.createElement(t.p,null,"However, in multivariate or high-dimensional contexts, the z-score is less useful, and we typically rely on more elaborate distributional assumptions or robust alternatives (e.g., ",i.createElement(l.A,null,"robust estimation")," of location and scale)."),"\n",i.createElement(t.h3,{id:"parametric-vs-non-parametric-techniques",style:{position:"relative"}},i.createElement(t.a,{href:"#parametric-vs-non-parametric-techniques","aria-label":"parametric vs non parametric techniques permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Parametric vs. non-parametric techniques"),"\n",i.createElement(t.p,null,"In parametric methods, we require a predefined distribution or family of distributions. For instance, we might assume that normal operating temperatures in an industrial sensor system follow a Gaussian distribution or a mixture of Gaussians. If the data is large and truly adheres to these assumptions, parametric methods can be extremely efficient and interpretable."),"\n",i.createElement(t.p,null,"Non-parametric methods, on the other hand, do not assume a specific distribution. Instead, they use data-driven strategies such as kernel density estimation (KDE) or rank-based tests. KDE, for instance, estimates the underlying probability density function by summing individual kernel functions centered on each data point:"),"\n",i.createElement(r.A,{text:"\\[\n\\hat{p}(x) = \\frac{1}{n\\,h^d} \\sum_{i=1}^n K \\Bigl( \\frac{x - x_i}{h} \\Bigr),\n\\]"}),"\n",i.createElement(t.p,null,"where ",i.createElement(r.A,{text:"\\(n\\)"})," is the number of samples, ",i.createElement(r.A,{text:"\\(h\\)"})," is the bandwidth (a smoothing parameter), and ",i.createElement(r.A,{text:"\\(K\\)"})," is a chosen kernel function (commonly Gaussian). Points ",i.createElement(r.A,{text:"\\(x\\)"})," for which ",i.createElement(r.A,{text:"\\(\\hat{p}(x)\\)"})," is exceedingly small can be flagged as anomalies."),"\n",i.createElement(t.p,null,"Variables here:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(r.A,{text:"\\(n\\)"})," is the total number of reference points in the dataset."),"\n",i.createElement(t.li,null,i.createElement(r.A,{text:"\\(d\\)"})," is the dimensionality of ",i.createElement(r.A,{text:"\\(x\\)"}),"."),"\n",i.createElement(t.li,null,i.createElement(r.A,{text:"\\(h\\)"})," is the kernel bandwidth controlling how wide each kernel is spread."),"\n",i.createElement(t.li,null,i.createElement(r.A,{text:"\\(K\\)"})," is typically a symmetric function integrated to 1, e.g. a Gaussian kernel."),"\n"),"\n",i.createElement(t.h3,{id:"time-series-considerations",style:{position:"relative"}},i.createElement(t.a,{href:"#time-series-considerations","aria-label":"time series considerations permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Time-series considerations"),"\n",i.createElement(t.p,null,"Anomaly detection in time-series settings introduces complexities such as trends, seasonality, and serial correlations. A purely distribution-based method that ignores time or ordering might misclassify cyclical or repeated patterns as anomalies. Thus, domain-aware approaches (e.g., ARIMA-based residual analysis, decomposition-based outlier detection) have been developed."),"\n",i.createElement(t.p,null,"In classical time-series anomaly detection, a model such as ARIMA is fit to the series, producing forecasts and confidence intervals. Observations lying far outside the forecast envelope are flagged. Alternatively, robust decomposition can separate the series into trend, seasonality, and remainder components, with anomalies identified in the remainder if they exceed certain thresholds."),"\n",i.createElement(t.h2,{id:"machine-learning-methods",style:{position:"relative"}},i.createElement(t.a,{href:"#machine-learning-methods","aria-label":"machine learning methods permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Machine learning methods"),"\n",i.createElement(t.p,null,'Machine learning-based anomaly detection methods often revolve around learning a representation of "normal" data and then scoring or classifying new observations as normal or abnormal. We can broadly categorize these approaches into ',i.createElement(l.A,null,"supervised"),", ",i.createElement(l.A,null,"unsupervised"),", ",i.createElement(l.A,null,"semi-supervised"),", and ",i.createElement(l.A,null,"deep learning-based")," techniques."),"\n",i.createElement(t.h3,{id:"41-supervised-anomaly-detection",style:{position:"relative"}},i.createElement(t.a,{href:"#41-supervised-anomaly-detection","aria-label":"41 supervised anomaly detection permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.1. Supervised anomaly detection"),"\n",i.createElement(t.p,null,"When labeled data is available and we have examples of both normal instances and anomalies, one can treat anomaly detection as a supervised classification problem. However, real-life anomaly detection often suffers from severe class imbalance: anomalies can be extremely rare, and in many domains, large sets of labeled anomalies do not exist. Nonetheless, when some labeled anomalies are indeed available, standard classification approaches (e.g., random forests, support vector machines, gradient boosting) can be adapted with specialized strategies for ",i.createElement(l.A,null,"imbalanced classification"),":"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,"Oversampling the minority class with methods like SMOTE (Synthetic Minority Over-sampling TEchnique)."),"\n",i.createElement(t.li,null,"Undersampling the majority class."),"\n",i.createElement(t.li,null,"Incorporating class weight adjustments in the classifier's training loss."),"\n",i.createElement(t.li,null,"Using specialized metrics such as the F2-Score, AUROC, or PRAUC that focus more on minority-class performance."),"\n"),"\n",i.createElement(t.p,null,'An example challenge with supervised learning arises when the anomalies are highly heterogeneous. In such a scenario, a single "anomaly class" might not represent the full variety of abnormal patterns. Another challenge is that new, previously unseen anomalies might go undetected.'),"\n",i.createElement(t.h4,{id:"performance-challenges-with-imbalanced-data",style:{position:"relative"}},i.createElement(t.a,{href:"#performance-challenges-with-imbalanced-data","aria-label":"performance challenges with imbalanced data permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Performance challenges with imbalanced data"),"\n",i.createElement(t.p,null,"Classification-based approaches for anomaly detection typically revolve around having many more normal examples than anomalies. If a naive model simply classifies all points as normal, it may achieve a deceptively high overall accuracy. Therefore, the focus shifts to metrics like:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"Precision, recall, and F1 (or F2) specifically for the anomaly class."),"\n",i.createElement(t.li,null,"The area under the precision-recall curve (PR AUC), which is particularly informative under class imbalance."),"\n",i.createElement(t.li,null,"The area under the ROC curve (ROC AUC), though it may sometimes overestimate performance in extremely imbalanced settings."),"\n"),"\n",i.createElement(t.h3,{id:"42-unsupervised-anomaly-detection",style:{position:"relative"}},i.createElement(t.a,{href:"#42-unsupervised-anomaly-detection","aria-label":"42 unsupervised anomaly detection permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.2. Unsupervised anomaly detection"),"\n",i.createElement(t.p,null,i.createElement(l.A,null,"Unsupervised methods")," constitute a major portion of anomaly detection algorithms, precisely because they do not require labels. They rely on the assumption that anomalies are rare and different from the bulk of the data."),"\n",i.createElement(t.h4,{id:"clustering-based-methods",style:{position:"relative"}},i.createElement(t.a,{href:"#clustering-based-methods","aria-label":"clustering based methods permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Clustering-based methods"),"\n",i.createElement(t.p,null,"Since anomalies are typically distinct or sparse, small or distant clusters in a clustering solution can be flagged as anomalies. For instance, with K-means, if a point's distance to the assigned cluster centroid is too large compared to an empirical threshold, it might be labeled as an anomaly. Alternatively, hierarchical clustering or density-based clustering (e.g., DBSCAN) can identify small clusters with few points as anomalies."),"\n",i.createElement(t.p,null,"Nonetheless, one must be mindful that if the dataset has multiple natural clusters of normal points, naive clustering-based approaches might misclassify valid clusters with fewer points as anomalies. Hybrid strategies or domain-driven thresholds can mitigate this."),"\n",i.createElement(t.h4,{id:"density-estimation",style:{position:"relative"}},i.createElement(t.a,{href:"#density-estimation","aria-label":"density estimation permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Density estimation"),"\n",i.createElement(t.p,null,"Density-based approaches, such as ",i.createElement(l.A,null,"Local Outlier Factor (LOF)"),", measure how isolated a point is with respect to its neighbors in the feature space. In LOF, each data point's local density is compared to the densities of its nearest neighbors. Points with significantly lower local densities than their neighbors receive higher outlier scores."),"\n",i.createElement(t.p,null,'Other well-known methods include robust kernel density estimation or the aforementioned parametric and non-parametric distribution modeling. However, in high dimensions, distance measures and density estimation can become less reliable due to the "curse of dimensionality."'),"\n",i.createElement(t.h3,{id:"43-semi-supervised-techniques",style:{position:"relative"}},i.createElement(t.a,{href:"#43-semi-supervised-techniques","aria-label":"43 semi supervised techniques permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.3. Semi-supervised techniques"),"\n",i.createElement(t.p,null,'Semi-supervised techniques in anomaly detection typically assume that we have access to a set of "normal" training samples but lack reliable labels for anomalies. The model attempts to learn a compact representation of normal behavior and flags deviations from it as anomalies.'),"\n",i.createElement(t.h4,{id:"one-class-svm",style:{position:"relative"}},i.createElement(t.a,{href:"#one-class-svm","aria-label":"one class svm permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"One-class SVM"),"\n",i.createElement(t.p,null,i.createElement(l.A,null,"One-class SVM")," is a powerful semi-supervised approach introduced in the late 1990s and widely popularized in the 2000s. It tries to separate the origin (in feature space) from the data with a maximum margin boundary, effectively isolating most of the data points in a particular region. New points lying outside this region are labeled as anomalies or novelties."),"\n",i.createElement(t.p,null,"In scikit-learn's implementation (",i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">sklearn.svm.OneClassSVM</code>'}}),"), the RBF (radial basis function) kernel is commonly used. The parameter ",i.createElement(r.A,{text:"\\( \\nu \\)"})," controls the upper bound on the fraction of training errors (i.e., the fraction of outliers) and influences the margin. This method is especially well-suited for ",i.createElement(l.A,null,"novelty detection")," — it learns from a training set that is assumed to be largely free of anomalies and then aims to detect anomalies in future data."),"\n",i.createElement(t.h4,{id:"isolation-forest",style:{position:"relative"}},i.createElement(t.a,{href:"#isolation-forest","aria-label":"isolation forest permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Isolation forest"),"\n",i.createElement(t.p,null,i.createElement(l.A,null,"Isolation forest")," is another widely adopted algorithm. Rather than modeling the distribution of normal data, it explicitly attempts to ",i.createElement(l.A,null,"isolate")," anomalies by selecting features and thresholds at random to recursively partition the feature space. Anomalies tend to be easier to isolate because they typically require fewer splits:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,"Build an ensemble of random trees where each tree is grown by randomly selecting features and splitting values."),"\n",i.createElement(t.li,null,"For a given point ",i.createElement(r.A,{text:"\\(x\\)"}),", measure the average path length across all trees."),"\n",i.createElement(t.li,null,"Points with shorter average path lengths are considered anomalies (easier to isolate)."),"\n"),"\n",i.createElement(t.p,null,"This idea resonates with the broader concept of ensemble learning: if many random partitions isolate a point early, it is likely abnormal. Isolation forests are relatively fast and can handle high-dimensional datasets."),"\n",i.createElement(t.h4,{id:"other-semi-supervised-approaches",style:{position:"relative"}},i.createElement(t.a,{href:"#other-semi-supervised-approaches","aria-label":"other semi supervised approaches permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Other semi-supervised approaches"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"RANSAC")," (RANdom SAmple Consensus) can be used to robustly fit a model to the majority of the data. Points that deviate significantly from the fitted model are flagged as outliers or anomalies."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Elliptic envelope"),' (in scikit-learn) models data as a Gaussian distribution and finds the "envelope" that fits most points. Points outside this ellipse are considered anomalies. This works best for roughly unimodal, elliptical distributions.'),"\n"),"\n",i.createElement(t.h3,{id:"44-deep-learning-based-methods",style:{position:"relative"}},i.createElement(t.a,{href:"#44-deep-learning-based-methods","aria-label":"44 deep learning based methods permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.4. Deep learning-based methods"),"\n",i.createElement(t.p,null,"In recent years, the anomaly detection landscape has been significantly influenced by deep learning research, offering advanced ways to learn representations in complex, high-dimensional data such as images, time series, and sensor networks."),"\n",i.createElement(t.h4,{id:"autoencoders-for-reconstruction-error",style:{position:"relative"}},i.createElement(t.a,{href:"#autoencoders-for-reconstruction-error","aria-label":"autoencoders for reconstruction error permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Autoencoders for reconstruction error"),"\n",i.createElement(t.p,null,"A popular approach is to train an ",i.createElement(l.A,null,"autoencoder"),' on "normal" data. The autoencoder, a neural network architecture with a bottleneck hidden layer, learns to reconstruct normal data. The reconstruction error is then used as an anomaly score:'),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,"Train the autoencoder using only normal examples (or mostly normal)."),"\n",i.createElement(t.li,null,"For new data, measure how well the autoencoder reconstructs the input."),"\n",i.createElement(t.li,null,"If the reconstruction error ",i.createElement(r.A,{text:"\\( \\lVert x - \\hat{x} \\rVert \\)"})," is above a certain threshold, the point is labeled as an anomaly."),"\n"),"\n",i.createElement(t.p,null,"Such approaches have been utilized extensively in fraud detection, defect detection in images, and industrial IoT sensor monitoring. Variants such as denoising autoencoders, variational autoencoders, and convolutional autoencoders are also widely applied."),"\n",i.createElement(t.h4,{id:"generative-adversarial-networks-gans",style:{position:"relative"}},i.createElement(t.a,{href:"#generative-adversarial-networks-gans","aria-label":"generative adversarial networks gans permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Generative adversarial networks (GANs)"),"\n",i.createElement(t.p,null,"GAN-based anomaly detection methods (e.g., ",i.createElement(l.A,null,"AnoGAN"),") train a generator-discriminator pair on normal data. The assumption is that the generator will fail to produce convincing replicas for anomalous inputs, while the discriminator's output or an auxiliary error metric can be used to detect anomalies. Although more complex to train, GANs have shown promising results in vision and other data modalities."),"\n",i.createElement(t.p,null,"A high-level outline:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,"Train a GAN using only normal data."),"\n",i.createElement(t.li,null,"For a new data point, find the latent representation ",i.createElement(r.A,{text:"\\(z\\)"})," in the generator's space that best reconstructs the data point."),"\n",i.createElement(t.li,null,"If the reconstruction is poor or the discriminator's confidence is low, classify the point as an anomaly."),"\n"),"\n",i.createElement(t.p,null,"Deep learning-based solutions can require substantial data and computational resources, yet they offer unmatched flexibility when dealing with unstructured or high-dimensional data like images, audio signals, or complex time series. Recent research has also explored graph neural networks for anomaly detection in graph-structured data, LSTM-based detection for sequential data, and transformer-based anomaly detection for complicated temporal patterns."),"\n",i.createElement(t.h2,{id:"feature-engineering-and-dimensionality-reduction",style:{position:"relative"}},i.createElement(t.a,{href:"#feature-engineering-and-dimensionality-reduction","aria-label":"feature engineering and dimensionality reduction permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Feature engineering and dimensionality reduction"),"\n",i.createElement(t.p,null,"High-dimensional data complicates anomaly detection for many algorithms, particularly distance- and density-based methods, due to the so-called ",i.createElement(l.A,null,"curse of dimensionality"),". Consequently, thoughtful feature engineering can be crucial to success."),"\n",i.createElement(t.h3,{id:"51-selecting-and-scaling-features",style:{position:"relative"}},i.createElement(t.a,{href:"#51-selecting-and-scaling-features","aria-label":"51 selecting and scaling features permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.1. Selecting and scaling features"),"\n",i.createElement(t.p,null,"In many anomaly detection scenarios, the appropriate choice and preprocessing of features is as essential as the choice of the detection algorithm:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Normalization or standardization"),": E.g., subtract mean, divide by standard deviation for each feature, especially important for distance-based methods."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Robust scaling"),": Re-scaling with robust measures such as median and interquartile range can mitigate the influence of outliers on feature scaling."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Domain-driven features"),": Domain knowledge often reveals specialized transformations. For instance, log-scaling or power transformations for data with heavy tails."),"\n"),"\n",i.createElement(t.h3,{id:"52-dimensionality-reduction-pca-t-sne-umap",style:{position:"relative"}},i.createElement(t.a,{href:"#52-dimensionality-reduction-pca-t-sne-umap","aria-label":"52 dimensionality reduction pca t sne umap permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.2. Dimensionality reduction (PCA, t-SNE, UMAP)"),"\n",i.createElement(t.p,null,"Reducing dimensionality can help highlight potential clusters of normal data or separate outliers in a latent projection."),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"PCA"),": A linear transformation that captures the directions of maximal variance. Points lying far from the principal subspace can be labeled as anomalies. The ",i.createElement(r.A,{text:"\\(Q\\)"}),"-statistic or the reconstruction error in PCA is sometimes used as an outlier score."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"t-SNE"),": A non-linear dimensionality reduction method often used for visualization. Although t-SNE is primarily a visualization tool, it can help practitioners spot outliers in a 2D embedding."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"UMAP"),": Another non-linear approach that preserves both local and global data structure. Like t-SNE, it is widely used for exploration and outlier discovery."),"\n"),"\n",i.createElement(t.h3,{id:"53-domain-specific-feature-transformations",style:{position:"relative"}},i.createElement(t.a,{href:"#53-domain-specific-feature-transformations","aria-label":"53 domain specific feature transformations permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.3. Domain-specific feature transformations"),"\n",i.createElement(t.p,null,"Financial transactions might require ratio features (e.g., comparing an expense to a user's average monthly expense). Network traffic anomalies might require features derived from packet sizes, protocol usage, or temporal frequency. Similarly, sensor data might benefit from aggregated features such as rolling averages, differences, or wavelet transform coefficients."),"\n",i.createElement(t.p,null,"In time-series anomaly detection, transformations that isolate cyclical or seasonal components can be helpful, as can features capturing changes from one time step to the next. The right transformation can reduce noise and highlight patterns that separate normal from abnormal."),"\n",i.createElement(t.h2,{id:"model-evaluation-and-validation",style:{position:"relative"}},i.createElement(t.a,{href:"#model-evaluation-and-validation","aria-label":"model evaluation and validation permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Model evaluation and validation"),"\n",i.createElement(t.p,null,"Evaluating anomaly detection models is challenging because of the extreme class imbalance, potential absence of labeled anomalies, and the possibility of new types of anomalies appearing in the future. Nevertheless, certain strategies and metrics apply generally:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Precision, recall, and F1-score")," for anomalies, if we have ground truth labels."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"ROC AUC")," or ",i.createElement(t.strong,null,"PR AUC")," if label distribution or threshold tuning is a concern. PR AUC is more insightful when positives (anomalies) are rare."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Cross-validation"),": K-fold cross-validation can be adapted to measure outlier detection performance, although ensuring a consistent ratio of anomalies in each fold can be tricky if anomalies are extremely rare."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Time-series splits"),": In time-series anomaly detection, standard random folds can lead to information leakage. Instead, one uses rolling or expanding window evaluation."),"\n"),"\n",i.createElement(t.p,null,"Moreover, when ground truth is scarce, domain expert feedback is invaluable to confirm or reject flagged anomalies. In certain industries, the cost of a missed anomaly might far exceed the cost of a false alarm, so the trade-off between false positives and false negatives must be carefully balanced."),"\n",i.createElement(t.h3,{id:"handling-class-imbalance",style:{position:"relative"}},i.createElement(t.a,{href:"#handling-class-imbalance","aria-label":"handling class imbalance permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Handling class imbalance"),"\n",i.createElement(t.p,null,"Class imbalance is a persistent issue. In a typical industrial scenario, anomalies may comprise well below 1% of the data. Techniques include:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Synthetic anomaly generation"),": If domain knowledge allows, artificially creating plausible anomalies can help in training or validating."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Cost-sensitive methods"),": Adjust the cost function to penalize missed anomalies more heavily."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Oversampling")," the known anomalies or undersampling the normal data, though care must be taken not to lose valuable information about normal patterns."),"\n"),"\n",i.createElement(t.p,null,"When unlabeled data is abundant, semi-supervised methods that learn normal patterns remain highly attractive."),"\n",i.createElement(t.h2,{id:"cases-applications-python-tools-examples",style:{position:"relative"}},i.createElement(t.a,{href:"#cases-applications-python-tools-examples","aria-label":"cases applications python tools examples permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Cases, applications, Python tools, examples"),"\n",i.createElement(t.p,null,"Anomaly detection is used across a variety of fields:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Credit card fraud"),": Real-time monitoring of transactions to detect potential fraud."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Intrusion detection"),": Identifying unauthorized access or suspicious behavior on networks."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Equipment health"),": Monitoring sensor data for unexpected vibrations, temperatures, or other signals that deviate from known healthy patterns."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Healthcare"),": Flagging unusual patient vital signs, new disease symptoms, or rare medical conditions."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Manufacturing"),": Spotting production irregularities, defective products, or malfunctioning machines."),"\n"),"\n",i.createElement(t.p,null,"Below is a quick Python code snippet illustrating how one might use isolation forests in ",i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">scikit-learn</code>'}}),":"),"\n",i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">&lt;Code text={`\nimport numpy as np\nfrom sklearn.ensemble import IsolationForest\n\n# Example: generate a small synthetic dataset\nrng = np.random.RandomState(42)\n# Normal data around mean=0, standard deviation=1\nX_normal = 0.3 * rng.randn(100, 2)\nX_normal = np.r_[X_normal + 2, X_normal - 2]\n\n# Add some outliers\nX_outliers = rng.uniform(low=-6, high=6, size=(20, 2))\n\nX = np.concatenate([X_normal, X_outliers], axis=0)\n\n# Fit isolation forest\niso_forest = IsolationForest(contamination=0.1, random_state=42)\niso_forest.fit(X)\n\n# Predict anomalies (-1 means outlier, 1 means inlier)\npreds = iso_forest.predict(X)\nscores = iso_forest.decision_function(X)\n\nprint("Predictions:", preds)\nprint("Anomaly scores:", scores)\n`}/></code></pre></div>'}}),"\n",i.createElement(t.p,null,"Here, we generate normal data in two clusters (centered at ",i.createElement(r.A,{text:"\\(\\pm 2\\)"}),") and a set of uniform outliers. We train an Isolation Forest with a contamination ratio of 0.1 (assuming ~10% anomalies). Points receiving a prediction of ",i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">-1</code>'}})," are outliers, while ",i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">1</code>'}})," indicates normal."),"\n",i.createElement(t.p,null,"Many other popular tools and libraries exist:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"scikit-learn")," includes ",i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">OneClassSVM</code>'}}),", ",i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">IsolationForest</code>'}}),", ",i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">LocalOutlierFactor</code>'}}),", and ",i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">EllipticEnvelope</code>'}}),"."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"PyOD")," (Python Outlier Detection) is a comprehensive library with numerous algorithms, including advanced ensembles and deep learning-based methods."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"TensorFlow")," and ",i.createElement(t.strong,null,"PyTorch")," can be used to build custom neural architectures for anomaly detection."),"\n"),"\n",i.createElement(t.h2,{id:"tools-frameworks-and-libraries",style:{position:"relative"}},i.createElement(t.a,{href:"#tools-frameworks-and-libraries","aria-label":"tools frameworks and libraries permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Tools, frameworks, and libraries"),"\n",i.createElement(t.p,null,"Besides the scikit-learn ecosystem, the following are noteworthy:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,i.createElement(l.A,null,"PyOD"),": One of the most extensive open-source Python libraries solely dedicated to anomaly detection, featuring well-known algorithms (Isolation Forest, LOF, One-Class SVM, etc.) and advanced ones (like COPOD, ECOD, SUOD)."),"\n",i.createElement(t.li,null,i.createElement(l.A,null,"River"),": A Python library for online/streaming anomaly detection, useful when data arrives continuously and the model must update in real time."),"\n",i.createElement(t.li,null,i.createElement(l.A,null,"MLlib")," in Apache Spark: Provides distributed implementations of a few anomaly detection or outlier detection algorithms, suitable for big data."),"\n",i.createElement(t.li,null,i.createElement(l.A,null,"H2O.ai"),": Offers an extensive platform with anomaly detection functionalities, often integrated with GPU acceleration and automated machine learning features."),"\n"),"\n",i.createElement(t.p,null,"For specialized domains — like anomaly detection in graphs or images — libraries such as PyTorch Geometric (for graph neural networks) or integrated frameworks for computer vision can be harnessed to build custom solutions."),"\n",i.createElement(t.h2,{id:"conclusion",style:{position:"relative"}},i.createElement(t.a,{href:"#conclusion","aria-label":"conclusion permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Conclusion"),"\n",i.createElement(t.p,null,'Anomaly detection is an indispensable technique in data science and machine learning, powering solutions ranging from fraud prevention and cybersecurity to industrial monitoring and healthcare diagnostics. By defining an anomaly as something that does not conform to an established "normal" pattern, practitioners must grapple with the inherent imbalance and diversity of anomalies.'),"\n",i.createElement(t.p,null,"From a historical perspective, statistical methods (z-score, parametric distribution fitting, robust statistics) laid the groundwork and remain crucial whenever domain assumptions apply or data is limited. Over time, unsupervised and semi-supervised machine learning approaches have brought more flexible, data-driven solutions, especially in settings where labeled anomalies are scarce or incomplete. Advanced algorithms such as one-class SVM, isolation forests, and local outlier factor have become mainstays, given their interpretability and relatively high performance across different domains."),"\n",i.createElement(t.p,null,"Simultaneously, deep learning has opened the door for new paradigms: learning a compressed representation of normal data (via autoencoders or generative models) can greatly amplify our ability to highlight anomalies in complex, high-dimensional data. This area continues to be one of active research, with developments in transformer-based or diffusion-based anomaly detection for time series, images, and text data."),"\n",i.createElement(t.p,null,"Developing a successful anomaly detection solution requires careful planning, including:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"Understanding the data domain, the types of anomalies of interest, and the implications of false positives versus false negatives."),"\n",i.createElement(t.li,null,"Engineering appropriate features and scaling methods that mitigate the curse of dimensionality and highlight distinguishing characteristics."),"\n",i.createElement(t.li,null,"Employing robust validation and evaluation strategies that take imbalance and domain feedback into account."),"\n"),"\n",i.createElement(t.p,null,"Although numerous open-source frameworks offer a wealth of ready-to-use anomaly detection algorithms, domain expertise often remains the linchpin of success. Cleverly chosen features and well-considered thresholds can be more critical than the choice between, say, isolation forest and local outlier factor."),"\n",i.createElement(t.p,null,"Looking ahead, research trends include self-supervised learning for anomaly detection (leveraging large unlabeled datasets to learn robust representations), few-shot and active learning approaches for leveraging small sets of labeled anomalies, and scalable distributed detection methods for massive streaming data. Together, these developments underscore the vibrant and evolving nature of anomaly detection as a discipline — one that continues to play a vital role in safeguarding systems, processes, and data-driven decision-making across the globe."))}var o=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,n.RP)(),e.components);return t?i.createElement(t,e,i.createElement(s,e)):s(e)},c=a(54506),m=a(88864),d=a(58481),h=a.n(d),u=a(5984),p=a(43672),g=a(27042),f=a(72031),v=a(81817),y=a(27105),b=a(17265),E=a(2043),w=a(95751),S=a(94328),x=a(80791),A=a(78137);const H=e=>{let{toc:t}=e;if(!t||!t.items)return null;return i.createElement("nav",{className:x.R},i.createElement("ul",null,t.items.map(((e,t)=>i.createElement("li",{key:t},i.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const a=t.replace("#",""),n=document.getElementById(a);n&&n.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&i.createElement(H,{toc:{items:e.items}}))))))};function k(e){let{data:{mdx:t,allMdx:l,allPostImages:r},children:s}=e;const{frontmatter:o,body:m,tableOfContents:d}=t,f=o.index,x=o.slug.split("/")[1],k=l.nodes.filter((e=>e.frontmatter.slug.includes(`/${x}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),C=k.findIndex((e=>e.frontmatter.index===f)),M=k[C+1],_=k[C-1],T=o.slug.replace(/\/$/,""),I=/[^/]*$/.exec(T)[0],z=`posts/${x}/content/${I}/`,{0:V,1:P}=(0,i.useState)(o.flagWideLayoutByDefault),{0:N,1:L}=(0,i.useState)(!1);var O;(0,i.useEffect)((()=>{L(!0);const e=setTimeout((()=>L(!1)),340);return()=>clearTimeout(e)}),[V]),"adventures"===x?O=b.cb:"research"===x?O=b.Qh:"thoughts"===x&&(O=b.T6);const B=h()(m).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,q=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),a=e%60;return a<=30?`~${t}${a>0?".5":""} h`:`~${t+1} h`}(Math.ceil(B/O)+(o.extraReadTimeMin||0)),F=[{flag:o.flagDraft,component:()=>Promise.all([a.e(5850),a.e(9833)]).then(a.bind(a,49833))},{flag:o.flagMindfuckery,component:()=>Promise.all([a.e(5850),a.e(7805)]).then(a.bind(a,27805))},{flag:o.flagRewrite,component:()=>Promise.all([a.e(5850),a.e(8916)]).then(a.bind(a,78916))},{flag:o.flagOffensive,component:()=>Promise.all([a.e(5850),a.e(6731)]).then(a.bind(a,49112))},{flag:o.flagProfane,component:()=>Promise.all([a.e(5850),a.e(3336)]).then(a.bind(a,83336))},{flag:o.flagMultilingual,component:()=>Promise.all([a.e(5850),a.e(2343)]).then(a.bind(a,62343))},{flag:o.flagUnreliably,component:()=>Promise.all([a.e(5850),a.e(6865)]).then(a.bind(a,11627))},{flag:o.flagPolitical,component:()=>Promise.all([a.e(5850),a.e(4417)]).then(a.bind(a,24417))},{flag:o.flagCognitohazard,component:()=>Promise.all([a.e(5850),a.e(8669)]).then(a.bind(a,18669))},{flag:o.flagHidden,component:()=>Promise.all([a.e(5850),a.e(8124)]).then(a.bind(a,48124))}],{0:D,1:R}=(0,i.useState)([]);return(0,i.useEffect)((()=>{F.forEach((e=>{let{flag:t,component:a}=e;t&&a().then((e=>{R((t=>[].concat((0,c.A)(t),[e.default])))}))}))}),[]),i.createElement(g.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},i.createElement(v.A,{postNumber:o.index,date:o.date,updated:o.updated,readTime:q,difficulty:o.difficultyLevel,title:o.title,desc:o.desc,banner:o.banner,section:x,postKey:I,isMindfuckery:o.flagMindfuckery,mainTag:o.mainTag}),i.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},o.otherTags.map(((e,t)=>i.createElement("span",{key:t,className:`noselect ${A.MW}`,style:{margin:"0 5px 5px 0"}},e)))),i.createElement("div",{className:"postBody"},i.createElement(H,{toc:d})),i.createElement("br",null),i.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},i.createElement(g.P.button,{className:`noselect ${S.pb}`,id:S.xG,onClick:()=>{P(!V)},whileTap:{scale:.93}},i.createElement(g.P.div,{className:w.DJ,key:V,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},V?"Switch to default layout":"Switch to wide layout"))),i.createElement("br",null),i.createElement("div",{className:"postBody",style:{margin:V?"0 -14%":"",maxWidth:V?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},i.createElement("div",{className:`${S.P_} ${N?S.Xn:S.qG}`},D.map(((e,t)=>i.createElement(e,{key:t}))),o.indexCourse?i.createElement(E.A,{index:o.indexCourse,category:o.courseCategoryName}):"",i.createElement(u.Z.Provider,{value:{images:r.nodes,basePath:z.replace(/\/$/,"")+"/"}},i.createElement(n.xA,{components:{Image:p.A}},s)))),i.createElement(y.A,{nextPost:M,lastPost:_,keyCurrent:I,section:x}))}function C(e){return i.createElement(k,e,i.createElement(o,e))}function M(e){var t,a,n,l,r;let{data:s}=e;const{frontmatter:o}=s.mdx,c=o.titleSEO||o.title,d=o.titleOG||c,h=o.titleTwitter||c,u=o.descSEO||o.desc,p=o.descOG||u,g=o.descTwitter||u,v=o.schemaType||"BlogPosting",y=o.keywordsSEO,b=o.date,E=o.updated||b,w=o.imageOG||(null===(t=o.banner)||void 0===t||null===(a=t.childImageSharp)||void 0===a||null===(n=a.gatsbyImageData)||void 0===n||null===(l=n.images)||void 0===l||null===(r=l.fallback)||void 0===r?void 0:r.src),S=o.imageAltOG||p,x=o.imageTwitter||w,A=o.imageAltTwitter||g,H=o.canonicalURL,k=o.flagHidden||!1,C=o.mainTag||"Posts",M=o.slug.split("/")[1]||"posts",{siteUrl:_}=(0,m.Q)(),T={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:_},{"@type":"ListItem",position:2,name:C,item:`${_}/${o.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${_}${o.slug}`}]};return i.createElement(f.A,{title:c+" - avrtt.blog",titleOG:d,titleTwitter:h,description:u,descriptionOG:p,descriptionTwitter:g,schemaType:v,keywords:y,datePublished:b,dateModified:E,imageOG:w,imageAltOG:S,imageTwitter:x,imageAltTwitter:A,canonicalUrl:H,flagHidden:k,mainTag:C,section:M,type:"article"},i.createElement("script",{type:"application/ld+json"},JSON.stringify(T)))}},90548:function(e,t,a){var n=a(96540),i=a(7978);t.A=e=>{let{text:t}=e;return n.createElement(i.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-tsx-content-file-path-src-pages-posts-research-anomaly-detection-mdx-86d931e08eb84e52cd6a.js.map