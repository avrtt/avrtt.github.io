"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[861],{5188:function(e,t,n){n.r(t),n.d(t,{Head:function(){return A},PostTemplate:function(){return C},default:function(){return _}});var a=n(54506),i=n(28453),r=n(96540),l=n(16886),o=n(46295),s=n(96098);function c(e){const t=Object.assign({p:"p",ul:"ul",li:"li",h2:"h2",a:"a",span:"span",h3:"h3",h4:"h4",strong:"strong",ol:"ol"},(0,i.RP)(),e.components),{Image:n}=t;return n||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),r.createElement(r.Fragment,null,"\n",r.createElement("br"),"\n","\n","\n",r.createElement(t.p,null,"Image processing is a cornerstone of modern machine learning and data science, especially in a world where multimedia data proliferates at breakneck speed. Working with images has become crucial for countless applications, including automated inspection, medical diagnostics, biometrics, robotics, document analysis, and advanced research topics like autonomous driving or human–machine collaboration. The capacity to process, enhance, and analyze images forms an essential pipeline for many sophisticated algorithms in computer vision. Whether using simple thresholding methods for quick feature extraction or building complex neural architectures to solve end-to-end vision problems, the domain of image processing offers an ever-expanding set of tools and theoretical frameworks."),"\n",r.createElement(t.p,null,"Machine learning (ML) systems rely on well-prepared and high-quality data, and when that data is visual or multimedia, it almost invariably requires image preprocessing, segmentation, or transformation steps. Additionally, the synergy between image processing and deep learning has led to dramatic improvements in accuracy for tasks such as image classification, object detection, semantic segmentation, and more. Enormous volumes of images, from social media photos to satellite imagery, demand efficient and robust processing. Researchers (e.g., Zhang and gang, CVPR 2023; Ramesh and gang, NeurIPS 2022) have consistently identified that well-structured preprocessing pipelines lead to higher model performance and fewer training difficulties."),"\n",r.createElement(t.p,null,"This article explores image processing in considerable detail, targeting a specialized readership: professionals and scientists with a strong background in machine learning, statistics, and data science who wish to solidify or expand their understanding of how image data can be transformed, enhanced, segmented, and ultimately used to power advanced machine learning pipelines. We will balance conceptual depth with an informal, learning-oriented voice, offering technical expansions on the fundamentals and advanced theoretical underpinnings of each concept. Although not a purely academic paper, we will reference relevant research from top conferences (like CVPR, ECCV, ICML, NeurIPS) and journals (like IEEE TPAMI, JMLR, and IJCV) whenever beneficial."),"\n",r.createElement(t.p,null,'This article adheres to the overall flow of the machine learning course outline, slotting in at section 27: "Image processing." However, we will make cross-references to relevant techniques covered in earlier or later sections (for example, references to cluster-based segmentation that appear again in chapter 21 on K-means, or connections to deep learning in chapters 48, 49, and 50 on neural network concepts).'),"\n",r.createElement(t.p,null,"Our goals are:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"Introduce essential image-processing terminology and principles."),"\n",r.createElement(t.li,null,"Discuss binarization, focusing on both global and local thresholding techniques such as Otsu's method, Niblack's approach, Bernsen's local thresholding, and variations of them that address inhomogeneous lighting or noise."),"\n",r.createElement(t.li,null,"Examine the role of image enhancement through morphological operations, histogram equalization, and edge-detection methods."),"\n",r.createElement(t.li,null,"Illustrate color processing and colorization strategies, bridging them to color-based ML tasks."),"\n",r.createElement(t.li,null,"Describe key feature extraction methods (SIFT, SURF, ORB, etc.) and specialized feature descriptors."),"\n",r.createElement(t.li,null,"Examine segmentation algorithms (threshold-based, region-based, clustering-based, etc.) and introduce the integration with object detection and location."),"\n",r.createElement(t.li,null,"Discuss the synergy between traditional image processing techniques and modern machine learning and deep learning frameworks."),"\n",r.createElement(t.li,null,"Highlight best practices for data augmentation and specialized evaluation metrics (e.g., Intersection over Union (IoU) for segmentation tasks)."),"\n"),"\n",r.createElement(t.p,null,"By the end, you should gain not only a deeper understanding of theoretical principles underpinning image processing but also a stronger intuition for how these methods integrate into broader machine learning pipelines."),"\n",r.createElement(t.h2,{id:"fundamentals",style:{position:"relative"}},r.createElement(t.a,{href:"#fundamentals","aria-label":"fundamentals permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Fundamentals"),"\n",r.createElement(t.h3,{id:"refresher-for-basic-terminology",style:{position:"relative"}},r.createElement(t.a,{href:"#refresher-for-basic-terminology","aria-label":"refresher for basic terminology permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Refresher for basic terminology"),"\n",r.createElement(t.p,null,"Images in digital form are typically represented as two-dimensional arrays (matrices) of pixel intensity values. Each pixel holds one or more components (also referred to as channels) that describe its color or intensity. A simple grayscale image has one channel, indicating the intensity at each spatial coordinate ",r.createElement(s.A,{text:"\\( (x,y) \\)"}),". Color images often have three channels in the RGB (Red, Green, Blue) model. Each channel is usually an integer in the range 0–255 (for 8-bit images), though higher bit depths and floating-point representations exist in more advanced systems."),"\n",r.createElement(t.p,null,"An alternative representation includes ",r.createElement(l.A,null,"multi-spectral")," or hyper-spectral images, where the number of channels can be in the dozens or even hundreds. Such representations appear in remote sensing, medical imaging, or advanced scientific domains. In these contexts, the fundamental principle remains the same: each pixel coordinate holds a vector of intensity/energy values."),"\n",r.createElement(t.h4,{id:"color-models",style:{position:"relative"}},r.createElement(t.a,{href:"#color-models","aria-label":"color models permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Color models"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"RGB"),": This is perhaps the most widely used model, especially for computer graphics and display. An RGB pixel is specified as a combination of Red, Green, and Blue intensities."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"HSV")," (Hue, Saturation, Value): Often used in color manipulation tasks, HSV can make certain operations (e.g., changing color brightness or saturation) more intuitive."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Grayscale"),": In grayscale images, each pixel is simply one scalar intensity, typically in the range 0–255 for 8-bit images. Grayscale conversion from an RGB image often follows a weighted formula such as ",r.createElement(s.A,{text:"\\( I = 0.2989 \\times R + 0.5870 \\times G + 0.1140 \\times B \\)"})," or similar."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Other color spaces"),": There are many others, such as YUV, YCbCr, LAB, etc. They can be more perceptually uniform or used in specific compression schemes (e.g., YUV in many video standards)."),"\n"),"\n",r.createElement(t.h4,{id:"image-file-formats-and-compression",style:{position:"relative"}},r.createElement(t.a,{href:"#image-file-formats-and-compression","aria-label":"image file formats and compression permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Image file formats and compression"),"\n",r.createElement(t.p,null,"Different storage formats abound, balancing ease of display, compression ratio, color precision, and metadata support. Examples include:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"JPEG"),": A lossy compression method widely used for photos. It exploits the limitations of human vision for high compression ratios but can introduce artifacts (blurriness or blockiness)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"PNG"),": A lossless format, typically used for web images, icons, or images needing an alpha channel."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"TIFF"),": Offers flexible color depth and is popular in professional photography and high-quality archiving."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"BMP"),": An older format that stores uncompressed or lightly compressed data."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"GIF"),": Historically used for animated images with a limited color palette."),"\n"),"\n",r.createElement(t.p,null,"In advanced machine learning systems, we frequently read images from these formats but process them in an internal uncompressed representation (e.g., in ",r.createElement(l.A,null,"NumPy arrays")," or ",r.createElement(l.A,null,"PyTorch tensors"),"). This ensures pixel-level transformations can be performed rapidly and without repeated decompression overhead."),"\n",r.createElement(t.h4,{id:"channels-and-bit-depth",style:{position:"relative"}},r.createElement(t.a,{href:"#channels-and-bit-depth","aria-label":"channels and bit depth permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Channels and bit depth"),"\n",r.createElement(t.p,null,'When referring to an image with "channels," we mean the separate color or intensity planes stored in that image. An 8-bit RGB image has three channels, each an 8-bit layer. A 16-bit RGB image doubles the per-channel precision. Meanwhile, hyper-spectral images can have tens or hundreds of channels, used in geospatial or medical contexts.'),"\n",r.createElement(t.p,null,"Bit depth is crucial for dynamic range. Many professional image pipelines (e.g., medical imaging) prefer 12-bit or 16-bit channels to capture subtle intensity variations without saturating. The trade-off, of course, is higher memory usage and potential computational overhead. In deep learning tasks, it is often beneficial to maintain higher precision in earlier pipeline stages if subtle variations matter for classification or detection."),"\n",r.createElement(t.h3,{id:"additional-remarks-on-image-representation",style:{position:"relative"}},r.createElement(t.a,{href:"#additional-remarks-on-image-representation","aria-label":"additional remarks on image representation permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Additional remarks on image representation"),"\n",r.createElement(t.p,null,"Image data is sometimes stored in row-major or column-major order. Software frameworks like OpenCV or TensorFlow might store channels last or channels first. This can affect indexing (e.g., ",r.createElement(s.A,{text:"\\( \\text{(height, width, channels)} \\)"})," vs. ",r.createElement(s.A,{text:"\\( \\text{(channels, height, width)} \\)"}),"). Always ensure you understand the memory layout, especially when bridging multiple libraries that each have different defaults."),"\n",r.createElement(t.h2,{id:"binarization-techniques",style:{position:"relative"}},r.createElement(t.a,{href:"#binarization-techniques","aria-label":"binarization techniques permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Binarization techniques"),"\n",r.createElement(t.p,null,'Binarization is the process of mapping a multi-level or color image to a two-level representation — often black and white (0 and 1). It is sometimes referred to as "thresholding." One sets an intensity threshold ',r.createElement(s.A,{text:"\\( t \\)"})," and assigns pixels with intensities above ",r.createElement(s.A,{text:"\\( t \\)"})," to one class (e.g., white) and below ",r.createElement(s.A,{text:"\\( t \\)"})," to another class (e.g., black). While seemingly simple, thresholding is an important technique for tasks like document analysis, license plate extraction, and shape detection."),"\n",r.createElement(t.h3,{id:"challenges-and-applications",style:{position:"relative"}},r.createElement(t.a,{href:"#challenges-and-applications","aria-label":"challenges and applications permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Challenges and applications"),"\n",r.createElement(t.p,null,"Binarization drastically reduces the information content of an image. A well-chosen threshold can make subsequent processing (e.g., connected components labeling, contour detection) simpler and more robust. However, a poorly chosen threshold can cause merges of distinct objects or fragmentation of single entities, thereby losing critical features."),"\n",r.createElement(t.p,null,"Binarization is especially prevalent in ",r.createElement(t.strong,null,"document processing"),", where text can be extracted from a background. In that domain, global thresholding might suffice for images with uniform illumination. However, real-world conditions frequently lead to non-uniform backgrounds — shadows, highlights, or local variations in illumination — where local binarization methods become essential."),"\n",r.createElement(t.h3,{id:"global-binarization",style:{position:"relative"}},r.createElement(t.a,{href:"#global-binarization","aria-label":"global binarization permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Global binarization"),"\n",r.createElement(t.p,null,"A ",r.createElement(t.strong,null,"global")," threshold-based approach uses the same ",r.createElement(s.A,{text:"\\( t \\)"})," across the entire image. The simplest approach picks ",r.createElement(s.A,{text:"\\( t \\)"})," heuristically. For instance, one can set ",r.createElement(s.A,{text:"\\( t \\)"})," to half the maximum dynamic range (",r.createElement(s.A,{text:"\\(127 \\)"})," for 8-bit grayscale) or derive it from statistical measures (e.g., using the mean or median intensity). A more robust approach is Otsu's method."),"\n",r.createElement(t.h4,{id:"otsus-method",style:{position:"relative"}},r.createElement(t.a,{href:"#otsus-method","aria-label":"otsus method permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Otsu's method"),"\n",r.createElement(t.p,null,"Otsu's method (Otsu, IEEE TSMC 1979) searches for a threshold ",r.createElement(s.A,{text:"\\( t \\)"})," that minimizes intra-class variance or maximizes inter-class variance. Assume a grayscale image with intensities from ",r.createElement(s.A,{text:"\\( 0 \\)"})," to ",r.createElement(s.A,{text:"\\( L-1 \\)"}),". Let ",r.createElement(s.A,{text:"\\( p(i) \\)"})," be the normalized histogram count for intensity ",r.createElement(s.A,{text:"\\( i \\)"}),". Then the probability of two classes (background ",r.createElement(s.A,{text:"\\( 0 \\)"})," and foreground ",r.createElement(s.A,{text:"\\( 1 \\)"}),"), for a threshold ",r.createElement(s.A,{text:"\\( t \\)"}),", is:"),"\n",r.createElement(s.A,{text:"\\[\n\\omega_0(t) = \\sum_{i=0}^{t-1} p(i), \n\\quad \n\\omega_1(t) = \\sum_{i=t}^{L-1} p(i).\n\\]"}),"\n",r.createElement(t.p,null,"The means of these classes (",r.createElement(s.A,{text:"\\( \\mu_0(t) \\)"})," and ",r.createElement(s.A,{text:"\\( \\mu_1(t) \\)"}),") and the global mean (",r.createElement(s.A,{text:"\\( \\mu_T \\)"}),") are:"),"\n",r.createElement(s.A,{text:"\\[\n\\mu_0(t) = \\frac{\\sum_{i=0}^{t-1} i\\, p(i)}{\\omega_0(t)}, \n\\quad\n\\mu_1(t) = \\frac{\\sum_{i=t}^{L-1} i\\, p(i)}{\\omega_1(t)},\n\\quad\n\\mu_T = \\sum_{i=0}^{L-1} i\\, p(i).\n\\]"}),"\n",r.createElement(t.p,null,"Otsu showed that maximizing the inter-class variance:"),"\n",r.createElement(s.A,{text:"\\[\n\\sigma_b^2(t) = \\omega_0(t)\\,\\omega_1(t)\\,[\\mu_0(t) - \\mu_1(t)]^2\n\\]"}),"\n",r.createElement(t.p,null,"is equivalent to minimizing the intra-class variance. The threshold that yields the largest ",r.createElement(s.A,{text:"\\( \\sigma_b^2(t) \\)"})," is the Otsu threshold ",r.createElement(s.A,{text:"\\( t^* \\)"}),". He also proposed a multi-threshold extension and noted that the method is akin to a 1D discrete variant of Fisher's linear discriminant analysis (LDA)."),"\n",r.createElement(t.p,null,"While Otsu's method is elegant and often works well for images with a bimodal intensity distribution, it can fail on images with heavy noise, uneven illumination, or more complex intensity histograms (Lee and gang, CVGIP 1990). Variations like ",r.createElement(t.strong,null,"two-dimensional Otsu")," (Jianzhuang and gang, 1991) consider a joint distribution of pixel intensity and local average to handle noisy scenarios better, though they come with increased computational cost."),"\n",r.createElement(t.p,null,"Below is a Python example showcasing Otsu's thresholding using ",r.createElement(l.A,null,"NumPy"),". This snippet uses a naive approach to find the threshold that maximizes inter-class variance:"),"\n",r.createElement(o.A,{text:'\nimport numpy as np\n\ndef otsu_threshold_naive(image):\n    """\n    image: 2D NumPy array (grayscale)\n    returns: threshold (int)\n    """\n    # Compute histogram\n    hist, bin_edges = np.histogram(image, bins=256, range=(0, 256))\n    total_pixels = image.size\n    \n    # Probabilities\n    p = hist / total_pixels\n    \n    best_threshold = 0\n    max_between_class_variance = -1\n    \n    # Precompute cumulative sums for faster iteration\n    cumulative_sum = np.cumsum(p)\n    cumulative_mean = np.cumsum(np.arange(256) * p)\n    global_mean = cumulative_mean[-1]\n    \n    for t in range(1, 256):\n        w0 = cumulative_sum[t-1]\n        w1 = 1 - w0\n        if w0 < 1e-6 or w1 < 1e-6:\n            # avoid division by zero\n            continue\n        \n        mu0 = cumulative_mean[t-1] / w0\n        mu1 = (global_mean - cumulative_mean[t-1]) / w1\n        \n        # inter-class variance\n        between_var = w0 * w1 * (mu0 - mu1)**2\n        \n        if between_var > max_between_class_variance:\n            max_between_class_variance = between_var\n            best_threshold = t\n    \n    return best_threshold\n'}),"\n",r.createElement(t.p,null,"This straightforward method can be replaced with optimized versions in OpenCV (",r.createElement(l.A,null,"cv2.threshold(image, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)"),") or scikit-image (",r.createElement(l.A,null,"skimage.filters.threshold_otsu"),")."),"\n",r.createElement(t.h3,{id:"local-adaptive-binarization",style:{position:"relative"}},r.createElement(t.a,{href:"#local-adaptive-binarization","aria-label":"local adaptive binarization permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Local (adaptive) binarization"),"\n",r.createElement(t.p,null,"Real-world images often have non-uniform lighting: some areas are brighter, some are darker. A single threshold might not suffice. Local methods compute a threshold for each pixel based on its neighborhood. This approach is sometimes termed ",r.createElement(t.strong,null,"adaptive")," thresholding. Common local binarization methods include:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Adaptive Gaussian thresholding"),": A threshold is computed per local region (often a block of size ",r.createElement(s.A,{text:"\\(w \\times w\\)"}),")."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Bernsen's method"),": In each local window, compute ",r.createElement(s.A,{text:"\\( \\text{min} \\)"})," and ",r.createElement(s.A,{text:"\\( \\text{max} \\)"})," intensities. The local threshold is ",r.createElement(s.A,{text:"\\( ( \\text{max} + \\text{min} ) / 2 \\)"}),"."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Niblack's method"),": Threshold is ",r.createElement(s.A,{text:"\\( \\mu(x,y) + k \\, s(x,y) \\)"}),", where ",r.createElement(s.A,{text:"\\( \\mu(x,y) \\)"})," is the mean intensity in a local window around ",r.createElement(s.A,{text:"\\( (x,y) \\)"}),", ",r.createElement(s.A,{text:"\\( s(x,y) \\)"})," is the standard deviation, and ",r.createElement(s.A,{text:"\\( k \\)"})," is an empirically chosen constant (e.g., -0.2 if the foreground is dark)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Bradley–Roth"),": Uses integral images to accelerate local summations. Each local threshold is the average intensity in a neighborhood minus some fraction (like 15%)."),"\n"),"\n",r.createElement(t.p,null,'When combined with morphological filtering or denoising, local binarization can excel at tasks where simple global thresholding fails. However, local thresholding may introduce "artificial boundaries" or small gaps if the window size is ill-chosen for the scale of the object, or if the image is extremely textured. Tuning the local window size, overlap, and constants (like ',r.createElement(s.A,{text:"\\( k \\)"})," in Niblack's approach) becomes critical."),"\n",r.createElement(t.h3,{id:"common-binarization-methods-in-practice",style:{position:"relative"}},r.createElement(t.a,{href:"#common-binarization-methods-in-practice","aria-label":"common binarization methods in practice permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Common binarization methods in practice"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Otsu's method"),": The classical global approach, good for many well-lit or nearly bimodal images."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Adaptive thresholding")," (mean or Gaussian): Often found in open-source libraries for images with moderate background variations."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Bernsen"),': Simple to implement, but can produce "phantom" noise in uniform areas.'),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Niblack")," and modifications (e.g., Sauvola, Wolf, NICK, Bradley–Roth): Very common in document processing pipelines, ID card scanning, or meter reading in low-light conditions."),"\n"),"\n",r.createElement(t.p,null,"The local approaches make sense when the background or foreground distribution is not homogeneous across the image. They remain a vibrant area of research, as every new real-world scenario (high dynamic range, night-vision, glare, shadows) brings unique binarization challenges."),"\n",r.createElement(t.h2,{id:"image-enhancement-and-preprocessing",style:{position:"relative"}},r.createElement(t.a,{href:"#image-enhancement-and-preprocessing","aria-label":"image enhancement and preprocessing permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Image enhancement and preprocessing"),"\n",r.createElement(t.p,null,"In many computer vision tasks, images acquired from sensors can be noisy, low-contrast, blurred, or corrupted by external factors like lens distortion and lighting variations. Image enhancement attempts to improve image quality, making subsequent feature extraction or classification steps easier and more accurate."),"\n",r.createElement(t.h3,{id:"noise-reduction-filtering-and-smoothing",style:{position:"relative"}},r.createElement(t.a,{href:"#noise-reduction-filtering-and-smoothing","aria-label":"noise reduction filtering and smoothing permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Noise reduction (filtering and smoothing)"),"\n",r.createElement(t.p,null,"Noise manifests in different ways: salt-and-pepper noise (random white or black pixels), Gaussian noise (arising from sensor electronics), speckle noise (in radar or ultrasound). Common noise reduction techniques include:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Averaging filter")," or box filter: Each pixel is replaced by the average of a ",r.createElement(s.A,{text:"\\( k \\times k \\)"})," neighborhood, effectively blurring the image."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Gaussian filter"),": A weighted average giving more weight to closer neighbors."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Median filter"),": Very effective at removing salt-and-pepper noise while preserving edges better than a box filter."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Bilateral filter"),": Preserves edges by also considering intensity differences between neighboring pixels."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Non-local means")," (Buades and gang, CVPR 2005): A more advanced technique that compares patches to reduce noise while maintaining structure."),"\n"),"\n",r.createElement(t.p,null,"Smoothing can remove small artifacts but must be used judiciously since oversmoothing can destroy sharp edges and degrade important structural details."),"\n",r.createElement(t.p,null,"Below is an example of applying a median filter in Python with OpenCV:"),"\n",r.createElement(o.A,{text:'\nimport cv2\nimport numpy as np\n\ndef denoise_median(image, kernel_size=3):\n    """\n    Apply a median filter to remove salt-and-pepper noise.\n    image: 2D or 3D NumPy array\n    kernel_size: must be odd, e.g., 3, 5, 7\n    """\n    return cv2.medianBlur(image, kernel_size)\n'}),"\n",r.createElement(t.h3,{id:"contrast-and-brightness-adjustments",style:{position:"relative"}},r.createElement(t.a,{href:"#contrast-and-brightness-adjustments","aria-label":"contrast and brightness adjustments permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Contrast and brightness adjustments"),"\n",r.createElement(t.p,null,"Contrast affects how large the difference in intensity or color is between the darkest and brightest parts of an image. In machine learning workflows, it is often necessary to correct for under-exposure or over-exposure before features are extracted."),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Linear transformations"),": ",r.createElement(s.A,{text:"\\( I_\\text{new} = \\alpha\\,I_\\text{old} + \\beta \\)"}),". The slope ",r.createElement(s.A,{text:"\\( \\alpha \\)"})," modifies contrast, and the intercept ",r.createElement(s.A,{text:"\\( \\beta \\)"})," modifies brightness."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Gamma correction"),": ",r.createElement(s.A,{text:"\\( I_\\text{out} = I_\\text{in}^{\\gamma} \\)"}),". Non-linear transformations are used to reduce or boost mid-intensity ranges."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"CLAHE")," (Contrast Limited Adaptive Histogram Equalization): A specialized technique that adaptively improves local contrast without amplifying noise excessively (Zuiderveld, 1994)."),"\n"),"\n",r.createElement(t.h3,{id:"histogram-equalization",style:{position:"relative"}},r.createElement(t.a,{href:"#histogram-equalization","aria-label":"histogram equalization permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Histogram equalization"),"\n",r.createElement(t.p,null,"Histogram equalization redistributes intensity values so they occupy a broader range. The transform ",r.createElement(s.A,{text:"\\( T \\)"})," is chosen such that the resulting histogram becomes (approximately) uniform. On a grayscale image ",r.createElement(s.A,{text:"\\( f \\)"}),", a common approach uses the cumulative distribution function (CDF) of the input intensities. In the discrete case, for a pixel intensity ",r.createElement(s.A,{text:"\\( r \\)"}),":"),"\n",r.createElement(s.A,{text:"\\[\nT(r) = (L - 1)\\sum_{i=0}^{r} p(i),\n\\]"}),"\n",r.createElement(t.p,null,"where ",r.createElement(s.A,{text:"\\( L \\)"})," is the number of possible intensity levels, and ",r.createElement(s.A,{text:"\\( p(i) \\)"})," is the PDF of intensities in ",r.createElement(s.A,{text:"\\( f \\)"}),". The result is that the intensities in the new image ",r.createElement(s.A,{text:"\\( g \\)"}),' are more evenly spread. This technique can dramatically improve the visibility of details in a low-contrast image, though it may "wash out" certain regions or amplify noise in others.'),"\n",r.createElement(t.h3,{id:"morphological-operations-erosion-dilation-opening-closing",style:{position:"relative"}},r.createElement(t.a,{href:"#morphological-operations-erosion-dilation-opening-closing","aria-label":"morphological operations erosion dilation opening closing permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Morphological operations (erosion, dilation, opening, closing)"),"\n",r.createElement(t.p,null,"Morphological filters originate from mathematical morphology, used extensively in binary images but applicable to grayscale and color. The primary morphological operations are:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Erosion"),": Shrinks foreground regions by removing boundary pixels. This is done by sliding a structuring element (e.g., a 3×3 square) over the image; a pixel is kept only if all corresponding pixels under the structuring element are foreground."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Dilation"),": Grows foreground regions by adding pixels to object boundaries."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Opening"),": An erosion followed by dilation. Used to remove small noise objects (foreground) while largely preserving the shape of bigger objects."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Closing"),": A dilation followed by erosion. Used to fill small holes in foreground objects."),"\n"),"\n",r.createElement(t.p,null,"For instance, an opening operation might be beneficial after binarization to remove specks of noise, while a closing might be used to smooth the boundaries of large text or shapes."),"\n",r.createElement(t.h3,{id:"edge-detection-and-feature-sharpening",style:{position:"relative"}},r.createElement(t.a,{href:"#edge-detection-and-feature-sharpening","aria-label":"edge detection and feature sharpening permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Edge detection and feature sharpening"),"\n",r.createElement(t.p,null,"Edges represent discontinuities in intensity or color, signifying boundaries of objects in an image. Classical detectors include:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Sobel"),": Computes horizontal and vertical gradients."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Prewitt"),": Similar principle to Sobel, slightly different kernels."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Canny"),": A multi-stage process that includes smoothing, gradient computation, non-maximum suppression, and hysteresis thresholding. Widely used for robust edge detection."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Laplacian"),": Computes the second derivative; often used in conjunction with Gaussian smoothing (LoG filter)."),"\n"),"\n",r.createElement(t.p,null,'For sharpening, unsharp masking is a popular approach, generating a "mask" of edges via a high-pass filter and adding it back to the original image.'),"\n",r.createElement(t.p,null,"Combining these techniques yields powerful pre-processing pipelines for tasks like digit recognition on meter displays, medical image analysis (MRI, CT scans), or robust shape-based object detection."),"\n",r.createElement(t.h2,{id:"color-processing-and-colorization",style:{position:"relative"}},r.createElement(t.a,{href:"#color-processing-and-colorization","aria-label":"color processing and colorization permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Color processing and colorization"),"\n",r.createElement(t.p,null,"Color can be an extremely informative feature for classification or segmentation. Color-based segmentation, for instance, might simplify detection of fruits in orchard images or lane markings on roads."),"\n",r.createElement(t.h3,{id:"color-spaces-conversion-transformations",style:{position:"relative"}},r.createElement(t.a,{href:"#color-spaces-conversion-transformations","aria-label":"color spaces conversion transformations permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Color spaces (conversion, transformations)"),"\n",r.createElement(t.p,null,"Beyond RGB, many color spaces exist to simplify certain tasks:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"HSV"),': Separates the color\'s hue (the "type" of color) from its saturation (purity) and value (brightness). To detect objects by color alone, focusing on hue can be easier than dealing with all three RGB channels.'),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Lab"),": Often used where perceptual uniformity is important. Distances in Lab space can better approximate how the human eye perceives color differences."),"\n"),"\n",r.createElement(t.p,null,"Conversion among color spaces is typically handled with known transformations. For example, from RGB to HSV, each pixel transforms with a set of piecewise equations to determine hue (an angle in [0, 360)), saturation, and value."),"\n",r.createElement(t.h3,{id:"image-colorization-techniques",style:{position:"relative"}},r.createElement(t.a,{href:"#image-colorization-techniques","aria-label":"image colorization techniques permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Image colorization techniques"),"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"Image colorization")," is the process of converting a grayscale image to a color image, typically by inferring plausible or context-relevant hues. In classical algorithms, colorization might be partially manual (e.g., scribble-based methods) or rely on user input for color hints. Modern deep learning approaches (e.g., Zhang and gang, ECCV 2016) treat colorization as a regression or classification problem in a color space such as Lab, training on large datasets of color images to learn plausible color assignments."),"\n",r.createElement(t.p,null,"Colorization can be used to restore old black-and-white photos or highlight features in medical or scientific images. In machine learning, colorization is often approached as an ",r.createElement(t.strong,null,"auxiliary self-supervised task"),', where the network "learns to colorize" as a way of learning robust feature representations (Larsson and gang, ECCV 2016).'),"\n",r.createElement(t.h3,{id:"applications-of-colorization-in-machine-learning",style:{position:"relative"}},r.createElement(t.a,{href:"#applications-of-colorization-in-machine-learning","aria-label":"applications of colorization in machine learning permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Applications of colorization in machine learning"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Data augmentation"),": Synthetic recoloring can enrich training sets by simulating lighting conditions or object color variations."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Self-supervised representation learning"),": Large unlabeled image sets can train colorization networks. The learned representations can then be transferred to downstream tasks (e.g., classification)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Artistic style transfer"),": Combined with techniques from style transfer to produce novel color schemes (e.g., painting from one image's color palette onto another)."),"\n"),"\n",r.createElement(t.h2,{id:"feature-extraction-and-representation",style:{position:"relative"}},r.createElement(t.a,{href:"#feature-extraction-and-representation","aria-label":"feature extraction and representation permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Feature extraction and representation"),"\n",r.createElement(t.p,null,"Feature extraction is central to traditional computer vision pipelines. Before the era of deep learning's convolutional encoders, practitioners relied on carefully designed local descriptors or global features."),"\n",r.createElement(t.h3,{id:"keypoint-detectors-and-descriptors-sift-surf-orb",style:{position:"relative"}},r.createElement(t.a,{href:"#keypoint-detectors-and-descriptors-sift-surf-orb","aria-label":"keypoint detectors and descriptors sift surf orb permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Keypoint detectors and descriptors (SIFT, SURF, ORB)"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"SIFT")," (Scale-Invariant Feature Transform; Lowe, IJCV 2004): Detects local keypoints in scale-space, robust to changes in scale, rotation, and moderate affine transformations. Each keypoint is described by a histogram of gradient orientations."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"SURF")," (Speeded-Up Robust Features): A faster approximation of SIFT's operator, using integral images and a box-filter approach for the scale-space representation."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"ORB")," (Oriented FAST and Rotated BRIEF; Rublee and gang, ICCV 2011): A fast keypoint descriptor that uses a corner detection approach (FAST) and a binary descriptor (BRIEF). Very efficient for real-time or embedded vision tasks."),"\n"),"\n",r.createElement(t.p,null,"Keypoints and descriptors remain relevant, especially for classical tasks like image matching, panorama stitching, or 3D reconstruction from multiple views. Even with deep features, SIFT-like methods can be simpler for certain geometry-based tasks."),"\n",r.createElement(t.h3,{id:"texture-features-glcm-lbp",style:{position:"relative"}},r.createElement(t.a,{href:"#texture-features-glcm-lbp","aria-label":"texture features glcm lbp permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Texture features (GLCM, LBP)"),"\n",r.createElement(t.p,null,"Texture refers to repeating patterns, granularity, or local variations in intensity. Two common texture descriptors are:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"GLCM")," (Gray-Level Co-occurrence Matrix): Captures how often pairs of intensity values occur at certain spatial offsets. Common statistical measures (contrast, energy, homogeneity, correlation) quantify the texture."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"LBP")," (Local Binary Patterns): A pixel's neighborhood is thresholded around the central pixel, creating a binary pattern. Summarizing these binary patterns yields a compact descriptor for texture classification."),"\n"),"\n",r.createElement(t.h3,{id:"shape-features-and-region-descriptors",style:{position:"relative"}},r.createElement(t.a,{href:"#shape-features-and-region-descriptors","aria-label":"shape features and region descriptors permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Shape features and region descriptors"),"\n",r.createElement(t.p,null,"In shape-based tasks, we often extract contours or boundaries after binarization or segmentation. Then we can compute:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Hu moments"),": Seven moment invariants that remain relatively stable under translation, rotation, and scale."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Fourier descriptors"),": A closed contour's shape can be approximated by a series expansion in the Fourier domain."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Region properties"),": Eccentricity, circularity, aspect ratio, etc., derived from raw or segmented objects."),"\n"),"\n",r.createElement(t.h3,{id:"dimensionality-reduction-pca-t-sne-umap",style:{position:"relative"}},r.createElement(t.a,{href:"#dimensionality-reduction-pca-t-sne-umap","aria-label":"dimensionality reduction pca t sne umap permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Dimensionality reduction (PCA, t-SNE, UMAP)"),"\n",r.createElement(t.p,null,"Once features are extracted (e.g., from SIFT or GLCM), we might want to reduce dimensionality to highlight discriminative aspects. This can help with visualization or reduce computational overhead in classification:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"PCA")," (Principal Component Analysis): Linear method that projects data onto directions of maximum variance."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"t-SNE")," (t-Distributed Stochastic Neighbor Embedding): Non-linear method that preserves local distances and is often used for cluster visualization in 2D or 3D."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"UMAP")," (Uniform Manifold Approximation and Projection): Another non-linear approach that preserves global structure better than t-SNE in some cases, while also offering faster performance."),"\n"),"\n",r.createElement(t.h2,{id:"image-segmentation-and-object-detection",style:{position:"relative"}},r.createElement(t.a,{href:"#image-segmentation-and-object-detection","aria-label":"image segmentation and object detection permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Image segmentation and object detection"),"\n",r.createElement(t.p,null,"Segmentation partitions an image into regions that share some similarity criterion (e.g., intensity, texture, color). Object detection locates and classifies instances of interest in the scene (e.g., bounding boxes of vehicles in a traffic camera feed)."),"\n",r.createElement(t.h3,{id:"threshold-based-segmentation",style:{position:"relative"}},r.createElement(t.a,{href:"#threshold-based-segmentation","aria-label":"threshold based segmentation permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Threshold-based segmentation"),"\n",r.createElement(t.p,null,"When objects can be separated by intensity, thresholding alone may suffice. Multi-threshold methods can create multi-labeled segmentation results. In volumetric medical data, thresholding can highlight certain tissue densities. However, real-world images frequently require more sophisticated methods due to noise, variable illumination, or overlapping intensity distributions."),"\n",r.createElement(t.h3,{id:"clustering-based-segmentation-k-means-mean-shift",style:{position:"relative"}},r.createElement(t.a,{href:"#clustering-based-segmentation-k-means-mean-shift","aria-label":"clustering based segmentation k means mean shift permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Clustering-based segmentation (k-means, mean shift)"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"k-means"),": You can treat each pixel's color or intensity as a vector in ",r.createElement(s.A,{text:"\\( \\mathbb{R}^d \\)"})," (depending on the color space). The algorithm partitions the image into k clusters. Each cluster can be assigned a unique label, effectively segmenting. This approach is intuitive and easy to implement but can fail if k is poorly chosen or if the distribution of intensities is complex."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Mean shift"),": Iteratively shifts each pixel toward the densest region of points within a kernel window. Regions that converge to the same density mode are clustered. ",r.createElement(l.A,null,"Mean shift")," can adapt to complex, arbitrarily shaped clusters but can be computationally heavier."),"\n"),"\n",r.createElement(t.h3,{id:"region-based-segmentation-watershed-region-growing",style:{position:"relative"}},r.createElement(t.a,{href:"#region-based-segmentation-watershed-region-growing","aria-label":"region based segmentation watershed region growing permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Region-based segmentation (watershed, region growing)"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Watershed"),': Imagines the image as a topographic surface. "Watersheds" form the boundaries between "catchment basins." The technique can over-segment unless you specify markers or use advanced modifications.'),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Region growing"),": Starts from seed points and merges pixels or regions that meet similarity criteria (e.g., intensity difference below a threshold). This can yield highly controllable segmentation but depends on good initial seeds."),"\n"),"\n",r.createElement(t.h3,{id:"object-detection-and-localization-traditional-vs-deep-learning",style:{position:"relative"}},r.createElement(t.a,{href:"#object-detection-and-localization-traditional-vs-deep-learning","aria-label":"object detection and localization traditional vs deep learning permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Object detection and localization (traditional vs. deep learning)"),"\n",r.createElement(t.p,null,"Traditional object detection methods often rely on carefully handcrafted features (e.g., HOG, Haar cascades) and sliding-window approaches. They are still used in embedded or real-time settings due to efficiency or interpretability. However, the modern wave of object detection relies heavily on convolutional neural networks:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Faster R-CNN"),": A region-proposal architecture that classifies bounding boxes."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"YOLO")," (You Only Look Once) or ",r.createElement(t.strong,null,"SSD")," (Single Shot Detector): Predict bounding boxes directly from the feature maps."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Transformers-based")," object detectors (Carion and gang, ECCV 2020) leverage self-attention to refine bounding boxes in a more global context."),"\n"),"\n",r.createElement(t.p,null,"Even though we are primarily discussing image processing fundamentals here, it is crucial to note that many modern detection pipelines still integrate classical pre-processing steps: resizing, color normalization, sometimes morphological operations if the domain is specialized (e.g., medical images with specific tissue structures)."),"\n",r.createElement(t.h2,{id:"mldl-integration",style:{position:"relative"}},r.createElement(t.a,{href:"#mldl-integration","aria-label":"mldl integration permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"ML/DL integration"),"\n",r.createElement(t.h3,{id:"image-data-preprocessing-for-machine-learning",style:{position:"relative"}},r.createElement(t.a,{href:"#image-data-preprocessing-for-machine-learning","aria-label":"image data preprocessing for machine learning permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Image data preprocessing for machine learning"),"\n",r.createElement(t.p,null,"Most machine learning algorithms, especially classical ML (SVM, random forests, logistic regression), expect tabular or vector inputs. Images must be vectorized or have their features extracted. With deep learning, raw images can be fed into a neural network after minimal transformations (resizing, normalization). However, advanced practitioners still use image processing (e.g., noise removal, color space transforms) to reduce domain-specific artifacts or to unify lighting conditions."),"\n",r.createElement(t.h3,{id:"classical-ml-algorithms-for-image-classification",style:{position:"relative"}},r.createElement(t.a,{href:"#classical-ml-algorithms-for-image-classification","aria-label":"classical ml algorithms for image classification permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Classical ML algorithms for image classification"),"\n",r.createElement(t.p,null,"Before the deep learning boom, a pipeline might look like:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Preprocess")," (denoise, normalize)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Extract features")," (SIFT descriptors, color histograms, GLCM texture features, etc.)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Represent")," them in a suitable dimensionality (possibly after PCA or LDA)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Train")," a classifier (SVM, random forest)."),"\n"),"\n",r.createElement(t.p,null,"This remains entirely valid for smaller datasets or real-time applications where neural networks might be too large or slow. Tools like scikit-learn can handle these tasks efficiently."),"\n",r.createElement(t.h3,{id:"transfer-learning-for-image-tasks",style:{position:"relative"}},r.createElement(t.a,{href:"#transfer-learning-for-image-tasks","aria-label":"transfer learning for image tasks permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Transfer learning for image tasks"),"\n",r.createElement(t.p,null,"Deep convolutional networks pre-trained on large datasets (e.g., ImageNet) are frequently fine-tuned for new tasks. Even so, pre-processing like histogram equalization or color normalization can reduce domain gaps (e.g., a medical dataset might have different intensity distributions than ImageNet's natural images)."),"\n",r.createElement(t.h3,{id:"data-augmentation",style:{position:"relative"}},r.createElement(t.a,{href:"#data-augmentation","aria-label":"data augmentation permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Data augmentation"),"\n",r.createElement(t.p,null,"Augmenting training images with random transformations can significantly improve generalization:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"Geometric transformations: rotations, translations, flips, perspective warping."),"\n",r.createElement(t.li,null,"Color transformations: random brightness, contrast, saturation, or hue changes."),"\n",r.createElement(t.li,null,"Noise injection: additive Gaussian noise, salt-and-pepper."),"\n",r.createElement(t.li,null,"Cutout or Mixup: advanced augmentation strategies that mask random portions or mix images at the pixel level."),"\n"),"\n",r.createElement(t.p,null,"For segmentation or detection tasks, these augmentations must be applied consistently to images and label maps (segmentation masks, bounding boxes, etc.)."),"\n",r.createElement(t.h3,{id:"specific-evaluation-metrics-eg-iou",style:{position:"relative"}},r.createElement(t.a,{href:"#specific-evaluation-metrics-eg-iou","aria-label":"specific evaluation metrics eg iou permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Specific evaluation metrics (e.g., IoU)"),"\n",r.createElement(t.p,null,"For tasks like segmentation, the Intersection over Union (IoU) or Jaccard Index is a standard measure:"),"\n",r.createElement(s.A,{text:"\\[\n\\text{IoU} = \\frac{\\text{Area of Overlap}}{\\text{Area of Union}}\n\\]"}),"\n",r.createElement(t.p,null,"In classification tasks, accuracy or F1-score might suffice, but segmentation demands more region-based or pixel-based metrics. For object detection, metrics like mean Average Precision (mAP) at certain IoU thresholds are common."),"\n",r.createElement(t.h2,{id:"extra-considerations-and-deeper-dives",style:{position:"relative"}},r.createElement(t.a,{href:"#extra-considerations-and-deeper-dives","aria-label":"extra considerations and deeper dives permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Extra considerations and deeper dives"),"\n",r.createElement(t.h3,{id:"advanced-binarization-research",style:{position:"relative"}},r.createElement(t.a,{href:"#advanced-binarization-research","aria-label":"advanced binarization research permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Advanced binarization research"),"\n",r.createElement(t.p,null,"Numerous specialized thresholding algorithms exist that incorporate gradient information, entropy-based thresholds, or region merging. For instance, methods relying on local gradient distribution first compute gradient magnitudes and then select thresholds based on the distribution of those gradients (sometimes integrated with Otsu-like cost functions). Another set of approaches uses entropy or mutual information as a measure for an optimal threshold (Kapur and gang, Computer Vision, Graphics, and Image Processing, 1985)."),"\n",r.createElement(t.p,null,"As documents or real-world scenes become more challenging (e.g., images with curved surfaces, extreme lighting, or partial occlusions), binarization research continuously evolves. Some newer approaches incorporate morphological scale-space analysis or even small neural networks that adapt thresholds locally, bridging classical image processing and deep learning."),"\n",r.createElement(t.h3,{id:"integral-images-and-fast-local-operations",style:{position:"relative"}},r.createElement(t.a,{href:"#integral-images-and-fast-local-operations","aria-label":"integral images and fast local operations permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Integral images and fast local operations"),"\n",r.createElement(t.p,null,"The concept of ",r.createElement(t.strong,null,"integral images")," (also known as summed area tables) is key to accelerating many local operations (Viola & Jones, CVPR 2001). Instead of performing direct convolution or summation in each local region, integral images allow constant-time retrieval of sums within rectangular regions. This principle underpins fast local thresholding (Bradley–Roth binarization) and speeds up box-filtering in algorithms like SURF and many real-time object detection systems."),"\n",r.createElement(t.p,null,"If an image is ",r.createElement(s.A,{text:"\\( I(x,y) \\)"}),", the integral image ",r.createElement(s.A,{text:"\\( S(x,y) \\)"})," is given by:"),"\n",r.createElement(s.A,{text:"\\[\nS(x,y) = \\sum_{i=0}^{x} \\sum_{j=0}^{y} I(i,j).\n\\]"}),"\n",r.createElement(t.p,null,"One can compute it efficiently via a recursive relationship:"),"\n",r.createElement(s.A,{text:"\\[\nS(x,y) = I(x,y) + S(x-1,y) + S(x,y-1) - S(x-1,y-1).\n\\]"}),"\n",r.createElement(t.p,null,"Then, the sum of a rectangular region ",r.createElement(s.A,{text:"\\( (x_1,y_1) \\)"})," to ",r.createElement(s.A,{text:"\\( (x_2,y_2) \\)"})," can be computed as:"),"\n",r.createElement(s.A,{text:"\\[\n\\text{Sum} = S(x_2,y_2) - S(x_1-1,y_2) - S(x_2,y_1-1) + S(x_1-1,y_1-1).\n\\]"}),"\n",r.createElement(t.p,null,"This reduces local summation from ",r.createElement(s.A,{text:"\\( O(k^2) \\)"})," to ",r.createElement(s.A,{text:"\\( O(1) \\)"})," after a single pass to build ",r.createElement(s.A,{text:"\\( S(x,y) \\)"}),". Bradley–Roth thresholding and many other adaptive methods leverage this for large performance gains."),"\n",r.createElement(t.h3,{id:"hybrid-methods-and-morphological-binarization",style:{position:"relative"}},r.createElement(t.a,{href:"#hybrid-methods-and-morphological-binarization","aria-label":"hybrid methods and morphological binarization permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Hybrid methods and morphological binarization"),"\n",r.createElement(t.p,null,'Some pipelines combine local thresholding with morphological steps to "clean up" the result. For instance:'),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,"Estimate a local threshold using Bernsen's approach."),"\n",r.createElement(t.li,null,"Binarize the image."),"\n",r.createElement(t.li,null,"Apply an opening to remove tiny noise or separate close objects."),"\n",r.createElement(t.li,null,"Optionally apply a closing to fill small gaps in the objects of interest."),"\n"),"\n",r.createElement(t.p,null,"Additionally, multi-stage morphological binarization can rely on connected components analysis to remove extraneous regions that do not meet size or shape criteria (e.g., in text detection, discard all connected components smaller than a minimum pixel area)."),"\n",r.createElement(t.h3,{id:"connections-with-deep-learning",style:{position:"relative"}},r.createElement(t.a,{href:"#connections-with-deep-learning","aria-label":"connections with deep learning permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Connections with deep learning"),"\n",r.createElement(t.p,null,"While deep convolutional networks can learn filters and segmentation masks end-to-end, classical image processing remains relevant. Often, an ",r.createElement(l.A,null,"ensemble approach")," combining classical methods and deep networks outperforms a purely neural approach when domain knowledge is strong (Cheng and gang, IEEE TMI 2020). For example, in certain forms of medical imaging:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"A pre-processing step might remove scanner artifacts or standardize intensities."),"\n",r.createElement(t.li,null,"Classical morphological operations might isolate an anatomical region."),"\n",r.createElement(t.li,null,"Then, a neural network is applied to classify or detect pathologies in that region."),"\n"),"\n",r.createElement(t.p,null,"This multi-stage design can reduce spurious false positives and improve interpretability."),"\n",r.createElement(t.h2,{id:"putting-it-all-together",style:{position:"relative"}},r.createElement(t.a,{href:"#putting-it-all-together","aria-label":"putting it all together permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Putting it all together"),"\n",r.createElement(t.p,null,"Building an end-to-end image processing pipeline for a real-world ML application might look like this:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Data collection"),": Gather images from cameras or other sensors. Possibly store them in a compressed format like PNG or JPEG."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Reading and conversions"),": Load images into arrays, maybe convert from BGR to RGB or to grayscale if color is not essential."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Enhancement"),": Remove noise (e.g., median filter or bilateral filter), adjust contrast or brightness if needed, or standardize the color distribution."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Segmentation or binarization"),": Depending on the task (e.g., reading meter digits), use local thresholding (Niblack, Bernsen, Bradley–Roth) or advanced morphological methods. Possibly combine multiple thresholds for multi-region segmentation."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Feature extraction"),": For classical ML, compute SIFT descriptors or GLCM-based texture features; for deep learning, maybe skip manual feature extraction or incorporate a basic morphological step first."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Model training/inference"),": Train your classification, detection, or recognition model. In the deep learning approach, data augmentation is integrated here to ensure robust training."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Evaluation"),": Use metrics like accuracy, F1-score for classification, IoU or mAP for segmentation/detection, etc."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Iteration"),": Tweak your enhancement parameters or model architecture based on performance or domain constraints."),"\n"),"\n",r.createElement(t.p,null,"Beyond these fundamentals, advanced methods continue to emerge, especially at the intersection of classical image processing and deep learning. Techniques such as unsupervised denoising with autoencoders, or combining morphological operators with differentiable modules within a neural network, are active research fronts (e.g., Diamond and gang, NeurIPS 2017)."),"\n",r.createElement(t.p,null,"For industrial or large-scale data pipelines, keep an eye on computational efficiency. High-volume tasks might require GPU-accelerated morphological filters or specialized libraries. Tools like OpenCV, scikit-image, TorchVision, and GPU-based libraries (e.g., NVIDIA VPI) all help accelerate standard image processing operations."),"\n",r.createElement(t.p,null,"In summary, image processing remains a vibrant, foundational domain in machine learning. Although deep neural networks can learn end-to-end from raw images, an understanding of thresholding, morphological filtering, color transformations, and classical feature extraction remains essential for building robust real-world solutions. By integrating these image processing fundamentals with advanced ML algorithms, practitioners can craft pipelines that handle diverse and challenging visual tasks reliably and efficiently."),"\n",r.createElement(n,{alt:"image-processing-flow",path:"",caption:"A conceptual diagram of an image processing pipeline, from reading raw data to applying morphological operations and advanced feature extraction.",zoom:"false"}))}var m=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,i.RP)(),e.components);return t?r.createElement(t,e,r.createElement(c,e)):c(e)};var h=n(36710),d=n(58481),g=n.n(d),u=n(36310),p=n(87245),f=n(27042),v=n(59849),b=n(5591),E=n(61122),y=n(9219),w=n(33203),x=n(95751),S=n(94328),k=n(80791),H=n(78137);const z=e=>{let{toc:t}=e;if(!t||!t.items)return null;return r.createElement("nav",{className:k.R},r.createElement("ul",null,t.items.map(((e,t)=>r.createElement("li",{key:t},r.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const n=t.replace("#",""),a=document.getElementById(n);a&&a.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&r.createElement(z,{toc:{items:e.items}}))))))};function C(e){let{data:{mdx:t,allMdx:l,allPostImages:o},children:s}=e;const{frontmatter:c,body:m,tableOfContents:h}=t,d=c.index,v=c.slug.split("/")[1],k=l.nodes.filter((e=>e.frontmatter.slug.includes(`/${v}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),C=k.findIndex((e=>e.frontmatter.index===d)),_=k[C+1],A=k[C-1],T=c.slug.replace(/\/$/,""),I=/[^/]*$/.exec(T)[0],M=`posts/${v}/content/${I}/`,{0:V,1:B}=(0,r.useState)(c.flagWideLayoutByDefault),{0:L,1:N}=(0,r.useState)(!1);var P;(0,r.useEffect)((()=>{N(!0);const e=setTimeout((()=>N(!1)),340);return()=>clearTimeout(e)}),[V]),"adventures"===v?P=y.cb:"research"===v?P=y.Qh:"thoughts"===v&&(P=y.T6);const O=g()(m).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,R=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),n=e%60;return n<=30?`~${t}${n>0?".5":""} h`:`~${t+1} h`}(Math.ceil(O/P)+(c.extraReadTimeMin||0)),G=[{flag:c.flagDraft,component:()=>Promise.all([n.e(3231),n.e(8809)]).then(n.bind(n,28809))},{flag:c.flagMindfuckery,component:()=>Promise.all([n.e(3231),n.e(2471)]).then(n.bind(n,67709))},{flag:c.flagRewrite,component:()=>Promise.all([n.e(3231),n.e(6764)]).then(n.bind(n,62002))},{flag:c.flagOffensive,component:()=>Promise.all([n.e(3231),n.e(2443)]).then(n.bind(n,17681))},{flag:c.flagProfane,component:()=>Promise.all([n.e(3231),n.e(8048)]).then(n.bind(n,53286))},{flag:c.flagMultilingual,component:()=>Promise.all([n.e(3231),n.e(4069)]).then(n.bind(n,78831))},{flag:c.flagUnreliably,component:()=>Promise.all([n.e(3231),n.e(3417)]).then(n.bind(n,8179))},{flag:c.flagPolitical,component:()=>Promise.all([n.e(3231),n.e(5195)]).then(n.bind(n,30433))},{flag:c.flagCognitohazard,component:()=>Promise.all([n.e(3231),n.e(3175)]).then(n.bind(n,8413))},{flag:c.flagHidden,component:()=>Promise.all([n.e(3231),n.e(9556)]).then(n.bind(n,14794))}],{0:q,1:j}=(0,r.useState)([]);return(0,r.useEffect)((()=>{G.forEach((e=>{let{flag:t,component:n}=e;t&&n().then((e=>{j((t=>[].concat((0,a.A)(t),[e.default])))}))}))}),[]),r.createElement(f.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},r.createElement(b.A,{postNumber:c.index,date:c.date,updated:c.updated,readTime:R,difficulty:c.difficultyLevel,title:c.title,desc:c.desc,banner:c.banner,section:v,postKey:I,isMindfuckery:c.flagMindfuckery,mainTag:c.mainTag}),r.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},c.otherTags.map(((e,t)=>r.createElement("span",{key:t,className:`noselect ${H.MW}`,style:{margin:"0 5px 5px 0"}},e)))),r.createElement("div",{className:"postBody"},r.createElement(z,{toc:h})),r.createElement("br"),r.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},r.createElement(f.P.button,{className:`noselect ${S.pb}`,id:S.xG,onClick:()=>{B(!V)},whileTap:{scale:.93}},r.createElement(f.P.div,{className:x.DJ,key:V,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},V?"Switch to default layout":"Switch to wide layout"))),r.createElement("br"),r.createElement("div",{className:"postBody",style:{margin:V?"0 -14%":"",maxWidth:V?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},r.createElement("div",{className:`${S.P_} ${L?S.Xn:S.qG}`},q.map(((e,t)=>r.createElement(e,{key:t}))),c.indexCourse?r.createElement(w.A,{index:c.indexCourse,category:c.courseCategoryName}):"",r.createElement(u.Z.Provider,{value:{images:o.nodes,basePath:M.replace(/\/$/,"")+"/"}},r.createElement(i.xA,{components:{Image:p.A}},s)))),r.createElement(E.A,{nextPost:_,lastPost:A,keyCurrent:I,section:v}))}function _(e){return r.createElement(C,e,r.createElement(m,e))}function A(e){var t,n,a,i,l;let{data:o}=e;const{frontmatter:s}=o.mdx,c=s.titleSEO||s.title,m=s.titleOG||c,d=s.titleTwitter||c,g=s.descSEO||s.desc,u=s.descOG||g,p=s.descTwitter||g,f=s.schemaType||"BlogPosting",b=s.keywordsSEO,E=s.date,y=s.updated||E,w=s.imageOG||(null===(t=s.banner)||void 0===t||null===(n=t.childImageSharp)||void 0===n||null===(a=n.gatsbyImageData)||void 0===a||null===(i=a.images)||void 0===i||null===(l=i.fallback)||void 0===l?void 0:l.src),x=s.imageAltOG||u,S=s.imageTwitter||w,k=s.imageAltTwitter||p,H=s.canonicalURL,z=s.flagHidden||!1,C=s.mainTag||"Posts",_=s.slug.split("/")[1]||"posts",{siteUrl:A}=(0,h.Q)(),T={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:A},{"@type":"ListItem",position:2,name:C,item:`${A}/${s.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${A}${s.slug}`}]};return r.createElement(v.A,{title:c+" - avrtt.blog",titleOG:m,titleTwitter:d,description:g,descriptionOG:u,descriptionTwitter:p,schemaType:f,keywords:b,datePublished:E,dateModified:y,imageOG:w,imageAltOG:x,imageTwitter:S,imageAltTwitter:k,canonicalUrl:H,flagHidden:z,mainTag:C,section:_,type:"article"},r.createElement("script",{type:"application/ld+json"},JSON.stringify(T)))}},96098:function(e,t,n){var a=n(96540),i=n(7978);t.A=e=>{let{text:t}=e;return a.createElement(i.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-js-content-file-path-src-pages-posts-research-image-processing-mdx-fdb6b6e3e971c8b6a454.js.map