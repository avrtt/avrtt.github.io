"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[2035],{3962:function(e,t){t.A="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iaXNvLTg4NTktMSI/Pgo8IS0tIEdlbmVyYXRvcjogQWRvYmUgSWxsdXN0cmF0b3IgMTYuMC4wLCBTVkcgRXhwb3J0IFBsdWctSW4gLiBTVkcgVmVyc2lvbjogNi4wMCBCdWlsZCAwKSAgLS0+CjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+CjxzdmcgdmVyc2lvbj0iMS4xIiBpZD0iQ2FwYV8xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB4PSIwcHgiIHk9IjBweCIKCSB3aWR0aD0iNDE2Ljk3OXB4IiBoZWlnaHQ9IjQxNi45NzlweCIgdmlld0JveD0iMCAwIDQxNi45NzkgNDE2Ljk3OSIgc3R5bGU9ImVuYWJsZS1iYWNrZ3JvdW5kOm5ldyAwIDAgNDE2Ljk3OSA0MTYuOTc5OyIKCSB4bWw6c3BhY2U9InByZXNlcnZlIj4KPGc+Cgk8cGF0aCBkPSJNMzU2LjAwNCw2MS4xNTZjLTgxLjM3LTgxLjQ3LTIxMy4zNzctODEuNTUxLTI5NC44NDgtMC4xODJjLTgxLjQ3LDgxLjM3MS04MS41NTIsMjEzLjM3OS0wLjE4MSwyOTQuODUKCQljODEuMzY5LDgxLjQ3LDIxMy4zNzgsODEuNTUxLDI5NC44NDksMC4xODFDNDM3LjI5MywyNzQuNjM2LDQzNy4zNzUsMTQyLjYyNiwzNTYuMDA0LDYxLjE1NnogTTIzNy42LDM0MC43ODYKCQljMCwzLjIxNy0yLjYwNyw1LjgyMi01LjgyMiw1LjgyMmgtNDYuNTc2Yy0zLjIxNSwwLTUuODIyLTIuNjA1LTUuODIyLTUuODIyVjE2Ny44ODVjMC0zLjIxNywyLjYwNy01LjgyMiw1LjgyMi01LjgyMmg0Ni41NzYKCQljMy4yMTUsMCw1LjgyMiwyLjYwNCw1LjgyMiw1LjgyMlYzNDAuNzg2eiBNMjA4LjQ5LDEzNy45MDFjLTE4LjYxOCwwLTMzLjc2Ni0xNS4xNDYtMzMuNzY2LTMzLjc2NQoJCWMwLTE4LjYxNywxNS4xNDctMzMuNzY2LDMzLjc2Ni0zMy43NjZjMTguNjE5LDAsMzMuNzY2LDE1LjE0OCwzMy43NjYsMzMuNzY2QzI0Mi4yNTYsMTIyLjc1NSwyMjcuMTA3LDEzNy45MDEsMjA4LjQ5LDEzNy45MDF6Ii8+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPC9zdmc+Cg=="},31178:function(e,t,n){n.r(t),n.d(t,{Head:function(){return C},PostTemplate:function(){return z},default:function(){return M}});var a=n(54506),i=n(28453),l=n(96540),r=n(66501),s=n(16886),o=n(46295),c=n(96098);function m(e){const t=Object.assign({p:"p",strong:"strong",h2:"h2",a:"a",span:"span",ul:"ul",li:"li",hr:"hr",h3:"h3",ol:"ol"},(0,i.RP)(),e.components),{Image:n}=t;return n||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),l.createElement(l.Fragment,null,"\n",l.createElement("br"),"\n","\n","\n",l.createElement(t.p,null,l.createElement(t.strong,null,"TL;DR"),": T-distributed stochastic neighbor embedding (",l.createElement(s.A,null,"t-SNE"),") is a powerful technique for mapping high-dimensional data into a lower-dimensional (typically 2D or 3D) space while preserving local structures. By employing a heavy-tailed ",l.createElement(r.A,{text:"A distribution whose tails (the extremities) have a higher probability mass compared to, for example, a Gaussian distribution."})," t-distribution in the low-dimensional embedding, it helps alleviate the well-known crowding problem and visually distinguishes clusters. This article explores the intuition, mathematical underpinnings, implementation details, visualization approaches, and key caveats of t-SNE, including parameter choices such as perplexity, early exaggeration, and learning rate. It also discusses popular variants (bh-SNE, fft-SNE), hybrid strategies with PCA or UMAP, and the challenges and limitations of using t-SNE for real-world applications."),"\n",l.createElement(t.h2,{id:"1-introduction",style:{position:"relative"}},l.createElement(t.a,{href:"#1-introduction","aria-label":"1 introduction permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"1. Introduction"),"\n",l.createElement(t.p,null,"The ",l.createElement(s.A,null,"t-distributed stochastic neighbor embedding (t-SNE)")," algorithm is one of the most widely used methods for visualizing high-dimensional data. Developed by Laurens van der Maaten and Geoffrey Hinton (Journal of Machine Learning Research, 2008), it represents a refinement of an earlier approach known as ",l.createElement(r.A,{text:"Stochastic neighbor embedding"}),"SNE (Hinton and Roweis, NeurIPS 2002). t-SNE's principal objective is to preserve local neighborhood structures from a high-dimensional space (",l.createElement(c.A,{text:"\\( \\mathbb{R}^D \\)"}),") by creating an intuitive, visually interpretable, low-dimensional map — often 2D, though 3D is also used for more nuanced interactive visualizations."),"\n",l.createElement(t.p,null,"Why do we need a specialized algorithm for high-dimensional data visualization? When dealing with large datasets with tens, hundreds, or even thousands of features, humans struggle to perceive patterns directly because our natural spatial intuition is bound to three dimensions. A well-known phenomenon called the ",l.createElement(s.A,null,"curse of dimensionality"),' tells us that many distance-based or density-based intuition fails as dimensionality grows, because the volume of space expands so quickly that data points often appear to be equidistant or vanish in a high-dimensional "void." Simply running classical dimensionality reduction (like principal component analysis, PCA) may not always reveal the kind of local clusters that we want to see for tasks like pattern discovery, anomaly detection, or sample clustering. In such scenarios, t-SNE is particularly handy because it places a strong emphasis on preserving small pairwise distances — i.e., the local neighborhoods — while relaxing constraints for points that are far apart.'),"\n",l.createElement(t.p,null,"Despite its popularity and impressive performance, t-SNE has important caveats. For instance, it is particularly good at revealing local structure (clusters) but is notorious for sometimes distorting global relationships (e.g., the relative distances between clusters). It also requires careful tuning of parameters such as perplexity, exaggeration factors, and the learning rate. Understanding these parameters is critical to producing a stable and meaningful visualization."),"\n",l.createElement(t.p,null,"In this article, I will discuss:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"The conceptual underpinnings of t-SNE, including why it introduces a ",l.createElement(s.A,null,"t-distribution")," in the low-dimensional embedding instead of a Gaussian distribution."),"\n",l.createElement(t.li,null,"The mathematical formulation and the idea of converting distances into probabilities."),"\n",l.createElement(t.li,null,"Implementation details, with an illustrative code snippet that performs t-SNE on a toy dataset."),"\n",l.createElement(t.li,null,"Variants and practical improvements: Barnes-Hut t-SNE (",l.createElement(s.A,null,"bh-SNE"),") and ",l.createElement(s.A,null,"fft-SNE"),"."),"\n",l.createElement(t.li,null,"Tips for effectively visualizing t-SNE results, color-coding, clustering, and the interplay with other dimensionality reduction methods (e.g., PCA or UMAP)."),"\n",l.createElement(t.li,null,"Challenges and limitations, especially regarding parameter sensitivity, runtime performance, and the potential misinterpretation of the emergent geometry in the embedding."),"\n"),"\n",l.createElement(t.p,null,"Let's dive into the foundations of the algorithm and uncover how t-SNE tackles some of the pitfalls present in earlier methods like SNE and ",l.createElement(r.A,{text:"Symmetric Stochastic Neighbor Embedding"}),"Symmetric SNE."),"\n",l.createElement(t.hr),"\n",l.createElement(t.h2,{id:"2-algorithm",style:{position:"relative"}},l.createElement(t.a,{href:"#2-algorithm","aria-label":"2 algorithm permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2. Algorithm"),"\n",l.createElement(t.p,null,"The t-SNE algorithm can be seen as a multi-stage process that transforms pairwise similarities in the original high-dimensional space into pairwise similarities in a low-dimensional space, then optimizes an objective function to make these two sets of similarities as close as possible. Below, I will present the conceptual and mathematical details step by step."),"\n",l.createElement(t.h3,{id:"21-high-dimensional-probability-distributions",style:{position:"relative"}},l.createElement(t.a,{href:"#21-high-dimensional-probability-distributions","aria-label":"21 high dimensional probability distributions permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.1 high-dimensional probability distributions"),"\n",l.createElement(t.p,null,"At the heart of the SNE family of algorithms is the concept of converting high-dimensional distances into conditional probabilities. For each data point ",l.createElement(c.A,{text:"\\( x_i \\in \\mathbb{R}^D \\)"})," in the original (high-dimensional) dataset, we define a conditional probability ",l.createElement(c.A,{text:"\\( p_{j|i} \\)"})," that represents how likely it is that ",l.createElement(c.A,{text:"\\( x_i \\)"}),' would "choose" ',l.createElement(c.A,{text:"\\( x_j \\)"})," as its neighbor if neighbors were picked in proportion to their probability density under a Gaussian centered on ",l.createElement(c.A,{text:"\\( x_i \\)"}),". Formally:"),"\n",l.createElement(c.A,{text:"\\( p_{j|i} = \\frac{\\exp \\left( - \\frac{\\| x_i - x_j\\|^2}{2 \\sigma_i^2} \\right)}{\\sum_{k \\neq i} \\exp \\left( - \\frac{\\| x_i - x_k\\|^2}{2 \\sigma_i^2} \\right)}. \\)"}),"\n",l.createElement(t.p,null,"Here:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(c.A,{text:"\\( \\sigma_i \\)"})," is a parameter that determines the width of the Gaussian centered on ",l.createElement(c.A,{text:"\\( x_i \\)"}),"."),"\n",l.createElement(t.li,null,"The similarity ",l.createElement(c.A,{text:"\\( p_{j|i} \\)"})," increases as the distance between ",l.createElement(c.A,{text:"\\( x_i \\)"})," and ",l.createElement(c.A,{text:"\\( x_j \\)"})," decreases, matching our intuition that closer points in the high-dimensional space should remain close in some sense in the low-dimensional map."),"\n"),"\n",l.createElement(t.p,null,"The distribution of ",l.createElement(c.A,{text:"\\( p_{j|i} \\)"})," is chosen so that each ",l.createElement(c.A,{text:"\\( x_i \\)"})," has a local neighborhood structure that sums up to 1 over all j. Typically, ",l.createElement(c.A,{text:"\\( p_{i|i} \\)"})," is defined to be 0."),"\n",l.createElement(t.p,null,"Furthermore, we typically define a ",l.createElement(s.A,null,"joint probability distribution")," in the high-dimensional space by symmetrizing:"),"\n",l.createElement(c.A,{text:"\\( p_{ij} = \\frac{p_{j|i} + p_{i|j}}{2N}, \\)"}),"\n",l.createElement(t.p,null,"where ",l.createElement(c.A,{text:"\\( N \\)"})," is the number of data points (the term ",l.createElement(c.A,{text:"\\( 2N \\)"})," normalizes the probabilities to sum to 1 across pairs ",l.createElement(c.A,{text:"\\( i,j \\)"}),"). This symmetric formulation is particularly relevant for t-SNE, though it was introduced initially in Symmetric SNE to avoid certain pathologies of purely one-sided conditional probabilities."),"\n",l.createElement(t.h3,{id:"22-the-use-of-the-t-distribution-in-low-dimensional-space",style:{position:"relative"}},l.createElement(t.a,{href:"#22-the-use-of-the-t-distribution-in-low-dimensional-space","aria-label":"22 the use of the t distribution in low dimensional space permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.2 the use of the t-distribution in low-dimensional space"),"\n",l.createElement(t.p,null,"In standard SNE or Symmetric SNE, the similarities in the low-dimensional map are also modeled by a Gaussian distribution, typically with a fixed variance for all points. However, this approach suffers from the ",l.createElement(s.A,null,"crowding problem"),": if you preserve many small distances well, it is difficult to proportionally represent large distances on a 2D or 3D plane. As dimensionality grows, local volumes are dwarfed by the volume at the periphery (the surface of a high-dimensional hypersphere), so many points inevitably appear too close once projected into low dimensions."),"\n",l.createElement(t.p,null,"t-SNE's key idea is: Instead of using a Gaussian to model pairwise interactions in the low-dimensional space, use a ",l.createElement(c.A,{text:"\\( t \\)"}),"-distribution with one degree of freedom. In other words, the ",l.createElement(c.A,{text:"\\( q_{ij} \\)"})," distribution in the embedded space is not Gaussian, but is given by:"),"\n",l.createElement(c.A,{text:"\\[\nq_{ij} = \\frac{\\left(1 + \\| y_i - y_j \\|^2 \\right)^{-1}}{\\sum_{k \\neq l} \\left(1 + \\| y_k - y_l \\|^2 \\right)^{-1}}.\n\\]"}),"\n",l.createElement(t.p,null,"Here ",l.createElement(c.A,{text:"\\( y_i \\in \\mathbb{R}^2 \\)"})," (or ",l.createElement(c.A,{text:"\\( \\mathbb{R}^3 \\)"})," in a 3D embedding) is the low-dimensional representation of the high-dimensional data point ",l.createElement(c.A,{text:"\\( x_i \\)"}),". The t-distribution has heavier tails than a Gaussian, so it places a higher probability on moderate or even large distances in the low-dimensional embedding. This property helps remedy the crowding issue by allowing points that are far apart in the original space to repel each other more strongly when mapped into fewer dimensions."),"\n",l.createElement(t.h3,{id:"23-similarities-and-distances-revisited",style:{position:"relative"}},l.createElement(t.a,{href:"#23-similarities-and-distances-revisited","aria-label":"23 similarities and distances revisited permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.3 similarities and distances revisited"),"\n",l.createElement(t.p,null,"We have two sets of probabilities:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(c.A,{text:"\\( P = \\{ p_{ij} \\} \\)"})," capturing neighborhood relationships in the high-dimensional space."),"\n",l.createElement(t.li,null,l.createElement(c.A,{text:"\\( Q = \\{ q_{ij} \\} \\)"})," capturing neighborhood relationships in the low-dimensional space."),"\n"),"\n",l.createElement(t.p,null,"The original impetus is to choose an embedding ",l.createElement(c.A,{text:"\\( \\{ y_i \\} \\)"})," such that ",l.createElement(c.A,{text:"\\( Q \\)"})," is as similar to ",l.createElement(c.A,{text:"\\( P \\)"})," as possible, focusing primarily on the small distances (close neighbors). This is formalized by using the ",l.createElement(s.A,null,"Kullback-Leibler (KL) divergence"),":"),"\n",l.createElement(c.A,{text:"\\[\nC = KL(P \\| Q) = \\sum_{i \\neq j} p_{ij} \\log_2 \\frac{p_{ij}}{q_{ij}}.\n\\]"}),"\n",l.createElement(t.p,null,"Minimizing ",l.createElement(c.A,{text:"\\( KL(P \\| Q) \\)"})," ensures that if ",l.createElement(c.A,{text:"\\( p_{ij} \\)"})," is large (i.e., if ",l.createElement(c.A,{text:"\\( x_j \\)"})," is a close neighbor to ",l.createElement(c.A,{text:"\\( x_i \\)"})," in the original space), ",l.createElement(c.A,{text:"\\( q_{ij} \\)"})," should also be large (i.e., ",l.createElement(c.A,{text:"\\( y_j \\)"})," should be close to ",l.createElement(c.A,{text:"\\( y_i \\)"})," in the embedding). Because ",l.createElement(c.A,{text:"\\( KL \\)"})," is ",l.createElement(r.A,{text:"An asymmetric measure of difference between two probability distributions"}),', the penalty for "missing" a close neighbor is higher than the penalty for accidentally placing two distant points close together. This is often desirable for data visualization.'),"\n",l.createElement(t.h3,{id:"24-mathematical-formulation",style:{position:"relative"}},l.createElement(t.a,{href:"#24-mathematical-formulation","aria-label":"24 mathematical formulation permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.4 mathematical formulation"),"\n",l.createElement(t.p,null,"The objective function for t-SNE is:"),"\n",l.createElement(c.A,{text:"\\[\nC = \\sum_{i} \\sum_{j} p_{ij} \\log_2 \\frac{p_{ij}}{q_{ij}}\n\\]"}),"\n",l.createElement(t.p,null,"where"),"\n",l.createElement(c.A,{text:"\\[\nq_{ij} = \\frac{(1 + \\| y_i - y_j \\|^2)^{-1}}{\\sum_{k \\neq l} (1 + \\| y_k - y_l \\|^2)^{-1}}.\n\\]"}),"\n",l.createElement(t.p,null,"To optimize ",l.createElement(c.A,{text:"\\( C \\)"}),", we use (stochastic) gradient descent. The gradient with respect to ",l.createElement(c.A,{text:"\\( y_i \\)"})," in t-SNE is:"),"\n",l.createElement(c.A,{text:"\\[\n\\frac{\\delta C}{\\delta y_i} = 4 \\sum_j (p_{ij} - q_{ij}) (y_i - y_j)\\, (1 + \\| y_i - y_j \\|^2)^{-1}.\n\\]"}),"\n",l.createElement(t.p,null,"We have a factor of 4 (as opposed to 2 in Symmetric SNE) because of the difference in how the t-distribution modifies the similarity measure and also due to symmetrical pairwise terms ",l.createElement(c.A,{text:"\\( (p_{ij} - q_{ij}) \\)"}),". Intuitively, if ",l.createElement(c.A,{text:"\\( p_{ij} > q_{ij} \\)"}),", then ",l.createElement(c.A,{text:"\\( y_i \\)"}),' is "pulled" closer to ',l.createElement(c.A,{text:"\\( y_j \\)"}),". Conversely, if ",l.createElement(c.A,{text:"\\( p_{ij} < q_{ij} \\)"}),", ",l.createElement(c.A,{text:"\\( y_i \\)"}),' is "pushed" away from ',l.createElement(c.A,{text:"\\( y_j \\)"}),". The factor ",l.createElement(c.A,{text:"\\( (1 + \\| y_i - y_j \\|^2)^{-1} \\)"})," ensures that distant points interact more strongly than they would under a Gaussian assumption, addressing the crowding problem."),"\n",l.createElement(t.h3,{id:"25-understanding-the-perplexity-parameter",style:{position:"relative"}},l.createElement(t.a,{href:"#25-understanding-the-perplexity-parameter","aria-label":"25 understanding the perplexity parameter permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.5 understanding the perplexity parameter"),"\n",l.createElement(t.p,null,"One of the most important hyperparameters in t-SNE is the ",l.createElement(s.A,null,"perplexity"),". Perplexity is closely related to the concept of ",l.createElement(r.A,{text:"Shannon entropy is often measured in bits, and perplexity is 2 to the power of the entropy."}),"entropy in information theory. For each data point ",l.createElement(c.A,{text:"\\( x_i \\)"}),", we define a conditional distribution ",l.createElement(c.A,{text:"\\( p_{j|i} \\)"})," with a Gaussian kernel whose bandwidth ",l.createElement(c.A,{text:"\\( \\sigma_i \\)"})," is chosen so that the Shannon entropy of ",l.createElement(c.A,{text:"\\( p_{j|i} \\)"})," equals some user-specified perplexity ",l.createElement(c.A,{text:"\\( \\mathrm{Perp} \\)"}),":"),"\n",l.createElement(c.A,{text:"\\[\n\\mathrm{Perp}(P_i) = 2^{H(P_i)},\n\\]"}),"\n",l.createElement(t.p,null,"where"),"\n",l.createElement(c.A,{text:"\\( H(P_i) = - \\sum_j p_{j|i} \\log_2 p_{j|i}. \\)"}),"\n",l.createElement(t.p,null,"Thus, a larger perplexity means a flatter ",l.createElement(c.A,{text:"\\( p_{j|i} \\)"})," distribution, implying that ",l.createElement(c.A,{text:"\\( x_i \\)"}),' has a wider notion of which points are considered "neighbors." Typically, perplexity values between 5 and 50 are used in practice, though certain data sets might require going lower or higher.'),"\n",l.createElement(t.h3,{id:"26-setting-perplexity-in-different-scenarios",style:{position:"relative"}},l.createElement(t.a,{href:"#26-setting-perplexity-in-different-scenarios","aria-label":"26 setting perplexity in different scenarios permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.6 setting perplexity in different scenarios"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Dense or overlapping clusters"),": If your dataset forms dense clusters with relatively small inter-cluster distances, you might start with a smaller perplexity (e.g., 5 — 10) because you want the local environment to be tight."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Broad or large-scale structures"),": If your dataset has multiple cluster scales or you suspect that you need to capture broader neighborhoods (for instance, social networks with different connectivity densities), you may choose a higher perplexity (e.g., 30 — 50)."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Heterogeneous data"),": If you suspect that different data subsets have drastically different local densities, you might explore multiple perplexity values or a range-based approach to ensure the method doesn't break the dataset into artificially separated clusters."),"\n"),"\n",l.createElement(t.h3,{id:"27-early-exaggeration-phase",style:{position:"relative"}},l.createElement(t.a,{href:"#27-early-exaggeration-phase","aria-label":"27 early exaggeration phase permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.7 early exaggeration phase"),"\n",l.createElement(t.p,null,"Another hallmark of the t-SNE algorithm is the ",l.createElement(s.A,null,"early exaggeration")," phase. During the first few iterations of gradient descent, the probabilities ",l.createElement(c.A,{text:"\\( p_{ij} \\)"})," are multiplied by a constant factor (e.g., 4 or 12) that increases the overall attractive forces among points that are truly neighbors in the high-dimensional space. The effect is to create tight clusters that are relatively well-separated from each other early on. Over time, this separation allows for clusters to rearrange more easily without getting stuck in local minima. After a certain number of iterations, the exaggeration is turned off, and the optimization continues at the normal scale of ",l.createElement(c.A,{text:"\\( p_{ij} \\)"}),"."),"\n",l.createElement(t.h3,{id:"28-choosing-the-right-learning-rate",style:{position:"relative"}},l.createElement(t.a,{href:"#28-choosing-the-right-learning-rate","aria-label":"28 choosing the right learning rate permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.8 choosing the right learning rate"),"\n",l.createElement(t.p,null,"t-SNE uses (stochastic) gradient descent, which requires setting a ",l.createElement(s.A,null,"learning rate"),". Choosing a suboptimal learning rate can cause:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Too large"),": The algorithm might overshoot local minima, leading to large point movements that never settle, causing chaotic or even degenerate embeddings."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Too small"),": Convergence becomes painfully slow, and the local minima might not be effectively escaped."),"\n"),"\n",l.createElement(t.p,null,"A typical default learning rate is in the range of 100 — 200 for many standard t-SNE implementations, but this is heavily data- and scale-dependent. As you move beyond toy examples (like MNIST) to more complex or bigger data, you might need to adjust the learning rate. If you see that your final plot has points extremely cluttered or clumped into a single mass, the learning rate may be too high (or your perplexity may be mis-specified). Conversely, if you see a slow, partial formation of structure but the algorithm fails to produce well-separated clusters, you might try increasing the learning rate."),"\n",l.createElement(t.hr),"\n",l.createElement(t.h2,{id:"3-implementation",style:{position:"relative"}},l.createElement(t.a,{href:"#3-implementation","aria-label":"3 implementation permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3. Implementation"),"\n",l.createElement(t.p,null,"In this section, I provide a conceptual, step-by-step guide to implementing t-SNE, along with detailed code snippets. The purpose here is to illuminate the main steps and typical usage patterns rather than produce a hyper-optimized production-level code."),"\n",l.createElement(t.h3,{id:"31-step-by-step-with-detailed-code-snippets",style:{position:"relative"}},l.createElement(t.a,{href:"#31-step-by-step-with-detailed-code-snippets","aria-label":"31 step by step with detailed code snippets permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.1 step-by-step with detailed code snippets"),"\n",l.createElement(t.p,null,"Let's assume we have ",l.createElement(c.A,{text:"\\( X \\)"})," as an array of shape ",l.createElement(c.A,{text:"\\( (N, D) \\)"}),", where ",l.createElement(c.A,{text:"\\( N \\)"})," is the number of data points and ",l.createElement(c.A,{text:"\\( D \\)"})," is the dimension of each data vector. We want to embed this data into ",l.createElement(c.A,{text:"\\( Y \\in \\mathbb{R}^{N \\times 2} \\)"}),". Below is a conceptual code snippet in Python, using NumPy for illustration. In practice, one often uses a library like scikit-learn, PyTorch, or specialized implementations like openTSNE or Multicore-TSNE."),"\n",l.createElement(o.A,{text:'\nimport numpy as np\n\ndef compute_pairwise_affinities(X, perplexity=30.0):\n    """\n    Compute p_{j|i} for all i, j in the dataset.\n    Use binary search to find sigma_i that matches the desired perplexity for each point.\n    Returns a matrix P of shape (N, N).\n    """\n    N = X.shape[0]\n    # Squared Euclidean distances\n    sum_X = np.sum(np.square(X), axis=1)\n    # Expand to create an NxN distance matrix\n    dists = -2 * np.dot(X, X.T) + np.expand_dims(sum_X, 1) + np.expand_dims(sum_X, 0)\n    \n    # Initialize result\n    P = np.zeros((N, N))\n    log_perp = np.log2(perplexity)\n    \n    for i in range(N):\n        beta_min, beta_max = -np.inf, np.inf\n        beta = 1.0  # 1 / (2 * sigma^2)\n        \n        # Exclude self-distance\n        mask = np.arange(N) != i\n        distances_i = dists[i, mask]\n        \n        # Binary search to match perplexity\n        max_tries = 50\n        for _ in range(max_tries):\n            # Compute p_{j|i} with current beta\n            p_i = np.exp(-distances_i * beta)\n            p_i_sum = np.sum(p_i)\n            p_i = p_i / p_i_sum\n            \n            # Shannon entropy in log base 2\n            H = -np.sum(p_i * np.log2(p_i))\n            H_diff = H - log_perp\n            \n            # If within tolerance, break\n            if np.abs(H_diff) < 1e-5:\n                break\n            \n            # If entropy is too high, increase beta; else decrease\n            if H_diff > 0:\n                beta_min = beta\n                if beta_max == np.inf or beta_max == -np.inf:\n                    beta *= 2.\n                else:\n                    beta = (beta + beta_max) / 2.\n            else:\n                beta_max = beta\n                if beta_min == -np.inf:\n                    beta /= 2.\n                else:\n                    beta = (beta + beta_min) / 2.\n        \n        # Assign final p_{j|i}\n        P[i, mask] = p_i\n    \n    # Symmetrize & normalize\n    P = 0.5 * (P + P.T)\n    P /= np.sum(P)\n    \n    return P\n\ndef tsne(X, perplexity=30.0, dim=2, lr=200.0, n_iter=1000, early_exaggeration=12.0):\n    """\n    Simple demonstration of t-SNE with gradient descent in Python (NumPy).\n    """\n    np.random.seed(42)\n    N, D = X.shape\n    \n    # Step 1: Compute high-dimensional affinities\n    P = compute_pairwise_affinities(X, perplexity=perplexity)\n    P *= early_exaggeration\n    \n    # Step 2: Initialize Y randomly\n    Y = 0.01 * np.random.randn(N, dim)\n    \n    # Setup gradient descent parameters\n    momentum = 0.9\n    Y_inc = np.zeros_like(Y)\n    \n    # Number of iterations for early exaggeration\n    n_early = 250\n    for it in range(n_iter):\n        # Compute low-dimensional affinities Q\n        # t-distribution with 1 degree of freedom\n        sum_Y = np.sum(np.square(Y), axis=1)\n        dists = -2 * np.dot(Y, Y.T) + np.expand_dims(sum_Y, 1) + np.expand_dims(sum_Y, 0)\n        # (1 + distance)^{-1}\n        num = 1.0 / (1.0 + dists)\n        np.fill_diagonal(num, 0.0)\n        Q = num / np.sum(num)\n        \n        # Compute gradient\n        PQdiff = (P - Q)  # shape (N,N)\n        \n        # Weighted version: factor 4 as in t-SNE\n        # Expand dims to broadcast in subtraction\n        dY = np.zeros_like(Y)\n        for i in range(N):\n            # sum_j( (P-Q)_{ij} * (y_i - y_j) * (1 + dist_ij)^{-1} )\n            # We can vectorize, but let\'s do it more explicitly for clarity\n            diff_i = (Y[i] - Y)\n            grad_i = 4.0 * np.sum(\n                np.expand_dims(PQdiff[i], axis=1) * diff_i * num[i].reshape(-1,1),\n                axis=0\n            )\n            dY[i] = grad_i\n        \n        # Update with momentum\n        Y_inc = momentum * Y_inc - lr * dY\n        Y += Y_inc\n        \n        # Turn off early exaggeration after n_early iterations\n        if it == n_early:\n            P /= early_exaggeration\n        \n        # Optionally: decrease learning rate or update momentum, etc.\n        \n        # Printing or logging intermediate results:\n        if (it + 1) % 100 == 0:\n            cost = np.sum(P * np.log2((P + 1e-7) / (Q + 1e-7)))\n            print(f"Iteration {it+1}, KL-divergence cost: {cost:.5f}")\n    \n    return Y\n\n\n# Example usage on a toy dataset\nif __name__ == "__main__":\n    from sklearn.datasets import load_iris\n    X = load_iris().data  # shape (150, 4)\n    Y_embedding = tsne(X, perplexity=30.0, n_iter=1000)\n    print("Final embedding shape:", Y_embedding.shape)\n'}),"\n",l.createElement(t.p,null,"This code is for educational illustration. In real-world practice, you would use a robust, optimized library. Notice that we:"),"\n",l.createElement(t.ol,null,"\n",l.createElement(t.li,null,"Compute ",l.createElement(c.A,{text:"\\( P \\)"})," via binary search to match the specified perplexity for each point."),"\n",l.createElement(t.li,null,"Multiply ",l.createElement(c.A,{text:"\\( P \\)"})," by the early exaggeration factor for the initial ",l.createElement(c.A,{text:"\\( n_{early} \\)"})," iterations."),"\n",l.createElement(t.li,null,"Iteratively compute ",l.createElement(c.A,{text:"\\( Q \\)"})," in the low-dimensional space, take gradients, and update ",l.createElement(c.A,{text:"\\( Y \\)"}),"."),"\n"),"\n",l.createElement(t.p,null,"This code also demonstrates how one can incorporate momentum, a typical practice to accelerate convergence and smooth out gradient updates."),"\n",l.createElement(t.hr),"\n",l.createElement(t.h2,{id:"4-visualization-and-interpretation",style:{position:"relative"}},l.createElement(t.a,{href:"#4-visualization-and-interpretation","aria-label":"4 visualization and interpretation permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4. Visualization and interpretation"),"\n",l.createElement(t.p,null,"The principal output of t-SNE is a set of coordinates ",l.createElement(c.A,{text:"\\( \\{ y_i \\} \\)"})," that you can plot on a 2D plane or in 3D. Often, these are used to highlight clusters, identify outliers, or glean broad relationships among data points. However, interpreting t-SNE plots can be tricky if you do not keep in mind the algorithm's biases."),"\n",l.createElement(t.h3,{id:"41-color-coding-and-clustering",style:{position:"relative"}},l.createElement(t.a,{href:"#41-color-coding-and-clustering","aria-label":"41 color coding and clustering permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.1 color coding and clustering"),"\n",l.createElement(t.p,null,"One popular approach is to color-code each data point by some known label or other metadata. For instance, if you embed the famous MNIST dataset of handwritten digits, you might color each point by its digit label (0 — 9). This typically shows distinct digit clusters or transitions between visually similar digits (like 3 and 8). Even if your dataset does not come with labels, you can use an unsupervised clustering algorithm (e.g., DBSCAN or K-means in the projected 2D space) and color-code by cluster membership. Just remember that the geometry in a t-SNE plot emphasizes local relationships, so distance between two clusters might not necessarily reflect true global distances in the original space."),"\n",l.createElement(t.h3,{id:"42-multiple-runs-and-stability-checks",style:{position:"relative"}},l.createElement(t.a,{href:"#42-multiple-runs-and-stability-checks","aria-label":"42 multiple runs and stability checks permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.2 multiple runs and stability checks"),"\n",l.createElement(t.p,null,"t-SNE uses random initialization and a stochastic optimization procedure, so each run can produce somewhat different layouts. Clusters typically remain the same, but their arrangement on the plane can vary. To ensure that your results are consistent, do:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Multiple runs")," with different random seeds."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameter sweeps")," of perplexity, learning rate, etc."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Visual consistency checks")," to confirm that cluster groupings and broad structures are stable."),"\n"),"\n",l.createElement(t.h3,{id:"43-combining-t-sne-with-other-techniques",style:{position:"relative"}},l.createElement(t.a,{href:"#43-combining-t-sne-with-other-techniques","aria-label":"43 combining t sne with other techniques permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.3 combining t-SNE with other techniques"),"\n",l.createElement(t.p,null,"In many workflows, one might combine t-SNE with complementary methods:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Dimensionality reduction pre-processing"),": For large datasets, applying PCA or truncated SVD to reduce dimensionality from thousands to, say, 50 or 100 can drastically speed up t-SNE. This can also help mitigate some noise in the data."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"UMAP"),": Another popular manifold learning approach. Some researchers combine ",l.createElement(r.A,{text:"Uniform Manifold Approximation and Projection"}),"UMAP or t-SNE for different stages of analysis or for cross-validation of cluster findings. UMAP tends to emphasize both local and some global structures more effectively, but t-SNE can produce more visually separated clusters in some cases."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Clustering"),": You can run a separate clustering algorithm (e.g., GMM, DBSCAN, K-means) on the 2D or 3D embedding to label or characterize groups. This helps interpret patterns visually discovered in the t-SNE map."),"\n"),"\n",l.createElement(t.hr),"\n",l.createElement(t.h2,{id:"5-variants-like-bh-sne-and-fft-sne",style:{position:"relative"}},l.createElement(t.a,{href:"#5-variants-like-bh-sne-and-fft-sne","aria-label":"5 variants like bh sne and fft sne permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5. variants like bh-SNE and fft-SNE"),"\n",l.createElement(t.p,null,"t-SNE as outlined above has a time complexity of about ",l.createElement(c.A,{text:"\\( O(N^2) \\)"})," per iteration, due to the need to compute pairwise interactions among ",l.createElement(c.A,{text:"\\( N \\)"})," points. For moderate datasets, this is acceptable, but for extremely large datasets (tens or hundreds of thousands of points), it becomes infeasible."),"\n",l.createElement(t.p,null,"Two major variants that address scalability are:"),"\n",l.createElement(t.ol,null,"\n",l.createElement(t.li,null,l.createElement(s.A,null,"Barnes-Hut t-SNE (bh-SNE)")," (Van der Maaten, 2014). This approach uses a ",l.createElement(r.A,{text:"An algorithm commonly used in n-body simulations to approximate forces between a large number of particles."}),"Barnes-Hut tree-based approximation to compute ",l.createElement(c.A,{text:"\\( Q \\)"})," and the gradient more efficiently, reducing the complexity to about ",l.createElement(c.A,{text:"\\( O(N \\log N) \\)"}),"."),"\n",l.createElement(t.li,null,l.createElement(s.A,null,"fft-SNE")," (Linderman and gang, 2019). This leverages a fast Fourier transform–based approach to approximate the ",l.createElement(c.A,{text:"\\( n \\)"}),"-body computations in an even more efficient manner, making it possible to process large datasets on single or multi-GPU systems."),"\n"),"\n",l.createElement(t.p,null,"Both variants significantly speed up the calculation of the gradient by grouping distant points together, approximating their combined effect on each point."),"\n",l.createElement(t.hr),"\n",l.createElement(t.h2,{id:"6-hybrid-approaches-with-umap-or-pca",style:{position:"relative"}},l.createElement(t.a,{href:"#6-hybrid-approaches-with-umap-or-pca","aria-label":"6 hybrid approaches with umap or pca permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6. hybrid approaches with UMAP or PCA"),"\n",l.createElement(t.p,null,"Even with Barnes-Hut or FFT-based optimizations, t-SNE can still be expensive for extremely large datasets (e.g., millions of points). One commonly used workaround is:"),"\n",l.createElement(t.ol,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Pre-process with PCA"),": Reduce dimensionality from ",l.createElement(c.A,{text:"\\( D \\)"})," down to some intermediate dimension ",l.createElement(c.A,{text:"\\( d \\)"})," (like 50 or 100)."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Apply t-SNE")," on these ",l.createElement(c.A,{text:"\\( d \\)"}),"-dim representations. The intuition is that PCA retains as much of the global variance as possible, thereby denoising the data, while t-SNE focuses on preserving local structure in the final low-dimensional embedding."),"\n",l.createElement(t.li,null,"Alternatively, use ",l.createElement(s.A,null,"UMAP")," instead of PCA for the initial or final stage. UMAP can handle large-scale data quite efficiently and also provides a measure of global structure."),"\n"),"\n",l.createElement(t.p,null,"Such hybrid approaches are widely reported to produce embeddings that are both computationally feasible and visually coherent. In many machine learning pipelines, the final 2D or 3D layout from t-SNE is used primarily for visualization rather than for subsequent modeling, so the approximate global structure is often acceptable."),"\n",l.createElement(t.hr),"\n",l.createElement(t.h2,{id:"7-challenges-and-limitations",style:{position:"relative"}},l.createElement(t.a,{href:"#7-challenges-and-limitations","aria-label":"7 challenges and limitations permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7. challenges and limitations"),"\n",l.createElement(t.p,null,"t-SNE is excellent at producing visually appealing 2D representations. However, it does come with drawbacks that you should be aware of before applying it to your data."),"\n",l.createElement(t.h3,{id:"71-preserving-global-structure",style:{position:"relative"}},l.createElement(t.a,{href:"#71-preserving-global-structure","aria-label":"71 preserving global structure permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.1 preserving global structure"),"\n",l.createElement(t.p,null,"A fundamental challenge is that t-SNE is heavily biased toward preserving local neighborhoods. Large distances are less faithfully preserved, meaning you can easily misinterpret how far apart the clusters truly are in the original space. In some cases, clusters that appear well-separated in a t-SNE plot might actually lie in a continuum if one transitions across multiple dimensions. As a result, t-SNE is not recommended for tasks requiring accurate global distances or arrangement."),"\n",l.createElement(t.h3,{id:"72-parameter-sensitivity",style:{position:"relative"}},l.createElement(t.a,{href:"#72-parameter-sensitivity","aria-label":"72 parameter sensitivity permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.2 parameter sensitivity"),"\n",l.createElement(t.p,null,"Picking parameters such as perplexity, early exaggeration factor, learning rate, and the number of iterations can drastically change the final embedding. A user unfamiliar with these hyperparameters may produce misleading or suboptimal visualizations if the chosen perplexity is too low, or if the learning rate is too high. Thus, some experimentation (parameter sweeps, cross-validation, or domain knowledge–based heuristics) is usually needed."),"\n",l.createElement(t.h3,{id:"73-scalability-issues",style:{position:"relative"}},l.createElement(t.a,{href:"#73-scalability-issues","aria-label":"73 scalability issues permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.3 scalability issues"),"\n",l.createElement(t.p,null,"A naive ",l.createElement(c.A,{text:"\\( O(N^2) \\)"})," complexity can quickly become a bottleneck for large datasets (e.g., > 50,000 points). Techniques like bh-SNE and fft-SNE reduce the complexity significantly to approximately ",l.createElement(c.A,{text:"\\( O(N \\log N) \\)"}),", but the memory footprint might still be large, and the constant factors are not negligible. On GPU-based systems, specialized libraries can produce significant speed-ups, but large-scale dimensionality reduction remains non-trivial."),"\n",l.createElement(t.h3,{id:"74-potential-solutions-and-alternatives",style:{position:"relative"}},l.createElement(t.a,{href:"#74-potential-solutions-and-alternatives","aria-label":"74 potential solutions and alternatives permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.4 potential solutions and alternatives"),"\n",l.createElement(t.p,null,"When the main limitations become problematic, consider:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(s.A,null,"UMAP")," as a more scalable alternative that also has interpretive benefits."),"\n",l.createElement(t.li,null,l.createElement(s.A,null,"Parametric t-SNE")," using neural networks to learn an explicit mapping from high-dimensional input to 2D/3D output."),"\n",l.createElement(t.li,null,l.createElement(s.A,null,"Tri-SNE")," or advanced approaches that incorporate constraints to preserve certain global relationships."),"\n",l.createElement(t.li,null,l.createElement(s.A,null,"Hierarchical SNE"),", which handles data in a hierarchical manner, providing multi-scale visualization."),"\n",l.createElement(t.li,null,l.createElement(s.A,null,"Other manifold learning methods")," such as Isomap, LLE, or brand-new approaches introduced regularly at top-tier conferences (e.g., NeurIPS, ICML)."),"\n"),"\n",l.createElement(t.p,null,"Ultimately, the right approach depends on your dataset size, the structure in your data, and your downstream objectives."),"\n",l.createElement(t.hr),"\n",l.createElement(n,{alt:"t-SNE Visualization Example",path:"",caption:"An example of t-SNE on a high-dimensional dataset, illustrating tight clusters in a 2D embedding. Notice that local neighbor preservation is excellent, but global distances may not be faithful.",zoom:"false"}),"\n",l.createElement(t.hr),"\n",l.createElement(t.p,null,"Below, I have expanded certain points with additional commentary or reminders of how this topic connects to the broader ML pipeline:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Connection to the rest of the course"),": t-SNE is taught after fundamental courses on linear algebra, basic probability, and machine learning. Students are expected to already be aware of ",l.createElement(c.A,{text:"\\( O(N^2) \\)"})," complexity issues in large-scale algorithms (Chapters 25, 31, 32), the concept of gradient-based optimization (Chapters 31, 32), and the idea of ",l.createElement(r.A,{text:"Matrix factorization methods like SVD or PCA are introduced earlier in the course."}),"matrix decompositions from earlier linear algebra sessions (Chapters 7, 17)."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Relation to MLOps"),": While t-SNE is typically used for data exploration rather than in production, MLOps practitioners might incorporate t-SNE–generated visual insights in monitoring or anomaly detection dashboards (Chapter 29). For instance, you can embed training data and newly incoming data to see if new data distributions drift significantly from the original training distribution."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Embedding interpretability"),": t-SNE alone is not guaranteed to preserve hierarchical or global relationships. If interpretability of distances or cluster adjacency is critical, you might incorporate additional steps or constraints (or choose an alternative method, as indicated above)."),"\n"),"\n",l.createElement(t.p,null,"Overall, t-SNE stands as a shining example of how non-linear dimensionality reduction can be harnessed to glean new insights about high-dimensional data. By focusing on local structures and employing a heavy-tailed distribution in the low-dimensional space, it addresses some of the fundamental challenges of earlier approaches while providing mesmerizingly clean cluster separation in many real-world applications. Nonetheless, mindful usage, parameter tuning, and awareness of its limitations are crucial for ensuring that your final 2D or 3D plot is as meaningful as it is visually compelling."),"\n",l.createElement(t.p,null,"If you want to learn more about advanced t-SNE usages, consider reading the original Van der Maaten and Hinton JMLR 2008 paper as well as more recent follow-ups on large-scale t-SNE improvements."))}var h=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,i.RP)(),e.components);return t?l.createElement(t,e,l.createElement(m,e)):m(e)};var d=n(36710),p=n(58481),u=n.n(p),g=n(36310),f=n(87245),y=n(27042),E=n(59849),b=n(5591),v=n(61122),w=n(9219),x=n(33203),S=n(95751),_=n(94328),N=n(80791),A=n(78137);const j=e=>{let{toc:t}=e;if(!t||!t.items)return null;return l.createElement("nav",{className:N.R},l.createElement("ul",null,t.items.map(((e,t)=>l.createElement("li",{key:t},l.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const n=t.replace("#",""),a=document.getElementById(n);a&&a.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&l.createElement(j,{toc:{items:e.items}}))))))};function z(e){let{data:{mdx:t,allMdx:r,allPostImages:s},children:o}=e;const{frontmatter:c,body:m,tableOfContents:h}=t,d=c.index,p=c.slug.split("/")[1],E=r.nodes.filter((e=>e.frontmatter.slug.includes(`/${p}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),N=E.findIndex((e=>e.frontmatter.index===d)),z=E[N+1],M=E[N-1],C=c.slug.replace(/\/$/,""),T=/[^/]*$/.exec(C)[0],L=`posts/${p}/content/${T}/`,{0:k,1:H}=(0,l.useState)(c.flagWideLayoutByDefault),{0:I,1:P}=(0,l.useState)(!1);var D;(0,l.useEffect)((()=>{P(!0);const e=setTimeout((()=>P(!1)),340);return()=>clearTimeout(e)}),[k]),"adventures"===p?D=w.cb:"research"===p?D=w.Qh:"thoughts"===p&&(D=w.T6);const V=u()(m).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,B=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),n=e%60;return n<=30?`~${t}${n>0?".5":""} h`:`~${t+1} h`}(Math.ceil(V/D)+(c.extraReadTimeMin||0)),O=[{flag:c.flagDraft,component:()=>Promise.all([n.e(3231),n.e(8809)]).then(n.bind(n,28809))},{flag:c.flagMindfuckery,component:()=>Promise.all([n.e(3231),n.e(2471)]).then(n.bind(n,67709))},{flag:c.flagRewrite,component:()=>Promise.all([n.e(3231),n.e(6764)]).then(n.bind(n,62002))},{flag:c.flagOffensive,component:()=>Promise.all([n.e(3231),n.e(2443)]).then(n.bind(n,17681))},{flag:c.flagProfane,component:()=>Promise.all([n.e(3231),n.e(8048)]).then(n.bind(n,53286))},{flag:c.flagMultilingual,component:()=>Promise.all([n.e(3231),n.e(4069)]).then(n.bind(n,78831))},{flag:c.flagUnreliably,component:()=>Promise.all([n.e(3231),n.e(3417)]).then(n.bind(n,8179))},{flag:c.flagPolitical,component:()=>Promise.all([n.e(3231),n.e(5195)]).then(n.bind(n,30433))},{flag:c.flagCognitohazard,component:()=>Promise.all([n.e(3231),n.e(3175)]).then(n.bind(n,8413))},{flag:c.flagHidden,component:()=>Promise.all([n.e(3231),n.e(9556)]).then(n.bind(n,14794))}],{0:Y,1:G}=(0,l.useState)([]);return(0,l.useEffect)((()=>{O.forEach((e=>{let{flag:t,component:n}=e;t&&n().then((e=>{G((t=>[].concat((0,a.A)(t),[e.default])))}))}))}),[]),l.createElement(y.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},l.createElement(b.A,{postNumber:c.index,date:c.date,updated:c.updated,readTime:B,difficulty:c.difficultyLevel,title:c.title,desc:c.desc,banner:c.banner,section:p,postKey:T,isMindfuckery:c.flagMindfuckery,mainTag:c.mainTag}),l.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},c.otherTags.map(((e,t)=>l.createElement("span",{key:t,className:`noselect ${A.MW}`,style:{margin:"0 5px 5px 0"}},e)))),l.createElement("div",{class:"postBody"},l.createElement(j,{toc:h})),l.createElement("br"),l.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},l.createElement(y.P.button,{class:"noselect",className:_.pb,id:_.xG,onClick:()=>{H(!k)},whileTap:{scale:.93}},l.createElement(y.P.div,{className:S.DJ,key:k,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},k?"Switch to default layout":"Switch to wide layout"))),l.createElement("br"),l.createElement("div",{class:"postBody",style:{margin:k?"0 -14%":"",maxWidth:k?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},l.createElement("div",{className:`${_.P_} ${I?_.Xn:_.qG}`},Y.map(((e,t)=>l.createElement(e,{key:t}))),c.indexCourse?l.createElement(x.A,{index:c.indexCourse,category:c.courseCategoryName}):"",l.createElement(g.Z.Provider,{value:{images:s.nodes,basePath:L.replace(/\/$/,"")+"/"}},l.createElement(i.xA,{components:{Image:f.A}},o)))),l.createElement(v.A,{nextPost:z,lastPost:M,keyCurrent:T,section:p}))}function M(e){return l.createElement(z,e,l.createElement(h,e))}function C(e){var t,n,a,i,r;let{data:s}=e;const{frontmatter:o}=s.mdx,c=o.titleSEO||o.title,m=o.titleOG||c,h=o.titleTwitter||c,p=o.descSEO||o.desc,u=o.descOG||p,g=o.descTwitter||p,f=o.schemaType||"BlogPosting",y=o.keywordsSEO,b=o.date,v=o.updated||b,w=o.imageOG||(null===(t=o.banner)||void 0===t||null===(n=t.childImageSharp)||void 0===n||null===(a=n.gatsbyImageData)||void 0===a||null===(i=a.images)||void 0===i||null===(r=i.fallback)||void 0===r?void 0:r.src),x=o.imageAltOG||u,S=o.imageTwitter||w,_=o.imageAltTwitter||g,N=o.canonicalURL,A=o.flagHidden||!1,j=o.mainTag||"Posts",z=o.slug.split("/")[1]||"posts",{siteUrl:M}=(0,d.Q)(),C={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:M},{"@type":"ListItem",position:2,name:j,item:`${M}/${o.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${M}${o.slug}`}]};return l.createElement(E.A,{title:c+" - avrtt.blog",titleOG:m,titleTwitter:h,description:p,descriptionOG:u,descriptionTwitter:g,schemaType:f,keywords:y,datePublished:b,dateModified:v,imageOG:w,imageAltOG:x,imageTwitter:S,imageAltTwitter:_,canonicalUrl:N,flagHidden:A,mainTag:j,section:z,type:"article"},l.createElement("script",{type:"application/ld+json"},JSON.stringify(C)))}},66501:function(e,t,n){n.d(t,{A:function(){return r}});var a=n(96540),i=n(3962),l="styles-module--tooltiptext--a263b";var r=e=>{let{text:t,isBadge:n=!1}=e;const{0:r,1:s}=(0,a.useState)(!1),o=(0,a.useRef)(null);return(0,a.useEffect)((()=>{function e(e){o.current&&!o.current.contains(e.target)&&s(!1)}return document.addEventListener("click",e),()=>{document.removeEventListener("click",e)}}),[]),a.createElement("span",{className:"styles-module--tooltipWrapper--75ebf",ref:o},a.createElement("img",{id:n?"styles-module--infoBadge--e3d66":"styles-module--info--26c1f",src:i.A,alt:"info",onClick:e=>{e.stopPropagation(),s((e=>!e))}}),a.createElement("span",{className:r?`${l} styles-module--visible--c063c`:l},t))}},96098:function(e,t,n){var a=n(96540),i=n(7978);t.A=e=>{let{text:t}=e;return a.createElement(i.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-js-content-file-path-src-pages-posts-research-t-distributed-sne-mdx-b10d025a4156ff87214d.js.map