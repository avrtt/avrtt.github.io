"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[1205],{39325:function(e,t,n){n.r(t),n.d(t,{Head:function(){return T},PostTemplate:function(){return S},default:function(){return C}});var a=n(54506),i=n(28453),l=n(96540),r=(n(16886),n(46295)),o=n(96098);function s(e){const t=Object.assign({p:"p",h3:"h3",a:"a",span:"span",ul:"ul",li:"li",strong:"strong",h2:"h2",ol:"ol"},(0,i.RP)(),e.components),{Image:n}=t;return n||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),l.createElement(l.Fragment,null,"\n",l.createElement("br"),"\n","\n","\n","\n",l.createElement(t.p,null,"Convolutional neural networks, often referred to as CNNs or ConvNets, have gained remarkable prominence in the field of machine learning and deep learning in recent decades. At their core, CNNs are biologically inspired models—researchers were originally motivated by discoveries in neuroscience that certain cells in the visual cortex respond strongly to localized regions of a stimulus. As such, convolutional neural networks are largely inspired by the visual processing system and have demonstrated an uncanny ability to learn hierarchical representations from images, audio signals, and other high-dimensional data sources. In this chapter, I want to dive into the rationale behind CNNs, their historical background, and the reasons for their enduring popularity. I will highlight the road from early neural network attempts to the watershed moment when they took center stage in computer vision challenges."),"\n",l.createElement(t.p,null,"The conceptual underpinnings of convolution-based processing in neural architectures can be traced back to the 1960s, when Hubel and Wiesel discovered the existence of simple and complex cells in the visual cortex of the cat's brain. These specialized cells responded selectively to edges or oriented lines in specific regions of the visual field. This finding laid the neurological foundation for the idea of localized receptive fields and hierarchical feature extraction."),"\n",l.createElement(t.p,null,"Fast-forward to the 1980s, when researchers like Yann LeCun started applying backpropagation-based training to feedforward neural networks with convolutional layers for image classification tasks. One of the earliest successes was LeCun's LeNet-5 architecture (LeCun and gang, 1989), designed primarily for digit recognition (for example, on the MNIST dataset). LeNet-5 used convolutional layers interspersed with subsampling (pooling) layers and fully connected layers to classify handwritten digits with unprecedented accuracy. At the time, limited computational resources and insufficient data restricted the widespread use of CNNs to more demanding tasks."),"\n",l.createElement(t.p,null,"Then, a key milestone occurred in 2012, when AlexNet (Krizhevsky, Sutskever, and Hinton, NeurIPS 2012) won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) by a significant margin. AlexNet demonstrated that, given a large dataset (ImageNet) and the computational power of GPUs, deep convolutional neural networks could outperform traditional computer vision methods by a wide margin. This triumph catalyzed a wave of research interest in CNNs, leading to subsequent improved architectures such as VGG, GoogLeNet, ResNet, DenseNet, and more. Each iteration introduced novel methods for deeper and more efficient network designs, fueling the sustained success of convolutional neural networks in numerous tasks."),"\n",l.createElement(t.h3,{id:"key-advantages-and-reasons-for-popularity",style:{position:"relative"}},l.createElement(t.a,{href:"#key-advantages-and-reasons-for-popularity","aria-label":"key advantages and reasons for popularity permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Key advantages and reasons for popularity"),"\n",l.createElement(t.p,null,"CNNs have become nearly synonymous with image classification, object detection, and other vision tasks because they exploit the spatial structure and local correlations in data. Convolutional filters can detect local features such as edges, corners, or textures, which then combine hierarchically to detect more global and abstract concepts. Here are some highlights of why CNNs rose to prominence:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Localized Receptive Fields"),": By only connecting each neuron to a local region of the input, convolutional filters reduce the number of parameters and exploit spatially local correlations—this is less prone to overfitting compared to a fully connected structure for high-dimensional data."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameter Sharing"),": In convolutional layers, filters are applied across different spatial positions, which reuses the same set of weights. This drastically reduces the parameter count, making the model more data-efficient and allowing deeper architectures."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Hierarchical Feature Extraction"),": Stacking multiple convolutional layers yields a hierarchy of progressively more abstract and semantic features. Early layers capture simple edges and shapes, while deeper layers can capture specific object parts or entire objects."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Translation Equivariance"),": Convolution naturally preserves the notion of translation invariance—or more precisely, translation equivariance. A feature learned in one part of the image can be recognized in another location thanks to the weight sharing mechanism."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Hardware Optimization"),": Modern GPU architectures (and more recent specialized hardware like TPUs) are well-suited to performing large numbers of parallel operations on arrays (tensors). Convolutions map neatly onto GPU operations, giving CNNs a massive computational advantage."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Transfer Learning"),": CNNs trained on large-scale data like ImageNet can be fine-tuned easily for new tasks with fewer data, thanks to feature generality in early to mid layers, improving performance on domain-specific tasks dramatically."),"\n"),"\n",l.createElement(t.p,null,"These advantages, combined with large labeled datasets, improved computing power, and algorithmic advances (like better weight initializations and normalization techniques), propelled CNNs to become the de facto approach for many computer vision tasks—and indeed a building block in many other areas, including speech recognition, text classification (via character embeddings), and beyond."),"\n",l.createElement(t.h2,{id:"2-core-concepts",style:{position:"relative"}},l.createElement(t.a,{href:"#2-core-concepts","aria-label":"2 core concepts permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2. Core concepts"),"\n",l.createElement(t.p,null,"In this section, I'll step through the fundamental ideas that make convolutional neural networks unique. The essence is the convolution operation, which can be understood as a small filter scanning across an input (such as an image) to detect features. Then we'll examine hyperparameters like stride and padding that significantly influence the network's behavior and dimension transformations. We'll cover pooling operations, activation functions, and also get into the mechanics of how backpropagation is applied to CNN layers. Altogether, these core concepts form the bedrock on which modern CNN architectures stand."),"\n",l.createElement(t.h3,{id:"convolution-operation-and-feature-maps",style:{position:"relative"}},l.createElement(t.a,{href:"#convolution-operation-and-feature-maps","aria-label":"convolution operation and feature maps permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Convolution operation and feature maps"),"\n",l.createElement(t.p,null,"In standard fully connected neural networks, every neuron is connected to all elements of the previous layer. By contrast, in CNNs, the fundamental building block is the convolutional layer, where a set of learnable filters (or kernels) is convolved with the input. If we consider a single filter ",l.createElement(o.A,{text:"\\(F\\)"})," of size ",l.createElement(o.A,{text:"\\(k \\times k\\)"})," applied to a 2D input (like a grayscale image of size ",l.createElement(o.A,{text:"\\(H \\times W\\)"}),"), the convolution operation can be expressed as follows:"),"\n",l.createElement(o.A,{text:"\\[\n(\\text{feature map})(i, j) = \\sum_{p=0}^{k-1} \\sum_{q=0}^{k-1} F(p, q) \\cdot X(i+p, j+q)\n\\]"}),"\n",l.createElement(t.p,null,"Where:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(o.A,{text:"\\(X\\)"})," is the input (an image or an activation map from a previous layer)."),"\n",l.createElement(t.li,null,l.createElement(o.A,{text:"\\(F\\)"})," is the filter or kernel of size ",l.createElement(o.A,{text:"\\(k \\times k\\)"}),"."),"\n",l.createElement(t.li,null,l.createElement(o.A,{text:"\\(i, j\\)"})," are spatial coordinates in the output feature map."),"\n",l.createElement(t.li,null,l.createElement(o.A,{text:"\\(p, q\\)"})," index the kernel's rows and columns."),"\n"),"\n",l.createElement(t.p,null,"This summation is done element-wise, and a bias term is often added at the end. Each filter in a convolutional layer typically slides (convolves) across the entire input volume, producing a 2D feature map capturing the presence of that filter's learned pattern at each location. If the input has multiple channels (for instance, an RGB image has three channels), the filter extends across all channels, and a similar summation is performed along that channel dimension. The output of applying multiple filters is stacked channel-wise, forming a 3D volume known as the output feature map."),"\n",l.createElement(t.p,null,"Importantly, when the network is trained, these filters are updated via gradient descent (or related optimization algorithms), so they adapt to detect features that minimize the overall loss function. After convolution, a non-linear activation function is usually applied, and the result is fed forward to subsequent layers."),"\n",l.createElement(t.h3,{id:"strides-padding-and-other-hyperparameters",style:{position:"relative"}},l.createElement(t.a,{href:"#strides-padding-and-other-hyperparameters","aria-label":"strides padding and other hyperparameters permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Strides, padding, and other hyperparameters"),"\n",l.createElement(t.p,null,"In implementing a convolutional layer, several hyperparameters govern the behavior of the convolution operation:"),"\n",l.createElement(t.ol,null,"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Kernel size")," (",l.createElement(o.A,{text:"\\(k\\)"}),"): Defines the spatial size of the convolution filter. Common choices include 3×3, 5×5, or 7×7 for image tasks. Smaller filters typically capture fine-grained features and require fewer parameters. Larger filters capture more context but can be more expensive in terms of parameters and computation."),"\n"),"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Stride")," (",l.createElement(o.A,{text:"\\(s\\)"}),"): Dictates how far the filter moves each time it is applied. A stride of 1 means the filter is applied at every adjacent position; a stride of 2 means the filter hops over positions, effectively halving the spatial dimension of the output (approximately). A larger stride leads to a smaller output size and can reduce computational cost but also might skip potentially important details."),"\n"),"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Padding"),": Often the input is padded with zeros (or reflections, in some cases) to preserve spatial dimensions during convolution. For a ",l.createElement(o.A,{text:"\\(k \\times k\\)"})," filter, a typical choice of padding ",l.createElement(o.A,{text:"\\(p = \\lfloor (k-1)/2 \\rfloor\\)"})," preserves the input size, making the output dimension equal to the input dimension, especially with a stride of 1. Padding ensures that edge pixels get convolved the same number of times as central pixels, preventing shrinkage of the output volume as the network goes deeper."),"\n"),"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Dilation"),": Dilation (or atrous convolution) effectively spaces out the kernel elements to capture a larger receptive field without increasing the number of parameters. It is sometimes used in tasks requiring large receptive fields (e.g., semantic segmentation)."),"\n"),"\n"),"\n",l.createElement(t.p,null,"These hyperparameters have a direct impact on the shape of the output feature maps, the computational cost, and the network's capacity to capture details at different scales."),"\n",l.createElement(t.h3,{id:"pooling-mechanisms-max-average-global",style:{position:"relative"}},l.createElement(t.a,{href:"#pooling-mechanisms-max-average-global","aria-label":"pooling mechanisms max average global permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Pooling mechanisms (max, average, global)"),"\n",l.createElement(t.p,null,"Pooling layers are typically interleaved between convolutional layers to reduce spatial dimensions, thereby summarizing regions of the feature maps. This not only helps to lower the computational burden and number of parameters but also provides some translational invariance by aggregating features within local neighborhoods."),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Max pooling"),": Takes the maximum value within each pooling region. For instance, a 2×2 max pooling operation with stride 2 returns the maximum value of every 2×2 block in the input."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Average pooling"),": Computes the average value within each pooling region. Historically, average pooling was common in some early architectures (e.g., LeNet-5), but max pooling tends to perform better in many modern CNNs for tasks like image classification."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Global pooling"),": A special case that aggregates over the entire spatial dimension, resulting in a single feature vector per channel. Global average pooling is sometimes used in place of fully connected layers at the end of a CNN (e.g., in certain architectures like Network in Network, or in modern classification networks). This drastically reduces parameter count and can help mitigate overfitting."),"\n"),"\n",l.createElement(t.p,null,"Pooling helps the network focus on the most salient features by discarding less relevant details. However, the choice between max and average pooling (and whether or not to even include pooling) depends on the specific application. Some advanced architectures rely on strided convolutions instead of pooling or adopt a combination of multiple approaches."),"\n",l.createElement(t.h3,{id:"activation-functions-in-case-of-cnn",style:{position:"relative"}},l.createElement(t.a,{href:"#activation-functions-in-case-of-cnn","aria-label":"activation functions in case of cnn permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Activation functions in case of CNN"),"\n",l.createElement(t.p,null,"After the convolution and pooling steps, we need a non-linear activation function to ensure that the network can learn complex, non-linear mappings. In the early days, ",l.createElement(o.A,{text:"(\\mathrm{tanh})"})," or sigmoid activations were used, but these have largely been superseded by the ReLU (Rectified Linear Unit) family:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"ReLU"),": ",l.createElement(o.A,{text:"\\( \\mathrm{ReLU}(z) = \\max(0, z) \\)"}),", which sets negative values to zero. ReLU addresses the vanishing gradient problem encountered with sigmoid or tanh for deeper networks, enabling training of much deeper models."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Leaky ReLU"),": ",l.createElement(o.A,{text:"\\( \\mathrm{LeakyReLU}(z) = \\max(0.01 \\cdot z, z) \\)"}),' attempts to mitigate the so-called "dying ReLU" problem by allowing a small, non-zero gradient for negative values.'),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"ELU"),", ",l.createElement(t.strong,null,"SELU"),", ",l.createElement(t.strong,null,"GELU"),", etc.: Numerous variations have been proposed, each with theoretical or empirical advantages in certain contexts. SELU (Scaled Exponential Linear Unit) is designed for self-normalizing neural networks. GELU (Gaussian Error Linear Unit) is sometimes popular in transformers, although it can be used in CNNs as well."),"\n"),"\n",l.createElement(t.p,null,"The choice of activation can affect training stability, convergence speed, and final accuracy, but ReLU and its variants remain some of the most widely used in CNN applications."),"\n",l.createElement(t.h3,{id:"how-backpropagation-and-parameter-updating-work-in-case-of-cnn",style:{position:"relative"}},l.createElement(t.a,{href:"#how-backpropagation-and-parameter-updating-work-in-case-of-cnn","aria-label":"how backpropagation and parameter updating work in case of cnn permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"How backpropagation and parameter updating work in case of CNN"),"\n",l.createElement(t.p,null,"The backpropagation algorithm in CNNs is conceptually the same as in fully connected networks but with the additional complexity of weight sharing and local connectivity. During the forward pass:"),"\n",l.createElement(t.ol,null,"\n",l.createElement(t.li,null,"Each filter is convolved over the input."),"\n",l.createElement(t.li,null,"An activation function is applied to each element of the resulting feature maps."),"\n",l.createElement(t.li,null,"Downstream layers (pooling, additional convolutions, fully connected layers, etc.) transform this output."),"\n",l.createElement(t.li,null,"The final layer typically produces a prediction such as a probability distribution over classes."),"\n"),"\n",l.createElement(t.p,null,"During the backward pass:"),"\n",l.createElement(t.ol,null,"\n",l.createElement(t.li,null,"The gradient of the loss with respect to the output of each layer is computed (using chain rule)."),"\n",l.createElement(t.li,null,"At the convolutional layer, these gradients are aggregated with respect to each filter and position where the filter was applied. Since the same filter is slid across the input (weight sharing), the gradient for the filter is accumulated over all spatial locations."),"\n",l.createElement(t.li,null,"The filters' weights and biases are updated accordingly, typically using optimizers like stochastic gradient descent (SGD), Adam, RMSProp, etc."),"\n"),"\n",l.createElement(t.p,null,"In code, frameworks like TensorFlow, PyTorch, and Keras handle this process automatically under the hood, so you only need to define the forward pass, the loss function, and the optimizer. However, understanding how backprop works is crucial for diagnosing problems like exploding or vanishing gradients, especially in deeper CNN architectures."),"\n",l.createElement(t.h3,{id:"etc",style:{position:"relative"}},l.createElement(t.a,{href:"#etc","aria-label":"etc permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Etc."),"\n",l.createElement(t.p,null,"Beyond the above fundamentals, some additional topics often interwoven with the CNN architecture discussion include:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Padding modes"),": Zero-padding is the most common, but reflection or replicate padding can be used to reduce boundary artifacts."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Weight initialization"),": He initialization or Glorot (Xavier) initialization is typically recommended for ReLU-based or linear-based CNNs, respectively, which helps control signal variance across layers."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Regularization"),": CNNs have a large number of parameters; hence, techniques like weight decay (L2 regularization), dropout, or data augmentation are often employed to reduce overfitting."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Normalization"),": Normalizing activations can speed up training and improve stability, which leads into our next chapter that covers batch normalization in detail."),"\n"),"\n",l.createElement(t.h2,{id:"3-building-cnn",style:{position:"relative"}},l.createElement(t.a,{href:"#3-building-cnn","aria-label":"3 building cnn permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3. Building CNN"),"\n",l.createElement(t.p,null,"Having established the core ideas, let's look at the main building blocks in detail. A typical CNN can be thought of as a combination of multiple stages of convolutional + (optionally) pooling + normalization layers, culminating in a few fully connected layers. We will also discuss batch normalization and dropout for better training stability and generalization. Finally, we'll walk through a simple example in TensorFlow/Keras that illustrates how to build a CNN step by step."),"\n",l.createElement(t.h3,{id:"convolutional-layer-in-detail",style:{position:"relative"}},l.createElement(t.a,{href:"#convolutional-layer-in-detail","aria-label":"convolutional layer in detail permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Convolutional layer in detail"),"\n",l.createElement(t.p,null,"A convolutional layer is characterized by the following parameters:"),"\n",l.createElement(t.ol,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Number of filters")," (",l.createElement(o.A,{text:"\\(M\\)"}),"): The layer will learn ",l.createElement(o.A,{text:"\\(M\\)"})," distinct filters that each produce a distinct 2D feature map output. These feature maps are stacked depth-wise, forming the output volume with shape ",l.createElement(o.A,{text:"\\(H_{\\text{out}} \\times W_{\\text{out}} \\times M\\)"}),"."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Filter size")," (",l.createElement(o.A,{text:"\\(k \\times k\\)"}),"): Determines the spatial region in which the layer will scan. As mentioned, 3×3 filters are extremely common in modern architectures."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Stride")," (",l.createElement(o.A,{text:"\\(s\\)"}),"), ",l.createElement(t.strong,null,"Padding")," (",l.createElement(o.A,{text:"\\(p\\)"}),"), and other hyperparameters as discussed."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Non-linear activation"),": Usually applied right after the convolution."),"\n"),"\n",l.createElement(t.p,null,"In some architectures, you might find grouped convolutions (splitting channels into groups processed separately) or depthwise separable convolutions (factorizing standard convolution into depthwise and pointwise convolutions). These advanced techniques reduce computational burden and can help the network learn more efficiently."),"\n",l.createElement(t.p,null,"When designing a CNN from scratch, I usually begin with a small set of filters in early layers. Deeper into the network, the number of channels typically increases (e.g., doubling with each major block in some architectures). This pattern recognizes the fact that deeper layers can abstract more complex features but also need more representational capacity."),"\n",l.createElement(t.h3,{id:"pooling-layer-in-detail",style:{position:"relative"}},l.createElement(t.a,{href:"#pooling-layer-in-detail","aria-label":"pooling layer in detail permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Pooling layer in detail"),"\n",l.createElement(t.p,null,"The pooling layer is relatively simple. For max pooling, for instance, consider a region ",l.createElement(o.A,{text:"\\(k_p \\times k_p\\)"}),":"),"\n",l.createElement(o.A,{text:"\\[\n(\\text{pooled map})(i, j) = \\max_{p,q \\in \\{0, \\ldots, k_p-1\\}} X(s \\cdot i + p, s \\cdot j + q)\n\\]"}),"\n",l.createElement(t.p,null,"Where:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(o.A,{text:"\\(X\\)"})," is the input feature map."),"\n",l.createElement(t.li,null,l.createElement(o.A,{text:"\\(s\\)"})," is the stride for pooling."),"\n",l.createElement(t.li,null,l.createElement(o.A,{text:"\\(k_p\\)"})," is the pooling kernel size."),"\n"),"\n",l.createElement(t.p,null,"The effect is a reduction in the spatial dimension. Pooling drastically lowers the number of activations that flow into subsequent layers, thereby improving computational efficiency and offering local translational invariance."),"\n",l.createElement(t.h3,{id:"fully-connected-layer",style:{position:"relative"}},l.createElement(t.a,{href:"#fully-connected-layer","aria-label":"fully connected layer permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Fully connected layer"),"\n",l.createElement(t.p,null,'Typically, near the network\'s output, the spatial dimension becomes sufficiently small that the feature maps can be "flattened" into a single vector. That vector then feeds into one or more fully connected layers (also called dense layers in frameworks like Keras). For classification, the final layer might be a fully connected layer with ',l.createElement(o.A,{text:"\\(C\\)"})," outputs, where ",l.createElement(o.A,{text:"\\(C\\)"})," is the number of classes, followed by a softmax. In older CNN architectures (like AlexNet or VGG), these fully connected layers often contain the majority of network parameters. Modern architectures sometimes replace the last fully connected layer with global average pooling and a smaller classifier to reduce overfitting."),"\n",l.createElement(t.h3,{id:"batch-normalization-for-cnn",style:{position:"relative"}},l.createElement(t.a,{href:"#batch-normalization-for-cnn","aria-label":"batch normalization for cnn permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Batch normalization for CNN"),"\n",l.createElement(t.p,null,'Batch normalization (Ioffe and Szegedy, 2015) was a breakthrough technique that normalizes intermediate activations. Conceptually, it helps avoid the problem of "internal covariate shift" by normalizing mini-batch statistics, thus stabilizing and speeding up training. Mathematically, for each activation channel ',l.createElement(o.A,{text:"\\(c\\)"})," in a layer, batch normalization computes:"),"\n",l.createElement(o.A,{text:"\\[\n\\hat{x}_c = \\frac{x_c - \\mu_c}{\\sqrt{\\sigma_c^2 + \\epsilon}},\n\\quad\ny_c = \\gamma_c \\hat{x}_c + \\beta_c\n\\]"}),"\n",l.createElement(t.p,null,"Where:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(o.A,{text:"\\(x_c\\)"})," is the pre-activation value of channel ",l.createElement(o.A,{text:"\\(c\\)"}),"."),"\n",l.createElement(t.li,null,l.createElement(o.A,{text:"\\(\\mu_c\\)"})," and ",l.createElement(o.A,{text:"\\(\\sigma_c^2\\)"})," are the mean and variance of channel ",l.createElement(o.A,{text:"\\(c\\)"})," computed over the mini-batch."),"\n",l.createElement(t.li,null,l.createElement(o.A,{text:"\\(\\gamma_c\\)"})," and ",l.createElement(o.A,{text:"\\(\\beta_c\\)"})," are learnable parameters that scale and shift the normalized value."),"\n",l.createElement(t.li,null,l.createElement(o.A,{text:"\\(\\epsilon\\)"})," is a small constant to avoid division by zero."),"\n"),"\n",l.createElement(t.p,null,"Batch normalization is typically inserted after the convolutional operation and before the non-linear activation (although some variants place it after activation). It provides a more stable gradient flow and often allows for higher learning rates, reducing the sensitivity to weight initialization."),"\n",l.createElement(t.h3,{id:"dropout-for-regularization",style:{position:"relative"}},l.createElement(t.a,{href:"#dropout-for-regularization","aria-label":"dropout for regularization permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Dropout for regularization"),"\n",l.createElement(t.p,null,'Dropout is another important strategy used in CNNs to address overfitting. The idea is to "drop" (i.e., zero out) a random subset of activations during training, preventing the network from relying too heavily on any particular set of co-adapted features. In fully connected layers, dropout is often placed right before or after the layer. Convolutional dropout can also be applied, though its usage patterns can differ. The standard dropout formula for an activation ',l.createElement(o.A,{text:"\\(z\\)"})," is:"),"\n",l.createElement(o.A,{text:"\\[\nz_{\\text{dropped}} =\n\\begin{cases}\n0, & \\text{with probability } p \\\\\n\\frac{z}{1-p}, & \\text{with probability } (1-p)\n\\end{cases}\n\\]"}),"\n",l.createElement(t.p,null,"Where ",l.createElement(o.A,{text:"\\(p\\)"})," is the dropout rate. This means a fraction ",l.createElement(o.A,{text:"\\(p\\)"})," of the neurons is set to zero. During inference, the activations are scaled accordingly, so the final predictions remain consistent."),"\n",l.createElement(t.h3,{id:"step-by-step-example-with-code-snippets-in-tensorflowkeras",style:{position:"relative"}},l.createElement(t.a,{href:"#step-by-step-example-with-code-snippets-in-tensorflowkeras","aria-label":"step by step example with code snippets in tensorflowkeras permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Step-by-step example with code snippets in TensorFlow/Keras"),"\n",l.createElement(t.p,null,"Let's walk through a straightforward example of constructing a CNN in TensorFlow/Keras for image classification (e.g., MNIST). This is a relatively simple dataset, but the mechanics can be extended to more complex tasks."),"\n",l.createElement(r.A,{text:"\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# 1. Load dataset\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n\n# MNIST images are 28x28, grayscale. Reshape for a channel dimension.\nx_train = x_train.reshape((-1, 28, 28, 1)).astype(\"float32\") / 255.0\nx_test = x_test.reshape((-1, 28, 28, 1)).astype(\"float32\") / 255.0\n\n# 2. Define a simple CNN model\nmodel = keras.Sequential([\n    layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu',\n                  input_shape=(28, 28, 1)),\n    layers.MaxPooling2D(pool_size=(2, 2)),\n    layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n    layers.MaxPooling2D(pool_size=(2, 2)),\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(10, activation='softmax')\n])\n\n# 3. Compile the model\nmodel.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# 4. Train the model\nmodel.fit(x_train, y_train, batch_size=64, epochs=5, validation_split=0.1)\n\n# 5. Evaluate on test set\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(f\"Test Accuracy: {test_acc}\")\n"}),"\n",l.createElement(t.p,null,"Here's the step-by-step breakdown:"),"\n",l.createElement(t.ol,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Dataset"),": We load the MNIST dataset from Keras. Each image is 28×28 pixels, and there is a single channel (grayscale). We normalize pixel values to the range [0, 1] by dividing by 255."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Model Architecture"),":","\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Conv2D(32 filters, 3×3, ReLU)"),": The first convolutional layer takes the input (28×28×1) and outputs 32 feature maps. The kernel size is 3×3."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"MaxPooling2D(2×2)"),": Reduces each spatial dimension by a factor of 2."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Conv2D(64 filters, 3×3, ReLU)"),": A second convolutional layer with more filters."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"MaxPooling2D(2×2)"),": Further dimensionality reduction."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Flatten"),": Flattens the 2D feature maps to a 1D vector."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Dense(128, ReLU)"),": A fully connected layer of 128 hidden units."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Dropout(0.5)"),": Randomly drops 50% of the units to mitigate overfitting."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Dense(10, softmax)"),": Outputs probabilities for 10 classes."),"\n"),"\n"),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Compilation"),": We use Adam optimizer and the standard cross-entropy loss for multi-class classification. We also track accuracy as a metric."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Training"),": We train for 5 epochs with a batch size of 64 and a validation split of 10%."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Evaluation"),": Finally, we evaluate on the held-out test set."),"\n"),"\n",l.createElement(t.h3,{id:"hyperparameter-tuning",style:{position:"relative"}},l.createElement(t.a,{href:"#hyperparameter-tuning","aria-label":"hyperparameter tuning permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Hyperparameter tuning"),"\n",l.createElement(t.p,null,"Designing CNN architectures involves many hyperparameters and design choices. Here are some you might experiment with:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Number of filters")," per layer and how this number grows or remains constant across layers."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Filter size"),": Trying different kernel sizes (e.g., 3×3 vs. 5×5)."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Pooling strategy"),": Using max pooling vs. average pooling vs. strided convolutions, plus the size and stride of pooling."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Network depth"),": The number of convolutional + pooling layers. Deeper networks can learn more features but are more prone to overfitting or vanishing gradients if not properly regularized and normalized."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Learning rate"),": Possibly scheduling or decaying the learning rate as training progresses, or using advanced optimizers such as Adam, SGD with momentum, RMSProp, etc."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Batch size"),": Larger batch sizes may require careful tuning of the learning rate. Smaller batches can cause noisy gradients but can improve generalization in some scenarios."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Regularization"),": Varying dropout rates, weight decay, or data augmentation strategies."),"\n"),"\n",l.createElement(t.p,null,"In practice, systematic hyperparameter optimization (grid search, random search, Bayesian optimization, or more sophisticated methods) can be employed to find the best combination for a given dataset and problem setup."),"\n",l.createElement(t.h2,{id:"4-transfer-learning-in-cnns",style:{position:"relative"}},l.createElement(t.a,{href:"#4-transfer-learning-in-cnns","aria-label":"4 transfer learning in cnns permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4. Transfer learning in CNNs"),"\n",l.createElement(t.p,null,"Transfer learning is a technique that leverages a model pre-trained on a large dataset (e.g., ImageNet) and adapts it to a different target task, typically with fewer data. This approach is extremely powerful in domains where gathering a sufficiently large labeled dataset from scratch is challenging or impossible. The intuition is that the early layers of a CNN learn general features (e.g., edges, corners, textures), which can be reused across various image-recognition tasks, while the later layers learn more domain-specific features."),"\n",l.createElement(t.p,null,"The general steps are:"),"\n",l.createElement(t.ol,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Select a pre-trained model"),": Popular choices include VGG16, ResNet50, Inception, etc., all pre-trained on ImageNet."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Freeze early layers"),': The first few layers are left unmodified (their weights are "frozen") to preserve their learned representations.'),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Replace and fine-tune later layers"),": Remove the old classification layer(s) and add a new output layer (with the number of classes for the new task). Optionally, unfreeze some top layers if deeper fine-tuning is needed."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Train on the new data"),": Since fewer parameters are learned from scratch, the risk of overfitting is reduced."),"\n"),"\n",l.createElement(t.p,null,"Here's a simplified code snippet demonstrating how you might fine-tune a pre-trained model from Keras:"),"\n",l.createElement(r.A,{text:"\nbase_model = keras.applications.VGG16(\n    weights='imagenet',\n    include_top=False,  # exclude the ImageNet classifier\n    input_shape=(224, 224, 3)\n)\n\n# Freeze base_model\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# Add new trainable layers on top\nx = layers.Flatten()(base_model.output)\nx = layers.Dense(256, activation='relu')(x)\nx = layers.Dropout(0.5)(x)\noutputs = layers.Dense(num_classes, activation='softmax')(x)\n\nmodel = keras.Model(inputs=base_model.input, outputs=outputs)\n\n# Compile and train\nmodel.compile(optimizer='adam', \n              loss='categorical_crossentropy', \n              metrics=['accuracy'])\nmodel.fit(new_data, new_labels, epochs=10, batch_size=32)\n"}),"\n",l.createElement(t.p,null,"This simple strategy allows one to benefit from powerful feature representations learned on large-scale data. It's particularly common in practice for tasks in medical imaging, fine-grained object recognition, and more, where data is either rare or expensive to label."),"\n",l.createElement(t.h2,{id:"5-motivation-for-more-advanced-cnn-architectures-transition-to-cnn-architecture-part-2",style:{position:"relative"}},l.createElement(t.a,{href:"#5-motivation-for-more-advanced-cnn-architectures-transition-to-cnn-architecture-part-2","aria-label":"5 motivation for more advanced cnn architectures transition to cnn architecture part 2 permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),'5. Motivation for more advanced CNN architectures (transition to "CNN architecture, part 2")'),"\n",l.createElement(t.p,null,"To wrap up this introduction to CNN architectures, it's worth mentioning that the straightforward feedforward pattern we've discussed—convolution, pooling, normalization, dropout, and fully connected layers—serves as the foundation of more advanced architectures. Over time, researchers have sought ways to make CNNs deeper, more parameter-efficient, and better regularized. Innovations like residual connections (ResNet), inception modules, dense connections (DenseNet), and various attention mechanisms have significantly boosted performance and training stability in tasks ranging from simple classification to object detection, instance segmentation, and beyond."),"\n",l.createElement(t.p,null,"In \"CNN architecture, pt. 2\", I'll dive into these more complex patterns. We'll explore how skip connections mitigate vanishing gradients in deep networks, how inception modules help the network look at multiple filter sizes in parallel, how depthwise separable convolutions can drastically reduce computational overhead, and other state-of-the-art ideas that push the boundaries of CNN performance."),"\n",l.createElement(n,{alt:"Typical CNN diagram",path:"",caption:"An illustrative diagram of a simple CNN with convolution, activation, pooling, and fully connected layers.",zoom:"false"}),"\n",l.createElement(t.p,null,"Ultimately, understanding these advanced architectures is paramount if your work involves state-of-the-art image tasks, specialized object detection systems, or more efficient CNN designs for real-time processing on devices with limited computational resources. The core principles remain the same—localized feature extraction via convolution, dimensionality reduction via pooling, progressive abstraction in deeper layers—but the structural variations have proved to be game changers."),"\n",l.createElement(t.p,null,"I believe this thorough overview provides the necessary building blocks for you to fully grasp modern convolutional neural networks, not only as black-box function approximators but as architectures that elegantly exploit the spatial correlations in their input. Equipped with knowledge of the convolution operation, pooling strategies, activation functions, backpropagation mechanics, and regularization techniques, you are now ready to tackle more intricate designs and push the frontiers of CNN applications."))}var c=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,i.RP)(),e.components);return t?l.createElement(t,e,l.createElement(s,e)):s(e)};var m=n(36710),d=n(58481),h=n.n(d),u=n(36310),p=n(87245),g=n(27042),f=n(59849),v=n(5591),y=n(61122),E=n(9219),b=n(33203),w=n(95751),k=n(94328),x=n(80791),N=n(78137);const z=e=>{let{toc:t}=e;if(!t||!t.items)return null;return l.createElement("nav",{className:x.R},l.createElement("ul",null,t.items.map(((e,t)=>l.createElement("li",{key:t},l.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const n=t.replace("#",""),a=document.getElementById(n);a&&a.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&l.createElement(z,{toc:{items:e.items}}))))))};function S(e){let{data:{mdx:t,allMdx:r,allPostImages:o},children:s}=e;const{frontmatter:c,body:m,tableOfContents:d}=t,f=c.index,x=c.slug.split("/")[1],S=r.nodes.filter((e=>e.frontmatter.slug.includes(`/${x}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),C=S.findIndex((e=>e.frontmatter.index===f)),T=S[C+1],_=S[C-1],H=c.slug.replace(/\/$/,""),A=/[^/]*$/.exec(H)[0],I=`posts/${x}/content/${A}/`,{0:L,1:M}=(0,l.useState)(c.flagWideLayoutByDefault),{0:P,1:D}=(0,l.useState)(!1);var V;(0,l.useEffect)((()=>{D(!0);const e=setTimeout((()=>D(!1)),340);return()=>clearTimeout(e)}),[L]),"adventures"===x?V=E.cb:"research"===x?V=E.Qh:"thoughts"===x&&(V=E.T6);const R=h()(m).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,B=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),n=e%60;return n<=30?`~${t}${n>0?".5":""} h`:`~${t+1} h`}(Math.ceil(R/V)+(c.extraReadTimeMin||0)),G=[{flag:c.flagDraft,component:()=>Promise.all([n.e(3231),n.e(8809)]).then(n.bind(n,28809))},{flag:c.flagMindfuckery,component:()=>Promise.all([n.e(3231),n.e(2471)]).then(n.bind(n,67709))},{flag:c.flagRewrite,component:()=>Promise.all([n.e(3231),n.e(6764)]).then(n.bind(n,62002))},{flag:c.flagOffensive,component:()=>Promise.all([n.e(3231),n.e(2443)]).then(n.bind(n,17681))},{flag:c.flagProfane,component:()=>Promise.all([n.e(3231),n.e(8048)]).then(n.bind(n,53286))},{flag:c.flagMultilingual,component:()=>Promise.all([n.e(3231),n.e(4069)]).then(n.bind(n,78831))},{flag:c.flagUnreliably,component:()=>Promise.all([n.e(3231),n.e(3417)]).then(n.bind(n,8179))},{flag:c.flagPolitical,component:()=>Promise.all([n.e(3231),n.e(5195)]).then(n.bind(n,30433))},{flag:c.flagCognitohazard,component:()=>Promise.all([n.e(3231),n.e(3175)]).then(n.bind(n,8413))},{flag:c.flagHidden,component:()=>Promise.all([n.e(3231),n.e(9556)]).then(n.bind(n,14794))}],{0:U,1:F}=(0,l.useState)([]);return(0,l.useEffect)((()=>{G.forEach((e=>{let{flag:t,component:n}=e;t&&n().then((e=>{F((t=>[].concat((0,a.A)(t),[e.default])))}))}))}),[]),l.createElement(g.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},l.createElement(v.A,{postNumber:c.index,date:c.date,updated:c.updated,readTime:B,difficulty:c.difficultyLevel,title:c.title,desc:c.desc,banner:c.banner,section:x,postKey:A,isMindfuckery:c.flagMindfuckery,mainTag:c.mainTag}),l.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},c.otherTags.map(((e,t)=>l.createElement("span",{key:t,className:`noselect ${N.MW}`,style:{margin:"0 5px 5px 0"}},e)))),l.createElement("div",{class:"postBody"},l.createElement(z,{toc:d})),l.createElement("br"),l.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},l.createElement(g.P.button,{class:"noselect",className:k.pb,id:k.xG,onClick:()=>{M(!L)},whileTap:{scale:.93}},l.createElement(g.P.div,{className:w.DJ,key:L,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},L?"Switch to default layout":"Switch to wide layout"))),l.createElement("br"),l.createElement("div",{class:"postBody",style:{margin:L?"0 -14%":"",maxWidth:L?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},l.createElement("div",{className:`${k.P_} ${P?k.Xn:k.qG}`},U.map(((e,t)=>l.createElement(e,{key:t}))),c.indexCourse?l.createElement(b.A,{index:c.indexCourse,category:c.courseCategoryName}):"",l.createElement(u.Z.Provider,{value:{images:o.nodes,basePath:I.replace(/\/$/,"")+"/"}},l.createElement(i.xA,{components:{Image:p.A}},s)))),l.createElement(y.A,{nextPost:T,lastPost:_,keyCurrent:A,section:x}))}function C(e){return l.createElement(S,e,l.createElement(c,e))}function T(e){var t,n,a,i,r;let{data:o}=e;const{frontmatter:s}=o.mdx,c=s.titleSEO||s.title,d=s.titleOG||c,h=s.titleTwitter||c,u=s.descSEO||s.desc,p=s.descOG||u,g=s.descTwitter||u,v=s.schemaType||"BlogPosting",y=s.keywordsSEO,E=s.date,b=s.updated||E,w=s.imageOG||(null===(t=s.banner)||void 0===t||null===(n=t.childImageSharp)||void 0===n||null===(a=n.gatsbyImageData)||void 0===a||null===(i=a.images)||void 0===i||null===(r=i.fallback)||void 0===r?void 0:r.src),k=s.imageAltOG||p,x=s.imageTwitter||w,N=s.imageAltTwitter||g,z=s.canonicalURL,S=s.flagHidden||!1,C=s.mainTag||"Posts",T=s.slug.split("/")[1]||"posts",{siteUrl:_}=(0,m.Q)(),H={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:_},{"@type":"ListItem",position:2,name:C,item:`${_}/${s.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${_}${s.slug}`}]};return l.createElement(f.A,{title:c+" - avrtt.blog",titleOG:d,titleTwitter:h,description:u,descriptionOG:p,descriptionTwitter:g,schemaType:v,keywords:y,datePublished:E,dateModified:b,imageOG:w,imageAltOG:k,imageTwitter:x,imageAltTwitter:N,canonicalUrl:z,flagHidden:S,mainTag:C,section:T,type:"article"},l.createElement("script",{type:"application/ld+json"},JSON.stringify(H)))}},96098:function(e,t,n){var a=n(96540),i=n(7978);t.A=e=>{let{text:t}=e;return a.createElement(i.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-js-content-file-path-src-pages-posts-research-cnn-architecture-mdx-903e434af482c92cf26f.js.map