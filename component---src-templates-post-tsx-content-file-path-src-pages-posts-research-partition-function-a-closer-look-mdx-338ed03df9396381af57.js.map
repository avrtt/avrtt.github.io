{"version":3,"file":"component---src-templates-post-tsx-content-file-path-src-pages-posts-research-partition-function-a-closer-look-mdx-338ed03df9396381af57.js","mappings":"4GAAA,ooD,gFCIWA,EAAc,oCC2CzB,MA/BwCC,IAA8B,IAA7B,KAAEC,EAAI,QAAEC,GAAQ,GAAOF,EAC5D,MAAM,EAACG,EAAO,EAACC,IAAaC,EAAAA,EAAAA,WAAS,GAC/BC,GAAaC,EAAAA,EAAAA,QAA+B,MAmBlD,OAZAC,EAAAA,EAAAA,YAAU,KACN,SAASC,EAAmBC,GACpBJ,EAAWK,SAAWD,EAAME,kBAAkBC,OAASP,EAAWK,QAAQG,SAASJ,EAAME,SACzFR,GAAU,EAElB,CAEA,OADAW,SAASC,iBAAiB,QAASP,GAC5B,KACHM,SAASE,oBAAoB,QAASR,EAAmB,CAC5D,GACF,IAGCS,EAAAA,cAAA,QAAMC,UDnCc,uCCmCoBC,IAAKd,GACzCY,EAAAA,cAAA,OAAKG,GAAInB,EDrCE,kCADL,6BCsC6CoB,IAAKC,EAAAA,EAAMC,IAAI,OAAOC,QAnBxDC,IACrBA,EAAEC,kBACFvB,GAAWwB,IAAUA,GAAK,IAkBtBV,EAAAA,cAAA,QAAMC,UAAWhB,EAAS,GAAG0B,kCAAyCA,GACjE5B,GAEF,C,sMCgBf,SAAS6B,EAAkBC,GACzB,MAAMC,EAAcC,OAAOC,OAAO,CAChCC,EAAG,IACHC,GAAI,KACJC,EAAG,IACHC,KAAM,OACNC,GAAI,KACJC,GAAI,KACJC,GAAI,KACJC,OAAQ,SACRC,GAAI,KACJC,GAAI,OACHC,EAAAA,EAAAA,MAAsBd,EAAMe,aAAa,MAACC,GAASf,EAEtD,OADKe,GA+zBP,SAA8B1B,EAAI2B,GAChC,MAAM,IAAIC,MAAM,aAAeD,EAAY,YAAc,UAAY,KAAO3B,EAAK,qEACnF,CAj0Bc6B,CAAqB,SAAS,GACnChC,EAAAA,cAAoBA,EAAAA,SAAgB,KAAM,KAAMA,EAAAA,cAAoB,MAAO,KAAM,KAAM,KAAMA,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,+VAAgWjB,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,iCAAkC,cAAejC,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,wBAAyB,0NAA2NjC,EAAAA,cAAoBkC,EAAAA,EAAO,CACr3BnD,KAAM,YACJ,4YAA6Y,KAAMiB,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,oWAAqWjB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC9zBnD,KAAM,YACJ,gLAAiLiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC9MnD,KAAM,YACJ,0HAA2H,KAAMiB,EAAAA,cAAoBc,EAAYI,GAAI,CACvKf,GAAI,oEACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,qEACN,aAAc,8EACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,qEAAsE,KAAMvC,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,sBAAuBjB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC1KnD,KAAM,YACJ,yEAA0EiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACvGnD,KAAM,kCACJ,WAAYiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACzCnD,KAAM,sBACJ,oBAAqBiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAClDnD,KAAM,kBACJ,0DAA2DiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACxFnD,KAAM,sBACJ,gEAAiEiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC9FnD,KAAM,sBACJ,yBAA0B,KAAMiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC7DnD,KAAM,8FACJ,KAAMiB,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,0BAA2BjB,EAAAA,cAAoBkC,EAAAA,EAAO,CACvGnD,KAAM,qBACJ,uBAAwB,KAAMiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC3DnD,KAAM,4FACJ,KAAMiB,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,MAAOjB,EAAAA,cAAoBkC,EAAAA,EAAO,CACnFnD,KAAM,sBACJ,oBAAqB,KAAMiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACxDnD,KAAM,6FACJ,KAAMiB,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,MAAOjB,EAAAA,cAAoBkC,EAAAA,EAAO,CACnFnD,KAAM,sBACJ,iEAAkEiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC/FnD,KAAM,kCACJ,aAAc,KAAMiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACjDnD,KAAM,8FACJ,KAAMiB,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,UAAWjB,EAAAA,cAAoBkC,EAAAA,EAAO,CACvFnD,KAAM,qBACJ,iCAAkCiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC/DnD,KAAM,qBACJ,qGAAsGiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACnInD,KAAM,kCACJ,oCAAqCiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAClEnD,KAAM,kBACJ,KAAM,KAAMiB,EAAAA,cAAoBc,EAAYI,GAAI,CAClDf,GAAI,wCACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,yCACN,aAAc,kDACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,yCAA0C,KAAMvC,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,uHAAwHjB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC/OnD,KAAM,qBACJ,2EAA4EiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACzGnD,KAAM,sBACJ,8OAA+OiB,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,0BAA2B,QAASjC,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,gCAAiC,qDAAsDjC,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,gCAAiC,KAAM,KAAMjC,EAAAA,cAAoBc,EAAYI,GAAI,CAC5iBf,GAAI,gDACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,iDACN,aAAc,0DACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,iDAAkD,KAAMvC,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,qEAAsEjB,EAAAA,cAAoBkC,EAAAA,EAAO,CACrMnD,KAAM,YACJ,6GAA8GiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC3InD,KAAM,qBACJ,qCAAsCiB,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,oBAAqB,QAASjC,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,kBAAmB,qMAAsMjC,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,qBAAsB,+OAAgPjC,EAAAA,cAAoBwC,EAAAA,EAAS,CACjrBzD,KAAM,4BACJ,oDAAqD,KAAMiB,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,+UAAgVjB,EAAAA,cAAoBkC,EAAAA,EAAO,CACjdnD,KAAM,qBACJ,wKAAyK,KAAMiB,EAAAA,cAAoBc,EAAYO,GAAI,CACrNlB,GAAI,gCACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,iCACN,aAAc,0CACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,kCAAmC,KAAMvC,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,0JAA2J,KAAMjB,EAAAA,cAAoBkC,EAAAA,EAAO,CACjRnD,KAAM,yHACJ,KAAMiB,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,SAAUjB,EAAAA,cAAoBkC,EAAAA,EAAO,CACtFnD,KAAM,8BACJ,WAAYiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACzCnD,KAAM,YACJ,sBAAuBiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACpDnD,KAAM,YACJ,0CAA2CiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACxEnD,KAAM,wCACJ,aAAc,KAAMiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACjDnD,KAAM,2FACJ,KAAMiB,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,uDAAwD,KAAMjB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC1InD,KAAM,yGACJ,KAAMiB,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,qBAAsBjB,EAAAA,cAAoBkC,EAAAA,EAAO,CAClGnD,KAAM,YACJ,0HAA2H,KAAMiB,EAAAA,cAAoBc,EAAYI,GAAI,CACvKf,GAAI,+BACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,gCACN,aAAc,yCACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,gCAAiC,KAAMvC,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,mBAAoBjB,EAAAA,cAAoBkC,EAAAA,EAAO,CAClInD,KAAM,gCACJ,yIAA8I,KAAMiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACjLnD,KAAM,yJACJ,KAAMiB,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,qFAAsF,KAAMjB,EAAAA,cAAoBkC,EAAAA,EAAO,CACxKnD,KAAM,qNACJ,KAAMiB,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,2BAA4BjB,EAAAA,cAAoBkC,EAAAA,EAAO,CACxGnD,KAAM,2BACJ,uEAAwE,KAAMiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC3GnD,KAAM,kMACJ,KAAMiB,EAAAA,cAAoBc,EAAYQ,GAAI,KAAM,KAAMtB,EAAAA,cAAoBc,EAAYS,GAAI,KAAMvB,EAAAA,cAAoBc,EAAYU,OAAQ,KAAM,kBAAmB,KAAMxB,EAAAA,cAAoBkC,EAAAA,EAAO,CACpMnD,KAAM,uEACJ,0HAA2HiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACxJnD,KAAM,4BACJ,KAAM,KAAMiB,EAAAA,cAAoBc,EAAYS,GAAI,KAAMvB,EAAAA,cAAoBc,EAAYU,OAAQ,KAAM,kBAAmB,KAAMxB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC1JnD,KAAM,yGACJ,0MAA2M,MAAO,KAAMiB,EAAAA,cAAoBc,EAAYI,GAAI,CAC9Pf,GAAI,0DACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,2DACN,aAAc,oEACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,2DAA4D,KAAMvC,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,iDAAkDjB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC3LnD,KAAM,qGACJ,yDAA0DiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACvFnD,KAAM,kCACJ,sEAAuEiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACpGnD,KAAM,sBACJ,oEAAqEiB,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,4BAA6B,yJAA0J,KAAMjC,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,qCAAsCjB,EAAAA,cAAoBkC,EAAAA,EAAO,CACnZnD,KAAM,2CACJ,uJAAwJiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACrLnD,KAAM,qBACJ,6EAA8EiB,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,oBAAqB,KAAMjC,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,kBAAmB,SAAUjC,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,gCAAiC,qJAAsJ,KAAMjC,EAAAA,cAAoBc,EAAYI,GAAI,CACxdf,GAAI,qCACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,sCACN,aAAc,+CACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,sCAAuC,KAAMvC,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,kCAAmCjB,EAAAA,cAAoBkC,EAAAA,EAAO,CACvJnD,KAAM,qBACJ,yOAA4OiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACzQnD,KAAM,kBACJ,mTAAoT,KAAMiB,EAAAA,cAAoBc,EAAYO,GAAI,CAChWlB,GAAI,6DACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,8DACN,aAAc,uEACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,+DAAgE,KAAMvC,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,oJAAqJjB,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,0BAA2B,mDAAoDjC,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,qCAAsC,aAAcjC,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,iCAAkC,qKAAsKjC,EAAAA,cAAoBkC,EAAAA,EAAO,CAC5tBnD,KAAM,2BACJ,mBAAoB,KAAMiB,EAAAA,cAAoBc,EAAYI,GAAI,CAChEf,GAAI,wBACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,yBACN,aAAc,kCACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,yBAA0B,KAAMvC,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,sIAAuIjB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC9OnD,KAAM,sBACJ,+OAAgPiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC7QnD,KAAM,kBACJ,8QAA+Q,KAAMiB,EAAAA,cAAoBc,EAAYI,GAAI,CAC3Tf,GAAI,8BACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,+BACN,aAAc,wCACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,iCAAkC,KAAMvC,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,6QAAgRjB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC/XnD,KAAM,YACJ,0OAA6OiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC1QnD,KAAM,kCACJ,4GAA6G,KAAMiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAChJnD,KAAM,0LACJ,KAAMiB,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,SAAUjB,EAAAA,cAAoBkC,EAAAA,EAAO,CACtFnD,KAAM,6BACJ,iCAAkCiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC/DnD,KAAM,YACJ,0CAA2CiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACxEnD,KAAM,4BACJ,kEAAqE,KAAMiB,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,2IAA4IjB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC7RnD,KAAM,eACJ,iJAAkJiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC/KnD,KAAM,YACJ,uOAAwO,KAAMiB,EAAAA,cAAoBc,EAAYI,GAAI,CACpRf,GAAI,6EACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,8EACN,aAAc,uFACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,kFAAmF,KAAMvC,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,yMAA4MjB,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,qCAAsC,OAAQjC,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,iCAAkC,wBAAyB,KAAMjC,EAAAA,cAAoBc,EAAYW,GAAI,KAAM,KAAMzB,EAAAA,cAAoBc,EAAYS,GAAI,KAAM,2EAA4E,KAAMvB,EAAAA,cAAoBc,EAAYS,GAAI,KAAM,wEAAyE,KAAMvB,EAAAA,cAAoBc,EAAYS,GAAI,KAAM,gJAAiJ,KAAMvB,EAAAA,cAAoBc,EAAYS,GAAI,KAAM,kHAAmH,KAAMvB,EAAAA,cAAoBc,EAAYS,GAAI,KAAM,uEAAwE,MAAO,KAAMvB,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,6bAA8b,KAAMjB,EAAAA,cAAoBc,EAAYI,GAAI,CACnyDf,GAAI,wBACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,yBACN,aAAc,kCACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,yBAA0B,KAAMvC,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,8GAA+GjB,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,gCAAiC,qXAAsX,KAAMjC,EAAAA,cAAoBc,EAAYO,GAAI,CACjqBlB,GAAI,qBACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,sBACN,aAAc,+BACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,uBAAwB,KAAMvC,EAAAA,cAAoBc,EAAYG,EAAG,KAAMjB,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,oBAAqB,2GAA4GjC,EAAAA,cAAoBkC,EAAAA,EAAO,CAC3QnD,KAAM,qBACJ,sCAAuCiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACpEnD,KAAM,kCACJ,iIAAkIiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC/JnD,KAAM,2CACJ,8BAA+B,KAAMiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAClEnD,KAAM,+EACJ,KAAMiB,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,SAAUjB,EAAAA,cAAoBkC,EAAAA,EAAO,CACtFnD,KAAM,qCACJ,gCAAkCiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC/DnD,KAAM,cACJ,4CAA8C,KAAMiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACjFnD,KAAM,0JACJ,KAAMiB,EAAAA,cAAoBc,EAAYI,GAAI,CAC5Cf,GAAI,gEACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,iEACN,aAAc,0EACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,iEAAkE,KAAMvC,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,qGAAsGjB,EAAAA,cAAoBkC,EAAAA,EAAO,CACrPnD,KAAM,qBACJ,wEAAyE,KAAMiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC5GnD,KAAM,2NACJ,KAAMiB,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,0EAA2EjB,EAAAA,cAAoBkC,EAAAA,EAAO,CACvJnD,KAAM,cACJ,WAAYiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACzCnD,KAAM,iBACJ,oGAAqGiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAClInD,KAAM,qBACJ,mCAAoCiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACjEnD,KAAM,2BACJ,oBAAqB,KAAMiB,EAAAA,cAAoBc,EAAYI,GAAI,CACjEf,GAAI,gCACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,iCACN,aAAc,0CACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,iCAAkC,KAAMvC,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,+YAAgZjB,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,2BAA4B,qHAAsH,KAAMjC,EAAAA,cAAoBc,EAAYI,GAAI,CACrsBf,GAAI,oCACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,qCACN,aAAc,8CACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,qCAAsC,KAAMvC,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,sUAAuUjB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC1bnD,KAAM,kCACJ,qUAAsU,KAAMiB,EAAAA,cAAoBc,EAAYI,GAAI,CAClXf,GAAI,yBACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,0BACN,aAAc,mCACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,0BAA2B,KAAMvC,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,ycAA0c,KAAMjB,EAAAA,cAAoBc,EAAYO,GAAI,CACjkBlB,GAAI,sCACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,uCACN,aAAc,gDACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,wCAAyC,KAAMvC,EAAAA,cAAoBc,EAAYG,EAAG,KAAMjB,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,kBAAmB,8MAA+MjC,EAAAA,cAAoBkC,EAAAA,EAAO,CAC7XnD,KAAM,wCACJ,kFAAqFiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAClHnD,KAAM,sBACJ,0FAA2FiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACxHnD,KAAM,qBACJ,KAAM,KAAMiB,EAAAA,cAAoBc,EAAYI,GAAI,CAClDf,GAAI,oCACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,qCACN,aAAc,8CACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,qCAAsC,KAAMvC,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,OAAQjB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC3HnD,KAAM,uFACJ,8FAA+FiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC5HnD,KAAM,sBACJ,kCAAmCiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAChEnD,KAAM,kBACJ,aAAc,KAAMiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACjDnD,KAAM,yIACJ,KAAMiB,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,uBAAwBjB,EAAAA,cAAoBkC,EAAAA,EAAO,CACpGnD,KAAM,wCACJ,6XAA8XiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC3ZnD,KAAM,qBACJ,KAAM,KAAMiB,EAAAA,cAAoBc,EAAYI,GAAI,CAClDf,GAAI,iBACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,kBACN,aAAc,2BACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,kBAAmB,KAAMvC,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,8YAA+YjB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC/enD,KAAM,kCACJ,gCAAiC,KAAMiB,EAAAA,cAAoBc,EAAYI,GAAI,CAC7Ef,GAAI,gDACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,iDACN,aAAc,0DACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,iDAAkD,KAAMvC,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,uHAAwHjB,EAAAA,cAAoBkC,EAAAA,EAAO,CACvPnD,KAAM,kCACJ,yLAA0LiB,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,kBAAmB,uDAAwD,KAAMjC,EAAAA,cAAoBc,EAAYI,GAAI,CACtVf,GAAI,iBACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,kBACN,aAAc,2BACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,kBAAmB,KAAMvC,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,iCAAkCjB,EAAAA,cAAoBkC,EAAAA,EAAO,CAClInD,KAAM,yEACJ,sRAAuRiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACpTnD,KAAM,wCACJ,kJAAmJiB,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,gCAAiC,OAAQjC,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,0BAA2B,wBAAyB,KAAMjC,EAAAA,cAAoBc,EAAYO,GAAI,CACtWlB,GAAI,6BACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,8BACN,aAAc,uCACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,+BAAgC,KAAMvC,EAAAA,cAAoBc,EAAYG,EAAG,KAAMjB,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,4BAA6B,4GAA6GjC,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,0BAA2B,wGAAyGjC,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,4CAA6C,KAAM,KAAMjC,EAAAA,cAAoBc,EAAYI,GAAI,CAC5iBf,GAAI,8BACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,+BACN,aAAc,wCACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,+BAAgC,KAAMvC,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,oLAAqLjB,EAAAA,cAAoBkC,EAAAA,EAAO,CAClSnD,KAAM,sBACJ,mDAAoDiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACjFnD,KAAM,sBACJ,2TAA4T,KAAMiB,EAAAA,cAAoBc,EAAYI,GAAI,CACxWf,GAAI,oBACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,qBACN,aAAc,8BACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,qBAAsB,KAAMvC,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,6HAA8HjB,EAAAA,cAAoBkC,EAAAA,EAAO,CACjOnD,KAAM,sCACJ,uDAAwDiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACrFnD,KAAM,sBACJ,8DAA+DiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC5FnD,KAAM,sBACJ,gIAAiI,KAAMiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACpKnD,KAAM,2RACJ,KAAMiB,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,UAAWjB,EAAAA,cAAoBkC,EAAAA,EAAO,CACvFnD,KAAM,yDACJ,kJAAmJiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAChLnD,KAAM,qBACJ,KAAM,KAAMiB,EAAAA,cAAoBc,EAAYI,GAAI,CAClDf,GAAI,uCACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,wCACN,aAAc,iDACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,wCAAyC,KAAMvC,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,8fAA+fjB,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,mCAAoC,sCAAuCjC,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,oBAAqB,iIAAkI,KAAMjC,EAAAA,cAAoBc,EAAYO,GAAI,CACh7BlB,GAAI,iCACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,kCACN,aAAc,2CACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,mCAAoC,KAAMvC,EAAAA,cAAoBc,EAAYG,EAAG,KAAMjB,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,gCAAiC,yTAAgU,KAAMjC,EAAAA,cAAoBc,EAAYI,GAAI,CACtgBf,GAAI,qCACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,sCACN,aAAc,+CACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,sCAAuC,KAAMvC,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,kDAAqDjB,EAAAA,cAAoBkC,EAAAA,EAAO,CACzKnD,KAAM,yBACJ,mJAAoJiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACjLnD,KAAM,cACJ,yBAA0BiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACvDnD,KAAM,cACJ,qHAAsHiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACnJnD,KAAM,wCACJ,UAAWiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACxCnD,KAAM,+BACJ,8BAA+B,KAAMiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAClEnD,KAAM,kHACJ,KAAMiB,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,SAAUjB,EAAAA,cAAoBkC,EAAAA,EAAO,CACtFnD,KAAM,kBACJ,yCAA0CiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACvEnD,KAAM,kCACJ,uBAAwBiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACrDnD,KAAM,2CACJ,mBAAoBiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACjDnD,KAAM,2BACJ,+NAAgOiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC7PnD,KAAM,kBACJ,uCAAwC,KAAMiB,EAAAA,cAAoBc,EAAYI,GAAI,CACpFf,GAAI,+DACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,gEACN,aAAc,yEACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,gEAAiE,KAAMvC,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,sGAAuGjB,EAAAA,cAAoBkC,EAAAA,EAAO,CACrPnD,KAAM,2BACJ,sOAAuOiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACpQnD,KAAM,yBACJ,0CAA2C,KAAMiB,EAAAA,cAAoBc,EAAYI,GAAI,CACvFf,GAAI,2BACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,4BACN,aAAc,qCACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,4BAA6B,KAAMvC,EAAAA,cAAoBc,EAAYQ,GAAI,KAAM,KAAMtB,EAAAA,cAAoBc,EAAYS,GAAI,KAAMvB,EAAAA,cAAoBc,EAAYU,OAAQ,KAAM,cAAe,kRAAmR,KAAMxB,EAAAA,cAAoBc,EAAYS,GAAI,KAAMvB,EAAAA,cAAoBc,EAAYU,OAAQ,KAAM,aAAc,wGAAyGxB,EAAAA,cAAoBkC,EAAAA,EAAO,CAChsBnD,KAAM,yBACJ,2BAA4BiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACzDnD,KAAM,yBACJ,8SAA+SiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC5UnD,KAAM,sBACJ,0EAA2E,MAAO,KAAMiB,EAAAA,cAAoBc,EAAYO,GAAI,CAC9HlB,GAAI,sCACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,uCACN,aAAc,gDACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,wCAAyC,KAAMvC,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,6DAA8DjB,EAAAA,cAAoBkC,EAAAA,EAAO,CACpLnD,KAAM,qBACJ,uEAAwEiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACrGnD,KAAM,qBACJ,iHAAkHiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC/InD,KAAM,wCACJ,wGAAyGiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACtInD,KAAM,2BACJ,mJAAoJ,KAAMiB,EAAAA,cAAoBc,EAAYI,GAAI,CAChMf,GAAI,sBACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,uBACN,aAAc,gCACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,uBAAwB,KAAMvC,EAAAA,cAAoBc,EAAYG,EAAG,KAAMjB,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,uBAAwB,sGAAuGjC,EAAAA,cAAoBkC,EAAAA,EAAO,CACzQnD,KAAM,kCACJ,gEAAiEiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC9FnD,KAAM,yBACJ,sBAAuB,KAAMiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC1DnD,KAAM,8FACJ,KAAMiB,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,iBAAkB,KAAMjB,EAAAA,cAAoBkC,EAAAA,EAAO,CACpGnD,KAAM,qIACJ,KAAMiB,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,uBAAwBjB,EAAAA,cAAoBkC,EAAAA,EAAO,CACpGnD,KAAM,0CACJ,gBAAiBiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC9CnD,KAAM,yBACJ,+BAAgC,KAAMiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACnEnD,KAAM,qIACJ,KAAMiB,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,YAAajB,EAAAA,cAAoBkC,EAAAA,EAAO,CACzFnD,KAAM,yBACJ,yBAA0BiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACvDnD,KAAM,sDACJ,kGAAmGiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAChInD,KAAM,yBACJ,uBAAwBiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACrDnD,KAAM,yBACJ,4GAA6G,KAAMiB,EAAAA,cAAoBc,EAAYI,GAAI,CACzJf,GAAI,mCACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,oCACN,aAAc,6CACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,sCAAuC,KAAMvC,EAAAA,cAAoBc,EAAYG,EAAG,KAAMjB,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,gCAAiC,0EAA2EjC,EAAAA,cAAoBkC,EAAAA,EAAO,CACrQnD,KAAM,qBACJ,qKAAsKiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACnMnD,KAAM,kCACJ,gDAAiDiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC9EnD,KAAM,iBACJ,0CAA2C,KAAMiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC9EnD,KAAM,8FACJ,KAAMiB,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,SAAUjB,EAAAA,cAAoBkC,EAAAA,EAAO,CACtFnD,KAAM,0DACJ,gFAAiFiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC9GnD,KAAM,+BACJ,OAAQiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACrCnD,KAAM,2BACJ,8FAA+FiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC5HnD,KAAM,kCACJ,WAAYiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACzCnD,KAAM,cACJ,qRAAsR,KAAMiB,EAAAA,cAAoBc,EAAYI,GAAI,CAClUf,GAAI,kBACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,mBACN,aAAc,4BACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,mBAAoB,KAAMvC,EAAAA,cAAoBc,EAAYG,EAAG,KAAMjB,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,mBAAoB,yEAA4EjC,EAAAA,cAAoBkC,EAAAA,EAAO,CACtOnD,KAAM,yBACJ,gGAAiGiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC9HnD,KAAM,qBACJ,8QAA+Q,KAAMiB,EAAAA,cAAoBc,EAAYI,GAAI,CAC3Tf,GAAI,oDACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,qDACN,aAAc,8DACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,qDAAsD,KAAMvC,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,oCAAqCjB,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,8BAA+B,KAAMjC,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,0BAA2B,uQAAwQjC,EAAAA,cAAoBkC,EAAAA,EAAO,CAC1jBnD,KAAM,qBACJ,QAASiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACtCnD,KAAM,2BACJ,oGAAqG,KAAMiB,EAAAA,cAAoBc,EAAYI,GAAI,CACjJf,GAAI,2BACJgC,MAAO,CACLC,SAAU,aAEXpC,EAAAA,cAAoBc,EAAYK,EAAG,CACpCkB,KAAM,4BACN,aAAc,qCACdpC,UAAW,iBACVD,EAAAA,cAAoBc,EAAYM,KAAM,CACvCkB,wBAAyB,CACvBC,OAAQ,meAEP,4BAA6B,KAAMvC,EAAAA,cAAoBc,EAAYQ,GAAI,KAAM,KAAMtB,EAAAA,cAAoBc,EAAYS,GAAI,KAAMvB,EAAAA,cAAoBc,EAAYU,OAAQ,KAAM,oBAAqBxB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC9NnD,KAAM,aACH,kGAAmGiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACjInD,KAAM,wCACJ,0BAA2BiB,EAAAA,cAAoBkC,EAAAA,EAAO,CACxDnD,KAAM,YACJ,oFAAqFiB,EAAAA,cAAoBkC,EAAAA,EAAO,CAClHnD,KAAM,YACJ,gBAAiB,KAAMiB,EAAAA,cAAoBc,EAAYS,GAAI,KAAMvB,EAAAA,cAAoBc,EAAYU,OAAQ,KAAM,sBAAuB,+HAAgI,KAAMxB,EAAAA,cAAoBc,EAAYS,GAAI,KAAMvB,EAAAA,cAAoBc,EAAYU,OAAQ,KAAM,oBAAqB,mKAAoKxB,EAAAA,cAAoBiC,EAAAA,EAAW,KAAM,2CAA4C,uEAAwE,MAAO,KAAMjC,EAAAA,cAAoB6B,EAAO,CAChuBvB,IAAK,+BACLmC,KAAM,GACNC,QAAS,wRACTC,KAAM,UACJ,KAAM3C,EAAAA,cAAoBc,EAAYY,IAAK,KAAM1B,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,kTAAmTjB,EAAAA,cAAoBkC,EAAAA,EAAO,CAC1anD,KAAM,iBACJ,KAAM,KAAMiB,EAAAA,cAAoB4C,EAAAA,EAAM,CACxC7D,KAAM,s4EAyDJ,KAAMiB,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,2TAA4TjB,EAAAA,cAAoBkC,EAAAA,EAAO,CACxYnD,KAAM,qBACJ,wJAAyJ,KAAMiB,EAAAA,cAAoBc,EAAYY,IAAK,KAAM1B,EAAAA,cAAoBc,EAAYG,EAAG,KAAM,grBACzP,CAKA,MAJA,SAAoBJ,QAAK,IAALA,IAAAA,EAAQ,CAAC,GAC3B,MAAOgC,QAASC,GAAa/B,OAAOC,OAAO,CAAC,GAAGW,EAAAA,EAAAA,MAAsBd,EAAMe,YAC3E,OAAOkB,EAAY9C,EAAAA,cAAoB8C,EAAWjC,EAAOb,EAAAA,cAAoBY,EAAmBC,IAAUD,EAAkBC,EAC9H,E,gLCr2BA,MAAMkC,EAAkBjE,IACtB,IAAI,IAACkE,GAAOlE,EACZ,IAAKkE,IAAQA,EAAIC,MAAO,OAAO,KAY/B,OAAOjD,EAAAA,cAAoB,MAAO,CAChCC,UAAWiD,EAAAA,GACVlD,EAAAA,cAAoB,KAAM,KAAMgD,EAAIC,MAAME,KAAI,CAACC,EAAMC,IAAUrD,EAAAA,cAAoB,KAAM,CAC1FsD,IAAKD,GACJrD,EAAAA,cAAoB,IAAK,CAC1BqC,KAAMe,EAAKG,IACXhD,QAASC,GAjBSgD,EAAChD,EAAG+C,KACtB/C,EAAEiD,iBACF,MAAMC,EAAWH,EAAII,QAAQ,IAAK,IAC5BC,EAAgB/D,SAASgE,eAAeH,GAC1CE,GACFA,EAAcE,eAAe,CAC3BC,SAAU,SACVC,MAAO,SAEX,EAQcR,CAAYhD,EAAG4C,EAAKG,MACjCH,EAAKa,OAAQb,EAAKH,OAASjD,EAAAA,cAAoB+C,EAAiB,CACjEC,IAAK,CACHC,MAAOG,EAAKH,aAEV,EAED,SAASiB,EAAaC,GAC3B,IAAKC,MAAM,IAACC,EAAG,OAAEC,EAAM,cAAEC,GAAc,SAAEC,GAAYL,EACrD,MAAM,YAACM,EAAW,KAAEC,EAAI,gBAAEC,GAAmBN,EACvChB,EAAQoB,EAAYpB,MAEpBuB,EADOH,EAAYI,KACJC,MAAM,KAAK,GAE1BC,EADQT,EAAOU,MAAMC,QAAOC,GAAQA,EAAKT,YAAYI,KAAKM,SAAS,IAAIP,QACnDQ,MAAK,CAACjE,EAAGkE,IAAMlE,EAAEsD,YAAYpB,MAAQgC,EAAEZ,YAAYpB,QACvEiC,EAAeP,EAAYQ,WAAUL,GAAQA,EAAKT,YAAYpB,QAAUA,IACxEmC,EAAWT,EAAYO,EAAe,GACtCG,EAAWV,EAAYO,EAAe,GACtCI,EAAcjB,EAAYI,KAAKlB,QAAQ,MAAO,IAC9CgC,EAAc,SAAUC,KAAKF,GAAa,GAC1CG,EAAW,SAASjB,aAAmBe,MACtC,EAAGG,EAAc,EAAGC,IAAmB5G,EAAAA,EAAAA,UAASsF,EAAYuB,0BAC5D,EAAGC,EAAa,EAAGC,IAAkB/G,EAAAA,EAAAA,WAAS,GASrD,IAAIgH,GALJ7G,EAAAA,EAAAA,YAAU,KACR4G,GAAe,GACf,MAAME,EAAQC,YAAW,IAAMH,GAAe,IAAQ,KACtD,MAAO,IAAMI,aAAaF,EAAM,GAC/B,CAACN,IAEY,eAAZlB,EACFuB,EAAiBI,EAAAA,GACI,aAAZ3B,EACTuB,EAAiBK,EAAAA,GACI,aAAZ5B,IACTuB,EAAiBM,EAAAA,IAEnB,MACMC,EADgBC,IAAejC,GAAMf,QAAQ,wBAAyB,IAAIA,QAAQ,SAAU,IAAIA,QAAQ,wBAAyB,IAAIiD,OAC3G9B,MAAM,OAAO+B,OAIvCC,EA9ER,SAAwBC,GACtB,GAAIA,GAAW,GAAI,MAAO,UAC1B,GAAIA,GAAW,GAAI,MAAO,UAC1B,GAAIA,GAAW,GAAI,MAAO,UAC1B,GAAIA,GAAW,GAAI,MAAO,UAC1B,GAAIA,GAAW,GAAI,MAAO,UAC1B,GAAIA,GAAW,GAAI,MAAO,OAC1B,MAAMC,EAAQC,KAAKC,MAAMH,EAAU,IAC7BI,EAAYJ,EAAU,GAC5B,OAAII,GAAa,GACR,IAAIH,IAAQG,EAAY,EAAI,KAAO,OAErC,IAAIH,EAAQ,KACrB,CAiEmBI,CAHWH,KAAKI,KAAKX,EAAYP,IAChC1B,EAAY6C,kBAAoB,IAG5CC,EAAU,CAAC,CACfC,KAAM/C,EAAYgD,UAClB3F,UAAWA,IAAM,0DAChB,CACD0F,KAAM/C,EAAYiD,gBAClB5F,UAAWA,IAAM,0DAChB,CACD0F,KAAM/C,EAAYkD,YAClB7F,UAAWA,IAAM,0DAChB,CACD0F,KAAM/C,EAAYmD,cAClB9F,UAAWA,IAAM,0DAChB,CACD0F,KAAM/C,EAAYoD,YAClB/F,UAAWA,IAAM,0DAChB,CACD0F,KAAM/C,EAAYqD,iBAClBhG,UAAWA,IAAM,0DAChB,CACD0F,KAAM/C,EAAYsD,eAClBjG,UAAWA,IAAM,0DAChB,CACD0F,KAAM/C,EAAYuD,cAClBlG,UAAWA,IAAM,0DAChB,CACD0F,KAAM/C,EAAYwD,kBAClBnG,UAAWA,IAAM,0DAChB,CACD0F,KAAM/C,EAAYyD,WAClBpG,UAAWA,IAAM,4DAEZ,EAAGqG,EAAe,EAAGC,IAAoBjJ,EAAAA,EAAAA,UAAS,IAWzD,OAVAG,EAAAA,EAAAA,YAAU,KACRiI,EAAQc,SAAQC,IACd,IAAI,KAACd,EAAI,UAAE1F,GAAawG,EACpBd,GACF1F,IAAYyG,MAAKC,IACfJ,GAAiB1H,GAAQ,GAAG+H,QAAOC,EAAAA,EAAAA,GAAmBhI,GAAO,CAAC8H,EAAOG,WAAU,GAEnF,GACA,GACD,IACI3I,EAAAA,cAAoB4I,EAAAA,EAAOC,IAAK,CACrCC,QAAS,CACPC,QAAS,GAEXC,QAAS,CACPD,QAAS,GAEXE,KAAM,CACJF,QAAS,GAEXG,WAAY,CACVC,SAAU,MAEXnJ,EAAAA,cAAoBoJ,EAAAA,EAAY,CACjCC,WAAY5E,EAAYpB,MACxBiG,KAAM7E,EAAY6E,KAClBC,QAAS9E,EAAY8E,QACrBzC,SAAUA,EACV0C,WAAY/E,EAAYgF,gBACxBxF,MAAOQ,EAAYR,MACnByF,KAAMjF,EAAYiF,KAClBC,OAAQlF,EAAYkF,OACpB/E,QAASA,EACTgF,QAASjE,EACTkE,cAAepF,EAAYiD,gBAC3BoC,QAASrF,EAAYqF,UACnB9J,EAAAA,cAAoB,MAAO,CAC7BmC,MAAO,CACL4H,QAAS,OACTC,eAAgB,WAChBC,SAAU,OACVC,SAAU,MACVC,WAAY,OACZC,aAAc,MACdC,UAAW,OACXC,aAAc,QAEf7F,EAAY8F,UAAUpH,KAAI,CAACqH,EAAKnH,IAAUrD,EAAAA,cAAoB,OAAQ,CACvEsD,IAAKD,EACLpD,UAAW,YAAYwK,EAAAA,KACvBtI,MAAO,CACLuI,OAAQ,gBAETF,MAAQxK,EAAAA,cAAoB,MAAO,CACpCC,UAAW,YACVD,EAAAA,cAAoB+C,EAAiB,CACtCC,IAAK2B,KACF3E,EAAAA,cAAoB,KAAM,MAAOA,EAAAA,cAAoB,MAAO,CAC/DmC,MAAO,CACLuI,OAAQ,iBACRC,UAAW,UAEZ3K,EAAAA,cAAoB4I,EAAAA,EAAOgC,OAAQ,CACpC3K,UAAW,YAAY4K,EAAAA,KACvB1K,GAAI0K,EAAAA,GACJtK,QAvHmBuK,KACnB/E,GAAiBD,EAAa,EAuH9BiF,SAAU,CACRC,MAAO,MAERhL,EAAAA,cAAoB4I,EAAAA,EAAOC,IAAK,CACjC5I,UAAWgL,EAAAA,GACX3H,IAAKwC,EACLgD,QAAS,CACPC,QAAS,GAEXC,QAAS,CACPD,QAAS,GAEXE,KAAM,CACJF,QAAS,GAEXG,WAAY,CACVC,SAAU,GACV+B,KAAM,cAEPpF,EAAe,2BAA6B,2BAA4B9F,EAAAA,cAAoB,KAAM,MAAOA,EAAAA,cAAoB,MAAO,CACrIC,UAAW,WACXkC,MAAO,CACLuI,OAAQ5E,EAAe,SAAW,GAClCoE,SAAUpE,EAAe,OAAS,GAClCoD,WAAY,uDAEblJ,EAAAA,cAAoB,MAAO,CAC5BC,UAAW,GAAG4K,EAAAA,MAAuC5E,EAAc4E,EAAAA,GAAkCA,EAAAA,MACpG1C,EAAchF,KAAI,CAACgI,EAAiB9H,IAAUrD,EAAAA,cAAoBmL,EAAiB,CACpF7H,IAAKD,MACFoB,EAAY2G,YAAcpL,EAAAA,cAAoBqL,EAAAA,EAAoB,CACrEhI,MAAOoB,EAAY2G,YACnBE,SAAU7G,EAAY8G,qBACnB,GAAIvL,EAAAA,cAAoBwL,EAAAA,EAAaC,SAAU,CAClDC,MAAO,CACLC,OAAQpH,EAAcS,MACtBa,SAAUA,EAASlC,QAAQ,MAAO,IAAM,MAEzC3D,EAAAA,cAAoB4L,EAAAA,GAAa,CAClChK,WAAY,CACVC,MAAKA,EAAAA,IAEN2C,MAAcxE,EAAAA,cAAoB6L,EAAAA,EAAY,CAC/CrG,SAAUA,EACVC,SAAUA,EACVE,WAAYA,EACZf,QAASA,IAEb,CAEe,SAASkH,EAAiBjL,GACvC,OAAOb,EAAAA,cAAoBkE,EAAcrD,EAAOb,EAAAA,cAAoB+L,EAAqBlL,GAC3F,CACO,SAASmL,EAAKC,GACnB,IAAIC,EAAqBC,EAAuBC,EAAwBC,EAAwBC,EAChG,IAAI,KAAClI,GAAQ6H,EACb,MAAM,YAACxH,GAAeL,EAAKC,IACrBJ,EAAQQ,EAAY8H,UAAY9H,EAAYR,MAC5CuI,EAAU/H,EAAY+H,SAAWvI,EACjCwI,EAAehI,EAAYgI,cAAgBxI,EAC3CyI,EAAcjI,EAAYkI,SAAWlI,EAAYiF,KACjDkD,EAAgBnI,EAAYoI,QAAUH,EACtCI,EAAqBrI,EAAYsI,aAAeL,EAChDM,EAAavI,EAAYuI,YAAc,cACvCC,EAAWxI,EAAYyI,YACvBC,EAAgB1I,EAAY6E,KAC5B8D,EAAe3I,EAAY8E,SAAW4D,EACtCE,EAAU5I,EAAY4I,UAA2D,QAA9CnB,EAAsBzH,EAAYkF,cAA4C,IAAxBuC,GAA4G,QAAjEC,EAAwBD,EAAoBoB,uBAAuD,IAA1BnB,GAAiH,QAApEC,EAAyBD,EAAsBoB,uBAAwD,IAA3BnB,GAA0G,QAA5DC,EAAyBD,EAAuBT,cAA+C,IAA3BU,GAA4G,QAA9DC,EAAyBD,EAAuBmB,gBAAiD,IAA3BlB,OAAlb,EAA+dA,EAAuBlM,KAChnBqN,EAAahJ,EAAYgJ,YAAcb,EACvCc,EAAejJ,EAAYiJ,cAAgBL,EAC3CM,EAAkBlJ,EAAYkJ,iBAAmBb,EACjDc,EAAenJ,EAAYoJ,aAC3B3F,EAAazD,EAAYyD,aAAc,EACvC4B,EAAUrF,EAAYqF,SAAW,QACjClF,EAAUH,EAAYI,KAAKC,MAAM,KAAK,IAAM,SAE5C,QAACgJ,IAAWC,EAAAA,EAAAA,KACZC,EAAiB,CACrB,WAAY,qBACZ,QAAS,iBACT,gBAAmB,CAAC,CAClB,QAAS,WACT,SAAY,EACZ,KAAQ,OACR,KAAQF,GACP,CACD,QAAS,WACT,SAAY,EACZ,KAAQhE,EACR,KAAQ,GAAGgE,KAAWrJ,EAAYI,KAAKC,MAAM,KAAK,MACjD,CACD,QAAS,WACT,SAAY,EACZ,KAAQb,EACR,KAAQ,GAAG6J,IAAUrJ,EAAYI,UAGrC,OAAO7E,EAAAA,cAAoBiO,EAAAA,EAAK,CAC9BhK,MAAOA,EAAQ,gBACfuI,QAASA,EACTC,aAAcA,EACdC,YAAaA,EACbE,cAAeA,EACfE,mBAAoBA,EACpBE,WAAYA,EACZC,SAAUA,EACVE,cAAeA,EACfC,aAAcA,EACdC,QAASA,EACTI,WAAYA,EACZC,aAAcA,EACdC,gBAAiBA,EACjBC,aAAcA,EACd1F,WAAYA,EACZ4B,QAASA,EACTlF,QAASA,EACTsJ,KAzCW,WA0CVlO,EAAAA,cAAoB,SAAU,CAC/BkO,KAAM,uBACLC,KAAKC,UAAUJ,IACpB,C,iDCvSA,IALUlP,IAA2B,IAA1B,KAAEC,GAAkBD,EAC7B,OACEkB,EAAAA,cAACkC,EAAAA,EAAK,KAAEnD,EAAa,C","sources":["webpack://avrtt.blog/./src/images/goals/info.svg","webpack://avrtt.blog/./src/components/Tooltip/styles.module.scss","webpack://avrtt.blog/./src/components/Tooltip/index.tsx","webpack://avrtt.blog/./src/pages/posts/research/partition_function_a_closer_look.mdx","webpack://avrtt.blog/./src/templates/post.tsx","webpack://avrtt.blog/./src/components/Latex/index.tsx"],"sourcesContent":["export default \"data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iaXNvLTg4NTktMSI/Pgo8IS0tIEdlbmVyYXRvcjogQWRvYmUgSWxsdXN0cmF0b3IgMTYuMC4wLCBTVkcgRXhwb3J0IFBsdWctSW4gLiBTVkcgVmVyc2lvbjogNi4wMCBCdWlsZCAwKSAgLS0+CjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+CjxzdmcgdmVyc2lvbj0iMS4xIiBpZD0iQ2FwYV8xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB4PSIwcHgiIHk9IjBweCIKCSB3aWR0aD0iNDE2Ljk3OXB4IiBoZWlnaHQ9IjQxNi45NzlweCIgdmlld0JveD0iMCAwIDQxNi45NzkgNDE2Ljk3OSIgc3R5bGU9ImVuYWJsZS1iYWNrZ3JvdW5kOm5ldyAwIDAgNDE2Ljk3OSA0MTYuOTc5OyIKCSB4bWw6c3BhY2U9InByZXNlcnZlIj4KPGc+Cgk8cGF0aCBkPSJNMzU2LjAwNCw2MS4xNTZjLTgxLjM3LTgxLjQ3LTIxMy4zNzctODEuNTUxLTI5NC44NDgtMC4xODJjLTgxLjQ3LDgxLjM3MS04MS41NTIsMjEzLjM3OS0wLjE4MSwyOTQuODUKCQljODEuMzY5LDgxLjQ3LDIxMy4zNzgsODEuNTUxLDI5NC44NDksMC4xODFDNDM3LjI5MywyNzQuNjM2LDQzNy4zNzUsMTQyLjYyNiwzNTYuMDA0LDYxLjE1NnogTTIzNy42LDM0MC43ODYKCQljMCwzLjIxNy0yLjYwNyw1LjgyMi01LjgyMiw1LjgyMmgtNDYuNTc2Yy0zLjIxNSwwLTUuODIyLTIuNjA1LTUuODIyLTUuODIyVjE2Ny44ODVjMC0zLjIxNywyLjYwNy01LjgyMiw1LjgyMi01LjgyMmg0Ni41NzYKCQljMy4yMTUsMCw1LjgyMiwyLjYwNCw1LjgyMiw1LjgyMlYzNDAuNzg2eiBNMjA4LjQ5LDEzNy45MDFjLTE4LjYxOCwwLTMzLjc2Ni0xNS4xNDYtMzMuNzY2LTMzLjc2NQoJCWMwLTE4LjYxNywxNS4xNDctMzMuNzY2LDMzLjc2Ni0zMy43NjZjMTguNjE5LDAsMzMuNzY2LDE1LjE0OCwzMy43NjYsMzMuNzY2QzI0Mi4yNTYsMTIyLjc1NSwyMjcuMTA3LDEzNy45MDEsMjA4LjQ5LDEzNy45MDF6Ii8+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPC9zdmc+Cg==\"","// extracted by mini-css-extract-plugin\nexport var info = \"styles-module--info--26c1f\";\nexport var infoBadge = \"styles-module--infoBadge--e3d66\";\nexport var tooltipWrapper = \"styles-module--tooltipWrapper--75ebf\";\nexport var tooltiptext = \"styles-module--tooltiptext--a263b\";\nexport var visible = \"styles-module--visible--c063c\";","/* \n\nCopyright © 2022  Vladislav Averett (avrtt)\nDistributed under the GNU AGPLv3 license. For details and source code, please refer to <https://github.com/avrtt/avrtt.github.io>.\n\n*/\n\nimport React, { useState, useEffect, useRef } from \"react\";\nimport info from \"../../images/goals/info.svg\"\nimport * as styles from \"./styles.module.scss\"\n\ninterface TooltipProps {\n  text: React.ReactNode;\n  isBadge?: boolean;\n}\n\nconst Tooltip: React.FC<TooltipProps> = ({ text, isBadge=false }) => {\n    const [isOpen, setIsOpen] = useState(false);\n    const tooltipRef = useRef<HTMLSpanElement | null>(null);\n\n    const handleIconClick = (e: React.MouseEvent) => {\n        e.stopPropagation();\n        setIsOpen((prev) => !prev);\n    };\n\n    useEffect(() => {\n        function handleClickOutside(event: MouseEvent) {\n            if (tooltipRef.current && event.target instanceof Node && !tooltipRef.current.contains(event.target)) {\n                setIsOpen(false);\n            }\n        }\n        document.addEventListener(\"click\", handleClickOutside);\n        return () => {\n            document.removeEventListener(\"click\", handleClickOutside);\n        };\n    }, []);\n\n    return (\n        <span className={styles.tooltipWrapper} ref={tooltipRef}>\n            <img id={isBadge ? styles.infoBadge : styles.info} src={info} alt='info' onClick={handleIconClick}/>\n            <span className={isOpen ? `${styles.tooltiptext} ${styles.visible}` : styles.tooltiptext}>\n                {text}\n            </span>\n        </span>\n    );\n};\n\nexport default Tooltip;\n","/*@jsxRuntime classic @jsx React.createElement @jsxFrag React.Fragment*/\n/**(intro: a quote, catchphrase, joke, etc.)**/\n/*\n\nhttps://www.deeplearningbook.org/contents/partition.html\n\n*/\n/*\n\n1. Introduction\n- Why dedicating a separate article to partition function?\n- Overview of undirected probabilistic models and the role of the partition function in normalizing unnormalized probabilities\n- Challenges arising from intractable sums or integrals in high-dimensional spaces\n- Importance of methods that circumvent or approximate the partition function\n2. The Log-Likelihood Gradient\n- Decomposition of the log-likelihood gradient into positive and negative phases\n- Why the partition function term makes the gradient difficult to compute\n- Role of the gradient of log Z and its interpretation in learning\n- (other related information)\n3. Stochastic Maximum Likelihood and Contrastive Divergence\n- Naive MCMC approaches and their computational drawbacks\n- Contrastive Divergence (CD-k): initialization from data, advantages, and bias issues\n- Stochastic Maximum Likelihood (SML) or Persistent Contrastive Divergence (PCD-k): maintaining a set of persistent chains for the negative phase\n- (other related information)\n4. Pseudolikelihood\n- Use of conditional probabilities to avoid direct computation of the partition function\n- Strengths of pseudolikelihood in certain tasks (e.g., missing data imputation) versus its limitations in density estimation\n- Trade-offs in computational cost and accuracy compared to maximum likelihood\n- (other related information)\n5. Score Matching and Ratio Matching\n- Score matching as a method to match gradients (the \"score\") of model and data without computing normalizers\n- Incompatibility with certain deeper or discrete-variable models that require only a lower bound on log p̃\n- Ratio matching for discrete data and its connection to local neighborhoods around training examples\n- (other related information)\n6. Denoising Score Matching\n- Motivation for smoothing distributions via added noise\n- How denoising modifies standard score matching to combat overfitting\n- Relationship to certain autoencoder training objectives\n- (other related information)\n7. Noise-Contrastive Estimation\n- Reduction of unsupervised learning to binary classification between data and noise samples\n- Simultaneous learning of model parameters and partition function offset\n- Advantages in some high-vocabulary or single-variable contexts and drawbacks in large multi-variable settings\n- (other related information)\n8. Estimating the Partition Function\n- Importance sampling: direct ratio estimation between proposal and target distribution\n- Annealed Importance Sampling (AIS): bridging from a known distribution to the target distribution with incremental \"temperatures\"\n- Bridge sampling: single \"bridge\" distribution strategy and relation to AIS\n- Linked importance sampling and other improvements for variance reduction\n- Practical considerations: when to estimate Z explicitly versus relying on partition-function-free training methods\n- (other related information)\n\n*/\nimport {useMDXComponents as _provideComponents} from \"@mdx-js/react\";\nimport React from \"react\";\nimport Tooltip from \"../../../components/Tooltip\";\nimport Highlight from \"../../../components/Highlight\";\nimport Code from \"../../../components/Code\";\nimport Latex from \"../../../components/Latex\";\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    h3: \"h3\",\n    a: \"a\",\n    span: \"span\",\n    h2: \"h2\",\n    ul: \"ul\",\n    li: \"li\",\n    strong: \"strong\",\n    ol: \"ol\",\n    hr: \"hr\"\n  }, _provideComponents(), props.components), {Image} = _components;\n  if (!Image) _missingMdxReference(\"Image\", true);\n  return React.createElement(React.Fragment, null, \"\\n\", React.createElement(\"br\"), \"\\n\", \"\\n\", \"\\n\", React.createElement(_components.p, null, \"When diving into probabilistic models — especially those that are undirected or energy-based — there is a central concept we cannot avoid: the partition function. I find it crucial to shine a bright spotlight on this because the partition function is at once ubiquitous and famously troublesome. It appears in virtually all undirected models, from \", React.createElement(Highlight, null, \"Restricted Boltzmann Machines\"), \" (RBMs) to \", React.createElement(Highlight, null, \"Markov Random Fields\"), \" (MRFs) and beyond, serving as the normalizing constant that transforms raw energy functions (or unnormalized log probabilities) into well-defined probability distributions. The partition function, often denoted as \", React.createElement(Latex, {\n    text: \"\\\\(Z\\\\)\"\n  }), \", is often the direct cause of computational nightmares: it involves summing (or integrating) a potentially astronomical number of configurations across a high-dimensional space. Because of its fundamental importance — as well as its notorious intractability — an entire ecosystem of approximation strategies, alternative learning objectives, and algorithmic cleverness has emerged around it.\"), \"\\n\", React.createElement(_components.p, null, \"The reason I dedicate a separate article to the partition function is the sheer depth of theoretical and practical methods that revolve around either avoiding it, approximating it, or circumventing its direct computation. In many advanced machine learning contexts, especially in the realm of energy-based models, the presence of the partition function \", React.createElement(Latex, {\n    text: \"\\\\(Z\\\\)\"\n  }), \" in the log-likelihood invites an elaborate interplay of gradient terms, sampling techniques, and approximate inference strategies. If we don't fully understand the role of \", React.createElement(Latex, {\n    text: \"\\\\(Z\\\\)\"\n  }), \" — and the difficulty it poses — our grasp of these learning algorithms will remain incomplete or superficial at best.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"role-of-the-partition-function-in-undirected-probabilistic-models\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#role-of-the-partition-function-in-undirected-probabilistic-models\",\n    \"aria-label\": \"role of the partition function in undirected probabilistic models permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Role of the partition function in undirected probabilistic models\"), \"\\n\", React.createElement(_components.p, null, \"To understand what \", React.createElement(Latex, {\n    text: \"\\\\(Z\\\\)\"\n  }), \" is, let's consider a general undirected model that assigns an energy \", React.createElement(Latex, {\n    text: \"\\\\(E(\\\\mathbf{x}; \\\\theta)\\\\)\"\n  }), \" (where \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathbf{x}\\\\)\"\n  }), \" is the data and \", React.createElement(Latex, {\n    text: \"\\\\(\\\\theta\\\\)\"\n  }), \" denotes parameters) to each possible configuration of \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathbf{x}\\\\)\"\n  }), \". The unnormalized probability of a particular configuration \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathbf{x}\\\\)\"\n  }), \" can be expressed as:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\tilde{p}(\\\\mathbf{x}; \\\\theta) = \\\\exp\\\\bigl(-E(\\\\mathbf{x}; \\\\theta)\\\\bigr).\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"The partition function \", React.createElement(Latex, {\n    text: \"\\\\(Z(\\\\theta)\\\\)\"\n  }), \" is then defined as\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\nZ(\\\\theta) = \\\\sum_{\\\\mathbf{x}} \\\\exp\\\\bigl(-E(\\\\mathbf{x}; \\\\theta)\\\\bigr),\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"if \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathbf{x}\\\\)\"\n  }), \" is discrete, or\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\nZ(\\\\theta) = \\\\int \\\\exp\\\\bigl(-E(\\\\mathbf{x}; \\\\theta)\\\\bigr)\\\\, d\\\\mathbf{x}\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"if \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathbf{x}\\\\)\"\n  }), \" is continuous. The fully normalized probability distribution \", React.createElement(Latex, {\n    text: \"\\\\(p(\\\\mathbf{x}; \\\\theta)\\\\)\"\n  }), \" is then:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\np(\\\\mathbf{x}; \\\\theta) = \\\\frac{\\\\tilde{p}(\\\\mathbf{x}; \\\\theta)}{Z(\\\\theta)}.\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"Hence, \", React.createElement(Latex, {\n    text: \"\\\\(Z(\\\\theta)\\\\)\"\n  }), \" is the global normalizer. If \", React.createElement(Latex, {\n    text: \"\\\\(Z(\\\\theta)\\\\)\"\n  }), \" is very difficult to compute or approximate, it complicates any method that relies on evaluating \", React.createElement(Latex, {\n    text: \"\\\\(p(\\\\mathbf{x}; \\\\theta)\\\\)\"\n  }), \" or its gradient with respect to \", React.createElement(Latex, {\n    text: \"\\\\(\\\\theta\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"challenges-in-high-dimensional-spaces\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#challenges-in-high-dimensional-spaces\",\n    \"aria-label\": \"challenges in high dimensional spaces permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Challenges in high-dimensional spaces\"), \"\\n\", React.createElement(_components.p, null, \"In high-dimensional spaces — typical for image data, large-scale textual data, or other complex domains — computing \", React.createElement(Latex, {\n    text: \"\\\\(Z(\\\\theta)\\\\)\"\n  }), \" exactly is typically out of the question. The number of configurations \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathbf{x}\\\\)\"\n  }), \" can be astronomically large. Even approximate summation or integration can be burdensome without careful sampling or specialized methods. This intractability is what led to the development of widely used approximation strategies like \", React.createElement(Highlight, null, \"Contrastive Divergence\"), \" and \", React.createElement(Highlight, null, \"Noise-Contrastive Estimation\"), \", as well as specialized sampling methods such as \", React.createElement(Highlight, null, \"Annealed Importance Sampling\"), \".\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"importance-of-partition-function-free-methods\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#importance-of-partition-function-free-methods\",\n    \"aria-label\": \"importance of partition function free methods permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Importance of partition-function-free methods\"), \"\\n\", React.createElement(_components.p, null, \"An important thread in the literature is the desire to circumvent \", React.createElement(Latex, {\n    text: \"\\\\(Z\\\\)\"\n  }), \" altogether. One might consider alternative objectives or surrogate losses that do not explicitly include \", React.createElement(Latex, {\n    text: \"\\\\(Z(\\\\theta)\\\\)\"\n  }), \". This has led to methods such as \", React.createElement(Highlight, null, \"Pseudolikelihood\"), \" and \", React.createElement(Highlight, null, \"Score Matching\"), \", each with its own nuances and domain of applicability. These methods are often deeply connected with fundamental statistical theory. For instance, score matching has roots in older ideas from \", React.createElement(Highlight, null, \"Fisher divergence\"), \" and leads to interesting ways to estimate parameters by matching model gradients to data gradients. Meanwhile, pseudolikelihood builds upon the factorization properties of conditional distributions and is sometimes used in fields like \", React.createElement(Tooltip, {\n    text: \"social network analysis\"\n  }), \" to handle large, complex networks of variables.\"), \"\\n\", React.createElement(_components.p, null, \"By the end of this article, I hope you will see not only the specific reasons why the partition function demands so much care, but also the variety of ways researchers have tackled it. The theoretical insights, the sampling-based methods, and the approximate objectives form a multifaceted toolbox. When confronted with the dreaded \", React.createElement(Latex, {\n    text: \"\\\\(Z(\\\\theta)\\\\)\"\n  }), \" in your own projects, you can decide whether to approximate it directly, bypass it with an alternative learning criterion, or try something more exotic altogether.\"), \"\\n\", React.createElement(_components.h2, {\n    id: \"2-the-log-likelihood-gradient\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#2-the-log-likelihood-gradient\",\n    \"aria-label\": \"2 the log likelihood gradient permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"2. The log-likelihood gradient\"), \"\\n\", React.createElement(_components.p, null, \"When we train an undirected model such as an RBM or a Markov Random Field using maximum likelihood, we typically consider the log-likelihood function:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\mathcal{L}(\\\\theta) = \\\\log p(\\\\mathbf{X}; \\\\theta) = \\\\sum_{i=1}^N \\\\log p(\\\\mathbf{x}^{(i)}; \\\\theta),\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"where \", React.createElement(Latex, {\n    text: \"\\\\( \\\\mathbf{x}^{(i)} \\\\)\"\n  }), \" is the \", React.createElement(Latex, {\n    text: \"\\\\(i\\\\)\"\n  }), \"-th data point and \", React.createElement(Latex, {\n    text: \"\\\\(N\\\\)\"\n  }), \" is the size of the dataset. Expanding \", React.createElement(Latex, {\n    text: \"\\\\(\\\\log p(\\\\mathbf{x}; \\\\theta)\\\\)\"\n  }), \", we get:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\log p(\\\\mathbf{x}; \\\\theta) = -E(\\\\mathbf{x}; \\\\theta) - \\\\log Z(\\\\theta).\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"Thus, the log-likelihood for the entire dataset is:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\mathcal{L}(\\\\theta) = -\\\\sum_{i=1}^N E(\\\\mathbf{x}^{(i)}; \\\\theta) - N \\\\log Z(\\\\theta),\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"assuming a single \", React.createElement(Latex, {\n    text: \"\\\\(E\\\\)\"\n  }), \" for all data points, though in some contexts we might have a sum of energies if the model factorizes in certain ways.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"positive-and-negative-phases\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#positive-and-negative-phases\",\n    \"aria-label\": \"positive and negative phases permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Positive and negative phases\"), \"\\n\", React.createElement(_components.p, null, \"The gradient of \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathcal{L}(\\\\theta)\\\\)\"\n  }), \" typically separates into what many in the Boltzmann machine literature call the \\\"positive phase\\\" and \\\"negative phase\\\". Specifically,\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\nabla_\\\\theta \\\\mathcal{L}(\\\\theta) = -\\\\sum_{i=1}^N \\\\nabla_\\\\theta E(\\\\mathbf{x}^{(i)}; \\\\theta) - N \\\\nabla_\\\\theta \\\\log Z(\\\\theta).\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"When distributing the negative sign, you might see it more commonly expressed as:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\nabla_\\\\theta \\\\mathcal{L}(\\\\theta) \\n= -\\\\sum_{i=1}^N \\\\nabla_\\\\theta E(\\\\mathbf{x}^{(i)}; \\\\theta) + N \\\\mathbb{E}_{p(\\\\mathbf{x}; \\\\theta)}\\\\bigl[\\\\nabla_\\\\theta E(\\\\mathbf{x}; \\\\theta)\\\\bigr].\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"Here the term involving \", React.createElement(Latex, {\n    text: \"\\\\(\\\\log Z(\\\\theta)\\\\)\"\n  }), \" has been rewritten as an expectation under the model distribution:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\nabla_\\\\theta \\\\log Z(\\\\theta) \\n= \\\\frac{\\\\nabla_\\\\theta Z(\\\\theta)}{Z(\\\\theta)} \\n= \\\\mathbb{E}_{p(\\\\mathbf{x}; \\\\theta)}\\\\bigl[\\\\nabla_\\\\theta E(\\\\mathbf{x}; \\\\theta)\\\\bigr].\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Positive phase\"), \": \", React.createElement(Latex, {\n    text: \"\\\\(-\\\\sum_{i=1}^N \\\\nabla_\\\\theta E(\\\\mathbf{x}^{(i)}; \\\\theta)\\\\)\"\n  }), \" corresponds to matching the model's parameters so that it assigns low energy (thus high probability) to observed data \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathbf{x}^{(i)}\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Negative phase\"), \": \", React.createElement(Latex, {\n    text: \"\\\\(+ N \\\\mathbb{E}_{p(\\\\mathbf{x}; \\\\theta)}\\\\bigl[\\\\nabla_\\\\theta E(\\\\mathbf{x}; \\\\theta)\\\\bigr]\\\\)\"\n  }), \" corresponds to ensuring that the overall probability mass does not blow up, by nudging the parameters so that the model distribution does not assign too much probability to non-data configurations.\"), \"\\n\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"why-the-partition-function-term-makes-the-gradient-hard\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#why-the-partition-function-term-makes-the-gradient-hard\",\n    \"aria-label\": \"why the partition function term makes the gradient hard permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Why the partition function term makes the gradient hard\"), \"\\n\", React.createElement(_components.p, null, \"The crux of the difficulty is that to compute \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathbb{E}_{p(\\\\mathbf{x}; \\\\theta)}\\\\bigl[\\\\nabla_\\\\theta E(\\\\mathbf{x}; \\\\theta)\\\\bigr]\\\\)\"\n  }), \", one must sample from the current model distribution \", React.createElement(Latex, {\n    text: \"\\\\(p(\\\\mathbf{x}; \\\\theta)\\\\)\"\n  }), \" or otherwise approximate it. Direct summation or integration over \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathbf{x}\\\\)\"\n  }), \" is generally intractable, so we need to rely on methods such as \", React.createElement(Highlight, null, \"Monte Carlo Markov Chain\"), \" (MCMC) to approximate this expectation. If the MCMC chain mixes slowly or dimension is large, accurate sampling can become an expensive proposition.\"), \"\\n\", React.createElement(_components.p, null, \"The partition function's gradient \", React.createElement(Latex, {\n    text: \"\\\\(\\\\nabla_\\\\theta \\\\log Z(\\\\theta)\\\\)\"\n  }), \" thus stands out as the key computational burden. Indeed, the entire impetus behind many alternative objectives is to avoid explicit computation of \", React.createElement(Latex, {\n    text: \"\\\\(Z(\\\\theta)\\\\)\"\n  }), \" or its gradient. As we shall see in subsequent chapters, approaches like \", React.createElement(Highlight, null, \"Pseudolikelihood\"), \", \", React.createElement(Highlight, null, \"Score Matching\"), \", and \", React.createElement(Highlight, null, \"Noise-Contrastive Estimation\"), \" are creative attempts to get around the intractable nature of the partition function while still capturing meaningful statistics about the data.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"interpreting-the-gradient-of-log-z\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#interpreting-the-gradient-of-log-z\",\n    \"aria-label\": \"interpreting the gradient of log z permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Interpreting the gradient of log Z\"), \"\\n\", React.createElement(_components.p, null, \"Another lens to see this from: \", React.createElement(Latex, {\n    text: \"\\\\(Z(\\\\theta)\\\\)\"\n  }), \" can be interpreted in thermodynamic or statistical-physics terms as a normalizing factor ensuring that the \\\"energy landscape\\\" is consistent with a proper probability distribution. Its gradient effectively measures how changes in \", React.createElement(Latex, {\n    text: \"\\\\(\\\\theta\\\\)\"\n  }), \" stretch or squash that landscape globally. Understanding this helps to see why we might approximate it locally (as in pseudolikelihood), or in a more global sense with approximate sampling (Contrastive Divergence), or by turning the problem into a classification problem (Noise-Contrastive Estimation).\"), \"\\n\", React.createElement(_components.h2, {\n    id: \"3-stochastic-maximum-likelihood-and-contrastive-divergence\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#3-stochastic-maximum-likelihood-and-contrastive-divergence\",\n    \"aria-label\": \"3 stochastic maximum likelihood and contrastive divergence permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"3. Stochastic maximum likelihood and contrastive divergence\"), \"\\n\", React.createElement(_components.p, null, \"Historically, one of the early successes in dealing with the partition function within RBMs and other Boltzmann machines was the introduction of \", React.createElement(Highlight, null, \"Contrastive Divergence\"), \" (CD) by Hinton and subsequent refinements like \", React.createElement(Highlight, null, \"Persistent Contrastive Divergence\"), \" (PCD) or \", React.createElement(Highlight, null, \"Stochastic Maximum Likelihood\"), \" (SML). These methods revolve around MCMC sampling, but they do so in ways that reduce the computational cost and, crucially, the burden of having to approximate \", React.createElement(Latex, {\n    text: \"\\\\(\\\\log Z(\\\\theta)\\\\)\"\n  }), \" too precisely.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"naive-mcmc-approaches\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#naive-mcmc-approaches\",\n    \"aria-label\": \"naive mcmc approaches permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Naive MCMC approaches\"), \"\\n\", React.createElement(_components.p, null, \"Let us consider the naive approach to maximum likelihood. We know that the gradient has this negative-phase term requiring samples \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathbf{x}\\\\)\"\n  }), \" from the model distribution. One might try a straightforward Markov chain approach: for each gradient update, run a Markov chain from scratch (starting from a random initialization) to approximate the model distribution at the current \", React.createElement(Latex, {\n    text: \"\\\\(\\\\theta\\\\)\"\n  }), \". But in many high-dimensional models, this chain could take a prohibitive number of steps to mix properly. Doing that at each parameter update is unrealistic. The result is that naive MCMC maximum likelihood can be extremely slow, often so slow as to be infeasible.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"contrastive-divergence-cd-k\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#contrastive-divergence-cd-k\",\n    \"aria-label\": \"contrastive divergence cd k permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Contrastive divergence (CD-k)\"), \"\\n\", React.createElement(_components.p, null, \"Contrastive Divergence proposed a more practical compromise: rather than sampling from the model distribution until full mixing, one initializes the chain with the actual data (often called the \\\"clamped\\\" state) and then runs a small number of Gibbs sampling steps — \", React.createElement(Latex, {\n    text: \"\\\\(k\\\\)\"\n  }), \" steps is typical — to get a \\\"negative sample\\\". The intuition is that starting from real data hopefully puts us in a region of high probability under the model, so we do not need as many steps to reach some approximate sample from \", React.createElement(Latex, {\n    text: \"\\\\(p(\\\\mathbf{x}; \\\\theta)\\\\)\"\n  }), \". Then we use that sample to estimate the negative phase. Formally, the CD-k gradient update looks like:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\nabla_\\\\theta \\\\mathcal{L}(\\\\theta) \\\\approx -\\\\sum_{i=1}^N \\\\nabla_\\\\theta E(\\\\mathbf{x}^{(i)}; \\\\theta) + \\\\sum_{i=1}^N \\\\nabla_\\\\theta E(\\\\mathbf{x}'^{(i)}; \\\\theta),\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"where \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathbf{x}'^{(i)}\\\\)\"\n  }), \" is the sample obtained after \", React.createElement(Latex, {\n    text: \"\\\\(k\\\\)\"\n  }), \" steps of Gibbs sampling starting from \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathbf{x}^{(i)}\\\\)\"\n  }), \". This is sometimes referred to as a form of \\\"short-run MCMC\\\".\"), \"\\n\", React.createElement(_components.p, null, \"CD-k is fast, especially for an RBM where Gibbs sampling can be split into an alternating procedure between hidden and visible units in \", React.createElement(Latex, {\n    text: \"\\\\(O(k)\\\\)\"\n  }), \" time. However, it introduces bias: the samples you get may not match the true stationary distribution of the model. Interestingly, for small \", React.createElement(Latex, {\n    text: \"\\\\(k\\\\)\"\n  }), \", the method often works well in practice (particularly in generative pretraining of deep networks). Yet for pure maximum likelihood, it is no longer guaranteed that we are performing gradient ascent on the true log-likelihood.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"stochastic-maximum-likelihood-sml-or-persistent-contrastive-divergence-pcd\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#stochastic-maximum-likelihood-sml-or-persistent-contrastive-divergence-pcd\",\n    \"aria-label\": \"stochastic maximum likelihood sml or persistent contrastive divergence pcd permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Stochastic maximum likelihood (SML) or persistent contrastive divergence (PCD)\"), \"\\n\", React.createElement(_components.p, null, \"A refinement to mitigate the short-run bias is to maintain a set of \\\"persistent\\\" MCMC chains across parameter updates, rather than re-initializing from the data each time. This approach is known as \", React.createElement(Highlight, null, \"Persistent Contrastive Divergence\"), \" or \", React.createElement(Highlight, null, \"Stochastic Maximum Likelihood\"), \" (SML). The idea is:\"), \"\\n\", React.createElement(_components.ol, null, \"\\n\", React.createElement(_components.li, null, \"Initialize one (or more) Markov chains at random once at the beginning.\"), \"\\n\", React.createElement(_components.li, null, \"For each update, sample a mini-batch of data for the positive phase.\"), \"\\n\", React.createElement(_components.li, null, \"Use the current state(s) of the persistent Markov chain(s) for the negative phase, running a few Gibbs steps on these chains to update them.\"), \"\\n\", React.createElement(_components.li, null, \"Update parameters based on the difference between the positive-phase gradient and the negative-phase gradient.\"), \"\\n\", React.createElement(_components.li, null, \"Keep the updated chain states around for the next parameter update.\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, \"Because these chains are not reset to the data at each iteration, they can potentially explore the model distribution more globally. SML is an (asymptotically) unbiased estimator of the gradient under certain conditions (e.g., if the chain is long-lived enough to approximate the stationary distribution). Still, the success of SML in practice depends heavily on the mixing properties of the chain and the complexity of the energy landscape.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"other-related-details\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#other-related-details\",\n    \"aria-label\": \"other related details permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Other related details\"), \"\\n\", React.createElement(_components.p, null, \"Many research papers have proposed improvements, from parallel tempering to advanced sampling methods like \", React.createElement(Highlight, null, \"Annealed Importance Sampling\"), \" (which we will see in a later section) to reduce bias further and speed up mixing. In general, though, if your model is high-dimensional and has complicated energy contours, even these advanced MCMC-based approaches can be slow or get stuck in local modes. This persistent difficulty is a major driver of alternative methods that avoid the partition function entirely.\"), \"\\n\", React.createElement(_components.h2, {\n    id: \"4-pseudolikelihood\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#4-pseudolikelihood\",\n    \"aria-label\": \"4 pseudolikelihood permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"4. Pseudolikelihood\"), \"\\n\", React.createElement(_components.p, null, React.createElement(Highlight, null, \"Pseudolikelihood\"), \" is a venerable approach introduced by Besag in the 1970s. It seeks to circumvent direct computation of \", React.createElement(Latex, {\n    text: \"\\\\(Z(\\\\theta)\\\\)\"\n  }), \" by replacing the joint likelihood \", React.createElement(Latex, {\n    text: \"\\\\(p(\\\\mathbf{x}; \\\\theta)\\\\)\"\n  }), \" with a product of conditional probabilities of each variable given all the others. Specifically, for a discrete variable set \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathbf{x} = (x_1, \\\\dots, x_d)\\\\)\"\n  }), \", the pseudolikelihood is:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\prod_{j=1}^d p(x_j \\\\mid \\\\mathbf{x}_{\\\\setminus j}; \\\\theta),\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"where \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathbf{x}_{\\\\setminus j}\\\\)\"\n  }), \" means \\\"all variables except \", React.createElement(Latex, {\n    text: \"\\\\(x_j\\\\)\"\n  }), \"\\\". Taking the log of that product gives:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\log \\\\mathrm{PL}(\\\\theta) \\n= \\\\sum_{j=1}^d \\\\sum_{i=1}^N \\\\log p\\\\bigl(x_j^{(i)} \\\\mid \\\\mathbf{x}_{\\\\setminus j}^{(i)}; \\\\theta\\\\bigr).\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.h3, {\n    id: \"avoiding-the-partition-function-via-conditional-probabilities\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#avoiding-the-partition-function-via-conditional-probabilities\",\n    \"aria-label\": \"avoiding the partition function via conditional probabilities permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Avoiding the partition function via conditional probabilities\"), \"\\n\", React.createElement(_components.p, null, \"Why does this help? Each conditional probability can often be computed without requiring the full \", React.createElement(Latex, {\n    text: \"\\\\(Z(\\\\theta)\\\\)\"\n  }), \". For instance, in an Ising model or a discrete MRF, one might have:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\np(x_j \\\\mid \\\\mathbf{x}_{\\\\setminus j}; \\\\theta) \\n= \\\\frac{\\\\exp\\\\bigl(-E(x_j, \\\\mathbf{x}_{\\\\setminus j}; \\\\theta)\\\\bigr)}{\\\\sum_{x_j'} \\\\exp\\\\bigl(-E(x_j', \\\\mathbf{x}_{\\\\setminus j}; \\\\theta)\\\\bigr)},\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"and that summation in the denominator is only over the local states of \", React.createElement(Latex, {\n    text: \"\\\\(x_j\\\\)\"\n  }), \" (e.g., \", React.createElement(Latex, {\n    text: \"\\\\(\\\\pm 1\\\\)\"\n  }), \" in an Ising spin system or a small discrete set). That is far easier to compute than the global \", React.createElement(Latex, {\n    text: \"\\\\(Z(\\\\theta)\\\\)\"\n  }), \", which sums over all variables \", React.createElement(Latex, {\n    text: \"\\\\(x_1, \\\\dots, x_d\\\\)\"\n  }), \" simultaneously.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"strengths-of-pseudolikelihood\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#strengths-of-pseudolikelihood\",\n    \"aria-label\": \"strengths of pseudolikelihood permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Strengths of pseudolikelihood\"), \"\\n\", React.createElement(_components.p, null, \"One immediate benefit is that pseudolikelihood drastically cuts computation time for models with discrete local variables — especially if the local variable state space is small. The method also has theoretical guarantees in certain asymptotic regimes, meaning that it can yield consistent parameter estimates under some assumptions on the dependency structure. In some specialized tasks such as \", React.createElement(Highlight, null, \"missing data imputation\"), \", optimizing conditional distributions may actually be more natural than focusing on the full joint distribution.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"limitations-in-density-estimation\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#limitations-in-density-estimation\",\n    \"aria-label\": \"limitations in density estimation permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Limitations in density estimation\"), \"\\n\", React.createElement(_components.p, null, \"However, pseudolikelihood can be a poor approximation to the true log-likelihood in settings where global dependencies matter a great deal. It does not necessarily place the correct amount of mass on joint configurations if each variable's local conditionals do not adequately capture the multi-way interactions present in \", React.createElement(Latex, {\n    text: \"\\\\(p(\\\\mathbf{x}; \\\\theta)\\\\)\"\n  }), \". Thus, for pure density modeling or generative tasks, pseudolikelihood might fail to produce samples that match the global structure of the data. In fields like image modeling or any domain with complex dependencies, pseudolikelihood can produce subpar generative performance even if the local conditionals are well fit.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"implementation-details\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#implementation-details\",\n    \"aria-label\": \"implementation details permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Implementation details\"), \"\\n\", React.createElement(_components.p, null, \"In practice, one typically just replaces the negative log-likelihood with the negative log-pseudolikelihood and does standard optimization. Gradient-based methods remain feasible, and indeed are often simpler than MCMC-based maximum likelihood. Another use case is in certain forms of cross validation or hyperparameter tuning, where the pseudolikelihood can be computed quickly and used as a proxy objective in place of the intractable true likelihood.\"), \"\\n\", React.createElement(_components.h2, {\n    id: \"5-score-matching-and-ratio-matching\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#5-score-matching-and-ratio-matching\",\n    \"aria-label\": \"5 score matching and ratio matching permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"5. Score matching and ratio matching\"), \"\\n\", React.createElement(_components.p, null, React.createElement(Highlight, null, \"Score Matching\"), \", introduced by Hyvärinen (2005), is another strategy for models that might have an intractable normalizing constant. The idea is elegantly different from pseudolikelihood. Instead of trying to maximize \", React.createElement(Latex, {\n    text: \"\\\\(\\\\log p(\\\\mathbf{x}; \\\\theta)\\\\)\"\n  }), \" directly, you match the gradient (\\\"score\\\") of the log-density with respect to \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathbf{x}\\\\)\"\n  }), \" between model and data. A key advantage is that it can be done without ever computing \", React.createElement(Latex, {\n    text: \"\\\\(Z(\\\\theta)\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"basic-principle-of-score-matching\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#basic-principle-of-score-matching\",\n    \"aria-label\": \"basic principle of score matching permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Basic principle of score matching\"), \"\\n\", React.createElement(_components.p, null, \"Let \", React.createElement(Latex, {\n    text: \"\\\\(s_\\\\theta(\\\\mathbf{x}) = \\\\nabla_{\\\\mathbf{x}} \\\\log p(\\\\mathbf{x}; \\\\theta)\\\\)\"\n  }), \" be the score function, i.e., the gradient of the log-probability with respect to the data \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathbf{x}\\\\)\"\n  }), \". Score matching tries to find \", React.createElement(Latex, {\n    text: \"\\\\(\\\\theta\\\\)\"\n  }), \" so that:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\mathbb{E}_{p_{\\\\text{data}}(\\\\mathbf{x})}\\\\bigl[\\\\| s_\\\\theta(\\\\mathbf{x}) - s_{\\\\text{data}}(\\\\mathbf{x}) \\\\|^2 \\\\bigr]\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"is minimized, where \", React.createElement(Latex, {\n    text: \"\\\\(s_{\\\\text{data}}(\\\\mathbf{x})\\\\)\"\n  }), \" is the true gradient of the data distribution (in practice, we do not know this function, but the method sets up an alternative objective that is computable). Hyvärinen derived a neat trick that the difference of these gradients can be simplified so that the partition function disappears from the resulting formula. In short, the objective for score matching does not require \", React.createElement(Latex, {\n    text: \"\\\\(Z(\\\\theta)\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"interpretation\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#interpretation\",\n    \"aria-label\": \"interpretation permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Interpretation\"), \"\\n\", React.createElement(_components.p, null, \"Intuitively, if two distributions match in terms of the derivatives of their log-densities, they must be the same distribution (under suitable regularity conditions). So matching the score function is a proxy for matching the distribution itself. Score matching can be particularly elegant in continuous spaces, e.g., in modeling natural images or other real-valued data, provided we can define \", React.createElement(Latex, {\n    text: \"\\\\(E(\\\\mathbf{x}; \\\\theta)\\\\)\"\n  }), \" in a differentiable manner.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"incompatibility-with-discrete-variable-models\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#incompatibility-with-discrete-variable-models\",\n    \"aria-label\": \"incompatibility with discrete variable models permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Incompatibility with discrete-variable models\"), \"\\n\", React.createElement(_components.p, null, \"A known drawback: standard score matching is not well-defined for discrete-variable models because you cannot treat \", React.createElement(Latex, {\n    text: \"\\\\(p(\\\\mathbf{x}; \\\\theta)\\\\)\"\n  }), \" as a smooth function in a continuous domain. One can attempt modifications for discrete data, but this is less straightforward and can degrade into complexity. Indeed, methods like \", React.createElement(Highlight, null, \"ratio matching\"), \" have been proposed for certain discrete scenarios.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"ratio-matching\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#ratio-matching\",\n    \"aria-label\": \"ratio matching permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Ratio matching\"), \"\\n\", React.createElement(_components.p, null, \"Ratio matching tries to match \", React.createElement(Latex, {\n    text: \"\\\\(\\\\frac{p(\\\\mathbf{x}; \\\\theta)}{p_{\\\\text{data}}(\\\\mathbf{x})}\\\\)\"\n  }), \" in local neighborhoods. As with score matching, the idea is to circumvent direct normalizing constants. Ratio matching sometimes finds use in specialized contexts, but it is far less mainstream than either pseudolikelihood or score matching. One might see it in cases where \", React.createElement(Latex, {\n    text: \"\\\\(p_{\\\\text{data}}(\\\\mathbf{x})\\\\)\"\n  }), \" is partially known or can be replaced with local empirical densities. For the majority of large-scale discrete tasks, though, methods such as \", React.createElement(Highlight, null, \"Noise-Contrastive Estimation\"), \" or \", React.createElement(Highlight, null, \"Contrastive Divergence\"), \" remain more common.\"), \"\\n\", React.createElement(_components.h2, {\n    id: \"6-denoising-score-matching\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#6-denoising-score-matching\",\n    \"aria-label\": \"6 denoising score matching permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"6. Denoising score matching\"), \"\\n\", React.createElement(_components.p, null, React.createElement(Highlight, null, \"Denoising score matching\"), \" extends the core idea of score matching by introducing an explicit noise model. It is deeply related to \", React.createElement(Highlight, null, \"Denoising Autoencoders\"), \" and has seen a recent surge of interest for tasks like generative modeling of images in the form of \", React.createElement(Highlight, null, \"Denoising Diffusion Probabilistic Models\"), \".\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"motivation-for-adding-noise\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#motivation-for-adding-noise\",\n    \"aria-label\": \"motivation for adding noise permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Motivation for adding noise\"), \"\\n\", React.createElement(_components.p, null, \"One practical issue with score matching is that the model might overfit the data's local geometry, especially if there is insufficient coverage of the space. By adding noise to \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathbf{x}\\\\)\"\n  }), \" — essentially observing a perturbed version of \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathbf{x}\\\\)\"\n  }), \" — you gain a smoothed version of the data distribution. The method trains the model to predict the score of the underlying clean distribution from the noisy samples. This also helps address regions of the space where the data is sparse, since you do not want your model to blow up arbitrarily in small pockets.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"mathematical-form\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#mathematical-form\",\n    \"aria-label\": \"mathematical form permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Mathematical form\"), \"\\n\", React.createElement(_components.p, null, \"The standard formulation, as introduced by Vincent in the context of denoising autoencoders, is that given a noisy sample \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathbf{x}_{\\\\text{noisy}}\\\\)\"\n  }), \" created by adding a small Gaussian perturbation to \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathbf{x}\\\\)\"\n  }), \", the network (or model) tries to reconstruct the original \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathbf{x}\\\\)\"\n  }), \", or equivalently, to learn the gradient of the log of the clean data density. This can be cast as a form of score matching:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\min_\\\\theta \\\\mathbb{E}_{\\\\mathbf{x} \\\\sim p_{\\\\text{data}}, \\\\mathbf{x}_{\\\\text{noisy}} \\\\mid \\\\mathbf{x}} \\n\\\\bigl[\\\\| s_\\\\theta(\\\\mathbf{x}_{\\\\text{noisy}}) - \\\\nabla_{\\\\mathbf{x}_{\\\\text{noisy}}}\\\\log p_{\\\\text{noisy}}(\\\\mathbf{x}_{\\\\text{noisy}}) \\\\|^2 \\\\bigr].\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"Often, \", React.createElement(Latex, {\n    text: \"\\\\(p_{\\\\text{noisy}}(\\\\mathbf{x}_{\\\\text{noisy}})\\\\)\"\n  }), \" is a known Gaussian corruption process. The partition function of the underlying distribution still never enters explicitly, so we circumvent \", React.createElement(Latex, {\n    text: \"\\\\(Z(\\\\theta)\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"relationship-to-autoencoder-training\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#relationship-to-autoencoder-training\",\n    \"aria-label\": \"relationship to autoencoder training permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Relationship to autoencoder training\"), \"\\n\", React.createElement(_components.p, null, \"Denoising autoencoders similarly attempt to reconstruct the clean data from noisy input, effectively learning a latent representation that discerns the manifold of real data. The gradient-based viewpoint of denoising score matching unifies these ideas: in certain architectures, learning to denoise is analogous to learning local directions pointing back toward the data manifold, i.e., the negative gradient of the energy. This synergy has led to state-of-the-art generative modeling approaches, including \", React.createElement(Highlight, null, \"score-based generative modeling\"), \" in continuous time (also known as \", React.createElement(Highlight, null, \"diffusion models\"), \") that unify the notion of iterative denoising with a continuous Markov process that can be reversed to generate new samples.\"), \"\\n\", React.createElement(_components.h2, {\n    id: \"7-noise-contrastive-estimation\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#7-noise-contrastive-estimation\",\n    \"aria-label\": \"7 noise contrastive estimation permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"7. Noise-contrastive estimation\"), \"\\n\", React.createElement(_components.p, null, React.createElement(Highlight, null, \"Noise-Contrastive Estimation\"), \" (NCE), introduced by Gutmann and Hyvärinen, is a technique that recasts unsupervised density estimation as a supervised binary classification problem. The partition function is learned as a parameter (often called the \\\"bias\\\" or offset) that helps discriminate between \\\"real data samples\\\" and \\\"noise samples\\\".\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"reduction-to-binary-classification\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#reduction-to-binary-classification\",\n    \"aria-label\": \"reduction to binary classification permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Reduction to binary classification\"), \"\\n\", React.createElement(_components.p, null, \"The key idea is: create a \\\"noisy distribution\\\" \", React.createElement(Latex, {\n    text: \"\\\\(q(\\\\mathbf{x})\\\\)\"\n  }), \" that you can sample from easily, such as a uniform distribution or some other convenient distribution. Then combine real data samples (labeled \", React.createElement(Latex, {\n    text: \"\\\\(y=1\\\\)\"\n  }), \") with noise samples (\", React.createElement(Latex, {\n    text: \"\\\\(y=0\\\\)\"\n  }), \") into a single dataset. Train a classifier that tries to predict whether a sample is real or noise by looking at \", React.createElement(Latex, {\n    text: \"\\\\(\\\\log p(\\\\mathbf{x}; \\\\theta)\\\\)\"\n  }), \" minus \", React.createElement(Latex, {\n    text: \"\\\\(\\\\log q(\\\\mathbf{x})\\\\)\"\n  }), \". More concretely, define:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\nD_\\\\theta(\\\\mathbf{x}) = \\\\sigma\\\\bigl(\\\\log p(\\\\mathbf{x}; \\\\theta) - \\\\log q(\\\\mathbf{x})\\\\bigr),\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"where \", React.createElement(Latex, {\n    text: \"\\\\(\\\\sigma\\\\)\"\n  }), \" is the logistic sigmoid function. If \", React.createElement(Latex, {\n    text: \"\\\\(p(\\\\mathbf{x}; \\\\theta)\\\\)\"\n  }), \" is unnormalized as \", React.createElement(Latex, {\n    text: \"\\\\(\\\\tilde{p}(\\\\mathbf{x}; \\\\theta)\\\\)\"\n  }), \", you can treat \", React.createElement(Latex, {\n    text: \"\\\\(\\\\log Z(\\\\theta)\\\\)\"\n  }), \" as a parameter that is learned via gradient-based classification. Minimizing the cross-entropy classification loss with a balanced set of real and noise examples yields consistent estimates of both the model parameters \", React.createElement(Latex, {\n    text: \"\\\\(\\\\theta\\\\)\"\n  }), \" and the partition function offset.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"simultaneous-estimation-of-parameters-and-partition-function\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#simultaneous-estimation-of-parameters-and-partition-function\",\n    \"aria-label\": \"simultaneous estimation of parameters and partition function permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Simultaneous estimation of parameters and partition function\"), \"\\n\", React.createElement(_components.p, null, \"The partition function effectively shows up in the log-odds for distinguishing real from noise. If \", React.createElement(Latex, {\n    text: \"\\\\(\\\\log Z(\\\\theta)\\\\)\"\n  }), \" is a free parameter, gradient descent will adjust it so that the separation between real and noise samples is maximized. One advantage is that you never explicitly sum over the entire data space. Another is that we can choose \", React.createElement(Latex, {\n    text: \"\\\\(q(\\\\mathbf{x})\\\\)\"\n  }), \" in a way that makes sampling trivial.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"advantages-and-drawbacks\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#advantages-and-drawbacks\",\n    \"aria-label\": \"advantages and drawbacks permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Advantages and drawbacks\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Advantages\"), \": NCE can be an elegant approach when dealing with large vocabulary but relatively simpler single-variable or low-dimensional contexts, e.g., certain language modeling tasks. It's also straightforward to implement since it reuses standard binary classification routines.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Drawbacks\"), \": In large multi-variable contexts with complex dependencies, choosing a suitable noise distribution \", React.createElement(Latex, {\n    text: \"\\\\(q(\\\\mathbf{x})\\\\)\"\n  }), \" can be non-trivial. If \", React.createElement(Latex, {\n    text: \"\\\\(q(\\\\mathbf{x})\\\\)\"\n  }), \" is too simplistic, the classifier finds the discrimination task too easy, leading to poor density estimates. On the other hand, generating noise samples that mimic real data intricacies might defeat the purpose. Moreover, the approach typically scales poorly if the dimensionality or complexity of \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathbf{x}\\\\)\"\n  }), \" is high unless you have a very clever or domain-specific noise model.\"), \"\\n\"), \"\\n\", React.createElement(_components.h2, {\n    id: \"8-estimating-the-partition-function\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#8-estimating-the-partition-function\",\n    \"aria-label\": \"8 estimating the partition function permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"8. Estimating the partition function\"), \"\\n\", React.createElement(_components.p, null, \"Even though many methods circumvent direct computation of \", React.createElement(Latex, {\n    text: \"\\\\(Z(\\\\theta)\\\\)\"\n  }), \", there are still scenarios where we may want an actual estimate of \", React.createElement(Latex, {\n    text: \"\\\\(Z(\\\\theta)\\\\)\"\n  }), \". For instance, in evaluating the absolute log-likelihood of a generative model, or comparing models by their \", React.createElement(Latex, {\n    text: \"\\\\(\\\\log p(\\\\mathbf{X}; \\\\theta)\\\\)\"\n  }), \" on a validation set, or computing certain Bayesian model selection criteria, we need an estimate of \", React.createElement(Latex, {\n    text: \"\\\\(\\\\log Z(\\\\theta)\\\\)\"\n  }), \". This final chapter surveys some of the widely used estimation techniques — mostly based on importance sampling variants and bridging methods.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"importance-sampling\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#importance-sampling\",\n    \"aria-label\": \"importance sampling permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Importance sampling\"), \"\\n\", React.createElement(_components.p, null, React.createElement(Highlight, null, \"Importance sampling\"), \" is a standard technique for estimating integrals or sums with respect to a difficult distribution \", React.createElement(Latex, {\n    text: \"\\\\(p(\\\\mathbf{x}; \\\\theta)\\\\)\"\n  }), \" by using samples drawn from an easier proposal distribution \", React.createElement(Latex, {\n    text: \"\\\\(r(\\\\mathbf{x})\\\\)\"\n  }), \". Suppose we want:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\nZ(\\\\theta) = \\\\int \\\\exp\\\\bigl(-E(\\\\mathbf{x}; \\\\theta)\\\\bigr)\\\\, d\\\\mathbf{x}.\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"We can write:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\nZ(\\\\theta) = \\\\int \\\\frac{\\\\exp\\\\bigl(-E(\\\\mathbf{x}; \\\\theta)\\\\bigr)}{r(\\\\mathbf{x})} r(\\\\mathbf{x})\\\\, d\\\\mathbf{x}.\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"Hence, if we sample \", React.createElement(Latex, {\n    text: \"\\\\(\\\\{\\\\mathbf{x}^{(i)}\\\\}_{i=1}^M\\\\)\"\n  }), \" i.i.d. from \", React.createElement(Latex, {\n    text: \"\\\\(r(\\\\mathbf{x})\\\\)\"\n  }), \", an unbiased estimator is:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\hat{Z} = \\\\frac{1}{M} \\\\sum_{i=1}^M \\\\frac{\\\\exp\\\\bigl(-E(\\\\mathbf{x}^{(i)}; \\\\theta)\\\\bigr)}{r(\\\\mathbf{x}^{(i)})}.\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"Choosing \", React.createElement(Latex, {\n    text: \"\\\\(r(\\\\mathbf{x})\\\\)\"\n  }), \" so that it resembles \", React.createElement(Latex, {\n    text: \"\\\\(\\\\exp\\\\bigl(-E(\\\\mathbf{x}; \\\\theta)\\\\bigr)\\\\)\"\n  }), \" can reduce variance. But for high-dimensional distributions, finding or sampling from such an \", React.createElement(Latex, {\n    text: \"\\\\(r(\\\\mathbf{x})\\\\)\"\n  }), \" is not trivial. If \", React.createElement(Latex, {\n    text: \"\\\\(r(\\\\mathbf{x})\\\\)\"\n  }), \" does not adequately cover the important regions, the estimator can suffer from extremely high variance.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"annealed-importance-sampling-ais\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#annealed-importance-sampling-ais\",\n    \"aria-label\": \"annealed importance sampling ais permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Annealed importance sampling (AIS)\"), \"\\n\", React.createElement(_components.p, null, React.createElement(Highlight, null, \"Annealed Importance Sampling\"), \", proposed by Neal (2001), is a significant improvement for evaluating \", React.createElement(Latex, {\n    text: \"\\\\(Z(\\\\theta)\\\\)\"\n  }), \". The method constructs a sequence of intermediate distributions bridging a tractable base distribution (e.g., a factorized Gaussian) and the target distribution \", React.createElement(Latex, {\n    text: \"\\\\(p(\\\\mathbf{x}; \\\\theta)\\\\)\"\n  }), \". One typically uses a temperature parameter \", React.createElement(Latex, {\n    text: \"\\\\(\\\\beta\\\\)\"\n  }), \" that goes from 0 to 1 in small steps:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\np_k(\\\\mathbf{x}) \\\\propto \\\\exp\\\\bigl(-\\\\beta_k E(\\\\mathbf{x}; \\\\theta)\\\\bigr),\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"where \", React.createElement(Latex, {\n    text: \"\\\\(0 = \\\\beta_0 < \\\\beta_1 < \\\\dots < \\\\beta_K = 1\\\\)\"\n  }), \". For each step, you do importance sampling or MCMC transitions to move from \", React.createElement(Latex, {\n    text: \"\\\\(p_{k-1}(\\\\mathbf{x})\\\\)\"\n  }), \" to \", React.createElement(Latex, {\n    text: \"\\\\(p_k(\\\\mathbf{x})\\\\)\"\n  }), \". The final product of importance weights across the chain yields an estimate of the ratio \", React.createElement(Latex, {\n    text: \"\\\\(\\\\frac{Z(\\\\theta)}{Z_0}\\\\)\"\n  }), \", where \", React.createElement(Latex, {\n    text: \"\\\\(Z_0\\\\)\"\n  }), \" is the known normalizer of the base distribution. AIS often yields lower-variance estimates than a single-step importance sampling, given the chain of intermediate bridging distributions allows a more incremental adaptation from the easy distribution to the difficult one.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"bridge-sampling\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#bridge-sampling\",\n    \"aria-label\": \"bridge sampling permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Bridge sampling\"), \"\\n\", React.createElement(_components.p, null, React.createElement(Highlight, null, \"Bridge sampling\"), \" is a related idea in which you define a single \\\"bridge\\\" distribution \", React.createElement(Latex, {\n    text: \"\\\\(r(\\\\mathbf{x})\\\\)\"\n  }), \" that lies somewhere between the base and target distributions. One sets up an estimator for \", React.createElement(Latex, {\n    text: \"\\\\(Z(\\\\theta)\\\\)\"\n  }), \" that uses samples from both the base distribution and the target distribution with a bridging function that helps reduce variance. AIS can be viewed as an extension of bridge sampling in which you use multiple bridging distributions in a chain rather than just one.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"linked-importance-sampling-and-variance-reduction\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#linked-importance-sampling-and-variance-reduction\",\n    \"aria-label\": \"linked importance sampling and variance reduction permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Linked importance sampling and variance reduction\"), \"\\n\", React.createElement(_components.p, null, \"Numerous variants exist, such as \", React.createElement(Highlight, null, \"linked importance sampling\"), \", \", React.createElement(Highlight, null, \"sequential Monte Carlo\"), \", and other advanced techniques that attempt to reduce variance by carefully correlating samples or controlling the shape of the bridging distributions. Each has its own set of pros and cons. The main takeaway is that if you truly need an accurate estimate of \", React.createElement(Latex, {\n    text: \"\\\\(Z(\\\\theta)\\\\)\"\n  }), \" (or \", React.createElement(Latex, {\n    text: \"\\\\(\\\\log Z(\\\\theta)\\\\)\"\n  }), \"), these bridging or annealing approaches usually perform better than naive importance sampling.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"practical-considerations\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#practical-considerations\",\n    \"aria-label\": \"practical considerations permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Practical considerations\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"When to estimate \", React.createElement(Latex, {\n    text: \"\\\\(Z\\\\)\"\n  })), \": If your downstream task only depends on unnormalized probabilities or partial derivatives of \", React.createElement(Latex, {\n    text: \"\\\\(\\\\log p(\\\\mathbf{x}; \\\\theta)\\\\)\"\n  }), \", you might circumvent \", React.createElement(Latex, {\n    text: \"\\\\(Z\\\\)\"\n  }), \". Indeed, many modern generative modeling frameworks do not bother with explicit \", React.createElement(Latex, {\n    text: \"\\\\(Z\\\\)\"\n  }), \" estimation.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Computational cost\"), \": AIS and related methods require carefully tuned MCMC steps and bridging schedules. They can be computationally expensive.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Model comparison\"), \": In practice, if you want to compare two models by their log-likelihood, approximate partition function estimates can have non-negligible variance. Tools like \", React.createElement(Highlight, null, \"PSRF (Potential Scale Reduction Factor)\"), \" from MCMC diagnostics or repeated runs can help gauge reliability.\"), \"\\n\"), \"\\n\", React.createElement(Image, {\n    alt: \"Partition function schematic\",\n    path: \"\",\n    caption: \"A schematic illustration of how an unnormalized model distribution compares to a known base distribution. The partition function is the ratio between the integral under the unnormalized function and the integral under the base distribution, bridging them is the essence of AIS.\",\n    zoom: \"false\"\n  }), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.p, null, \"Below, I include a brief code snippet showing how one might implement a simplified AIS procedure in Python for a toy 2D energy function — just to illustrate the concept. Naturally, for real-world high-dimensional data, one must incorporate more sophisticated MCMC transition operators and schedules for \", React.createElement(Latex, {\n    text: \"\\\\(\\\\beta\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(Code, {\n    text: `\nimport numpy as np\n\ndef energy_function(x, y):\n    # Simple 2D double well or ring structure as an example\n    r2 = x**2 + y**2\n    return 0.1 * (r2 - 1.0)**2  # Some contrived energy for demonstration\n\ndef base_log_prob(x, y):\n    # Base distribution: e.g., isotropic Gaussian with mean 0, variance=1\n    return -(x**2 + y**2) / 2.0\n\ndef ais(num_samples=10000, num_steps=1000):\n    samples = np.random.randn(num_samples, 2)  # from the base distribution\n    log_w = np.zeros(num_samples)\n    \n    # Beta schedule\n    betas = np.linspace(0, 1, num_steps)\n    delta_betas = betas[1:] - betas[:-1]\n\n    for i in range(num_steps-1):\n        beta0, beta1 = betas[i], betas[i+1]\n        \n        # Compute log unnormalized density difference\n        # which is [ -beta1 * E + beta1 * log base ] - [ -beta0 * E + beta0 * log base ]\n        # Because AIS is bridging from base to target\n        e_vals = np.array([energy_function(s[0], s[1]) for s in samples])\n        log_base = np.array([base_log_prob(s[0], s[1]) for s in samples])\n        \n        log_w += -beta1 * e_vals + beta1 * log_base - (-beta0 * e_vals + beta0 * log_base)\n        \n        # MCMC step (very naive: small random walk or something more advanced)\n        # We'll do a small Gaussian move\n        proposals = samples + 0.01 * np.random.randn(num_samples, 2)\n        \n        # Accept/reject with Metropolis-Hastings based on the intermediate distribution\n        current_energy = beta1 * np.array([energy_function(s[0], s[1]) for s in samples]) \\\n                         - beta1 * np.array([base_log_prob(s[0], s[1]) for s in samples])\n        proposal_energy = beta1 * np.array([energy_function(p[0], p[1]) for p in proposals]) \\\n                          - beta1 * np.array([base_log_prob(p[0], p[1]) for p in proposals])\n        \n        # Metropolis acceptance\n        accept_prob = np.exp(-(proposal_energy - current_energy))\n        rand_u = np.random.rand(num_samples)\n        accepts = rand_u < accept_prob\n        samples[accepts] = proposals[accepts]\n    \n    # Estimate ratio\n    z_base = 1.0  # The normalizer for the base distribution (2D Gaussian = 2*pi for sigma=1, ignoring constants for example)\n    w = np.exp(log_w)\n    z_est = np.mean(w) * z_base\n    return z_est\n\nif __name__ == \"__main__\":\n    z_est = ais(num_samples=20000, num_steps=500)\n    print(\"Estimated partition function:\", z_est)\n`\n  }), \"\\n\", React.createElement(_components.p, null, \"This snippet is not optimized, but it should convey the essence: we start from an easy-to-sample distribution (the base), gradually anneal toward the target distribution, track the change in unnormalized log probabilities, and combine them into a single importance weight. The method ends up with an estimate of \", React.createElement(Latex, {\n    text: \"\\\\(Z(\\\\theta)\\\\)\"\n  }), \". Of course, for a real model, the energy function and base distribution would be replaced accordingly, and the MCMC steps would be carefully tuned.\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.p, null, \"This concludes our in-depth journey through the partition function, the difficulties it imposes, and the wide array of methods used to either approximate it or circumvent it altogether. I hope this article has shed some light on why the partition function merits its own dedicated discussion: from Contrastive Divergence to Noise-Contrastive Estimation to sophisticated AIS procedures for explicit computation, the partition function is the pivotal factor in many advanced probabilistic models. Understanding it, and understanding the techniques for dealing with it, is key to mastering energy-based modeling and a host of other undirected graphical models in modern machine learning.\"));\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? React.createElement(MDXLayout, props, React.createElement(_createMdxContent, props)) : _createMdxContent(props);\n}\nexport default MDXContent;\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","import GATSBY_COMPILED_MDX from \"/home/avrtt/Repos/avrtt.github.io/src/pages/posts/research/partition_function_a_closer_look.mdx\";\nimport _toConsumableArray from \"@babel/runtime/helpers/esm/toConsumableArray\";\nimport React, {useState, useEffect} from 'react';\nimport {useSiteMetadata} from \"../hooks/useSiteMetadata\";\nimport RemoveMarkdown from 'remove-markdown';\nimport {ImageContext} from '../context/ImageContext';\nimport {MDXProvider} from '@mdx-js/react';\nimport Image from '../components/PostImage';\nimport {motion} from 'framer-motion';\nimport SEO from \"../components/seo\";\nimport PostBanner from '../components/PostBanner';\nimport PostBottom from '../components/PostBottom';\nimport {wordsPerMinuteAdventures, wordsPerMinuteResearch, wordsPerMinuteThoughts} from '../data/commonVariables';\nimport PartOfCourseNotice from \"../components/PartOfCourseNotice\";\nimport * as stylesButtonsCommon from \"../styles/buttons_common.module.scss\";\nimport * as stylesCustomPostLayouts from \"../styles/custom_post_layouts.module.scss\";\nimport * as stylesTableOfContents from \"../styles/table_of_contents.module.scss\";\nimport * as stylesTagBadges from \"../styles/tag_badges.module.scss\";\nfunction formatReadTime(minutes) {\n  if (minutes <= 10) return '~10 min';\n  if (minutes <= 20) return '~20 min';\n  if (minutes <= 30) return '~30 min';\n  if (minutes <= 40) return '~40 min';\n  if (minutes <= 50) return '~50 min';\n  if (minutes <= 60) return '~1 h';\n  const hours = Math.floor(minutes / 60);\n  const remainder = minutes % 60;\n  if (remainder <= 30) {\n    return `~${hours}${remainder > 0 ? '.5' : ''} h`;\n  }\n  return `~${hours + 1} h`;\n}\nconst TableOfContents = _ref => {\n  let {toc} = _ref;\n  if (!toc || !toc.items) return null;\n  const handleClick = (e, url) => {\n    e.preventDefault();\n    const targetId = url.replace('#', '');\n    const targetElement = document.getElementById(targetId);\n    if (targetElement) {\n      targetElement.scrollIntoView({\n        behavior: 'smooth',\n        block: 'start'\n      });\n    }\n  };\n  return React.createElement(\"nav\", {\n    className: stylesTableOfContents.toc\n  }, React.createElement(\"ul\", null, toc.items.map((item, index) => React.createElement(\"li\", {\n    key: index\n  }, React.createElement(\"a\", {\n    href: item.url,\n    onClick: e => handleClick(e, item.url)\n  }, item.title), item.items && React.createElement(TableOfContents, {\n    toc: {\n      items: item.items\n    }\n  })))));\n};\nexport function PostTemplate(_ref2) {\n  let {data: {mdx, allMdx, allPostImages}, children} = _ref2;\n  const {frontmatter, body, tableOfContents} = mdx;\n  const index = frontmatter.index;\n  const slug = frontmatter.slug;\n  const section = slug.split('/')[1];\n  const posts = allMdx.nodes.filter(post => post.frontmatter.slug.includes(`/${section}/`));\n  const sortedPosts = posts.sort((a, b) => a.frontmatter.index - b.frontmatter.index);\n  const currentIndex = sortedPosts.findIndex(post => post.frontmatter.index === index);\n  const nextPost = sortedPosts[currentIndex + 1];\n  const lastPost = sortedPosts[currentIndex - 1];\n  const trimmedSlug = frontmatter.slug.replace(/\\/$/, '');\n  const keyCurrent = (/[^/]*$/).exec(trimmedSlug)[0];\n  const basePath = `posts/${section}/content/${keyCurrent}/`;\n  const {0: isWideLayout, 1: setIsWideLayout} = useState(frontmatter.flagWideLayoutByDefault);\n  const {0: isAnimating, 1: setIsAnimating} = useState(false);\n  const toggleLayout = () => {\n    setIsWideLayout(!isWideLayout);\n  };\n  useEffect(() => {\n    setIsAnimating(true);\n    const timer = setTimeout(() => setIsAnimating(false), 340);\n    return () => clearTimeout(timer);\n  }, [isWideLayout]);\n  var wordsPerMinute;\n  if (section === \"adventures\") {\n    wordsPerMinute = wordsPerMinuteAdventures;\n  } else if (section === \"research\") {\n    wordsPerMinute = wordsPerMinuteResearch;\n  } else if (section === \"thoughts\") {\n    wordsPerMinute = wordsPerMinuteThoughts;\n  }\n  const plainTextBody = RemoveMarkdown(body).replace(/import .*? from .*?;/g, '').replace(/<.*?>/g, '').replace(/\\{\\/\\*[\\s\\S]*?\\*\\/\\}/g, '').trim();\n  const wordCount = plainTextBody.split(/\\s+/).length;\n  const baseReadTimeMinutes = Math.ceil(wordCount / wordsPerMinute);\n  const extraTime = frontmatter.extraReadTimeMin || 0;\n  const totalReadTime = baseReadTimeMinutes + extraTime;\n  const readTime = formatReadTime(totalReadTime);\n  const notices = [{\n    flag: frontmatter.flagDraft,\n    component: () => import(\"../components/NotFinishedNotice\")\n  }, {\n    flag: frontmatter.flagMindfuckery,\n    component: () => import(\"../components/MindfuckeryNotice\")\n  }, {\n    flag: frontmatter.flagRewrite,\n    component: () => import(\"../components/RewriteNotice\")\n  }, {\n    flag: frontmatter.flagOffensive,\n    component: () => import(\"../components/OffensiveNotice\")\n  }, {\n    flag: frontmatter.flagProfane,\n    component: () => import(\"../components/ProfanityNotice\")\n  }, {\n    flag: frontmatter.flagMultilingual,\n    component: () => import(\"../components/MultilingualNotice\")\n  }, {\n    flag: frontmatter.flagUnreliably,\n    component: () => import(\"../components/UnreliablyNotice\")\n  }, {\n    flag: frontmatter.flagPolitical,\n    component: () => import(\"../components/PoliticsNotice\")\n  }, {\n    flag: frontmatter.flagCognitohazard,\n    component: () => import(\"../components/CognitohazardNotice\")\n  }, {\n    flag: frontmatter.flagHidden,\n    component: () => import(\"../components/HiddenNotice\")\n  }];\n  const {0: loadedNotices, 1: setLoadedNotices} = useState([]);\n  useEffect(() => {\n    notices.forEach(_ref3 => {\n      let {flag, component} = _ref3;\n      if (flag) {\n        component().then(module => {\n          setLoadedNotices(prev => [].concat(_toConsumableArray(prev), [module.default]));\n        });\n      }\n    });\n  }, []);\n  return React.createElement(motion.div, {\n    initial: {\n      opacity: 0\n    },\n    animate: {\n      opacity: 1\n    },\n    exit: {\n      opacity: 0\n    },\n    transition: {\n      duration: 0.15\n    }\n  }, React.createElement(PostBanner, {\n    postNumber: frontmatter.index,\n    date: frontmatter.date,\n    updated: frontmatter.updated,\n    readTime: readTime,\n    difficulty: frontmatter.difficultyLevel,\n    title: frontmatter.title,\n    desc: frontmatter.desc,\n    banner: frontmatter.banner,\n    section: section,\n    postKey: keyCurrent,\n    isMindfuckery: frontmatter.flagMindfuckery,\n    mainTag: frontmatter.mainTag\n  }), React.createElement(\"div\", {\n    style: {\n      display: \"flex\",\n      justifyContent: \"flex-end\",\n      flexWrap: \"wrap\",\n      maxWidth: \"75%\",\n      marginLeft: \"auto\",\n      paddingRight: \"1vw\",\n      marginTop: \"-6vh\",\n      marginBottom: \"4vh\"\n    }\n  }, frontmatter.otherTags.map((tag, index) => React.createElement(\"span\", {\n    key: index,\n    className: `noselect ${stylesTagBadges.tagPosts}`,\n    style: {\n      margin: \"0 5px 5px 0\"\n    }\n  }, tag))), React.createElement(\"div\", {\n    className: \"postBody\"\n  }, React.createElement(TableOfContents, {\n    toc: tableOfContents\n  })), React.createElement(\"br\", null), React.createElement(\"div\", {\n    style: {\n      margin: \"0 10% -2vh 30%\",\n      textAlign: \"right\"\n    }\n  }, React.createElement(motion.button, {\n    className: `noselect ${stylesCustomPostLayouts.postButton}`,\n    id: stylesCustomPostLayouts.postLayoutSwitchButton,\n    onClick: toggleLayout,\n    whileTap: {\n      scale: 0.93\n    }\n  }, React.createElement(motion.div, {\n    className: stylesButtonsCommon.buttonTextWrapper,\n    key: isWideLayout,\n    initial: {\n      opacity: 0\n    },\n    animate: {\n      opacity: 1\n    },\n    exit: {\n      opacity: 0\n    },\n    transition: {\n      duration: 0.3,\n      ease: \"easeInOut\"\n    }\n  }, isWideLayout ? \"Switch to default layout\" : \"Switch to wide layout\"))), React.createElement(\"br\", null), React.createElement(\"div\", {\n    className: \"postBody\",\n    style: {\n      margin: isWideLayout ? \"0 -14%\" : \"\",\n      maxWidth: isWideLayout ? \"200%\" : \"\",\n      transition: \"margin 1s ease, max-width 1s ease, padding 1s ease\"\n    }\n  }, React.createElement(\"div\", {\n    className: `${stylesCustomPostLayouts.textContent} ${isAnimating ? stylesCustomPostLayouts.fadeOut : stylesCustomPostLayouts.fadeIn}`\n  }, loadedNotices.map((NoticeComponent, index) => React.createElement(NoticeComponent, {\n    key: index\n  })), frontmatter.indexCourse ? React.createElement(PartOfCourseNotice, {\n    index: frontmatter.indexCourse,\n    category: frontmatter.courseCategoryName\n  }) : \"\", React.createElement(ImageContext.Provider, {\n    value: {\n      images: allPostImages.nodes,\n      basePath: basePath.replace(/\\/$/, '') + '/'\n    }\n  }, React.createElement(MDXProvider, {\n    components: {\n      Image\n    }\n  }, children)))), React.createElement(PostBottom, {\n    nextPost: nextPost,\n    lastPost: lastPost,\n    keyCurrent: keyCurrent,\n    section: section\n  }));\n}\nPostTemplate\nexport default function GatsbyMDXWrapper(props) {\n  return React.createElement(PostTemplate, props, React.createElement(GATSBY_COMPILED_MDX, props));\n}\nexport function Head(_ref4) {\n  var _frontmatter$banner, _frontmatter$banner$c, _frontmatter$banner$c2, _frontmatter$banner$c3, _frontmatter$banner$c4;\n  let {data} = _ref4;\n  const {frontmatter} = data.mdx;\n  const title = frontmatter.titleSEO || frontmatter.title;\n  const titleOG = frontmatter.titleOG || title;\n  const titleTwitter = frontmatter.titleTwitter || title;\n  const description = frontmatter.descSEO || frontmatter.desc;\n  const descriptionOG = frontmatter.descOG || description;\n  const descriptionTwitter = frontmatter.descTwitter || description;\n  const schemaType = frontmatter.schemaType || \"BlogPosting\";\n  const keywords = frontmatter.keywordsSEO;\n  const datePublished = frontmatter.date;\n  const dateModified = frontmatter.updated || datePublished;\n  const imageOG = frontmatter.imageOG || ((_frontmatter$banner = frontmatter.banner) === null || _frontmatter$banner === void 0 ? void 0 : (_frontmatter$banner$c = _frontmatter$banner.childImageSharp) === null || _frontmatter$banner$c === void 0 ? void 0 : (_frontmatter$banner$c2 = _frontmatter$banner$c.gatsbyImageData) === null || _frontmatter$banner$c2 === void 0 ? void 0 : (_frontmatter$banner$c3 = _frontmatter$banner$c2.images) === null || _frontmatter$banner$c3 === void 0 ? void 0 : (_frontmatter$banner$c4 = _frontmatter$banner$c3.fallback) === null || _frontmatter$banner$c4 === void 0 ? void 0 : _frontmatter$banner$c4.src);\n  const imageAltOG = frontmatter.imageAltOG || descriptionOG;\n  const imageTwitter = frontmatter.imageTwitter || imageOG;\n  const imageAltTwitter = frontmatter.imageAltTwitter || descriptionTwitter;\n  const canonicalUrl = frontmatter.canonicalURL;\n  const flagHidden = frontmatter.flagHidden || false;\n  const mainTag = frontmatter.mainTag || \"Posts\";\n  const section = frontmatter.slug.split('/')[1] || \"posts\";\n  const type = \"article\";\n  const {siteUrl} = useSiteMetadata();\n  const breadcrumbJSON = {\n    \"@context\": \"https://schema.org\",\n    \"@type\": \"BreadcrumbList\",\n    \"itemListElement\": [{\n      \"@type\": \"ListItem\",\n      \"position\": 1,\n      \"name\": \"Home\",\n      \"item\": siteUrl\n    }, {\n      \"@type\": \"ListItem\",\n      \"position\": 2,\n      \"name\": mainTag,\n      \"item\": `${siteUrl}/${frontmatter.slug.split('/')[1]}`\n    }, {\n      \"@type\": \"ListItem\",\n      \"position\": 3,\n      \"name\": title,\n      \"item\": `${siteUrl}${frontmatter.slug}`\n    }]\n  };\n  return React.createElement(SEO, {\n    title: title + \" - avrtt.blog\",\n    titleOG: titleOG,\n    titleTwitter: titleTwitter,\n    description: description,\n    descriptionOG: descriptionOG,\n    descriptionTwitter: descriptionTwitter,\n    schemaType: schemaType,\n    keywords: keywords,\n    datePublished: datePublished,\n    dateModified: dateModified,\n    imageOG: imageOG,\n    imageAltOG: imageAltOG,\n    imageTwitter: imageTwitter,\n    imageAltTwitter: imageAltTwitter,\n    canonicalUrl: canonicalUrl,\n    flagHidden: flagHidden,\n    mainTag: mainTag,\n    section: section,\n    type: type\n  }, React.createElement(\"script\", {\n    type: \"application/ld+json\"\n  }, JSON.stringify(breadcrumbJSON)));\n}\nconst query = \"2571018839\";\n","/* \n\nCopyright © 2022  Vladislav Averett (avrtt)\nDistributed under the GNU AGPLv3 license. For details and source code, please refer to <https://github.com/avrtt/avrtt.github.io>.\n\n*/\n\nimport React from \"react\";\nimport Latex from 'react-latex-next';\nimport 'katex/dist/katex.min.css'; \n\ninterface LatexProps {\n  text: string;\n}\n  \nconst L = ({ text }: LatexProps) => {\n  return (\n    <Latex>{text}</Latex>\n  );\n};\nexport default L;\n"],"names":["tooltiptext","_ref","text","isBadge","isOpen","setIsOpen","useState","tooltipRef","useRef","useEffect","handleClickOutside","event","current","target","Node","contains","document","addEventListener","removeEventListener","React","className","ref","id","src","info","alt","onClick","e","stopPropagation","prev","styles","_createMdxContent","props","_components","Object","assign","p","h3","a","span","h2","ul","li","strong","ol","hr","_provideComponents","components","Image","component","Error","_missingMdxReference","Highlight","Latex","style","position","href","dangerouslySetInnerHTML","__html","Tooltip","path","caption","zoom","Code","wrapper","MDXLayout","TableOfContents","toc","items","stylesTableOfContents","map","item","index","key","url","handleClick","preventDefault","targetId","replace","targetElement","getElementById","scrollIntoView","behavior","block","title","PostTemplate","_ref2","data","mdx","allMdx","allPostImages","children","frontmatter","body","tableOfContents","section","slug","split","sortedPosts","nodes","filter","post","includes","sort","b","currentIndex","findIndex","nextPost","lastPost","trimmedSlug","keyCurrent","exec","basePath","isWideLayout","setIsWideLayout","flagWideLayoutByDefault","isAnimating","setIsAnimating","wordsPerMinute","timer","setTimeout","clearTimeout","wordsPerMinuteAdventures","wordsPerMinuteResearch","wordsPerMinuteThoughts","wordCount","RemoveMarkdown","trim","length","readTime","minutes","hours","Math","floor","remainder","formatReadTime","ceil","extraReadTimeMin","notices","flag","flagDraft","flagMindfuckery","flagRewrite","flagOffensive","flagProfane","flagMultilingual","flagUnreliably","flagPolitical","flagCognitohazard","flagHidden","loadedNotices","setLoadedNotices","forEach","_ref3","then","module","concat","_toConsumableArray","default","motion","div","initial","opacity","animate","exit","transition","duration","PostBanner","postNumber","date","updated","difficulty","difficultyLevel","desc","banner","postKey","isMindfuckery","mainTag","display","justifyContent","flexWrap","maxWidth","marginLeft","paddingRight","marginTop","marginBottom","otherTags","tag","stylesTagBadges","margin","textAlign","button","stylesCustomPostLayouts","toggleLayout","whileTap","scale","stylesButtonsCommon","ease","NoticeComponent","indexCourse","PartOfCourseNotice","category","courseCategoryName","ImageContext","Provider","value","images","MDXProvider","PostBottom","GatsbyMDXWrapper","GATSBY_COMPILED_MDX","Head","_ref4","_frontmatter$banner","_frontmatter$banner$c","_frontmatter$banner$c2","_frontmatter$banner$c3","_frontmatter$banner$c4","titleSEO","titleOG","titleTwitter","description","descSEO","descriptionOG","descOG","descriptionTwitter","descTwitter","schemaType","keywords","keywordsSEO","datePublished","dateModified","imageOG","childImageSharp","gatsbyImageData","fallback","imageAltOG","imageTwitter","imageAltTwitter","canonicalUrl","canonicalURL","siteUrl","useSiteMetadata","breadcrumbJSON","SEO","type","JSON","stringify"],"sourceRoot":""}