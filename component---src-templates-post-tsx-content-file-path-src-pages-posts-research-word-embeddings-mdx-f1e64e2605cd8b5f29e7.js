"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[1073],{38837:function(e,t,n){n.r(t),n.d(t,{Head:function(){return z},PostTemplate:function(){return _},default:function(){return A}});var a=n(28453),r=n(96540),i=n(61992),o=n(62087),l=n(90548);function s(e){const t=Object.assign({p:"p",h2:"h2",a:"a",span:"span",h3:"h3",ol:"ol",li:"li",strong:"strong",ul:"ul",hr:"hr"},(0,a.RP)(),e.components),{Image:n}=t;return n||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),r.createElement(r.Fragment,null,"\n",r.createElement("br"),"\n","\n","\n",r.createElement(t.p,null,"Embeddings are a powerful and versatile method of representing data — such as words, sentences, images, or audio — as dense numerical vectors in a high-dimensional space. The underlying purpose of these embeddings is to capture semantic meaning or other significant relationships in the data, which can then be leveraged by machine learning models for tasks ranging from text classification to recommendation systems. Unlike traditional methods that rely on sparse, high-dimensional encodings (e.g., one-hot vectors), embeddings make it feasible to map similar items close together in a continuous vector space, thereby greatly enhancing a model's ability to generalize, detect patterns, and make nuanced comparisons."),"\n",r.createElement(t.p,null,"In natural language processing (",r.createElement(i.A,null,"NLP"),'), word embeddings transform each word in a vocabulary into a fixed-length vector. Two words that often appear in similar contexts — such as "doctor" and "physician" — are placed relatively close to each other in the embedded space. This notion of "closeness" typically relies on a similarity measure, often ',r.createElement(l.A,{text:"\\( \\text{cosine similarity} \\)"}),". When these embeddings are plugged into machine learning models (e.g., for classification, question answering, or sentiment analysis), they facilitate a deeper understanding of text by highlighting semantic and syntactic relationships that discrete indexes simply cannot capture on their own."),"\n",r.createElement(t.p,null,"There has been a long evolution from naive bag-of-words encodings to more advanced contextual embeddings that adapt to the specific usage and context of a word. In modern ",r.createElement(i.A,null,"NLP")," pipelines, embeddings such as ",r.createElement(i.A,null,"Word2Vec"),", ",r.createElement(i.A,null,"FastText"),", ",r.createElement(i.A,null,"GloVe"),", ",r.createElement(i.A,null,"ELMo"),", and ",r.createElement(i.A,null,"BERT")," are a staple. These approaches have revolutionized the field by bringing in robust representational capabilities that drastically improve downstream tasks like named entity recognition, text classification, machine translation, and even question answering and summarization."),"\n",r.createElement(t.p,null,"When extended beyond text, the concept of embeddings can map entire sentences, paragraphs, or documents into vectors, and can similarly be used for images, audio, and video data. In short, embeddings serve as a powerful foundation for many advanced applications, enabling computers to interpret meaning in a manner that is much closer to how humans understand language and other data modalities."),"\n",r.createElement(t.p,null,"By the end of this article, the reader should have a solid grasp of word embeddings from the ground up, starting with simple one-hot representations and moving on to advanced neural-based or transformer-based approaches. I will also present code snippets, practical tips, and references to influential research that shaped these techniques. Let's dive deeper into how we get from raw text to semantically rich vectors."),"\n",r.createElement(t.h2,{id:"key-use-cases-of-embeddings",style:{position:"relative"}},r.createElement(t.a,{href:"#key-use-cases-of-embeddings","aria-label":"key use cases of embeddings permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Key use cases of embeddings"),"\n",r.createElement(t.p,null,"Embeddings excel in a diverse range of applications. While word embeddings are most often associated with NLP tasks, the general concept can be applied to any domain in which we need to capture nuanced relationships among entities. Below are some of the most prominent use cases for embeddings and why they have become indispensable in modern data science."),"\n",r.createElement(t.h3,{id:"semantic-search",style:{position:"relative"}},r.createElement(t.a,{href:"#semantic-search","aria-label":"semantic search permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Semantic search"),"\n",r.createElement(t.p,null,"Semantic search is a retrieval mechanism that goes beyond raw keyword matching. Instead of merely scanning for the exact word or phrase in a target document, a semantic search engine transforms both the query and the documents into vectors using an embedding model. These vectors ideally capture semantic information, so that two different phrases or queries referencing the same underlying concept will be placed close to each other in the embedding space."),"\n",r.createElement(t.p,null,'By comparing the distance (or similarity) between vectors, a semantic search system can match a query against documents or other entities even if the precise terms are not shared. This is particularly important in applications like legal and academic search, customer support chatbots, or large-scale knowledge base retrieval. An end user might ask for "the official guidelines on property maintenance" and retrieve documents that mention "building upkeep regulations," which is not guaranteed when performing a simple keyword search.'),"\n",r.createElement(t.p,null,"Common similarity metrics include ",r.createElement(l.A,{text:"\\( \\text{cosine similarity} \\)"}),", ",r.createElement(l.A,{text:"\\( L2 \\)"})," distance, or even learned distance measures in more advanced scenarios. The advantage of embeddings is their ability to group conceptually similar items together, greatly boosting retrieval performance in real-world systems."),"\n",r.createElement(t.h3,{id:"data-classification",style:{position:"relative"}},r.createElement(t.a,{href:"#data-classification","aria-label":"data classification permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Data classification"),"\n",r.createElement(t.p,null,"Word embeddings make data classification more powerful by creating a dense, information-rich input representation. Traditional classification pipelines (e.g., logistic regression or feed-forward neural networks) can suffer when the inputs are too sparse or lack semantic relationships. When text or other forms of raw data are embedded, the resulting vectors capture underlying patterns, thereby allowing simple classifiers to perform better."),"\n",r.createElement(t.p,null,'For example, in spam detection, an embedding-based approach can recognize that terms like "miracle cure," "free money," and "unbelievable offer" share a certain semantic domain of "promotional or suspicious content." Even if the text changes slightly (e.g., "100% free gift" vs. "completely free giveaway"), an embedding-based classification model can pick up that the messages carry similar meaning. This improves the model\'s recall and precision, and it often yields more robust results compared to purely keyword-based approaches.'),"\n",r.createElement(t.h3,{id:"recommendation-systems",style:{position:"relative"}},r.createElement(t.a,{href:"#recommendation-systems","aria-label":"recommendation systems permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Recommendation systems"),"\n",r.createElement(t.p,null,"In recommendation systems, embeddings have become a vital tool for representing both users and items in the same latent vector space. Here, the typical approach might be to embed users based on their interactions or preferences and items based on their properties. If a user's vector representation is close to an item's vector representation, we can infer that the user is likely to enjoy or consume that item. This approach is widely used in streaming media platforms, e-commerce sites, and social media recommendation feeds."),"\n",r.createElement(t.p,null,"The underlying logic is often the same: by converting the data — user tastes, item genres, textual descriptions, or even item images — into dense vectors, the model can measure similarity as a proxy for compatibility. Large-scale solutions, such as collaborative filtering or neural collaborative filtering methods, often rely on some variant of embeddings to capture underlying structures in user-item interactions."),"\n",r.createElement(t.h3,{id:"anomaly-detection",style:{position:"relative"}},r.createElement(t.a,{href:"#anomaly-detection","aria-label":"anomaly detection permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Anomaly detection"),"\n",r.createElement(t.p,null,'Anomaly detection with embeddings hinges on the principle that "normal" data clusters together in embedding space, whereas outliers or anomalies lie far from these clusters. In text-based anomaly detection — say, for fraud detection in textual logs — one can embed each log message or user query as a vector. If a new message is placed in a region that rarely appears in the training distribution, the system flags it as potentially suspicious.'),"\n",r.createElement(t.p,null,"This approach can be adapted across diverse domains, including network intrusion detection, insurance claims analysis, or manufacturing defect detection. The embedding step often reveals patterns that wouldn't be evident with simpler feature engineering, because embeddings can capture more subtle context and relationships."),"\n",r.createElement(t.h2,{id:"fundamentals-of-word-embeddings",style:{position:"relative"}},r.createElement(t.a,{href:"#fundamentals-of-word-embeddings","aria-label":"fundamentals of word embeddings permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Fundamentals of word embeddings"),"\n",r.createElement(t.p,null,"Word embeddings are a specialized form of embeddings used strictly for representing individual words within a language. In this paradigm, each word from the vocabulary is mapped to a dense vector in a relatively low-dimensional space (for instance, 50–300 dimensions, though some embeddings go even higher). These vectors are learned by analyzing the contexts in which words appear, under the assumption that words occurring in similar contexts are semantically related."),"\n",r.createElement(t.p,null,"A famous demonstration of the power of word embeddings is analogy reasoning:"),"\n",r.createElement(l.A,{text:"\\[\n\\text{king} - \\text{man} + \\text{woman} \\approx \\text{queen}\n\\]"}),"\n",r.createElement(t.p,null,'This arises not because the model "understands" the meaning of monarchy or gender but because it detects consistent contextual shifts in how these words are used in a large corpus.'),"\n",r.createElement(t.p,null,"These embeddings significantly simplify tasks that rely on lexical or semantic information. Before their introduction, manual feature engineering or large, sparse one-hot vectors were prevalent, creating difficulties in capturing word similarity or more nuanced linguistic phenomena. Modern NLP has effectively replaced those sparse representations with embeddings. Below, I go through early methods like one-hot encoding, transitioning into more sophisticated systems such as Word2Vec, FastText, GloVe, and the more recent contextual approaches, ELMO and BERT."),"\n",r.createElement(t.h2,{id:"one-hot-encoding",style:{position:"relative"}},r.createElement(t.a,{href:"#one-hot-encoding","aria-label":"one hot encoding permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"One-hot encoding"),"\n",r.createElement(t.h3,{id:"explanation",style:{position:"relative"}},r.createElement(t.a,{href:"#explanation","aria-label":"explanation permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Explanation"),"\n",r.createElement(t.p,null,"One-hot encoding is the simplest encoding scheme, in which each word is mapped to a vector of length ",r.createElement(l.A,{text:"\\( K \\)"})," — the size of the vocabulary. All entries are zero except for a single position set to 1, identifying the word's index in that vocabulary. For example, if we have a vocabulary of ",r.createElement(l.A,{text:"\\( K=5 \\)"}),' words: ["cat," "dog," "mouse," "banana," "car"], the word "dog" might be represented as ',r.createElement(l.A,{text:"\\( [0,1,0,0,0] \\)"}),"."),"\n",r.createElement(t.p,null,"One-hot vectors are easy to compute and straightforward to implement, but they suffer from major drawbacks:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Sparsity"),": For large vocabularies, each word vector can be enormous in dimension, and yet all but one entry is zero. This leads to memory inefficiency."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Lack of semantic similarity"),': One-hot vectors do not convey any notion of the distance between words. There is no built-in notion that "car" is more similar to "automobile" than "banana." Each word is represented as an equidistant point from every other word.'),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Vocabulary growth"),": In many real-world tasks, the vocabulary can be extremely large, and new words can appear frequently. Incorporating out-of-vocabulary words in one-hot encoding is impractical."),"\n"),"\n",r.createElement(t.p,null,"Due to these limitations, one-hot encoding is seldom used for advanced NLP pipelines. Instead, it primarily serves as the foundation upon which more nuanced encoding schemes are built (for instance, in the input layers of Word2Vec or CBOW)."),"\n",r.createElement(t.h2,{id:"word2vec",style:{position:"relative"}},r.createElement(t.a,{href:"#word2vec","aria-label":"word2vec permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Word2Vec"),"\n",r.createElement(t.h3,{id:"overview-of-word2vec",style:{position:"relative"}},r.createElement(t.a,{href:"#overview-of-word2vec","aria-label":"overview of word2vec permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Overview of Word2Vec"),"\n",r.createElement(t.p,null,r.createElement(i.A,null,"Word2Vec"),", introduced by Tomas Mikolov and colleagues (Mikolov and gang, NeurIPS 2013), is considered one of the first large breakthroughs in practical word embedding techniques. It uses a small, two-layer neural network that takes a textual corpus as input and produces word embeddings as its learned parameters. The model effectively captures co-occurrence statistics of words in local contexts without the heavy overhead of storing entire large co-occurrence matrices in memory (like some earlier matrix-factorization approaches might do)."),"\n",r.createElement(t.p,null,"There are two main architectures for Word2Vec:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Skip-gram"),': Predict surrounding (context) words given a central ("target") word.'),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Continuous Bag-of-Words (CBOW)"),": Predict a target word based on the words around it."),"\n"),"\n",r.createElement(t.h3,{id:"skip-gram-model",style:{position:"relative"}},r.createElement(t.a,{href:"#skip-gram-model","aria-label":"skip gram model permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Skip-gram model"),"\n",r.createElement(t.p,null,'Skip-gram is particularly good at capturing rare word relationships because it tries to predict multiple context words for each single target word. For instance, if the target word is "computer," the skip-gram model uses "computer" to predict words that might appear in its vicinity, such as "processor," "keyboard," or "memory," depending on the chosen window size. Over a large corpus, the model learns that certain words frequently co-occur with "computer," thereby associating them with similar vector directions.'),"\n",r.createElement(t.p,null,"Formally, suppose ",r.createElement(l.A,{text:"\\( w_t \\)"})," is the target word at position ",r.createElement(l.A,{text:"\\( t \\)"})," and ",r.createElement(l.A,{text:"\\( (w_{t-1}, w_{t-2}, \\ldots, w_{t-k}, w_{t+1}, w_{t+2}, \\ldots, w_{t+k}) \\)"})," are the surrounding context words within a fixed window size ",r.createElement(l.A,{text:"\\( k \\)"}),". The skip-gram model tries to maximize the likelihood:"),"\n",r.createElement(l.A,{text:"\\[\n\\prod_{t=1}^{T} \\prod_{j=-k, j \\neq 0}^{k} P(w_{t+j} \\mid w_t)\n\\]"}),"\n",r.createElement(t.p,null,"where ",r.createElement(l.A,{text:"\\( T \\)"})," is the total number of words in the corpus. Through training, it learns an embedding matrix whose rows (or columns, depending on the implementation) contain the final word vectors."),"\n",r.createElement(t.h3,{id:"cbow-continuous-bag-of-words",style:{position:"relative"}},r.createElement(t.a,{href:"#cbow-continuous-bag-of-words","aria-label":"cbow continuous bag of words permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"CBOW (continuous bag of words)"),"\n",r.createElement(t.p,null,"In contrast, the CBOW model does the inverse: it predicts the target word given its context words. Because each training instance pools the entire context into a single input, it can train faster and might work better for more frequent words. The objective is to maximize:"),"\n",r.createElement(l.A,{text:"\\[\n\\prod_{t=1}^{T} P(w_t \\mid w_{t-1}, w_{t-2}, \\ldots, w_{t-k}, w_{t+1}, \\ldots, w_{t+k})\n\\]"}),"\n",r.createElement(t.p,null,"While skip-gram can capture more subtle semantics (particularly for infrequent words), CBOW is often more efficient. In practice, the choice between skip-gram and CBOW might come down to the corpus size, the domain, or the coverage needed for rare words."),"\n",r.createElement(t.h3,{id:"negative-sampling-and-hierarchical-softmax",style:{position:"relative"}},r.createElement(t.a,{href:"#negative-sampling-and-hierarchical-softmax","aria-label":"negative sampling and hierarchical softmax permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Negative sampling and hierarchical softmax"),"\n",r.createElement(t.p,null,"For large vocabularies, computing the full softmax (the probability for each word in the vocabulary) can be extremely expensive in each training step. Word2Vec offers alternatives:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Negative sampling"),': Rather than updating parameters for all words in the vocabulary, negative sampling updates the model using only a few "negative" examples (words that are not in the true context). This drastically cuts down the computational cost.'),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Hierarchical softmax"),": A tree-based approach that estimates the softmax more efficiently. Instead of enumerating all words, it arranges them in a binary tree. The model updates only the path in the tree that leads to the relevant word."),"\n"),"\n",r.createElement(t.p,null,"Both approaches aim to approximate the full softmax distribution while maintaining tractable training time."),"\n",r.createElement(t.h3,{id:"cosine-similarity",style:{position:"relative"}},r.createElement(t.a,{href:"#cosine-similarity","aria-label":"cosine similarity permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Cosine similarity"),"\n",r.createElement(t.p,null,"Once Word2Vec is trained, the embedding vectors can be compared using ",r.createElement(l.A,{text:"\\( \\text{cosine similarity} \\)"}),":"),"\n",r.createElement(l.A,{text:"\\[\n\\text{similarity}(\\mathbf{A}, \\mathbf{B}) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|}\n\\]"}),"\n",r.createElement(t.p,null,"Where ",r.createElement(l.A,{text:"\\( \\mathbf{A}, \\mathbf{B} \\)"})," are word embedding vectors. Cosine similarity is often used because it normalizes for magnitude, focusing instead on the angle between vectors. This angle-based measure correlates well with semantic similarity in many embedding spaces: words used in comparable contexts typically have embedding vectors pointing in similar directions."),"\n",r.createElement(t.p,null,"Below is an illustrative placeholder image that often appears in tutorials, depicting how word vectors may align to reflect linear relationships such as ",r.createElement(l.A,{text:"\\( \\text{king} - \\text{man} + \\text{woman} \\approx \\text{queen} \\)"}),":"),"\n",r.createElement(n,{alt:"High-level 2D projection of Word2Vec embeddings showing semantic relationships",path:"",caption:"Conceptual diagram showing that semantic relationships among words often translate into linear vector arithmetic in embedded space.",zoom:"false"}),"\n",r.createElement(t.h2,{id:"fasttext",style:{position:"relative"}},r.createElement(t.a,{href:"#fasttext","aria-label":"fasttext permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"FastText"),"\n",r.createElement(t.h3,{id:"subword-n-gram-approach",style:{position:"relative"}},r.createElement(t.a,{href:"#subword-n-gram-approach","aria-label":"subword n gram approach permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Subword (n-gram) approach"),"\n",r.createElement(t.p,null,r.createElement(i.A,null,"FastText"),' (Bojanowski and gang, 2016) is an extension of Word2Vec created by Facebook AI Research. One of the issues with Word2Vec is that if a word does not appear in the training corpus ("out-of-vocabulary" word), you cannot derive a meaningful embedding for it. FastText addresses this by learning embeddings not just for entire words but also for ',r.createElement(l.A,{text:"\\( n \\)"}),"-grams of characters. A word's final embedding is effectively the sum of its ",r.createElement(l.A,{text:"\\( n \\)"}),"-gram embeddings."),"\n",r.createElement(t.p,null,'For example, for the word "banana" and ',r.createElement(l.A,{text:"\\( n=3 \\)"}),', the 3-gram subwords would be "ban," "ana," "nan," and so forth. Even if the entire word "banana" never appeared in the training set, each of these subwords might occur in other words. Therefore, FastText can offer reasonable approximations for new or rare words because it can look up embeddings of their subwords.'),"\n",r.createElement(t.h3,{id:"benefits-over-word2vec",style:{position:"relative"}},r.createElement(t.a,{href:"#benefits-over-word2vec","aria-label":"benefits over word2vec permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Benefits over Word2Vec"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"OOV words"),": Out-of-vocabulary words can be embedded by breaking them down into subwords that the model has already seen."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Morpheme-like subwords"),": In morphologically rich languages (e.g., Russian, Turkish, Finnish), words can have numerous forms. FastText's subword approach helps share parameters among these variations, improving accuracy and coverage."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Small additional overhead"),": While it has to store ",r.createElement(l.A,{text:"\\( n \\)"}),"-gram vectors in addition to full-word vectors, in many cases this overhead is negligible compared to the value gained."),"\n"),"\n",r.createElement(t.h3,{id:"typical-use-cases",style:{position:"relative"}},r.createElement(t.a,{href:"#typical-use-cases","aria-label":"typical use cases permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Typical use cases"),"\n",r.createElement(t.p,null,"FastText is especially useful for:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"Languages with large morphological variability."),"\n",r.createElement(t.li,null,"Scenarios where you expect to encounter or must handle many out-of-vocabulary words (e.g., user-generated content, social media text)."),"\n",r.createElement(t.li,null,"Environments that require real-time generation of embeddings for new terms (e.g., real-time chat applications, language learning platforms)."),"\n"),"\n",r.createElement(n,{alt:"Visualization of subword embeddings concept in FastText",path:"",caption:"In FastText, each word vector is formed as the sum of its subword vectors, facilitating better handling of rare words.",zoom:"false"}),"\n",r.createElement(t.h2,{id:"glove",style:{position:"relative"}},r.createElement(t.a,{href:"#glove","aria-label":"glove permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"GloVe"),"\n",r.createElement(t.h3,{id:"concept",style:{position:"relative"}},r.createElement(t.a,{href:"#concept","aria-label":"concept permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Concept"),"\n",r.createElement(t.p,null,r.createElement(i.A,null,"GloVe")," (Global Vectors for Word Representation) is another popular approach to word embeddings, proposed by Pennington and gang (2014) at Stanford. Unlike Word2Vec's local context-based predictions, GloVe relies on global corpus statistics. It constructs a large word-word co-occurrence matrix (or a condensed version of it) and factors this matrix to produce word embeddings."),"\n",r.createElement(t.p,null,'The main intuition is that ratios of co-occurrence probabilities encode unique semantic information. For instance, the probability that "ice" co-occurs with "cold" will be much higher than the probability that "ice" co-occurs with "hot." By learning a vector space that preserves these ratio relationships, GloVe captures both local context information and global corpus-wide statistics.'),"\n",r.createElement(t.h3,{id:"key-idea",style:{position:"relative"}},r.createElement(t.a,{href:"#key-idea","aria-label":"key idea permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Key idea"),"\n",r.createElement(t.p,null,"Let ",r.createElement(l.A,{text:"\\( X \\)"})," be the co-occurrence matrix, where ",r.createElement(l.A,{text:"\\( X_{ij} \\)"})," is the number of times word ",r.createElement(l.A,{text:"\\( j \\)"})," appears in the context of word ",r.createElement(l.A,{text:"\\( i \\)"}),". GloVe uses a weighted least squares objective that aims to factorize ",r.createElement(l.A,{text:"\\( X \\)"}),", producing embeddings ",r.createElement(l.A,{text:"\\( \\mathbf{w}_i \\)"})," and ",r.createElement(l.A,{text:"\\( \\mathbf{w}_j \\)"})," for words ",r.createElement(l.A,{text:"\\( i \\)"})," and ",r.createElement(l.A,{text:"\\( j \\)"})," such that:"),"\n",r.createElement(l.A,{text:"\\[\nf(X_{ij}) (\\mathbf{w}_i^\\top \\mathbf{\\tilde{w}}_j + b_i + \\tilde{b}_j - \\log X_{ij})^2\n\\]"}),"\n",r.createElement(t.p,null,"Here:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(l.A,{text:"\\( \\mathbf{w}_i, \\mathbf{\\tilde{w}}_j \\)"})," are the word vectors to be learned."),"\n",r.createElement(t.li,null,r.createElement(l.A,{text:"\\( b_i, \\tilde{b}_j \\)"})," are bias terms."),"\n",r.createElement(t.li,null,r.createElement(l.A,{text:"\\( X_{ij} \\)"})," is the co-occurrence count of words ",r.createElement(l.A,{text:"\\( i \\)"})," and ",r.createElement(l.A,{text:"\\( j \\)"}),"."),"\n",r.createElement(t.li,null,r.createElement(l.A,{text:"\\( f \\)"})," is a weighting function that lessens the effect of very large or very small co-occurrence counts."),"\n"),"\n",r.createElement(t.h3,{id:"differences-from-word2vec",style:{position:"relative"}},r.createElement(t.a,{href:"#differences-from-word2vec","aria-label":"differences from word2vec permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Differences from Word2Vec"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"GloVe explicitly uses global statistics (full co-occurrence counts across the entire corpus) rather than sampling local context windows alone."),"\n",r.createElement(t.li,null,"Training can sometimes be faster if the co-occurrence matrix is not extremely large."),"\n",r.createElement(t.li,null,"The resulting embeddings can capture certain global patterns more systematically."),"\n"),"\n",r.createElement(t.h3,{id:"typical-usage",style:{position:"relative"}},r.createElement(t.a,{href:"#typical-usage","aria-label":"typical usage permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Typical usage"),"\n",r.createElement(t.p,null,"GloVe embeddings are often used in NLP tasks that benefit from robust global relationships. The ",r.createElement(i.A,null,"Stanford GloVe")," site provides pre-trained embeddings (e.g., 50D, 100D, 200D, 300D) trained on corpora like Common Crawl or Wikipedia, which are commonly employed for downstream tasks in text analytics."),"\n",r.createElement(n,{alt:"Matrix factorization of co-occurrence counts in GloVe",path:"",caption:"GloVe embeddings factorize the global word co-occurrence matrix, seeking to preserve important ratio information among co-occurrence probabilities.",zoom:"false"}),"\n",r.createElement(t.h2,{id:"elmo",style:{position:"relative"}},r.createElement(t.a,{href:"#elmo","aria-label":"elmo permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"ELMO"),"\n",r.createElement(t.h3,{id:"contextual-embeddings",style:{position:"relative"}},r.createElement(t.a,{href:"#contextual-embeddings","aria-label":"contextual embeddings permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Contextual embeddings"),"\n",r.createElement(t.p,null,"Prior to ",r.createElement(i.A,null,"ELMo"),' and other contextual approaches, word embeddings were static: each word type had exactly one vector, regardless of how it was used in a sentence. This is a fundamental limitation, because words often have multiple senses or roles depending on context (e.g., "bank" can be a financial institution or the side of a river).'),"\n",r.createElement(t.p,null,r.createElement(i.A,null,"ELMo")," (Embeddings from Language Models), introduced by Peters and gang (2018), overcame this by generating embeddings that depend on the entire context in which a word appears. Instead of a single vector per word type, ELMo provides different vectors for each occurrence of the same word."),"\n",r.createElement(t.h3,{id:"bilstm-architecture",style:{position:"relative"}},r.createElement(t.a,{href:"#bilstm-architecture","aria-label":"bilstm architecture permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"BiLSTM architecture"),"\n",r.createElement(t.p,null,"ELMo is based on a deep bidirectional ",r.createElement(l.A,{text:"\\( LSTM \\)"})," language model. The idea is to train a stacked bidirectional ",r.createElement(l.A,{text:"\\( LSTM \\)"})," on a large corpus using a language modeling objective. In a simplified form, it tries to predict the next word given the previous words (forward direction) as well as the previous word given the next words (backward direction)."),"\n",r.createElement(t.p,null,"At each layer ",r.createElement(l.A,{text:"\\( j \\)"}),", the forward ",r.createElement(l.A,{text:"\\( \\overrightarrow{h_{k, j}^{LM}} \\)"})," and backward ",r.createElement(l.A,{text:"\\( \\overleftarrow{h_{k, j}^{LM}} \\)"})," hidden states together capture lexical, syntactic, and semantic information at different levels of abstraction. Lower layers capture more syntactic (e.g., part-of-speech) features, and higher layers capture more semantic aspects (e.g., word sense, discourse context)."),"\n",r.createElement(t.h3,{id:"weighted-layer-combination",style:{position:"relative"}},r.createElement(t.a,{href:"#weighted-layer-combination","aria-label":"weighted layer combination permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Weighted layer combination"),"\n",r.createElement(t.p,null,"At inference time, ELMo produces word embeddings for the ",r.createElement(l.A,{text:"\\( k \\)"}),"-th token by combining these hidden states with learned, task-specific weights:"),"\n",r.createElement(l.A,{text:"\\[\n\\text{ELMo}_{k}^{\\text{task}} = \\gamma^{\\text{task}} \\sum_{j=0}^{L} s_{j}^{\\text{task}} \\, h_{k,j}^{LM}\n\\]"}),"\n",r.createElement(t.p,null,"Where:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(l.A,{text:"\\( L \\)"})," is the number of layers in the ",r.createElement(l.A,{text:"\\( LSTM \\)"}),"."),"\n",r.createElement(t.li,null,r.createElement(l.A,{text:"\\( s_j^{\\text{task}} \\)"})," are softmax-normalized weights that indicate how much each layer contributes to the final embedding for that specific task (e.g., named entity recognition, sentiment analysis)."),"\n",r.createElement(t.li,null,r.createElement(l.A,{text:"\\( \\gamma^{\\text{task}} \\)"})," is a scalar scaling parameter."),"\n"),"\n",r.createElement(t.p,null,'This framework lets each downstream task "focus" on the layer or combination of layers that are most relevant. The result is a contextual embedding that can differentiate between "bank" in "I went to the bank to deposit money" vs. "He sat on the river bank," generating distinct vector representations for each usage.'),"\n",r.createElement(t.p,null,"ELMo is widely recognized for substantially boosting performance across a variety of NLP benchmarks. Researchers discovered that it allows even relatively simple models to incorporate contextual information that was previously difficult to encode."),"\n",r.createElement(n,{alt:"High-level overview of ELMo architecture",path:"",caption:"ELMo stacks bidirectional LSTM networks, creating context-sensitive embeddings for each word occurrence.",zoom:"false"}),"\n",r.createElement(t.h2,{id:"bert",style:{position:"relative"}},r.createElement(t.a,{href:"#bert","aria-label":"bert permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"BERT"),"\n",r.createElement(t.h3,{id:"transformer-based-model",style:{position:"relative"}},r.createElement(t.a,{href:"#transformer-based-model","aria-label":"transformer based model permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Transformer-based model"),"\n",r.createElement(t.p,null,r.createElement(i.A,null,"BERT")," (Bidirectional Encoder Representations from Transformers), introduced by Devlin and gang (2018), marked a significant leap forward in contextual embeddings. Unlike RNN-based approaches (e.g., ELMo), BERT employs a multi-layer bidirectional Transformer encoder. Transformers use a mechanism called self-attention, allowing the model to weigh the relevance of every token to every other token in a sentence, capturing context in a far more parallelizable manner than ",r.createElement(l.A,{text:"\\( LSTMs \\)"}),"."),"\n",r.createElement(t.h3,{id:"masked-language-modeling",style:{position:"relative"}},r.createElement(t.a,{href:"#masked-language-modeling","aria-label":"masked language modeling permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Masked language modeling"),"\n",r.createElement(t.p,null,'BERT\'s pre-training objective is "masked language modeling." It randomly masks a certain percentage (often 15%) of tokens in the input and tries to predict them from the unmasked tokens. This forces the model to learn contextual representations from both left and right contexts.'),"\n",r.createElement(l.A,{text:"\\[\n\\text{Loss}_{MLM} = -\\sum_{t \\in \\text{masked positions}} \\log P(\\text{actual token at } t \\mid \\text{context})\n\\]"}),"\n",r.createElement(t.p,null,"Because BERT sees context on both sides of each masked position, it is said to be a deeply bidirectional model, unlike older left-to-right language models (e.g., GPT-like models in their earlier forms)."),"\n",r.createElement(t.h3,{id:"next-sentence-prediction",style:{position:"relative"}},r.createElement(t.a,{href:"#next-sentence-prediction","aria-label":"next sentence prediction permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Next sentence prediction"),"\n",r.createElement(t.p,null,'During training, BERT also uses an auxiliary "next sentence prediction" objective. It pairs two sentences (A and B) and trains the model to predict whether B actually follows A in the original text. This helps BERT learn inter-sentence relationships, facilitating tasks such as question answering or natural language inference.'),"\n",r.createElement(t.p,null,"Although more recent variants (e.g., RoBERTa) have modified or removed this next sentence prediction objective, it remains a hallmark of the original BERT model."),"\n",r.createElement(t.h3,{id:"usage-in-downstream-tasks",style:{position:"relative"}},r.createElement(t.a,{href:"#usage-in-downstream-tasks","aria-label":"usage in downstream tasks permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Usage in downstream tasks"),"\n",r.createElement(t.p,null,"BERT's final outputs are richly contextualized token embeddings. Typically, for classification tasks, one takes the representation from the special ",r.createElement(i.A,null,"[CLS]")," token, which stands at the beginning of the input sequence, and passes it through a feed-forward layer to get the final prediction. For token-level tasks, each token's final embedding from BERT can be fed into a decoder for, say, named entity recognition or part-of-speech tagging."),"\n",r.createElement(t.p,null,"The success of BERT spurred a wave of Transformer-based language models (",r.createElement(i.A,null,"ALBERT"),", ",r.createElement(i.A,null,"DistilBERT"),", ",r.createElement(i.A,null,"RoBERTa"),", ",r.createElement(i.A,null,"DeBERTa"),", etc.), all of which rely on large-scale pre-training and can be fine-tuned on specific downstream tasks with minimal effort. These models significantly advanced the state-of-the-art in NLP by capturing deep contextual nuances in language."),"\n",r.createElement(n,{alt:"Diagram illustrating the BERT architecture",path:"",caption:"BERT uses multiple layers of bidirectional Transformers, each containing self-attention and feed-forward networks to yield rich contextual embeddings.",zoom:"false"}),"\n",r.createElement(t.h2,{id:"implementation-with-gensim-examples",style:{position:"relative"}},r.createElement(t.a,{href:"#implementation-with-gensim-examples","aria-label":"implementation with gensim examples permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Implementation with Gensim (examples)"),"\n",r.createElement(t.p,null,r.createElement(i.A,null,"Gensim")," is a popular Python library for topic modeling, document similarity, and — crucially for our focus — word embedding training and usage. Below are some practical snippets, illustrating both loading pre-trained models and training them from scratch on a corpus."),"\n",r.createElement(t.h3,{id:"loading-pre-trained-embeddings",style:{position:"relative"}},r.createElement(t.a,{href:"#loading-pre-trained-embeddings","aria-label":"loading pre trained embeddings permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Loading pre-trained embeddings"),"\n",r.createElement(t.p,null,'Many pre-trained embeddings exist for different languages. Gensim provides an easy way to download and load them. Below, I demonstrate loading a pre-trained Russian model, "word2vec-ruscorpora-300," which was trained on a large Russian corpus:'),"\n",r.createElement(o.A,{text:"\nimport gensim\nimport gensim.downloader as download_api\n\nrussian_model = download_api.load('word2vec-ruscorpora-300')\n\n# List the first 10 words in the model's vocabulary.\nlist(russian_model.vocab.keys())[:10]\n# Example output: ['весь_DET', 'человек_NOUN', 'мочь_VERB', 'год_NOUN', ...]\n\n# Finding similar words:\nsimilar_cats = russian_model.most_similar('кошка_NOUN')\nprint(similar_cats)\n# Example output might contain [('кот_NOUN', 0.757...), ('котенок_NOUN', 0.726...), ...]\n"}),"\n",r.createElement(t.p,null,"In the example above, the model has sub-tags on words (e.g., _NOUN, _VERB) to indicate their part-of-speech, which is beneficial in some NLP pipelines."),"\n",r.createElement(t.p,null,"You can also compute similarities between words, find the odd one out in a set, and perform arithmetic analogies:"),"\n",r.createElement(o.A,{text:"\n# Word similarity\nrussian_model.similarity('мужчина_NOUN', 'женщина_NOUN')\n\n# Odd one out\nrussian_model.doesnt_match('завтрак_NOUN хлопья_NOUN обед_NOUN ужин_NOUN'.split())\n\n# Word analogy:\nrussian_model.most_similar(positive=['король_NOUN', 'женщина_NOUN'],\n                           negative=['мужчина_NOUN'], topn=1)\n# This might return something like [('королева_NOUN', 0.731...)]\n"}),"\n",r.createElement(t.h3,{id:"training-word2vec-on-a-small-corpus",style:{position:"relative"}},r.createElement(t.a,{href:"#training-word2vec-on-a-small-corpus","aria-label":"training word2vec on a small corpus permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Training Word2Vec on a small corpus"),"\n",r.createElement(t.p,null,'If you do not have a large-scale pre-trained model that fits your domain, you can train a specialized one from scratch or continue fine-tuning an existing model. For demonstration, Gensim hosts a small corpus called "text8," which contains about 17 million characters of text from Wikipedia.'),"\n",r.createElement(o.A,{text:"\nfrom gensim.models.word2vec import Word2Vec\nimport gensim.downloader as download_api\n\n# Download the text8 corpus\ncorpus = download_api.load('text8')  # This returns an iterable of tokenized sentences\n\n# Train a Word2Vec model\nword2vec_model = Word2Vec(corpus, size=100, workers=4)\n\n# Check top 3 similar words to 'car'\nword2vec_model.most_similar('car')[:3]\n"}),"\n",r.createElement(t.p,null,"In this snippet, ",r.createElement(i.A,null,"size=100")," sets the dimensionality of the embeddings, and ",r.createElement(i.A,null,"workers=4")," uses 4 CPU threads for parallelization. You can also tune other parameters such as ",r.createElement(i.A,null,"window")," (the context window size), ",r.createElement(i.A,null,"min_count")," (minimum word frequency), ",r.createElement(i.A,null,"sg")," (use skip-gram if 1, else CBOW), ",r.createElement(i.A,null,"negative")," (number of negative samples), etc."),"\n",r.createElement(t.h3,{id:"training-fasttext",style:{position:"relative"}},r.createElement(t.a,{href:"#training-fasttext","aria-label":"training fasttext permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Training FastText"),"\n",r.createElement(t.p,null,"Similarly, one can train ",r.createElement(i.A,null,"FastText")," embeddings using Gensim, which will handle subwords for out-of-vocabulary tokens:"),"\n",r.createElement(o.A,{text:"\nfrom gensim.models.fasttext import FastText\n\nfasttext_model = FastText(corpus, size=100, workers=4)\nfasttext_model.most_similar('car')[:3]\n# Potentially returns subword-based expansions like ('boxcar', ...), etc.\n"}),"\n",r.createElement(t.p,null,'Even though "car" might appear in the corpus, some morphological variant or a closely related subword might not. FastText\'s ability to break words into subwords helps handle new or rare word forms more gracefully.'),"\n",r.createElement(t.h3,{id:"practical-code-snippets",style:{position:"relative"}},r.createElement(t.a,{href:"#practical-code-snippets","aria-label":"practical code snippets permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Practical code snippets"),"\n",r.createElement(o.A,{text:"\n# Finding word similarities\nprint(word2vec_model.wv.most_similar('queen'))\n\n# Odd one out (English)\nprint(word2vec_model.wv.doesnt_match(\"breakfast cereal lunch dinner\".split()))\n\n# Analogies\nprint(word2vec_model.wv.most_similar(positive=['king','woman'], negative=['man'], topn=1))\n"}),"\n",r.createElement(t.p,null,"Such queries give a tangible sense of how word embeddings encode relationships within their vector spaces."),"\n",r.createElement(t.h2,{id:"additional-considerations",style:{position:"relative"}},r.createElement(t.a,{href:"#additional-considerations","aria-label":"additional considerations permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Additional considerations"),"\n",r.createElement(t.h3,{id:"data-preprocessing",style:{position:"relative"}},r.createElement(t.a,{href:"#data-preprocessing","aria-label":"data preprocessing permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Data preprocessing"),"\n",r.createElement(t.p,null,"No matter which embedding algorithm you choose, data preprocessing remains critical. Often, you will:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Normalize text"),": lowercasing, removing extra spaces, and dealing with punctuation."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Tokenize"),": break sentences into meaningful tokens (words, subwords, or characters)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Filter"),": remove extremely rare words or noise, or handle them through subword techniques."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Handle domain-specific text"),": possibly incorporating domain knowledge for specialized tasks."),"\n"),"\n",r.createElement(t.p,null,"Poor preprocessing can introduce noise, degrade the quality of the learned embeddings, and hamper downstream performance."),"\n",r.createElement(t.h3,{id:"dimensionality-and-hyperparameters",style:{position:"relative"}},r.createElement(t.a,{href:"#dimensionality-and-hyperparameters","aria-label":"dimensionality and hyperparameters permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Dimensionality and hyperparameters"),"\n",r.createElement(t.p,null,"Selecting the dimensionality (",r.createElement(l.A,{text:"\\( d \\)"})," of embeddings) is a non-trivial decision. Higher dimensions can capture more nuanced relationships but may require more data to prevent overfitting. Typical dimensionalities range from 50 to 300 for many classical use cases. For deep contextual models (e.g., BERT), hidden sizes may be 768, 1024, or even larger in advanced applications."),"\n",r.createElement(t.p,null,"Additional hyperparameters include:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Window size"),": how many words to the left and right to consider as context."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Negative samples")," (for Word2Vec and FastText)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Number of training epochs"),"."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Learning rate")," and scheduling."),"\n"),"\n",r.createElement(t.p,null,"Each influences how embeddings form in the vector space, and slight changes can lead to substantial differences in performance."),"\n",r.createElement(t.h3,{id:"domain-specific-embeddings",style:{position:"relative"}},r.createElement(t.a,{href:"#domain-specific-embeddings","aria-label":"domain specific embeddings permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Domain-specific embeddings"),"\n",r.createElement(t.p,null,'When working with specialized texts (e.g., legal, medical, or technical corpora), general-purpose embeddings might not capture domain-specific vocabulary or sense distinctions. Training or fine-tuning on in-domain data can yield better results. For example, in a medical context, "cohort," "trial," "dosage," and "patient" have very domain-specific relationships that might not appear in standard English corpora.'),"\n",r.createElement(t.h3,{id:"model-interpretability",style:{position:"relative"}},r.createElement(t.a,{href:"#model-interpretability","aria-label":"model interpretability permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Model interpretability"),"\n",r.createElement(t.p,null,"Although embeddings can produce impressive results, one challenge is interpretability. The high-dimensional spaces are difficult to visualize beyond 2D or 3D projections. Researchers sometimes rely on techniques like ",r.createElement(i.A,null,"t-SNE")," or ",r.createElement(i.A,null,"UMAP")," to project embeddings to a lower-dimensional space for qualitative analysis. Another approach is to inspect nearest neighbors or track changes in embedding norms during training, but fully understanding or interpreting how embeddings encode meaning remains an ongoing area of research."),"\n",r.createElement(t.h3,{id:"going-beyond-words",style:{position:"relative"}},r.createElement(t.a,{href:"#going-beyond-words","aria-label":"going beyond words permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Going beyond words"),"\n",r.createElement(t.p,null,"Modern practice often moves from word-level embeddings to sentence-level or even document-level embeddings. Approaches such as ",r.createElement(i.A,null,"Doc2Vec")," or ",r.createElement(i.A,null,"Sentence-BERT")," (Reimers and Gurevych, 2019) aim to capture the meaning of entire sequences. These embeddings can then be used for tasks like sentence similarity, text retrieval, or summarization. Similarly, in vision tasks, we embed images into a latent space, allowing cross-modal comparisons if we also embed text (e.g., ",r.createElement(i.A,null,"CLIP")," from OpenAI, Radford and gang, 2021)."),"\n",r.createElement(t.p,null,"Finally, large language models can produce embeddings at the token level, sentence level, or entire passage level. As these models continue to grow, they capture more sophisticated aspects of semantics, context, and world knowledge."),"\n",r.createElement(n,{alt:"Visualization of how domain-specific embeddings differ from general embeddings",path:"",caption:"When building domain-specific models, focusing on an in-domain corpus can drastically alter the learned embedding space to highlight relevant semantics.",zoom:"false"}),"\n",r.createElement(t.h2,{id:"conclusion",style:{position:"relative"}},r.createElement(t.a,{href:"#conclusion","aria-label":"conclusion permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Conclusion"),"\n",r.createElement(t.p,null,"Word embeddings lie at the heart of many modern ",r.createElement(i.A,null,"NLP"),' systems. By leveraging dense vector representations, they radically improve a model\'s ability to "understand" the relationships among words and phrases, compared to older, more sparse encoding methods. The evolution of embeddings — from one-hot to Word2Vec, FastText, GloVe, and then to contextual models like ELMo and BERT — mirrors the broader trend in NLP toward capturing richer context and deeper language features.'),"\n",r.createElement(t.p,null,"Static embeddings such as Word2Vec, FastText, and GloVe remain highly valuable for many tasks, especially when resources are limited or when domain adaptation is relatively straightforward. Contextual embeddings, exemplified by ELMo, BERT, and their successors, have opened the door to a new era of performance gains and advanced capabilities such as multi-lingual understanding, zero-shot learning, and more."),"\n",r.createElement(t.p,null,'For practitioners, the best approach often depends on the size and scope of the data, the complexity of the task, and computational constraints. Pre-trained embeddings can serve as a powerful "universal" foundation, while domain-specific retraining or fine-tuning can help optimize performance. Furthermore, interpretability challenges persist: these vector spaces, though powerful, are not trivially transparent. Nonetheless, with the right tools and an understanding of how embeddings are formed, data scientists can harness them to build robust, state-of-the-art solutions in numerous NLP and broader machine learning applications.'),"\n",r.createElement(t.h2,{id:"references",style:{position:"relative"}},r.createElement(t.a,{href:"#references","aria-label":"references permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"References"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,'Mikolov and gang, 2013. "Efficient Estimation of Word Representations in Vector Space". ',r.createElement(i.A,null,"(arXiv:1301.3781)"),"."),"\n",r.createElement(t.li,null,'Bojanowski and gang, 2016. "Enriching Word Vectors with Subword Information". ',r.createElement(i.A,null,"(arXiv:1607.04606)"),"."),"\n",r.createElement(t.li,null,'Pennington, Socher, Manning, 2014. "GloVe: Global Vectors for Word Representation". ',r.createElement(i.A,null,"(EMNLP 2014)"),"."),"\n",r.createElement(t.li,null,'Peters and gang, 2018. "Deep Contextualized Word Representations". ',r.createElement(i.A,null,"(arXiv:1802.05365)"),"."),"\n",r.createElement(t.li,null,'Devlin and gang, 2018. "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding". ',r.createElement(i.A,null,"(arXiv:1810.04805)"),"."),"\n",r.createElement(t.li,null,"Gensim documentation: ",r.createElement(i.A,null,"https://radimrehurek.com/gensim")),"\n",r.createElement(t.li,null,"Gensim data repository: ",r.createElement(i.A,null,"https://github.com/RaRe-Technologies/gensim-data")),"\n",r.createElement(t.li,null,"Word2Vec code (original Google Code archive): ",r.createElement(i.A,null,"https://code.google.com/archive/p/word2vec")),"\n",r.createElement(t.li,null,"RusVectōrēs (online semantic relationships for Russian): ",r.createElement(i.A,null,"https://rusvectores.org/ru/")),"\n",r.createElement(t.li,null,"FastText site (Facebook AI Research): ",r.createElement(i.A,null,"https://fasttext.cc/")),"\n",r.createElement(t.li,null,'"word2vec-ruscorpora-300" model: ',r.createElement(i.A,null,"https://rusvectores.org/en/models")),"\n",r.createElement(t.li,null,"Additional tutorials: ",r.createElement(i.A,null,"https://rare-technologies.com/word2vec-tutorial"),", ",r.createElement(i.A,null,"https://towardsdatascience.com")," and various GitHub references for deeper examples."),"\n"),"\n",r.createElement(t.hr),"\n",r.createElement(t.p,null,"As the field continues to evolve, new embeddings — especially large-scale, multi-modal approaches — push the boundaries of what machines can do with language and other types of data. Yet the core insight remains: by embedding data in a meaningful way, we endow computational models with a powerful lens through which they can compare, retrieve, and generate information in a manner that feels increasingly intuitive."))}var c=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,a.RP)(),e.components);return t?r.createElement(t,e,r.createElement(s,e)):s(e)};var d=n(54506),m=n(88864),h=n(58481),u=n.n(h),p=n(5984),g=n(43672),f=n(27042),v=n(72031),b=n(81817),w=n(27105),E=n(17265),y=n(2043),x=n(95751),S=n(94328),k=n(80791),H=n(78137);const T=e=>{let{toc:t}=e;if(!t||!t.items)return null;return r.createElement("nav",{className:k.R},r.createElement("ul",null,t.items.map(((e,t)=>r.createElement("li",{key:t},r.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const n=t.replace("#",""),a=document.getElementById(n);a&&a.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&r.createElement(T,{toc:{items:e.items}}))))))};function _(e){let{data:{mdx:t,allMdx:i,allPostImages:o},children:l}=e;const{frontmatter:s,body:c,tableOfContents:m}=t,h=s.index,v=s.slug.split("/")[1],k=i.nodes.filter((e=>e.frontmatter.slug.includes(`/${v}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),_=k.findIndex((e=>e.frontmatter.index===h)),A=k[_+1],z=k[_-1],V=s.slug.replace(/\/$/,""),M=/[^/]*$/.exec(V)[0],C=`posts/${v}/content/${M}/`,{0:B,1:N}=(0,r.useState)(s.flagWideLayoutByDefault),{0:L,1:I}=(0,r.useState)(!1);var P;(0,r.useEffect)((()=>{I(!0);const e=setTimeout((()=>I(!1)),340);return()=>clearTimeout(e)}),[B]),"adventures"===v?P=E.cb:"research"===v?P=E.Qh:"thoughts"===v&&(P=E.T6);const W=u()(c).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,R=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),n=e%60;return n<=30?`~${t}${n>0?".5":""} h`:`~${t+1} h`}(Math.ceil(W/P)+(s.extraReadTimeMin||0)),O=[{flag:s.flagDraft,component:()=>Promise.all([n.e(5850),n.e(9833)]).then(n.bind(n,49833))},{flag:s.flagMindfuckery,component:()=>Promise.all([n.e(5850),n.e(7805)]).then(n.bind(n,27805))},{flag:s.flagRewrite,component:()=>Promise.all([n.e(5850),n.e(8916)]).then(n.bind(n,78916))},{flag:s.flagOffensive,component:()=>Promise.all([n.e(5850),n.e(6731)]).then(n.bind(n,49112))},{flag:s.flagProfane,component:()=>Promise.all([n.e(5850),n.e(3336)]).then(n.bind(n,83336))},{flag:s.flagMultilingual,component:()=>Promise.all([n.e(5850),n.e(2343)]).then(n.bind(n,62343))},{flag:s.flagUnreliably,component:()=>Promise.all([n.e(5850),n.e(6865)]).then(n.bind(n,11627))},{flag:s.flagPolitical,component:()=>Promise.all([n.e(5850),n.e(4417)]).then(n.bind(n,24417))},{flag:s.flagCognitohazard,component:()=>Promise.all([n.e(5850),n.e(8669)]).then(n.bind(n,18669))},{flag:s.flagHidden,component:()=>Promise.all([n.e(5850),n.e(8124)]).then(n.bind(n,48124))}],{0:F,1:j}=(0,r.useState)([]);return(0,r.useEffect)((()=>{O.forEach((e=>{let{flag:t,component:n}=e;t&&n().then((e=>{j((t=>[].concat((0,d.A)(t),[e.default])))}))}))}),[]),r.createElement(f.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},r.createElement(b.A,{postNumber:s.index,date:s.date,updated:s.updated,readTime:R,difficulty:s.difficultyLevel,title:s.title,desc:s.desc,banner:s.banner,section:v,postKey:M,isMindfuckery:s.flagMindfuckery,mainTag:s.mainTag}),r.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},s.otherTags.map(((e,t)=>r.createElement("span",{key:t,className:`noselect ${H.MW}`,style:{margin:"0 5px 5px 0"}},e)))),r.createElement("div",{className:"postBody"},r.createElement(T,{toc:m})),r.createElement("br",null),r.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},r.createElement(f.P.button,{className:`noselect ${S.pb}`,id:S.xG,onClick:()=>{N(!B)},whileTap:{scale:.93}},r.createElement(f.P.div,{className:x.DJ,key:B,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},B?"Switch to default layout":"Switch to wide layout"))),r.createElement("br",null),r.createElement("div",{className:"postBody",style:{margin:B?"0 -14%":"",maxWidth:B?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},r.createElement("div",{className:`${S.P_} ${L?S.Xn:S.qG}`},F.map(((e,t)=>r.createElement(e,{key:t}))),s.indexCourse?r.createElement(y.A,{index:s.indexCourse,category:s.courseCategoryName}):"",r.createElement(p.Z.Provider,{value:{images:o.nodes,basePath:C.replace(/\/$/,"")+"/"}},r.createElement(a.xA,{components:{Image:g.A}},l)))),r.createElement(w.A,{nextPost:A,lastPost:z,keyCurrent:M,section:v}))}function A(e){return r.createElement(_,e,r.createElement(c,e))}function z(e){var t,n,a,i,o;let{data:l}=e;const{frontmatter:s}=l.mdx,c=s.titleSEO||s.title,d=s.titleOG||c,h=s.titleTwitter||c,u=s.descSEO||s.desc,p=s.descOG||u,g=s.descTwitter||u,f=s.schemaType||"BlogPosting",b=s.keywordsSEO,w=s.date,E=s.updated||w,y=s.imageOG||(null===(t=s.banner)||void 0===t||null===(n=t.childImageSharp)||void 0===n||null===(a=n.gatsbyImageData)||void 0===a||null===(i=a.images)||void 0===i||null===(o=i.fallback)||void 0===o?void 0:o.src),x=s.imageAltOG||p,S=s.imageTwitter||y,k=s.imageAltTwitter||g,H=s.canonicalURL,T=s.flagHidden||!1,_=s.mainTag||"Posts",A=s.slug.split("/")[1]||"posts",{siteUrl:z}=(0,m.Q)(),V={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:z},{"@type":"ListItem",position:2,name:_,item:`${z}/${s.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${z}${s.slug}`}]};return r.createElement(v.A,{title:c+" - avrtt.blog",titleOG:d,titleTwitter:h,description:u,descriptionOG:p,descriptionTwitter:g,schemaType:f,keywords:b,datePublished:w,dateModified:E,imageOG:y,imageAltOG:x,imageTwitter:S,imageAltTwitter:k,canonicalUrl:H,flagHidden:T,mainTag:_,section:A,type:"article"},r.createElement("script",{type:"application/ld+json"},JSON.stringify(V)))}},90548:function(e,t,n){var a=n(96540),r=n(7978);t.A=e=>{let{text:t}=e;return a.createElement(r.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-tsx-content-file-path-src-pages-posts-research-word-embeddings-mdx-f1e64e2605cd8b5f29e7.js.map