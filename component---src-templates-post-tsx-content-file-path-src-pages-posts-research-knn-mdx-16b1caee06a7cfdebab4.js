"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[4989],{3962:function(e,t){t.A="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iaXNvLTg4NTktMSI/Pgo8IS0tIEdlbmVyYXRvcjogQWRvYmUgSWxsdXN0cmF0b3IgMTYuMC4wLCBTVkcgRXhwb3J0IFBsdWctSW4gLiBTVkcgVmVyc2lvbjogNi4wMCBCdWlsZCAwKSAgLS0+CjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+CjxzdmcgdmVyc2lvbj0iMS4xIiBpZD0iQ2FwYV8xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB4PSIwcHgiIHk9IjBweCIKCSB3aWR0aD0iNDE2Ljk3OXB4IiBoZWlnaHQ9IjQxNi45NzlweCIgdmlld0JveD0iMCAwIDQxNi45NzkgNDE2Ljk3OSIgc3R5bGU9ImVuYWJsZS1iYWNrZ3JvdW5kOm5ldyAwIDAgNDE2Ljk3OSA0MTYuOTc5OyIKCSB4bWw6c3BhY2U9InByZXNlcnZlIj4KPGc+Cgk8cGF0aCBkPSJNMzU2LjAwNCw2MS4xNTZjLTgxLjM3LTgxLjQ3LTIxMy4zNzctODEuNTUxLTI5NC44NDgtMC4xODJjLTgxLjQ3LDgxLjM3MS04MS41NTIsMjEzLjM3OS0wLjE4MSwyOTQuODUKCQljODEuMzY5LDgxLjQ3LDIxMy4zNzgsODEuNTUxLDI5NC44NDksMC4xODFDNDM3LjI5MywyNzQuNjM2LDQzNy4zNzUsMTQyLjYyNiwzNTYuMDA0LDYxLjE1NnogTTIzNy42LDM0MC43ODYKCQljMCwzLjIxNy0yLjYwNyw1LjgyMi01LjgyMiw1LjgyMmgtNDYuNTc2Yy0zLjIxNSwwLTUuODIyLTIuNjA1LTUuODIyLTUuODIyVjE2Ny44ODVjMC0zLjIxNywyLjYwNy01LjgyMiw1LjgyMi01LjgyMmg0Ni41NzYKCQljMy4yMTUsMCw1LjgyMiwyLjYwNCw1LjgyMiw1LjgyMlYzNDAuNzg2eiBNMjA4LjQ5LDEzNy45MDFjLTE4LjYxOCwwLTMzLjc2Ni0xNS4xNDYtMzMuNzY2LTMzLjc2NQoJCWMwLTE4LjYxNywxNS4xNDctMzMuNzY2LDMzLjc2Ni0zMy43NjZjMTguNjE5LDAsMzMuNzY2LDE1LjE0OCwzMy43NjYsMzMuNzY2QzI0Mi4yNTYsMTIyLjc1NSwyMjcuMTA3LDEzNy45MDEsMjA4LjQ5LDEzNy45MDF6Ii8+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPC9zdmc+Cg=="},9360:function(e,t,n){n.d(t,{A:function(){return r}});var a=n(96540),i=n(3962),l="styles-module--tooltiptext--a263b";var r=e=>{let{text:t,isBadge:n=!1}=e;const{0:r,1:s}=(0,a.useState)(!1),c=(0,a.useRef)(null);return(0,a.useEffect)((()=>{function e(e){c.current&&e.target instanceof Node&&!c.current.contains(e.target)&&s(!1)}return document.addEventListener("click",e),()=>{document.removeEventListener("click",e)}}),[]),a.createElement("span",{className:"styles-module--tooltipWrapper--75ebf",ref:c},a.createElement("img",{id:n?"styles-module--infoBadge--e3d66":"styles-module--info--26c1f",src:i.A,alt:"info",onClick:e=>{e.stopPropagation(),s((e=>!e))}}),a.createElement("span",{className:r?`${l} styles-module--visible--c063c`:l},t))}},90548:function(e,t,n){var a=n(96540),i=n(7978);t.A=e=>{let{text:t}=e;return a.createElement(i.A,null,t)}},96796:function(e,t,n){n.r(t),n.d(t,{Head:function(){return C},PostTemplate:function(){return M},default:function(){return z}});var a=n(28453),i=n(96540),l=n(9360),r=n(61992),s=n(62087),c=n(90548);function o(e){const t=Object.assign({p:"p",h3:"h3",a:"a",span:"span",ul:"ul",li:"li",strong:"strong",h2:"h2",ol:"ol",h4:"h4",hr:"hr",em:"em"},(0,a.RP)(),e.components),{Image:n}=t;return n||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),i.createElement(i.Fragment,null,"\n",i.createElement("br"),"\n","\n","\n","\n",i.createElement(t.p,null,"The ",i.createElement(r.A,null,"k-nearest neighbors (KNN)")," algorithm is a foundational technique in both classical and modern machine learning, valued for its conceptual simplicity, intuitive appeal, and effectiveness in certain application domains. In essence, this model belongs to the family of ",i.createElement(r.A,null,"instance-based or lazy-learning methods"),'. The key idea is to classify or predict the label of a new data point by looking at its "neighbors" in the feature space — that is, the most similar or closest points from the training set — and using their labels to make an inference.'),"\n",i.createElement(t.p,null,"This chapter dives into the mathematics, intuition, and theoretical properties that underlie the KNN approach. We will also discuss the ",i.createElement(r.A,null,"bias-variance tradeoff")," for KNN, potential pitfalls, convergence guarantees, and references to both traditional and cutting-edge research."),"\n",i.createElement(t.h3,{id:"overview-of-k-nearest-neighbors",style:{position:"relative"}},i.createElement(t.a,{href:"#overview-of-k-nearest-neighbors","aria-label":"overview of k nearest neighbors permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Overview of k-nearest neighbors"),"\n",i.createElement(t.p,null,"At a high level, KNN presupposes that ",i.createElement(l.A,{text:"Objects that are near each other in feature space are likely to share the same or similar labels"}),'. This often goes hand-in-hand with the so-called "smoothness" or "compactness" assumption: if two samples are sufficiently close under a suitable distance metric, they should belong to the same class (in classification) or have similar numeric values (in regression).'),"\n",i.createElement(t.p,null,"Concretely, the classification version of KNN works as follows:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"You are given a training set of ",i.createElement(c.A,{text:"\\( m \\)"})," labeled samples, ",i.createElement(c.A,{text:"\\( \\{(x_1, y_1), (x_2, y_2), \\ldots, (x_m, y_m)\\} \\)"}),". Each ",i.createElement(c.A,{text:"\\( x_i \\)"})," is typically an ",i.createElement(c.A,{text:"\\(n\\)"}),"-dimensional feature vector, and each ",i.createElement(c.A,{text:"\\( y_i \\)"})," is the label or target."),"\n",i.createElement(t.li,null,"You have a distance or similarity measure ",i.createElement(c.A,{text:"\\( \\rho(\\cdot,\\cdot) \\)"}),". For example, Euclidean distance is commonly used, but other metrics can be substituted."),"\n",i.createElement(t.li,null,"To predict the label of a new, unseen sample ",i.createElement(c.A,{text:"\\( x \\)"}),", you compute the distance from ",i.createElement(c.A,{text:"\\( x \\)"})," to all training samples ",i.createElement(c.A,{text:"\\( x_i \\)"}),"."),"\n",i.createElement(t.li,null,"You select the ",i.createElement(c.A,{text:"\\( k \\)"})," nearest points (i.e., the ",i.createElement(c.A,{text:"\\( k \\)"})," samples in the training set with the smallest distances to ",i.createElement(c.A,{text:"\\( x \\)"}),")."),"\n",i.createElement(t.li,null,"You aggregate the labels (for classification) or numeric targets (for regression) of those ",i.createElement(c.A,{text:"\\( k \\)"})," points in order to make your prediction."),"\n"),"\n",i.createElement(t.p,null,"Mathematically, for classification, a general KNN-based predictor ",i.createElement(c.A,{text:"\\( a(u) \\)"})," for a new point ",i.createElement(c.A,{text:"\\( u \\)"})," can be written as:"),"\n",i.createElement(c.A,{text:"\\[\na(u) = \\mathrm{arg}\\max_{y \\in Y} \\sum_{i=1}^{m} \\bigl[\\, y_{i;u} = y \\bigr] \\, w(i,u),\n\\]"}),"\n",i.createElement(t.p,null,"where ",i.createElement(c.A,{text:"\\( y_{i;u} \\)"})," denotes the label of the ",i.createElement(c.A,{text:"\\(i\\)"}),"-th nearest neighbor of ",i.createElement(c.A,{text:"\\( u \\)"}),", and ",i.createElement(c.A,{text:"\\( w(i,u) \\)"})," is a weight function that determines how much the ",i.createElement(c.A,{text:"\\(i\\)"}),"-th neighbor contributes to the classification of ",i.createElement(c.A,{text:"\\( u \\)"}),". The simplest scenario is:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Unweighted / majority vote KNN:")," ",i.createElement(c.A,{text:"\\( w(i,u) = [i \\leq k] \\)"}),", meaning that we only count the top ",i.createElement(c.A,{text:"\\(k\\)"})," neighbors equally, ignoring all others."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Weighted KNN:")," The neighbors are weighted inversely by their distance to ",i.createElement(c.A,{text:"\\( u \\)"}),", or perhaps by a kernel function, ensuring that closer neighbors exert a stronger influence."),"\n"),"\n",i.createElement(t.p,null,"For regression, the logic is much the same, except we compute (for instance) the mean of the ",i.createElement(c.A,{text:"\\( k \\)"})," nearest neighbors' numeric values, potentially weighting them as a function of distance."),"\n",i.createElement(t.h3,{id:"basic-principles",style:{position:"relative"}},i.createElement(t.a,{href:"#basic-principles","aria-label":"basic principles permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Basic principles"),"\n",i.createElement(t.p,null,"KNN is considered a ",i.createElement(r.A,null,"lazy learner"),' because it defers computation until inference time (classification or regression). There is no explicit training process in the same sense as parameter-based models (e.g., linear or logistic regression, neural networks, etc.). Instead, the entire training set is often stored, and queries require scanning or indexing that training set. This "train-later, predict-now" characteristic has implications for both memory consumption and computational cost at inference.'),"\n",i.createElement(t.p,null,"The underpinnings of KNN's consistency results (the fact that it can perform well asymptotically) trace back to the seminal work of T. Cover and P. Hart from 1967, who proved that the ",i.createElement(r.A,null,"1-nearest neighbor classifier")," is guaranteed to achieve at most twice the Bayes error rate in the limit of infinite samples (Cover and Hart, 1967, IEEE Transactions on Information Theory)."),"\n",i.createElement(t.h3,{id:"mathematical-formulation",style:{position:"relative"}},i.createElement(t.a,{href:"#mathematical-formulation","aria-label":"mathematical formulation permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Mathematical formulation"),"\n",i.createElement(t.p,null,"A rigorous way to describe KNN is to define, for each new input ",i.createElement(c.A,{text:"\\( x \\)"}),", the set ",i.createElement(c.A,{text:"\\( S_x \\subseteq D \\)"})," of its ",i.createElement(c.A,{text:"\\( k \\)"})," nearest neighbors. Concretely, ",i.createElement(c.A,{text:"\\( S_x \\)"})," is chosen so that ",i.createElement(c.A,{text:"\\( \\lvert S_x \\rvert = k \\)"})," and every point not in ",i.createElement(c.A,{text:"\\( S_x \\)"})," is at least as far from ",i.createElement(c.A,{text:"\\( x \\)"})," as the farthest point in ",i.createElement(c.A,{text:"\\( S_x \\)"}),". The KNN classifier ",i.createElement(c.A,{text:"\\( h \\)"})," can then be written as:"),"\n",i.createElement(c.A,{text:"\\[\nh(x) = \\text{mode} \\Bigl\\{\\, y'' \\;|\\; (x'', y'') \\in S_x \\Bigr\\}.\n\\]"}),"\n",i.createElement(t.p,null,"That is, you select the label that appears most frequently among the ",i.createElement(c.A,{text:"\\( k \\)"})," neighbors."),"\n",i.createElement(t.h3,{id:"bias-variance-tradeoff-in-knn",style:{position:"relative"}},i.createElement(t.a,{href:"#bias-variance-tradeoff-in-knn","aria-label":"bias variance tradeoff in knn permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Bias-variance tradeoff in KNN"),"\n",i.createElement(t.p,null,"In KNN, the ",i.createElement(r.A,null,"bias")," and ",i.createElement(r.A,null,"variance")," are primarily controlled by ",i.createElement(c.A,{text:"\\( k \\)"})," and by the choice of distance metric. Roughly speaking:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"For very small ",i.createElement(c.A,{text:"\\( k \\)"})," (e.g., ",i.createElement(c.A,{text:"\\(k = 1\\)"}),"), the model has ",i.createElement(r.A,null,"low bias")," (it can fit extremely local structures) but can have ",i.createElement(r.A,null,"high variance"),", as it is sensitive to noise or outlier points."),"\n",i.createElement(t.li,null,"For larger ",i.createElement(c.A,{text:"\\( k \\)"}),", the model has ",i.createElement(r.A,null,"higher bias")," (it effectively takes broader neighborhoods, smoothing out local fluctuations) and ",i.createElement(r.A,null,"lower variance"),", resulting in more stable predictions."),"\n"),"\n",i.createElement(t.p,null,"Because KNN has no parametric form, it can adapt to very complex decision boundaries. However, in high-dimensional spaces or in the presence of very large training data sets, naive KNN can become computationally expensive."),"\n",i.createElement(t.h2,{id:"distance-metrics",style:{position:"relative"}},i.createElement(t.a,{href:"#distance-metrics","aria-label":"distance metrics permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Distance metrics"),"\n",i.createElement(t.p,null,"One of the most critical components in KNN is the distance metric or similarity measure. The standard approach might be Euclidean distance, but the real power (and risk) of KNN lies in how well the chosen distance metric reflects semantic similarity among data points."),"\n",i.createElement(t.h3,{id:"euclidean-distance",style:{position:"relative"}},i.createElement(t.a,{href:"#euclidean-distance","aria-label":"euclidean distance permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Euclidean distance"),"\n",i.createElement(t.p,null,"Often the default choice, the Euclidean distance between two vectors ",i.createElement(c.A,{text:"\\( x = (x_1,\\ldots,x_n) \\)"})," and ",i.createElement(c.A,{text:"\\( y = (y_1,\\ldots,y_n) \\)"})," in an ",i.createElement(c.A,{text:"\\(n\\)"}),"-dimensional space is:"),"\n",i.createElement(c.A,{text:"\\[\n\\rho_{\\text{euclid}}(x, y) \\;=\\; \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}.\n\\]"}),"\n",i.createElement(t.p,null,"Euclidean distance has an intuitive geometric interpretation: the length of the straight line between the points in Euclidean space. However, it can be very sensitive to magnitude differences among features. To mitigate this, you often ",i.createElement(r.A,null,"normalize or standardize")," each feature dimension so that they are on a comparable scale."),"\n",i.createElement(t.h3,{id:"manhattan-distance",style:{position:"relative"}},i.createElement(t.a,{href:"#manhattan-distance","aria-label":"manhattan distance permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Manhattan distance"),"\n",i.createElement(t.p,null,'Also called the L1 metric or "taxicab" metric, the Manhattan distance is:'),"\n",i.createElement(c.A,{text:"\\[\n\\rho_{\\text{manhattan}}(x, y) \\;=\\; \\sum_{i=1}^n \\lvert x_i - y_i \\rvert.\n\\]"}),"\n",i.createElement(t.p,null,"When you have reason to assume that ",i.createElement(r.A,null,"coordinate-wise differences")," are more robust or meaningful for your task, or if you want the distance measure to be less sensitive to outliers in a single dimension, Manhattan distance can be beneficial."),"\n",i.createElement(t.h3,{id:"minkowski-distance",style:{position:"relative"}},i.createElement(t.a,{href:"#minkowski-distance","aria-label":"minkowski distance permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Minkowski distance"),"\n",i.createElement(t.p,null,"The Minkowski class of metrics generalizes multiple norms in a single formula:"),"\n",i.createElement(c.A,{text:"\\[\n\\rho_{p}(x, y) \\;=\\; \\Bigl(\\sum_{i=1}^n \\lvert x_i - y_i \\rvert^p \\Bigr)^{1/p}.\n\\]"}),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(c.A,{text:"\\( p=1 \\)"}),": recovers the Manhattan distance"),"\n",i.createElement(t.li,null,i.createElement(c.A,{text:"\\( p=2 \\)"}),": recovers the Euclidean distance"),"\n",i.createElement(t.li,null,i.createElement(c.A,{text:"\\( p \\to \\infty \\)"}),": recovers the Chebyshev distance (the maximum absolute difference along any dimension)"),"\n"),"\n",i.createElement(t.p,null,"This flexibility helps you experiment with different norms. Each might capture subtle differences in how distance is measured. However, one must carefully tune ",i.createElement(c.A,{text:"\\( p \\)"})," because the data geometry can vary drastically with different norms."),"\n",i.createElement(t.h3,{id:"cosine-similarity",style:{position:"relative"}},i.createElement(t.a,{href:"#cosine-similarity","aria-label":"cosine similarity permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Cosine similarity"),"\n",i.createElement(t.p,null,"Cosine similarity (or equivalently, cosine distance if you transform it into a distance measure) focuses on the angle between two vectors rather than their magnitude. It is commonly used in high-dimensional and sparse contexts such as text mining or natural language processing. If ",i.createElement(c.A,{text:"\\( x \\)"})," and ",i.createElement(c.A,{text:"\\( y \\)"})," are real vectors in ",i.createElement(c.A,{text:"\\(\\mathbb{R}^n\\)"}),", the ",i.createElement(r.A,null,"cosine similarity")," is:"),"\n",i.createElement(c.A,{text:"\\[\n\\text{sim}_{\\cos}(x,y) \\;=\\; \\frac{x \\cdot y}{\\|x\\| \\,\\|y\\|}.\n\\]"}),"\n",i.createElement(t.p,null,"One can convert this similarity into a distance by ",i.createElement(c.A,{text:"\\( \\rho_{\\cos}(x,y) = 1 - \\text{sim}_{\\cos}(x,y) \\)"}),". This is especially relevant in fields like information retrieval, textual clustering, or word embeddings, where directional closeness in the vector space is more important than raw Euclidean magnitude."),"\n",i.createElement(t.h3,{id:"impact-of-choosing-different-metrics",style:{position:"relative"}},i.createElement(t.a,{href:"#impact-of-choosing-different-metrics","aria-label":"impact of choosing different metrics permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Impact of choosing different metrics"),"\n",i.createElement(t.p,null,'Your choice of metric can profoundly affect performance. For example, with high-dimensional data, Euclidean distance might become less meaningful (the "curse of dimensionality"), so sometimes people turn to alternatives (e.g., ',i.createElement(r.A,null,"Mahalanobis distance")," with an appropriate covariance matrix, or even learned distance metrics like LMNN or FaceNet embeddings). Real-world success with KNN depends heavily on whether the distance measure aligns with how classes or underlying patterns separate in the feature space."),"\n",i.createElement(t.h2,{id:"classification-with-knn",style:{position:"relative"}},i.createElement(t.a,{href:"#classification-with-knn","aria-label":"classification with knn permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Classification with KNN"),"\n",i.createElement(t.h3,{id:"decision-boundaries",style:{position:"relative"}},i.createElement(t.a,{href:"#decision-boundaries","aria-label":"decision boundaries permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Decision boundaries"),"\n",i.createElement(t.p,null,"A classic visual of KNN is the ",i.createElement(r.A,null,"non-linear decision boundary")," that forms when you partition the feature space based on nearest neighbors. Because no explicit parametric shape is assumed, the boundary can be quite complicated."),"\n",i.createElement(t.p,null,"If you imagine a 2D or 3D feature space, you can see that for each point ",i.createElement(c.A,{text:"\\( x \\)"}),', KNN effectively claims: "the decision region around ',i.createElement(c.A,{text:"\\( x \\)"})," belongs to whichever class the majority of the ",i.createElement(c.A,{text:"\\( k \\)"}),' neighbors from the training data represent." The resulting boundary can be highly irregular and sensitive to noise if ',i.createElement(c.A,{text:"\\( k \\)"})," is small, or overly smooth (missing local details) if ",i.createElement(c.A,{text:"\\( k \\)"})," is large."),"\n",i.createElement(n,{alt:"knn-decision-boundary",path:"",caption:"An illustrative 2D example of a KNN decision boundary, where each region is colored according to its predicted class.",zoom:"false"}),"\n",i.createElement(t.h3,{id:"choosing-the-optimal-value-of-k",style:{position:"relative"}},i.createElement(t.a,{href:"#choosing-the-optimal-value-of-k","aria-label":"choosing the optimal value of k permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Choosing the optimal value of k"),"\n",i.createElement(t.p,null,"Selecting ",i.createElement(c.A,{text:"\\( k \\)"})," is crucial:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(c.A,{text:"\\( k=1 \\)"})," can perfectly classify training data but is prone to overfitting and very sensitive to noise."),"\n",i.createElement(t.li,null,"Larger ",i.createElement(c.A,{text:"\\( k \\)"})," reduces variance but can increase bias."),"\n",i.createElement(t.li,null,"There is often a sweet spot that balances the two extremes."),"\n"),"\n",i.createElement(t.p,null,"In practice, one typically performs ",i.createElement(r.A,null,"cross-validation")," (e.g., grid search over candidate values) to find the optimal ",i.createElement(c.A,{text:"\\( k \\)"})," that yields the best average validation accuracy."),"\n",i.createElement(t.h3,{id:"weighted-knn-for-improved-accuracy",style:{position:"relative"}},i.createElement(t.a,{href:"#weighted-knn-for-improved-accuracy","aria-label":"weighted knn for improved accuracy permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Weighted KNN for improved accuracy"),"\n",i.createElement(t.p,null,"One refinement is to weight each neighbor's contribution by a function of distance, so that closer neighbors have a stronger vote. A simple approach is:"),"\n",i.createElement(c.A,{text:"\\[\nw_i \\;=\\; \\frac{1}{\\rho(x, x_i) + \\epsilon},\n\\]"}),"\n",i.createElement(t.p,null,"where ",i.createElement(c.A,{text:"\\( \\rho(\\cdot,\\cdot) \\)"})," is the chosen metric and ",i.createElement(c.A,{text:"\\( \\epsilon \\)"})," is a small constant to avoid dividing by zero. More sophisticated approaches include kernels like the ",i.createElement(r.A,null,"Gaussian, tricube, or other smoothing kernels"),", reminiscent of Parzen window estimators."),"\n",i.createElement(t.h3,{id:"handling-class-imbalances",style:{position:"relative"}},i.createElement(t.a,{href:"#handling-class-imbalances","aria-label":"handling class imbalances permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Handling class imbalances"),"\n",i.createElement(t.p,null,"KNN on imbalanced data sets can be dominated by the majority class. Some strategies to address this:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Resampling")," the dataset (e.g., oversampling the minority class, undersampling the majority class)."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Applying class-dependent weighting")," to the KNN distances or to the final decision function."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Combining KNN with anomaly or outlier detection"),' so minority points do not get "washed out" by the majority.'),"\n"),"\n",i.createElement(t.p,null,"Sometimes, ",i.createElement(r.A,null,"SMOTE")," or other synthetic oversampling can help correct severe imbalance prior to fitting a KNN-based classifier."),"\n",i.createElement(t.h2,{id:"regression-with-knn",style:{position:"relative"}},i.createElement(t.a,{href:"#regression-with-knn","aria-label":"regression with knn permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Regression with KNN"),"\n",i.createElement(t.p,null,"KNN extends naturally to regression by considering the real-valued outputs of neighbors and combining them, typically via a weighted average:"),"\n",i.createElement(t.h3,{id:"averaging-approaches",style:{position:"relative"}},i.createElement(t.a,{href:"#averaging-approaches","aria-label":"averaging approaches permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Averaging approaches"),"\n",i.createElement(t.p,null,"A straightforward approach is to take the simple mean of the ",i.createElement(c.A,{text:"\\( k \\)"})," neighbors' numeric outputs:"),"\n",i.createElement(c.A,{text:"\\[\n\\hat{y}(x) \\;=\\; \\frac{1}{k} \\sum_{i \\in N_k(x)} y_i,\n\\]"}),"\n",i.createElement(t.p,null,"where ",i.createElement(c.A,{text:"\\( N_k(x) \\)"})," is the set of indices for the ",i.createElement(c.A,{text:"\\( k \\)"})," nearest neighbors. This yields a local, non-parametric estimate that can adapt to quite complicated data relationships."),"\n",i.createElement(t.h3,{id:"weighted-methods-for-regression",style:{position:"relative"}},i.createElement(t.a,{href:"#weighted-methods-for-regression","aria-label":"weighted methods for regression permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Weighted methods for regression"),"\n",i.createElement(t.p,null,"Analogous to the classification setting, you can weigh each neighbor's contribution by a function of distance. A common kernel-based approach is:"),"\n",i.createElement(c.A,{text:"\\[\n\\hat{y}(x) = \\frac{\\sum_{i \\in N_k(x)} K\\!\\bigl(\\rho(x,x_i)\\bigr)\\, y_i}{\\sum_{i \\in N_k(x)} K\\!\\bigl(\\rho(x,x_i)\\bigr)}.\n\\]"}),"\n",i.createElement(t.p,null,"In practice, you might define ",i.createElement(c.A,{text:"\\( K(r) = \\exp(-\\alpha\\, r^2) \\)"})," or some other shape. Weighted averaging can substantially enhance performance when the data's underlying relationship is fairly smooth and local neighborhoods are meaningful."),"\n",i.createElement(t.h2,{id:"advanced-techniques",style:{position:"relative"}},i.createElement(t.a,{href:"#advanced-techniques","aria-label":"advanced techniques permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Advanced techniques"),"\n",i.createElement(t.h3,{id:"dimensionality-reduction-and-feature-selection",style:{position:"relative"}},i.createElement(t.a,{href:"#dimensionality-reduction-and-feature-selection","aria-label":"dimensionality reduction and feature selection permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Dimensionality reduction and feature selection"),"\n",i.createElement(t.p,null,"A recurring difficulty in KNN is the ",i.createElement(r.A,null,"curse of dimensionality"),'. Distances in very high-dimensional spaces can be misleading (points tend to appear equidistant), making the notion of "nearest" less useful. Common strategies:'),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Dimensionality reduction")," (PCA, t-SNE, or more advanced methods) to project to a lower-dimensional subspace where the data is more compact."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Feature selection"),": systematically remove features that do not help discriminate or that add noise. In a real pipeline, you might apply domain knowledge or use wrapper/filter methods to identify a subset of meaningful features."),"\n"),"\n",i.createElement(t.h3,{id:"dealing-with-high-dimensional-data",style:{position:"relative"}},i.createElement(t.a,{href:"#dealing-with-high-dimensional-data","aria-label":"dealing with high dimensional data permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Dealing with high-dimensional data"),"\n",i.createElement(t.p,null,"When you cannot easily reduce dimensionality, you may turn to specialized metrics or learned embeddings that better reflect the notion of similarity for your domain. For instance, in computer vision tasks (like face recognition), networks such as ",i.createElement(r.A,null,"FaceNet")," or ",i.createElement(r.A,null,"ArcFace")," learn to embed images into a space where Euclidean distance correlates strongly with identity. Then, performing a KNN search in that space can work surprisingly well, even though the raw pixel dimension is extremely large."),"\n",i.createElement(t.h3,{id:"approximate-nearest-neighbor-searches",style:{position:"relative"}},i.createElement(t.a,{href:"#approximate-nearest-neighbor-searches","aria-label":"approximate nearest neighbor searches permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Approximate nearest neighbor searches"),"\n",i.createElement(t.p,null,"Because naive KNN queries require comparing the test point to all training points (",i.createElement(r.A,null,"O(n)")," complexity per query), large-scale usage can be prohibitive. This has motivated advanced data structures and algorithms that efficiently retrieve approximate neighbors:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"\n",i.createElement(r.A,null,"k-d trees"),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(r.A,null,"Ball trees"),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(r.A,null,"Product quantization (PQ)"),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(r.A,null,"Inverted file indices"),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(r.A,null,"Hierarchical Navigable Small World (HNSW)"),"\n"),"\n"),"\n",i.createElement(t.p,null,"Approximate nearest neighbor (ANN) methods trade off a small accuracy penalty for substantial speed boosts. They are critical in real-time systems such as image retrieval, recommendation, or large-scale clustering."),"\n",i.createElement(t.h3,{id:"hierarchical-navigable-small-world-hnsw",style:{position:"relative"}},i.createElement(t.a,{href:"#hierarchical-navigable-small-world-hnsw","aria-label":"hierarchical navigable small world hnsw permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Hierarchical navigable small world (HNSW)"),"\n",i.createElement(t.p,null,"The ",i.createElement(r.A,null,"HNSW"),' data structure (Malkov and Yashunin, 2018) is a sophisticated graph-based ANN technique. It organizes data points in a multi-layer "small-world" graph where edges provide both short-range and random long-range connections, ensuring that queries can be routed quickly to relevant regions of the graph. The search proceeds greedily layer by layer, drastically reducing search times compared to naive exhaustive scans.'),"\n",i.createElement(t.p,null,"The overall structure is layered as follows:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"Level 0 (lowest) contains all ",i.createElement(c.A,{text:"\\( N \\)"})," points."),"\n",i.createElement(t.li,null,"Each higher level is a sparser subset of points, randomly chosen."),"\n",i.createElement(t.li,null,"Searching begins at the top layer with a random entry point, then descends layer by layer, greedily moving closer to the query vector. On the final layer, it gathers candidate neighbors for a more refined local search."),"\n"),"\n",i.createElement(t.p,null,"The net effect is near ",i.createElement(r.A,null,"logarithmic complexity")," in many practical cases, enabling KNN queries on extremely large data sets (hundreds of millions or even billions of points) with feasible latency."),"\n",i.createElement(t.h4,{id:"code-snippet-example-in-python-with-hnswlib",style:{position:"relative"}},i.createElement(t.a,{href:"#code-snippet-example-in-python-with-hnswlib","aria-label":"code snippet example in python with hnswlib permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Code snippet (example in Python with hnswlib)"),"\n",i.createElement(n,{alt:"HNSW Graph Diagram",path:"",caption:"A hierarchical navigable small world structure for approximate nearest neighbor search.",zoom:"false"}),"\n",i.createElement(t.p,null,"Below is an illustrative snippet using the ",i.createElement(r.A,null,"hnswlib")," library in Python:"),"\n",i.createElement(s.A,{text:'\nimport hnswlib\nimport numpy as np\n\n# Dimensionality of the vectors.\ndim = 128\n\n# Number of elements (example).\nnum_elements = 10000\n\n# Create random data.\ndata = np.float32(np.random.random((num_elements, dim)))\nlabels = np.arange(num_elements)\n\n# Instantiate an HNSW index in L2 (Euclidean) space.\np = hnswlib.Index(space=\'l2\', dim=dim)  \n\n# Initialize the index\np.init_index(max_elements=num_elements, ef_construction=200, M=16)\n\n# Add the data\np.add_items(data, labels)\n\n# Set ef parameter for queries (controls recall/quality vs. speed).\np.set_ef(50)  # ef must be >= k\n\n# Query for the nearest neighbor\nneighbors, distances = p.knn_query(data, k=1)\nprint("Approximate neighbors shape:", neighbors.shape)\nprint("Distances shape:", distances.shape)\n'}),"\n",i.createElement(t.p,null,"This snippet illustrates how easy it is to build an HNSW graph structure and query neighbors with library support. In production, you might tweak ",i.createElement(c.A,{text:"\\( M \\)"})," and ",i.createElement(c.A,{text:"\\( ef \\)"})," to balance performance and accuracy."),"\n",i.createElement(t.h3,{id:"optimizing-knn-for-scalability",style:{position:"relative"}},i.createElement(t.a,{href:"#optimizing-knn-for-scalability","aria-label":"optimizing knn for scalability permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Optimizing KNN for scalability"),"\n",i.createElement(t.p,null,"Beyond approximate methods, additional strategies to make KNN scalable include:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Data compression or prototype selection:")," Removing redundant points, or representing clusters of points by their centroids (or medoids)."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Specialized hardware acceleration:")," Exploit GPU or vectorized instructions to speed up distance calculations."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Distributed or parallel indexing:")," For truly massive data sets, frameworks like Apache Spark or HPC libraries can parallelize neighbor searches."),"\n"),"\n",i.createElement(t.h2,{id:"practical-considerations",style:{position:"relative"}},i.createElement(t.a,{href:"#practical-considerations","aria-label":"practical considerations permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Practical considerations"),"\n",i.createElement(t.h3,{id:"data-preprocessing-normalization-missing-values",style:{position:"relative"}},i.createElement(t.a,{href:"#data-preprocessing-normalization-missing-values","aria-label":"data preprocessing normalization missing values permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Data preprocessing (normalization, missing values)"),"\n",i.createElement(t.p,null,"Data preprocessing is essential with KNN. Common guidelines:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Normalization:")," Transform each feature so that they all lie in similar ranges (e.g., 0 to 1 or ",i.createElement(c.A,{text:"\\(-1\\)"})," to ",i.createElement(c.A,{text:"\\(+1\\)"}),"), or so they have zero mean and unit variance. This prevents large-valued features from overpowering the distance measure."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Handling missing data:")," The distance metric must be defined carefully. Some practitioners impute missing values (e.g., mean imputation), while more advanced approaches skip distance computations on missing dimensions or infer them in a more sophisticated manner."),"\n"),"\n",i.createElement(t.h3,{id:"memory-usage-and-computational-complexity",style:{position:"relative"}},i.createElement(t.a,{href:"#memory-usage-and-computational-complexity","aria-label":"memory usage and computational complexity permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Memory usage and computational complexity"),"\n",i.createElement(t.p,null,"The classic (exact) KNN requires:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"Storing all training data (memory overhead)."),"\n",i.createElement(t.li,null,"A query time of ",i.createElement(c.A,{text:"\\( O(m) \\)"})," per test sample, where ",i.createElement(c.A,{text:"\\( m \\)"})," is the training set size."),"\n"),"\n",i.createElement(t.p,null,"As ",i.createElement(c.A,{text:"\\( m \\)"})," grows large, naive KNN quickly becomes impractical. Even for moderate ",i.createElement(c.A,{text:"\\( m \\)"}),", repeated queries can be expensive. Approximate methods or advanced data structures become critical."),"\n",i.createElement(t.h3,{id:"parameter-tuning-and-cross-validation",style:{position:"relative"}},i.createElement(t.a,{href:"#parameter-tuning-and-cross-validation","aria-label":"parameter tuning and cross validation permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Parameter tuning and cross-validation"),"\n",i.createElement(t.p,null,"KNN has relatively few hyperparameters:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,i.createElement(c.A,{text:"\\( k \\)"})," — the number of neighbors"),"\n",i.createElement(t.li,null,i.createElement(c.A,{text:"\\( p \\)"})," in Minkowski distance (if relevant)"),"\n",i.createElement(t.li,null,"Possibly weighting or kernel function parameters"),"\n"),"\n",i.createElement(t.p,null,"A typical best practice is to run ",i.createElement(r.A,null,"cross-validation")," (e.g., five-fold or ten-fold) across a grid of possible ",i.createElement(c.A,{text:"\\( k \\)"})," values (like 1, 3, 5, 7, ..., 31, etc.), as well as exploring different distance metrics or weighting schemes. You pick the combination that yields the highest validation accuracy (in classification) or the lowest MSE/MAE (in regression)."),"\n",i.createElement(t.h3,{id:"handling-noise-and-outliers",style:{position:"relative"}},i.createElement(t.a,{href:"#handling-noise-and-outliers","aria-label":"handling noise and outliers permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Handling noise and outliers"),"\n",i.createElement(t.p,null,"KNN can be very sensitive to noisy or mislabeled points, especially if ",i.createElement(c.A,{text:"\\( k \\)"})," is small. Some common practices:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Filtering or cleaning")," suspicious training points with domain knowledge."),"\n",i.createElement(t.li,null,"Using ",i.createElement(r.A,null,"STOLP"),' or other "prototype selection" methods to remove outliers that degrade classification.'),"\n",i.createElement(t.li,null,"Weighted KNN can reduce outlier impact by diminishing influence with distance."),"\n"),"\n",i.createElement(t.h3,{id:"model-interpretability-and-explainability",style:{position:"relative"}},i.createElement(t.a,{href:"#model-interpretability-and-explainability","aria-label":"model interpretability and explainability permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Model interpretability and explainability"),"\n",i.createElement(t.p,null,"KNN is sometimes praised for interpretability, at least from a ",i.createElement(r.A,null,'"reasoning by analogy"'),' perspective: "We predict the label of ',i.createElement(c.A,{text:"\\( x \\)"}),' to be Y because its nearest neighbors are labeled Y." That said, with a huge training set, it is less trivial to interpret or visualize the actual boundaries. The local explanation, however, remains straightforward: you can show examples in the training data that are close in distance to the new data point.'),"\n",i.createElement(t.h2,{id:"implementation",style:{position:"relative"}},i.createElement(t.a,{href:"#implementation","aria-label":"implementation permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Implementation"),"\n",i.createElement(t.p,null,"Below is a short example in Python (using ",i.createElement(r.A,null,"scikit-learn"),") for KNN classification. We assume you have data ",i.createElement(c.A,{text:"\\( X \\)"})," (features) and ",i.createElement(c.A,{text:"\\( y \\)"})," (labels), plus any needed preprocessing."),"\n",i.createElement(s.A,{text:"\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report\n\n# Suppose X is your feature matrix of shape (m, n)\n# Suppose y is a vector of class labels of shape (m,)\n# Example: You might load from a CSV or a dataset\n# X, y = ...\n\n# Split into train/test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# It's often useful to scale the features for KNN\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled  = scaler.transform(X_test)\n\n# Create a KNN classifier\nknn = KNeighborsClassifier(n_neighbors=5, metric='euclidean', weights='distance')\n\n# Fit on training data\nknn.fit(X_train_scaled, y_train)\n\n# Predict on test data\ny_pred = knn.predict(X_test_scaled)\n\n# Evaluate\nprint(classification_report(y_test, y_pred))\n"}),"\n",i.createElement(t.p,null,"This code snippet demonstrates a typical scikit-learn workflow:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,"Train/test split."),"\n",i.createElement(t.li,null,"Optional but recommended feature scaling (",i.createElement(r.A,null,"StandardScaler"),")."),"\n",i.createElement(t.li,null,"Instantiating ",i.createElement(r.A,null,"KNeighborsClassifier")," with your chosen hyperparameters (",i.createElement(c.A,{text:"\\( k \\) = 5"})," in this example)."),"\n",i.createElement(t.li,null,"Evaluating predictions with standard metrics."),"\n"),"\n",i.createElement(t.p,null,"For KNN regression, scikit-learn provides a ",i.createElement(r.A,null,"KNeighborsRegressor"),", used similarly but returning continuous outputs."),"\n",i.createElement(t.h2,{id:"use-cases",style:{position:"relative"}},i.createElement(t.a,{href:"#use-cases","aria-label":"use cases permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Use cases"),"\n",i.createElement(t.p,null,"Although KNN's naive variant can be slow for huge data sets, it remains a versatile tool in many domains. Here are some notable applications:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Image recognition and computer vision")),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(r.A,null,"Face recognition")," is a canonical example: one can embed faces into a lower-dimensional space (e.g., 128D from FaceNet) and then use KNN to classify or retrieve the identity."),"\n",i.createElement(t.li,null,i.createElement(r.A,null,"Content-based image retrieval")," often relies on KNN queries in a feature embedding space."),"\n"),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Recommendation systems")),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,'"User-based collaborative filtering" can be viewed as a form of KNN: find the users most similar to you, see which items they like, and recommend accordingly.'),"\n",i.createElement(t.li,null,"Item-based approaches also frequently rely on KNN among item embeddings."),"\n"),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Anomaly detection and fraud detection")),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"For credit-card transactions or intrusion detection, a KNN approach can isolate points that lack close neighbors in normal regions of the feature space. Weighted or distance-based outlier detection is related to KNN."),"\n"),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Healthcare and diagnostics")),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"KNN can be straightforwardly applied to diagnosing diseases from clinical metrics, gene expression data, or radiological images."),"\n",i.createElement(t.li,null,"Careful metric design or dimensionality reduction is often needed due to the complexity of medical data."),"\n"),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Other industry-specific scenarios")),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,'From retail analytics (predicting store demand by local "neighbors" of stores in feature space) to environmental modeling (predicting pollution at a site from the nearest sensor stations), KNN can be a building block.'),"\n"),"\n"),"\n"),"\n",i.createElement(t.hr),"\n",i.createElement(t.h2,{id:"extra-expansion-additional-theoretical-insights",style:{position:"relative"}},i.createElement(t.a,{href:"#extra-expansion-additional-theoretical-insights","aria-label":"extra expansion additional theoretical insights permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Extra expansion: Additional theoretical insights"),"\n",i.createElement(t.p,null,"Because the audience is advanced, it is worth highlighting a few deeper aspects of KNN theory:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Convergence to the Bayes optimal classifier")),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"For large ",i.createElement(c.A,{text:"\\( m \\)"})," and under certain assumptions (i.i.d. sampling, well-defined distance measure, etc.), ",i.createElement(r.A,null,"the KNN classifier's error will approach the Bayes error"),"."),"\n",i.createElement(t.li,null,"Specifically, the 1-NN classifier's error is bounded above by ",i.createElement(c.A,{text:"\\(2 \\times \\epsilon_{\\text{Bayes}}\\)"})," asymptotically."),"\n"),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Cover trees, vantage-point trees, and other data structures")),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"Beyond classic k-d trees, many specialized data structures exist to accelerate nearest neighbor searches in metric spaces with certain properties."),"\n"),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Prototype selection")),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,'Reducing the size of the training set to store only the most "informative" points can speed up inference dramatically. Algorithms like ',i.createElement(r.A,null,"Condensed Nearest Neighbor (CNN)"),", ",i.createElement(r.A,null,"Edited Nearest Neighbor (ENN)"),", and ",i.createElement(r.A,null,"STOLP")," systematically remove outliers or redundant points."),"\n"),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Relation to kernel density estimation")),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"Weighted KNN with a kernel that depends on distance is conceptually related to non-parametric density estimation. This close connection means you can interpret KNN in a probabilistic sense if you assume certain forms of local densities."),"\n"),"\n"),"\n"),"\n",i.createElement(t.hr),"\n",i.createElement(t.h2,{id:"extremely-extended-discussion-on-hnsw-and-large-scale-scenarios",style:{position:"relative"}},i.createElement(t.a,{href:"#extremely-extended-discussion-on-hnsw-and-large-scale-scenarios","aria-label":"extremely extended discussion on hnsw and large scale scenarios permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Extremely extended discussion on HNSW and large-scale scenarios"),"\n",i.createElement(t.p,null,"Because large-scale nearest neighbor queries are a big part of modern data science, we add more detail on ",i.createElement(r.A,null,"Hierarchical Navigable Small World (HNSW)"),":"),"\n",i.createElement(t.h3,{id:"main-idea",style:{position:"relative"}},i.createElement(t.a,{href:"#main-idea","aria-label":"main idea permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Main idea"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"Build a layered graph. The bottom layer (level 0) includes all points."),"\n",i.createElement(t.li,null,"Points also appear in higher layers with some probability ",i.createElement(c.A,{text:"\\( p \\)"}),"."),"\n",i.createElement(t.li,null,"Each point in a layer is connected to neighbors within that layer, using some bounding criterion on the number of edges."),"\n"),"\n",i.createElement(t.h3,{id:"searching-in-hnsw",style:{position:"relative"}},i.createElement(t.a,{href:"#searching-in-hnsw","aria-label":"searching in hnsw permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Searching in HNSW"),"\n",i.createElement(t.p,null,"When you query a new point ",i.createElement(c.A,{text:"\\( q \\)"}),":"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,"Start from a random entry point in the top layer."),"\n",i.createElement(t.li,null,"Greedily move to the neighbor that is closer to ",i.createElement(c.A,{text:"\\( q \\)"})," than your current node, until you can no longer improve."),"\n",i.createElement(t.li,null,"Descend to the layer below at that node, repeat the greedy search, now with possibly more candidates."),"\n",i.createElement(t.li,null,"Upon reaching the bottom layer, you carry out a final local search for the ",i.createElement(c.A,{text:"\\( k \\)"})," nearest neighbors."),"\n"),"\n",i.createElement(t.p,null,'Empirical results show that this yields extremely fast searches, with the "small world" property ensuring you do not get stuck in local minima too easily. The probability-based layering and the random links provide a near ',i.createElement(c.A,{text:"\\( O(\\log N) \\)"})," complexity in many real data sets, plus insertion or deletion of points is relatively efficient."),"\n",i.createElement(t.h3,{id:"real-world-example-face-recognition-at-scale",style:{position:"relative"}},i.createElement(t.a,{href:"#real-world-example-face-recognition-at-scale","aria-label":"real world example face recognition at scale permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Real-world example: Face recognition at scale"),"\n",i.createElement(t.p,null,"Consider a social network with ",i.createElement(c.A,{text:"\\( 10^{11} \\)"})," user images. Suppose each user face is embedded into a ",i.createElement(c.A,{text:"\\( 128 \\)"}),"-dimensional vector space. We can build an HNSW structure to store all those face embeddings. Then, for a new face, we:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,"Compute its embedding vector (using a deep CNN)."),"\n",i.createElement(t.li,null,"Perform an approximate KNN query in the HNSW to find the top few candidates."),"\n",i.createElement(t.li,null,"If the same user consistently appears in the top results, we predict that this face belongs to that user."),"\n"),"\n",i.createElement(t.p,null,"This approach is used in production systems at internet scale (Malkov and Yashunin, 2018)."),"\n",i.createElement(t.h2,{id:"final-remarks",style:{position:"relative"}},i.createElement(t.a,{href:"#final-remarks","aria-label":"final remarks permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Final remarks"),"\n",i.createElement(t.p,null,"The ",i.createElement(r.A,null,"k-nearest neighbors")," technique, while ancient in terms of machine learning history, remains relevant thanks to modern hardware, advanced indexing, and embedding-based distance metrics. It stands at the intersection of ",i.createElement(r.A,null,"simplicity")," and ",i.createElement(r.A,null,"potential complexity"),", bridging purely local, example-based reasoning with sophisticated large-scale system designs."),"\n",i.createElement(t.p,null,"KNN is conceptually easy to grasp, simple to implement, and surprisingly powerful with the right distance metric or feature embedding. Yet it is crucial to remember the computational burdens, the curse of dimensionality, and the careful engineering steps required for real-world, large-scale usage. By combining dimensionality reduction, approximate nearest neighbor techniques, and careful hyperparameter tuning, practitioners can push KNN well beyond its naive baseline and achieve state-of-the-art performance in many specialized tasks."),"\n",i.createElement(t.hr),"\n",i.createElement(t.h2,{id:"references-and-suggested-reading",style:{position:"relative"}},i.createElement(t.a,{href:"#references-and-suggested-reading","aria-label":"references and suggested reading permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"References and suggested reading"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Cover, T., & Hart, P. (1967).")," Nearest neighbor pattern classification. ",i.createElement(t.em,null,"IEEE Transactions on Information Theory"),", 13(1), 21–27."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Malkov, Yu. A., & Yashunin, D. A. (2018).")," Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs. ",i.createElement(t.em,null,"arXiv:1603.09320"),"."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Hastie, T., Tibshirani, R., & Friedman, J. (2009).")," ",i.createElement(t.em,null,"The Elements of Statistical Learning")," (2nd ed.). Springer. (A standard text for advanced machine learning and statistics, with coverage of KNN.)"),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Weinberger, K. Q., & Saul, L. K. (2009).")," Distance metric learning for large margin nearest neighbor classification. ",i.createElement(t.em,null,"Journal of Machine Learning Research"),". (Describes LMNN for learning metrics that improve KNN classification.)"),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"FaceNet:")," Schroff, F., Kalenichenko, D., & Philbin, J. (2015). FaceNet: A unified embedding for face recognition and clustering. In ",i.createElement(t.em,null,"CVPR"),"."),"\n"),"\n",i.createElement(t.hr),"\n",i.createElement(t.p,null,"This concludes our comprehensive treatment of the k-nearest neighbors algorithm — from the fundamental concept, through theoretical guarantees, to advanced approximate search structures such as HNSW, and on to practical considerations for real-world deployment."))}var m=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,a.RP)(),e.components);return t?i.createElement(t,e,i.createElement(o,e)):o(e)};var h=n(54506),d=n(88864),u=n(58481),p=n.n(u),g=n(5984),f=n(43672),v=n(27042),E=n(72031),y=n(81817),b=n(27105),w=n(17265),x=n(2043),N=n(95751),S=n(94328),k=n(80791),A=n(78137);const H=e=>{let{toc:t}=e;if(!t||!t.items)return null;return i.createElement("nav",{className:k.R},i.createElement("ul",null,t.items.map(((e,t)=>i.createElement("li",{key:t},i.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const n=t.replace("#",""),a=document.getElementById(n);a&&a.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&i.createElement(H,{toc:{items:e.items}}))))))};function M(e){let{data:{mdx:t,allMdx:l,allPostImages:r},children:s}=e;const{frontmatter:c,body:o,tableOfContents:m}=t,d=c.index,u=c.slug.split("/")[1],E=l.nodes.filter((e=>e.frontmatter.slug.includes(`/${u}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),k=E.findIndex((e=>e.frontmatter.index===d)),M=E[k+1],z=E[k-1],C=c.slug.replace(/\/$/,""),_=/[^/]*$/.exec(C)[0],T=`posts/${u}/content/${_}/`,{0:L,1:I}=(0,i.useState)(c.flagWideLayoutByDefault),{0:K,1:V}=(0,i.useState)(!1);var j;(0,i.useEffect)((()=>{V(!0);const e=setTimeout((()=>V(!1)),340);return()=>clearTimeout(e)}),[L]),"adventures"===u?j=w.cb:"research"===u?j=w.Qh:"thoughts"===u&&(j=w.T6);const B=p()(o).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,D=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),n=e%60;return n<=30?`~${t}${n>0?".5":""} h`:`~${t+1} h`}(Math.ceil(B/j)+(c.extraReadTimeMin||0)),P=[{flag:c.flagDraft,component:()=>Promise.all([n.e(5850),n.e(9833)]).then(n.bind(n,49833))},{flag:c.flagMindfuckery,component:()=>Promise.all([n.e(5850),n.e(7805)]).then(n.bind(n,27805))},{flag:c.flagRewrite,component:()=>Promise.all([n.e(5850),n.e(8916)]).then(n.bind(n,78916))},{flag:c.flagOffensive,component:()=>Promise.all([n.e(5850),n.e(6731)]).then(n.bind(n,49112))},{flag:c.flagProfane,component:()=>Promise.all([n.e(5850),n.e(3336)]).then(n.bind(n,83336))},{flag:c.flagMultilingual,component:()=>Promise.all([n.e(5850),n.e(2343)]).then(n.bind(n,62343))},{flag:c.flagUnreliably,component:()=>Promise.all([n.e(5850),n.e(6865)]).then(n.bind(n,11627))},{flag:c.flagPolitical,component:()=>Promise.all([n.e(5850),n.e(4417)]).then(n.bind(n,24417))},{flag:c.flagCognitohazard,component:()=>Promise.all([n.e(5850),n.e(8669)]).then(n.bind(n,18669))},{flag:c.flagHidden,component:()=>Promise.all([n.e(5850),n.e(8124)]).then(n.bind(n,48124))}],{0:O,1:W}=(0,i.useState)([]);return(0,i.useEffect)((()=>{P.forEach((e=>{let{flag:t,component:n}=e;t&&n().then((e=>{W((t=>[].concat((0,h.A)(t),[e.default])))}))}))}),[]),i.createElement(v.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},i.createElement(y.A,{postNumber:c.index,date:c.date,updated:c.updated,readTime:D,difficulty:c.difficultyLevel,title:c.title,desc:c.desc,banner:c.banner,section:u,postKey:_,isMindfuckery:c.flagMindfuckery,mainTag:c.mainTag}),i.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},c.otherTags.map(((e,t)=>i.createElement("span",{key:t,className:`noselect ${A.MW}`,style:{margin:"0 5px 5px 0"}},e)))),i.createElement("div",{className:"postBody"},i.createElement(H,{toc:m})),i.createElement("br",null),i.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},i.createElement(v.P.button,{className:`noselect ${S.pb}`,id:S.xG,onClick:()=>{I(!L)},whileTap:{scale:.93}},i.createElement(v.P.div,{className:N.DJ,key:L,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},L?"Switch to default layout":"Switch to wide layout"))),i.createElement("br",null),i.createElement("div",{className:"postBody",style:{margin:L?"0 -14%":"",maxWidth:L?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},i.createElement("div",{className:`${S.P_} ${K?S.Xn:S.qG}`},O.map(((e,t)=>i.createElement(e,{key:t}))),c.indexCourse?i.createElement(x.A,{index:c.indexCourse,category:c.courseCategoryName}):"",i.createElement(g.Z.Provider,{value:{images:r.nodes,basePath:T.replace(/\/$/,"")+"/"}},i.createElement(a.xA,{components:{Image:f.A}},s)))),i.createElement(b.A,{nextPost:M,lastPost:z,keyCurrent:_,section:u}))}function z(e){return i.createElement(M,e,i.createElement(m,e))}function C(e){var t,n,a,l,r;let{data:s}=e;const{frontmatter:c}=s.mdx,o=c.titleSEO||c.title,m=c.titleOG||o,h=c.titleTwitter||o,u=c.descSEO||c.desc,p=c.descOG||u,g=c.descTwitter||u,f=c.schemaType||"BlogPosting",v=c.keywordsSEO,y=c.date,b=c.updated||y,w=c.imageOG||(null===(t=c.banner)||void 0===t||null===(n=t.childImageSharp)||void 0===n||null===(a=n.gatsbyImageData)||void 0===a||null===(l=a.images)||void 0===l||null===(r=l.fallback)||void 0===r?void 0:r.src),x=c.imageAltOG||p,N=c.imageTwitter||w,S=c.imageAltTwitter||g,k=c.canonicalURL,A=c.flagHidden||!1,H=c.mainTag||"Posts",M=c.slug.split("/")[1]||"posts",{siteUrl:z}=(0,d.Q)(),C={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:z},{"@type":"ListItem",position:2,name:H,item:`${z}/${c.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:o,item:`${z}${c.slug}`}]};return i.createElement(E.A,{title:o+" - avrtt.blog",titleOG:m,titleTwitter:h,description:u,descriptionOG:p,descriptionTwitter:g,schemaType:f,keywords:v,datePublished:y,dateModified:b,imageOG:w,imageAltOG:x,imageTwitter:N,imageAltTwitter:S,canonicalUrl:k,flagHidden:A,mainTag:H,section:M,type:"article"},i.createElement("script",{type:"application/ld+json"},JSON.stringify(C)))}}}]);
//# sourceMappingURL=component---src-templates-post-tsx-content-file-path-src-pages-posts-research-knn-mdx-16b1caee06a7cfdebab4.js.map