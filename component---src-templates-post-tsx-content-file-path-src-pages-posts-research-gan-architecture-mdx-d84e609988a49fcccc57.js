"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[5296],{3962:function(e,t){t.A="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iaXNvLTg4NTktMSI/Pgo8IS0tIEdlbmVyYXRvcjogQWRvYmUgSWxsdXN0cmF0b3IgMTYuMC4wLCBTVkcgRXhwb3J0IFBsdWctSW4gLiBTVkcgVmVyc2lvbjogNi4wMCBCdWlsZCAwKSAgLS0+CjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+CjxzdmcgdmVyc2lvbj0iMS4xIiBpZD0iQ2FwYV8xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB4PSIwcHgiIHk9IjBweCIKCSB3aWR0aD0iNDE2Ljk3OXB4IiBoZWlnaHQ9IjQxNi45NzlweCIgdmlld0JveD0iMCAwIDQxNi45NzkgNDE2Ljk3OSIgc3R5bGU9ImVuYWJsZS1iYWNrZ3JvdW5kOm5ldyAwIDAgNDE2Ljk3OSA0MTYuOTc5OyIKCSB4bWw6c3BhY2U9InByZXNlcnZlIj4KPGc+Cgk8cGF0aCBkPSJNMzU2LjAwNCw2MS4xNTZjLTgxLjM3LTgxLjQ3LTIxMy4zNzctODEuNTUxLTI5NC44NDgtMC4xODJjLTgxLjQ3LDgxLjM3MS04MS41NTIsMjEzLjM3OS0wLjE4MSwyOTQuODUKCQljODEuMzY5LDgxLjQ3LDIxMy4zNzgsODEuNTUxLDI5NC44NDksMC4xODFDNDM3LjI5MywyNzQuNjM2LDQzNy4zNzUsMTQyLjYyNiwzNTYuMDA0LDYxLjE1NnogTTIzNy42LDM0MC43ODYKCQljMCwzLjIxNy0yLjYwNyw1LjgyMi01LjgyMiw1LjgyMmgtNDYuNTc2Yy0zLjIxNSwwLTUuODIyLTIuNjA1LTUuODIyLTUuODIyVjE2Ny44ODVjMC0zLjIxNywyLjYwNy01LjgyMiw1LjgyMi01LjgyMmg0Ni41NzYKCQljMy4yMTUsMCw1LjgyMiwyLjYwNCw1LjgyMiw1LjgyMlYzNDAuNzg2eiBNMjA4LjQ5LDEzNy45MDFjLTE4LjYxOCwwLTMzLjc2Ni0xNS4xNDYtMzMuNzY2LTMzLjc2NQoJCWMwLTE4LjYxNywxNS4xNDctMzMuNzY2LDMzLjc2Ni0zMy43NjZjMTguNjE5LDAsMzMuNzY2LDE1LjE0OCwzMy43NjYsMzMuNzY2QzI0Mi4yNTYsMTIyLjc1NSwyMjcuMTA3LDEzNy45MDEsMjA4LjQ5LDEzNy45MDF6Ii8+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPC9zdmc+Cg=="},9360:function(e,t,a){a.d(t,{A:function(){return l}});var n=a(96540),i=a(3962),r="styles-module--tooltiptext--a263b";var l=e=>{let{text:t,isBadge:a=!1}=e;const{0:l,1:o}=(0,n.useState)(!1),s=(0,n.useRef)(null);return(0,n.useEffect)((()=>{function e(e){s.current&&e.target instanceof Node&&!s.current.contains(e.target)&&o(!1)}return document.addEventListener("click",e),()=>{document.removeEventListener("click",e)}}),[]),n.createElement("span",{className:"styles-module--tooltipWrapper--75ebf",ref:s},n.createElement("img",{id:a?"styles-module--infoBadge--e3d66":"styles-module--info--26c1f",src:i.A,alt:"info",onClick:e=>{e.stopPropagation(),o((e=>!e))}}),n.createElement("span",{className:l?`${r} styles-module--visible--c063c`:r},t))}},36425:function(e,t,a){a.r(t),a.d(t,{Head:function(){return M},PostTemplate:function(){return G},default:function(){return T}});var n=a(28453),i=a(96540),r=a(61992),l=a(62087),o=a(90548),s=a(9360);function c(e){const t=Object.assign({p:"p",h3:"h3",a:"a",span:"span",ol:"ol",li:"li",hr:"hr",h2:"h2",ul:"ul",strong:"strong"},(0,n.RP)(),e.components),{Image:a}=t;return a||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),i.createElement(i.Fragment,null,"\n",i.createElement("br"),"\n","\n","\n",i.createElement(t.p,null,"Over the last decade, the field of deep generative modeling has surged in popularity, fueled in large part by breakthroughs in computational power, the availability of large-scale datasets, and methodological innovations within the machine learning community. In particular, generative models attempt to learn complex data distributions — often images, but also text, audio, and beyond — directly from raw datasets without needing explicit supervision for each sample. One of the most influential innovations in this space has been the introduction of ",i.createElement(r.A,null,"Generative Adversarial Networks")," (GANs), a family of methods that harness a two-player game between two neural networks in order to produce extremely realistic outputs that can rival real data in many domains."),"\n",i.createElement(t.p,null,"GANs address a fundamental challenge in generative modeling: how to efficiently train neural networks to create novel examples that belong to a certain distribution. Traditional generative approaches typically relied on direct density estimation or variational lower bounds on complex distributions, which often led to fuzzy samples or suboptimal training objectives. In contrast, GANs formulate an innovative scheme wherein a ",i.createElement(r.A,null,"generator")," aims to produce data samples that will fool an adversarially trained ",i.createElement(r.A,null,"discriminator"),". By learning to distinguish real from generated data, the discriminator provides the generator with a strong training signal about which aspects of the data distribution remain underrepresented or incorrectly modeled. This dynamic is at the heart of GANs' power."),"\n",i.createElement(t.p,null,"Ever since the publication of the seminal GAN paper by Ian Goodfellow and colleagues in 2014 (Goodfellow and gang, NeurIPS 2014), the machine learning community has witnessed an explosion in the use of adversarial training for tasks such as image synthesis, data augmentation, domain adaptation, and countless other applications where realistic data generation is a critical step. In parallel, the theoretical underpinnings of GANs have encouraged significant research into game-theoretic perspectives on machine learning, divergence measures, and robust optimization paradigms."),"\n",i.createElement(t.p,null,"By empowering a generator network to learn an implicit data distribution, GANs achieve results that are sometimes strikingly photorealistic. This success has established GANs as a leading class of generative algorithms, often surpassing alternative methods in generating samples of exceptional detail and clarity. Given these strengths, GANs have come to play a prominent role not just in image-based tasks (e.g., high-fidelity face generation, artwork synthesis, super-resolution) but also in other modalities, including music generation, speech synthesis, and even in reinforcement learning environments for sim-to-real transfer."),"\n",i.createElement(t.h3,{id:"historical-development-of-gans",style:{position:"relative"}},i.createElement(t.a,{href:"#historical-development-of-gans","aria-label":"historical development of gans permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Historical development of GANs"),"\n",i.createElement(t.p,null,"The original GAN framework introduced by Goodfellow and gang in 2014 laid out the idea of a minimax game between a generator ",i.createElement(o.A,{text:"\\( G \\)"})," and a discriminator ",i.createElement(o.A,{text:"\\( D \\)"}),". This approach quickly captured the attention of researchers who recognized the potential of adversarial training to circumvent some of the limitations of approaches like Variational Autoencoders (VAEs). Early successes were modest — the initial architectures were often small fully connected networks or simple convolutional structures, and the challenges were numerous: vanishing gradients, mode collapse, training instability, and sensitivity to hyperparameters, to name a few."),"\n",i.createElement(t.p,null,"A major milestone was the development of DCGAN (Deep Convolutional GAN) by Radford and gang (ICLR 2016), demonstrating that purely convolutional generator and discriminator networks could stabilize training substantially and yield sharper, higher-resolution images than the original formulations. This was followed by a flurry of work: LAPGAN, f-GAN, EBGAN, and many others that explored novel divergences, objective functions, and architectural variations. The introduction of Wasserstein GAN (WGAN) by Arjovsky and gang (ICML 2017) represented another breakthrough, as it replaced the Jensen–Shannon Divergence with the Earth Mover's (Wasserstein) distance for the training objective, aiming to mitigate issues like mode collapse and introduce smoother gradients. Enhancements like WGAN-GP (Gulrajani and gang, NeurIPS 2017) helped address gradient vanishing or explosion by introducing gradient penalties."),"\n",i.createElement(t.p,null,"Later on, self-attention mechanisms were integrated into GANs (SAGAN and BigGAN), significantly improving high-resolution image generation performance by allowing long-range dependencies. Progress in controlling and stabilizing large-scale GANs opened the door to methods such as ProGAN (Progressive Growing of GANs) for incremental resolution training, and eventually StyleGAN (Karras and gang, CVPR 2019, 2020) for high-fidelity, controllable image synthesis. Today, StyleGAN variants remain some of the most successful generative image models for high-resolution tasks, while the field as a whole continues to expand to text, 3D object generation, and multi-modal data."),"\n",i.createElement(t.h3,{id:"course-relevance-and-objectives",style:{position:"relative"}},i.createElement(t.a,{href:"#course-relevance-and-objectives","aria-label":"course relevance and objectives permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Course relevance and objectives"),"\n",i.createElement(t.p,null,"This article aims to give a detailed overview of the core ideas and architectures that define modern GAN frameworks and to equip advanced machine learning practitioners with the theoretical and practical grounding needed to build, evaluate, and improve GAN models in real projects. Mastering GANs involves not only understanding the adversarial objective and the role of the generator and discriminator but also appreciating the delicate interplay that emerges during training."),"\n",i.createElement(t.p,null,"By the end of this reading, I hope you will:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,"Possess an in-depth understanding of adversarial training dynamics and how they differ from other generative modeling paradigms."),"\n",i.createElement(t.li,null,"Recognize key architectural choices for both the generator and discriminator, including insights that have improved training stability over time."),"\n",i.createElement(t.li,null,"Be able to implement and debug a GAN pipeline in a deep learning framework, following best practices for hyperparameter selection, dataset preparation, and experiment logging."),"\n",i.createElement(t.li,null,"Gain a sense of where GAN research currently stands, how it interacts with other contemporary methods (such as diffusion models), and where it might be heading in the future."),"\n"),"\n",i.createElement(t.p,null,"GAN concepts tie into broader machine learning applications in many ways. Whether you need advanced data augmentation solutions, desire creative image-to-image translation capabilities, or plan to push the envelope on large-scale generative modeling, the adversarial paradigm can be a powerful piece of your toolkit. Let us now explore the fundamentals of how two neural networks — the generator and the discriminator — can learn from and challenge each other to produce astonishingly realistic and diverse data samples."),"\n",i.createElement(t.hr),"\n",i.createElement(t.h2,{id:"fundamentals-of-generative-adversarial-networks",style:{position:"relative"}},i.createElement(t.a,{href:"#fundamentals-of-generative-adversarial-networks","aria-label":"fundamentals of generative adversarial networks permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Fundamentals of generative adversarial networks"),"\n",i.createElement(t.h3,{id:"overview-of-the-adversarial-framework",style:{position:"relative"}},i.createElement(t.a,{href:"#overview-of-the-adversarial-framework","aria-label":"overview of the adversarial framework permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Overview of the adversarial framework"),"\n",i.createElement(t.p,null,"At the heart of a GAN is the notion of a ",i.createElement(r.A,null,"two-player minimax game")," between two neural networks: the generator ",i.createElement(o.A,{text:"\\( G \\)"})," and the discriminator ",i.createElement(o.A,{text:"\\( D \\)"}),". The generator, ",i.createElement(o.A,{text:"\\( G \\)"}),", takes as input a random sample from a latent distribution, often denoted by a latent variable ",i.createElement(o.A,{text:"\\( z \\sim p_z(z) \\)"}),", where ",i.createElement(o.A,{text:"\\( p_z(z) \\)"})," is typically chosen to be a simple prior distribution such as a Gaussian or uniform. Through a series of transformations (convolutions, fully connected layers, etc.), ",i.createElement(o.A,{text:"\\( G \\)"})," produces an output intended to mimic a sample from the real data distribution ",i.createElement(o.A,{text:"\\( p_{\\text{data}}(x) \\)"}),". Meanwhile, the discriminator, ",i.createElement(o.A,{text:"\\( D \\)"}),", is trained to distinguish whether a given sample is real or generated."),"\n",i.createElement(t.p,null,"Formally, the training objective can be expressed in the following form from the original GAN paper:"),"\n",i.createElement(o.A,{text:"\\[\n\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\bigl[ \\log D(x) \\bigr] + \\mathbb{E}_{z \\sim p_z(z)} \\bigl[ \\log \\bigl( 1 - D(G(z)) \\bigr) \\bigr].\n\\]"}),"\n",i.createElement(t.p,null,"Here,"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(o.A,{text:"\\( x \\)"})," represents real data samples from the data distribution ",i.createElement(o.A,{text:"\\( p_{\\text{data}}(x) \\)"}),"."),"\n",i.createElement(t.li,null,i.createElement(o.A,{text:"\\( z \\)"})," is a latent variable (often low-dimensional) drawn from a prior distribution ",i.createElement(o.A,{text:"\\( p_z(z) \\)"}),"."),"\n",i.createElement(t.li,null,i.createElement(o.A,{text:"\\( G \\)"})," maps ",i.createElement(o.A,{text:"\\( z \\)"})," to the data space, i.e. ",i.createElement(o.A,{text:"\\( G(z) \\)"}),"."),"\n",i.createElement(t.li,null,i.createElement(o.A,{text:"\\( D(x) \\)"})," outputs a scalar between 0 and 1 indicating the probability that ",i.createElement(o.A,{text:"\\( x \\)"})," is a real sample rather than a generated one."),"\n"),"\n",i.createElement(t.p,null,"In this game, ",i.createElement(o.A,{text:"\\( D \\)"})," tries to maximize the probability of correctly labeling real samples as real and generated samples as fake, while ",i.createElement(o.A,{text:"\\( G \\)"})," tries to minimize the ability of ",i.createElement(o.A,{text:"\\( D \\)"})," to distinguish ",i.createElement(o.A,{text:"\\( G(z) \\)"})," from real data. Over training, if ",i.createElement(o.A,{text:"\\( G \\)"})," and ",i.createElement(o.A,{text:"\\( D \\)"})," are balanced in capacity and well-tuned hyperparameters are chosen, the generator converges to producing samples that are nearly indistinguishable from real data."),"\n",i.createElement(t.h3,{id:"key-components-generator-and-discriminator",style:{position:"relative"}},i.createElement(t.a,{href:"#key-components-generator-and-discriminator","aria-label":"key components generator and discriminator permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Key components: generator and discriminator"),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Generator"),': The generator transforms noise in a latent space into samples that ideally come from the same distribution as the real data. Intuitively, one can think of the generator as an "artist" trying to imitate the style of the real data distribution. The generator\'s parameters adapt based on signals from the discriminator, which tells it which generated samples still appear fake.'),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Discriminator"),": The discriminator is a binary classifier that takes in a data sample (either from the real dataset or produced by the generator) and outputs a value in ",i.createElement(o.A,{text:"\\([0,1]\\)"}),'. The goal is to output a higher probability for real samples and a lower probability for generated ones. Essentially, the discriminator represents an "art critic" attempting to distinguish forgeries from authentic works, thus providing the feedback mechanism the generator needs in order to improve.'),"\n",i.createElement(t.h3,{id:"important-mathematical-foundations",style:{position:"relative"}},i.createElement(t.a,{href:"#important-mathematical-foundations","aria-label":"important mathematical foundations permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Important mathematical foundations"),"\n",i.createElement(t.p,null,"GANs have strong connections to fundamental divergence measures in probability theory, especially the Kullback–Leibler Divergence (KL Divergence) and Jensen–Shannon Divergence (JSD). In particular, the original GAN paper showed that the minimax objective can be interpreted as a process of minimizing the Jensen–Shannon Divergence between the real data distribution ",i.createElement(o.A,{text:"\\( p_{\\text{data}} \\)"})," and the distribution implicitly defined by ",i.createElement(o.A,{text:"\\( G \\)"}),"."),"\n",i.createElement(t.p,null,"In practice, the non-saturating version of the GAN objective is often used to address vanishing gradients. This variant modifies the generator's loss to:"),"\n",i.createElement(o.A,{text:"\\[\n\\min_G \\mathcal{L}_G = \\mathbb{E}_{z \\sim p_z(z)} \\bigl[ - \\log D(G(z)) \\bigr].\n\\]"}),"\n",i.createElement(t.p,null,"This alternative fosters stronger gradients for the generator when the discriminator easily rejects generated samples. Many other variants of the GAN objective exist (least-squares GAN, hinge loss, Wasserstein distance-based) to address issues like training instability, mode collapse, and gradient vanishing/explosion."),"\n",i.createElement(t.h3,{id:"link-to-other-generative-models",style:{position:"relative"}},i.createElement(t.a,{href:"#link-to-other-generative-models","aria-label":"link to other generative models permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Link to other generative models"),"\n",i.createElement(t.p,null,"Before GANs, many generative models relied heavily on explicit density estimation. Models such as ",i.createElement(s.A,{text:"Variational Autoencoders"})," (VAEs) optimize a variational lower bound that enforces a compressed latent representation while encouraging samples to be consistent with the observed data. Flow-based models (e.g., NICE, RealNVP, Glow) leverage invertible transformations to learn a direct mapping to a base distribution with known density."),"\n",i.createElement(t.p,null,"GANs differ in that they ",i.createElement(r.A,null,"do not explicitly model the density"),". Instead, they learn a transformation from the latent space to the data space by playing the adversarial game. This approach often yields higher-quality samples compared to methods that rely on likelihood-based training, though it provides fewer direct means to estimate exact likelihoods or measure coverage of the distribution."),"\n",i.createElement(t.p,null,"Some papers (e.g., Larsen and gang, 2016) have explored hybrid approaches combining VAEs and GANs in a single framework to leverage the representation learning strengths of VAEs and the sample-quality strengths of GANs. Throughout this article, I will reference such existing work whenever it clarifies the underlying ideas and highlights areas where practitioners can further refine or adapt adversarial training to specific tasks."),"\n",i.createElement(t.hr),"\n",i.createElement(t.h2,{id:"generator-architecture",style:{position:"relative"}},i.createElement(t.a,{href:"#generator-architecture","aria-label":"generator architecture permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Generator architecture"),"\n",i.createElement(t.h3,{id:"core-design-principles",style:{position:"relative"}},i.createElement(t.a,{href:"#core-design-principles","aria-label":"core design principles permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Core design principles"),"\n",i.createElement(t.p,null,"The generator typically starts with a ",i.createElement(r.A,null,"latent vector")," drawn from a simple distribution (e.g., a Gaussian ",i.createElement(o.A,{text:"\\( \\mathcal{N}(0, I) \\)"}),'). The generator must then progressively "decode" this latent representation into a sample in the original data space. For image generation tasks, this frequently involves a stack of transposed convolutional layers (sometimes referred to as deconvolutions or fractionally strided convolutions) that successively upsample the feature maps until they reach the desired output resolution.'),"\n",i.createElement(t.p,null,"This approach to upsampling stands in contrast to more classical ideas of using fully connected layers to project from the latent space directly into a high-dimensional pixel space. Using convolutional layers can help incorporate local spatial dependencies and helps produce sharper images with more structure. The generator is expected to produce samples that contain subtle details, textures, and shapes, so capturing these spatial relationships is key."),"\n",i.createElement(t.p,null,"The transposed convolution approach (ConvTranspose2D in many deep learning frameworks) ensures that learned filters can handle more nuanced patterns, as opposed to naive upsampling with fixed interpolation kernels. However, if not carefully designed, transposed convolutions can lead to checkerboard artifacts or other undesirable patterns. Hence, advanced generator architectures pay careful attention to the interplay of kernel sizes, strides, and padding during the upsampling process."),"\n",i.createElement(t.h3,{id:"common-layers-and-modules",style:{position:"relative"}},i.createElement(t.a,{href:"#common-layers-and-modules","aria-label":"common layers and modules permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Common layers and modules"),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Convolutional layers"),": For image-centric tasks, the generator often employs strided transposed convolutions with carefully selected kernel sizes and strides that smoothly scale feature map dimensions."),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Batch normalization"),": Introduced by Ioffe and Szegedy (ICML 2015), batch normalization helps stabilize training by normalizing the activations. In a GAN context, it can reduce mode collapse (where the generator produces a limited variety of outputs) and help ensure more consistent gradients."),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Activation functions"),": ReLU or LeakyReLU are the most frequently used activations in the generator. LeakyReLU can help propagate gradients in cases where the standard ReLU might saturate. For the output layer, a Tanh activation is common for image-related tasks when inputs are normalized to a ",i.createElement(o.A,{text:"\\([-1,1]\\)"})," range."),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Skip connections and residual blocks"),": Drawing from the success of residual networks in other vision tasks, some newer GAN architectures incorporate skip connections to facilitate gradient flow and enable deeper generator networks. Progressive Growing of GANs (ProGAN) and StyleGAN exploit the notion of incrementally growing the output resolution, effectively turning the architecture into a hierarchical approach to generation, which helps produce extremely high-resolution and coherent images."),"\n",i.createElement(a,{alt:"Generator structure",path:"",caption:"An illustration of a typical generator architecture for image generation tasks.",zoom:"false"}),"\n",i.createElement(t.h3,{id:"advanced-structural-variations",style:{position:"relative"}},i.createElement(t.a,{href:"#advanced-structural-variations","aria-label":"advanced structural variations permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Advanced structural variations"),"\n",i.createElement(t.p,null,"In some recent GAN architectures, the generator includes ",i.createElement(r.A,null,"self-attention")," layers to capture global relationships within the generated image. This helps the model consistently place features throughout the generated samples; for instance, in SAGAN (Self-Attention GAN), attention modules let the network focus on distant but related spatial regions."),"\n",i.createElement(t.p,null,"Another notable innovation is the incorporation of ",i.createElement(r.A,null,"adaptive instance normalization (AdaIN)")," layers, especially in StyleGAN, to achieve better control over style or other high-level attributes in the generated output. This approach modulates the generator's feature maps using statistics derived from style vectors. The result is an unprecedented level of manipulative control, letting one disentangle high-level concepts (pose, shape, semantic attributes) from low-level details (texture, color, lighting)."),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Progressive approaches"),": ProGAN proposed an incremental method to train the generator and discriminator starting from a low resolution (e.g., 4x4) and gradually increasing to a very high resolution (e.g., 1024x1024). This strategy helps stabilize training because the model first learns a coarse approximation and later refines details at higher resolutions."),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Hierarchical generation"),": For larger images, some architectures break the generation process into multiple stages. For instance, a generator could first produce a rough shape or layout at a lower resolution, then refine details in further stages. Hierarchical VAE-GAN hybrids exemplify such multi-stage designs, with each stage focusing on different aspects of image fidelity or structure."),"\n",i.createElement(t.h3,{id:"role-of-the-latent-space",style:{position:"relative"}},i.createElement(t.a,{href:"#role-of-the-latent-space","aria-label":"role of the latent space permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Role of the latent space"),"\n",i.createElement(t.p,null,"The latent vector ",i.createElement(o.A,{text:"\\( z \\)"})," essentially acts as the creative seed. By sampling different ",i.createElement(o.A,{text:"\\( z \\)"})," values from the prior distribution, the generator can produce a wide range of outputs. A properly trained generator maps distinct directions in latent space to different aspects of variation in the data distribution — for instance, controlling attributes like the orientation of a face, the color of hair, or the background environment in an image generation context."),"\n",i.createElement(t.p,null,"In certain applications, researchers may depart from the default Gaussian prior to incorporate more structured priors or even ",i.createElement(s.A,{text:"Variational inference techniques"})," that bring domain-specific knowledge to the generation process. Additionally, some designs use noise injection at multiple points within the generator rather than just at the input layer, which can help produce more complex patterns and reduce mode collapse by providing a continuous injection of randomness throughout the upsampling process."),"\n",i.createElement(t.hr),"\n",i.createElement(t.h2,{id:"discriminator-architecture",style:{position:"relative"}},i.createElement(t.a,{href:"#discriminator-architecture","aria-label":"discriminator architecture permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Discriminator architecture"),"\n",i.createElement(t.h3,{id:"core-design-principles-1",style:{position:"relative"}},i.createElement(t.a,{href:"#core-design-principles-1","aria-label":"core design principles 1 permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Core design principles"),"\n",i.createElement(t.p,null,"The discriminator typically mirrors the generator's approach with a downsampling network. Instead of transposed convolutions, the discriminator uses regular convolutions to reduce the input dimension step by step, ideally compressing the sample into a scalar that indicates whether it is real or generated. By successively mapping an input image to a lower dimensional embedding, the discriminator's goal is to discover the informative features that best separate real examples from generated ones."),"\n",i.createElement(t.p,null,'Like any classifier, the discriminator is trained via gradient-based optimization of a loss function (the adversarial loss). A key difference, though, is that this classifier must handle a continuously evolving distribution of "fake" samples coming from the generator. This dynamic environment demands robust design choices and stable hyperparameter settings to avoid overfitting to the generator\'s current state.'),"\n",i.createElement(t.h3,{id:"typical-layer-structures-for-classification",style:{position:"relative"}},i.createElement(t.a,{href:"#typical-layer-structures-for-classification","aria-label":"typical layer structures for classification permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Typical layer structures for classification"),"\n",i.createElement(t.p,null,"The standard practice — popularized by DCGAN — is to adopt a series of convolutional blocks where each block may be:"),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Conv → BatchNorm → LeakyReLU")),"\n",i.createElement(t.p,null,"with the occasional downsampling layer (strided convolution) or pooling operation. LeakyReLU is used instead of ReLU to keep gradients flowing for negative inputs. Typically, the final layer outputs a single scalar (or a patch-based array in some variants like PatchGAN from the pix2pix framework), representing the discriminator's confidence that the input is real."),"\n",i.createElement(t.p,null,'Using deeper networks can improve the discriminator\'s ability to detect finer differences between real and generated data, but it can also cause training to become more imbalanced if the discriminator becomes "too strong" too quickly. One must carefully tune the interplay with the generator capacity to avoid a scenario where the generator sees little to no meaningful gradient feedback.'),"\n",i.createElement(a,{alt:"Discriminator structure",path:"",caption:"A representation of how an input image is downsampled in the discriminator until it outputs a single real/fake probability.",zoom:"false"}),"\n",i.createElement(t.h3,{id:"handling-real-vs-generated-data",style:{position:"relative"}},i.createElement(t.a,{href:"#handling-real-vs-generated-data","aria-label":"handling real vs generated data permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Handling real vs. generated data"),"\n",i.createElement(t.p,null,"When training the discriminator, each batch typically consists of real samples (labeled as real) and generated samples (labeled as fake). A few techniques often come into play:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Label smoothing"),": Instead of labeling real samples with a ground truth label of 1, one might use 0.9 or 0.95 to prevent the discriminator from becoming overconfident. This helps reduce overfitting and can lead to more stable gradients for the generator."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Noisy labels"),": In some cases, random label flipping or adding noise to labels can help the discriminator remain robust and less prone to overfitting."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Minibatch discrimination"),": The discriminator can incorporate features derived from the entire batch rather than single samples in isolation. This technique detects if the generator is producing samples that, on their own, might appear plausible but lack diversity across the batch."),"\n"),"\n",i.createElement(t.h3,{id:"techniques-to-improve-discriminative-power",style:{position:"relative"}},i.createElement(t.a,{href:"#techniques-to-improve-discriminative-power","aria-label":"techniques to improve discriminative power permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Techniques to improve discriminative power"),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Spectral normalization"),": This method constrains the Lipschitz constant of the discriminator by normalizing the weight matrices. Originally introduced for WGAN variants, spectral normalization has proven effective in stabilizing training across a range of GAN formulations."),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Self-attention"),": Just as the generator can benefit from focusing on relationships between distant spatial areas, so can the discriminator. Integrating attention allows the discriminator to verify the global consistency of an image, verifying that far-apart details (like a person's face and background elements) match realistically."),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Gradient penalties"),": Techniques such as WGAN-GP apply a penalty on the gradient norm of the discriminator's output with respect to its input. This further enforces Lipschitz continuity and helps reduce mode collapse."),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Patch-based discrimination"),": Instead of producing a single global real/fake judgment, patch-based discriminators output a grid of local real/fake predictions. This approach, first widely used in image-to-image translation tasks (pix2pix, CycleGAN), can enhance local detail fidelity in generated samples."),"\n",i.createElement(t.hr),"\n",i.createElement(t.h2,{id:"training-process-and-key-techniques",style:{position:"relative"}},i.createElement(t.a,{href:"#training-process-and-key-techniques","aria-label":"training process and key techniques permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Training process and key techniques"),"\n",i.createElement(t.h3,{id:"minimax-objective-and-loss-functions",style:{position:"relative"}},i.createElement(t.a,{href:"#minimax-objective-and-loss-functions","aria-label":"minimax objective and loss functions permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Minimax objective and loss functions"),"\n",i.createElement(t.p,null,"The original GAN training objective is a minimax game. The discriminator's goal is to maximize:"),"\n",i.createElement(o.A,{text:"\\[\n\\mathcal{L}_D = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\bigl[\\log D(x)\\bigr] + \n\\mathbb{E}_{z \\sim p_z(z)} \\bigl[\\log\\bigl(1 - D(G(z))\\bigr)\\bigr],\n\\]"}),"\n",i.createElement(t.p,null,"while the generator's goal is to minimize:"),"\n",i.createElement(o.A,{text:"\\[\n\\mathcal{L}_G = \\mathbb{E}_{z \\sim p_z(z)} \\bigl[\\log\\bigl(1 - D(G(z))\\bigr)\\bigr],\n\\]"}),"\n",i.createElement(t.p,null,"although practically the non-saturating version is used for generator training, as mentioned before:"),"\n",i.createElement(o.A,{text:"\\[\n\\mathcal{L}_{G,\\text{non-sat}} = \\mathbb{E}_{z \\sim p_z(z)} \\bigl[-\\log D(G(z))\\bigr].\n\\]"}),"\n",i.createElement(t.p,null,"Alternative losses include ",i.createElement(r.A,null,"Least Squares GAN (LSGAN)"),', which replaces the binary cross-entropy losses with a least-squares criterion, thereby encouraging the discriminator to not only separate real from fake but also measure the "distance" between them. Another prominent variation is the ',i.createElement(r.A,null,"Wasserstein GAN (WGAN)")," loss:"),"\n",i.createElement(o.A,{text:"\\[\n\\min_G \\max_D \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\bigl[D(x)\\bigr] - \n\\mathbb{E}_{z \\sim p_z(z)} \\bigl[D(G(z))\\bigr],\n\\]"}),"\n",i.createElement(t.p,null,"where ",i.createElement(o.A,{text:"\\( D \\)"})," outputs a real-valued score (instead of a probability) and the Earth Mover's distance is used as the measure of discrepancy between ",i.createElement(o.A,{text:"\\( p_{\\text{data}} \\)"})," and the generator distribution. WGAN addresses the problem that the JSD may not provide meaningful gradients when the data distributions are disjoint."),"\n",i.createElement(t.h3,{id:"approaches-to-stabilize-training",style:{position:"relative"}},i.createElement(t.a,{href:"#approaches-to-stabilize-training","aria-label":"approaches to stabilize training permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Approaches to stabilize training"),"\n",i.createElement(t.p,null,"Stability is one of the greatest challenges in GAN research. Unstable training typically manifests as:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Mode collapse"),": The generator outputs a narrow range of samples, ignoring large regions of the real distribution."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Vanishing or exploding gradients"),": The learning signals weaken drastically or blow up, preventing effective optimization."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Discriminator overpowering"),": The discriminator quickly converges to near-perfect accuracy, giving the generator almost no signal to improve."),"\n"),"\n",i.createElement(t.p,null,"Prominent strategies to combat these issues include:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Wasserstein distance"),": As in WGAN, providing a smoother distance measure encourages continuous improvements to the generator."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Gradient penalty"),": WGAN-GP modifies WGAN by penalizing the norm of the discriminator's gradients to maintain Lipschitz continuity."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Orthogonal regularization"),": Some advanced techniques adopt orthogonal constraints on network weights to reduce degenerate solutions."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Two-time-scale updates"),": Some training recipes allow the discriminator to update multiple times per generator update or vice versa, ensuring a balanced improvement pace between the two adversaries."),"\n"),"\n",i.createElement(t.h3,{id:"balancing-generator-and-discriminator-performance",style:{position:"relative"}},i.createElement(t.a,{href:"#balancing-generator-and-discriminator-performance","aria-label":"balancing generator and discriminator performance permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Balancing generator and discriminator performance"),"\n",i.createElement(t.p,null,"I often stress the importance of monitoring the relative performance of ",i.createElement(o.A,{text:"\\( G \\)"})," and ",i.createElement(o.A,{text:"\\( D \\)"}),". If the discriminator is too strong, it might consistently yield near-1 for real samples and near-0 for generated samples, effectively saturating the generator's gradient. If the generator is too strong, it might easily fool the discriminator, causing the discriminator to provide uninformative signals."),"\n",i.createElement(t.p,null,"Common techniques to balance performance:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Learning rate tuning"),": Using separate learning rates for generator and discriminator can help keep training synchronized."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Training frequency"),": Sometimes the discriminator updates more frequently than the generator, or vice versa. In WGAN, it is recommended to update the discriminator several times per generator step in early training."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Early stopping / partial freeze"),": Temporarily freezing the discriminator's parameters can give the generator a chance to catch up."),"\n"),"\n",i.createElement(t.h3,{id:"hyperparameter-tuning-and-optimization-strategies",style:{position:"relative"}},i.createElement(t.a,{href:"#hyperparameter-tuning-and-optimization-strategies","aria-label":"hyperparameter tuning and optimization strategies permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Hyperparameter tuning and optimization strategies"),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Optimizer choice"),": Adam and RMSProp are common. Adam is popular because its adaptive nature helps maintain stable gradients in the face of widely varying updates. However, some versions of WGAN prefer RMSProp for theoretical reasons related to weight clipping."),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Momentum parameters"),": For Adam, the recommended ",i.createElement(o.A,{text:"\\(\\beta_1\\)"})," is often set to values such as 0.5 in DCGAN, rather than the default 0.9, which was found to improve training stability. ",i.createElement(o.A,{text:"\\(\\beta_2\\)"})," can remain at 0.999 in many cases, but it can be beneficial to experiment."),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Batch size"),": Large batch sizes can stabilize training by reducing gradient variance, but they also demand significant computational resources. Smaller batch sizes often exhibit more variance that can hamper stable learning."),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Learning rate scheduling"),": Some users adopt cyclical learning rates, gradually varying the learning rate between two extremes. This can occasionally help the networks escape local minima or ephemeral equilibria, though it is not as commonly used in GAN training as in classification tasks."),"\n",i.createElement(t.hr),"\n",i.createElement(t.h2,{id:"popular-gan-variants-and-applications",style:{position:"relative"}},i.createElement(t.a,{href:"#popular-gan-variants-and-applications","aria-label":"popular gan variants and applications permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Popular GAN variants and applications"),"\n",i.createElement(t.h3,{id:"dcgan-deep-convolutional-gan",style:{position:"relative"}},i.createElement(t.a,{href:"#dcgan-deep-convolutional-gan","aria-label":"dcgan deep convolutional gan permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"DCGAN (Deep Convolutional GAN)"),"\n",i.createElement(t.p,null,"DCGAN is widely regarded as a hallmark in stabilizing GAN training for image synthesis. Its architectural guidelines include:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,"Strided convolutions (and transposed convolutions) in generator and discriminator."),"\n",i.createElement(t.li,null,"Batch normalization in both networks (except in the output layer of the generator and input layer of the discriminator)."),"\n",i.createElement(t.li,null,"ReLU activations in the generator (except for Tanh in the final layer), and LeakyReLU in the discriminator."),"\n"),"\n",i.createElement(t.p,null,"These design decisions overcame some of the difficulties in early GAN implementations (such as using fully connected layers in the generator). DCGAN was instrumental in showcasing that a purely convolutional generator could learn complex structures like faces, bedrooms, and everyday objects from large image datasets."),"\n",i.createElement(t.h3,{id:"conditional-gan-cgan",style:{position:"relative"}},i.createElement(t.a,{href:"#conditional-gan-cgan","aria-label":"conditional gan cgan permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Conditional GAN (cGAN)"),"\n",i.createElement(t.p,null,"Conditional GANs augment the latent input with extra information, such as a class label or textual description. For instance, one might incorporate an embedded label vector ",i.createElement(o.A,{text:"\\( y \\)"})," by concatenating it with ",i.createElement(o.A,{text:"\\( z \\)"})," in the generator's input. The discriminator receives real/fake samples along with the associated condition. This approach allows direct control over which type of sample is generated. cGANs have led to successful text-to-image synthesis systems, among other applications."),"\n",i.createElement(t.p,null,"A famous extension is Pix2Pix (Isola and gang, CVPR 2017), which conditions on an input image to produce a translated version in another domain (e.g., edges→photos, day→night). Pix2Pix uses a patch-based discriminator (PatchGAN) that encourages high-frequency correctness in each local patch."),"\n",i.createElement(t.h3,{id:"cyclegan-and-image-to-image-translation",style:{position:"relative"}},i.createElement(t.a,{href:"#cyclegan-and-image-to-image-translation","aria-label":"cyclegan and image to image translation permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"CycleGAN and image-to-image translation"),"\n",i.createElement(t.p,null,"CycleGAN (Zhu and gang, ICCV 2017) solves the image-to-image translation problem in an unpaired setting by introducing cycle-consistency. Two generators map data from domain A to B and from B to A, with discriminators operating in each domain. A cycle-consistency loss enforces that translating an image from A to B, then back to A, yields the original image. This surprising result permits tasks like horse→zebra or Monet→photo transformations without requiring aligned training pairs."),"\n",i.createElement(t.h3,{id:"stylegan-and-high-resolution-image-generation",style:{position:"relative"}},i.createElement(t.a,{href:"#stylegan-and-high-resolution-image-generation","aria-label":"stylegan and high resolution image generation permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"StyleGAN and high-resolution image generation"),"\n",i.createElement(t.p,null,'StyleGAN (Karras and gang, CVPR 2019) introduced a new way to handle the latent code by mapping it to "style" parameters (in AdaIN layers), letting the network control different aspects of the generated image (coarse features, mid-level features, and fine details) at different layers of the generator. The progressive growing introduced in ProGAN was also retained to train on very high resolutions. StyleGAN and StyleGAN2 have become go-to methods for realistic face generation, with images sometimes indistinguishable from real photographs.'),"\n",i.createElement(t.h3,{id:"beyond-images-text-audio-and-more",style:{position:"relative"}},i.createElement(t.a,{href:"#beyond-images-text-audio-and-more","aria-label":"beyond images text audio and more permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Beyond images: text, audio, and more"),"\n",i.createElement(t.p,null,"GANs have expanded beyond vision tasks to myriad other domains. For instance, ",i.createElement(t.strong,null,"SeqGAN")," introduced adversarial training for sequence generation in text, addressing the mismatch between generating discrete tokens and backpropagation-based gradient updates. Although text-based GANs face extra difficulties (like discrete data and mode collapse in language models), progress is ongoing."),"\n",i.createElement(t.p,null,"In the audio domain, some attempts use adversarial training for music generation or speech synthesis (e.g., WaveGAN for raw audio). High-quality audio generation remains challenging, but advanced conditional and multi-scale approaches are showing promising results."),"\n",i.createElement(t.p,null,'Furthermore, adversarial methods can be used for domain adaptation, style transfer in speech or text, and even tasks in reinforcement learning where one agent\'s policy is "adversarial" to another. The flexible nature of the adversarial game suggests that as new modalities and tasks arise, GAN frameworks may continue evolving to accommodate them, often achieving results that push the boundaries of generative quality.'),"\n",i.createElement(t.hr),"\n",i.createElement(t.h2,{id:"implementation-details-and-best-practices",style:{position:"relative"}},i.createElement(t.a,{href:"#implementation-details-and-best-practices","aria-label":"implementation details and best practices permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Implementation details and best practices"),"\n",i.createElement(t.h3,{id:"framework-selection-tensorflow-pytorch-etc",style:{position:"relative"}},i.createElement(t.a,{href:"#framework-selection-tensorflow-pytorch-etc","aria-label":"framework selection tensorflow pytorch etc permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Framework selection (TensorFlow, PyTorch, etc.)"),"\n",i.createElement(t.p,null,"Popular deep learning frameworks for building and training GANs include TensorFlow, PyTorch, and JAX. PyTorch is often praised for its intuitive dynamic computation graph, making it a favorite among researchers for prototyping. TensorFlow (particularly with Keras) is also widely used in production contexts, featuring strong support for large-scale distributed training and a robust ecosystem for deployment."),"\n",i.createElement(t.p,null,"When selecting a framework, I suggest considering:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"Familiarity and community support."),"\n",i.createElement(t.li,null,"Availability of libraries or plugins that facilitate specific tasks (e.g., integrated monitoring tools, distributed training features)."),"\n",i.createElement(t.li,null,"Ease of debugging. PyTorch's dynamic graph approach often makes debugging simpler for many."),"\n"),"\n",i.createElement(t.p,null,"Ultimately, the differences in final performance between frameworks are typically negligible for standard architectures, so it comes down to developer preference and workflow needs."),"\n",i.createElement(t.h3,{id:"coding-architecture-for-modular-design",style:{position:"relative"}},i.createElement(t.a,{href:"#coding-architecture-for-modular-design","aria-label":"coding architecture for modular design permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Coding architecture for modular design"),"\n",i.createElement(t.p,null,"A well-structured GAN project often follows this layout:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Data loader"),": A separate module for loading and preprocessing data."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Model definition"),": Classes that define the generator and discriminator as separate modules (e.g., ",i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">Generator(nn.Module)</code>'}})," and ",i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">Discriminator(nn.Module)</code>'}})," in PyTorch)."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Loss functions"),": Implementations of the adversarial loss, possibly with variants for different GAN formulations."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Training script"),": A loop that orchestrates data loading, model updates, and checkpoint saving."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Evaluation script"),": Tools to generate samples for debugging or measuring metrics like FID and IS."),"\n"),"\n",i.createElement(t.p,null,"Having a modular design not only keeps the code cleaner but allows easy experiments with changes in one component (e.g., switching out the generator architecture) without interfering with the rest of the pipeline."),"\n",i.createElement(t.p,null,"Below is a minimal PyTorch-style skeleton to illustrate the main building blocks:"),"\n",i.createElement(l.A,{text:"\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# -- Generator Definition --\nclass Generator(nn.Module):\n    def __init__(self, latent_dim=100, ngf=64, out_channels=3):\n        super(Generator, self).__init__()\n        self.net = nn.Sequential(\n            # Example block: input is latent_dim x 1 x 1\n            nn.ConvTranspose2d(latent_dim, ngf*8, kernel_size=4, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(ngf*8),\n            nn.ReLU(True),\n            # Additional layers...\n            nn.ConvTranspose2d(ngf*8, out_channels, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.Tanh()\n        )\n        \n    def forward(self, x):\n        return self.net(x)\n\n# -- Discriminator Definition --\nclass Discriminator(nn.Module):\n    def __init__(self, in_channels=3, ndf=64):\n        super(Discriminator, self).__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(in_channels, ndf, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # Additional layers...\n            nn.Conv2d(ndf, 1, kernel_size=4, stride=1, padding=0, bias=False),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, x):\n        return self.net(x)\n\n# -- Instantiate models --\nz_dim = 100\ngen = Generator(latent_dim=z_dim)\ndisc = Discriminator()\n\n# -- Optimizers --\nlr = 2e-4\nbetas = (0.5, 0.999)\noptimizerG = optim.Adam(gen.parameters(), lr=lr, betas=betas)\noptimizerD = optim.Adam(disc.parameters(), lr=lr, betas=betas)\n\n# Example training step snippet\ndef train_step(real_images, gen, disc, optimizerG, optimizerD):\n    # Update Discriminator\n    optimizerD.zero_grad()\n    z = torch.randn(real_images.size(0), z_dim, 1, 1)\n    fake_images = gen(z)\n    disc_real = disc(real_images)\n    disc_fake = disc(fake_images.detach())\n    lossD = -torch.mean(torch.log(disc_real + 1e-8) + torch.log(1 - disc_fake + 1e-8))\n    lossD.backward()\n    optimizerD.step()\n    \n    # Update Generator\n    optimizerG.zero_grad()\n    disc_fake_for_gen = disc(fake_images)\n    lossG = -torch.mean(torch.log(disc_fake_for_gen + 1e-8))\n    lossG.backward()\n    optimizerG.step()\n    \n    return lossD.item(), lossG.item()\n"}),"\n",i.createElement(t.p,null,"This snippet demonstrates a bare-bones DCGAN-style implementation. In practice, you would:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"Adjust learning rates."),"\n",i.createElement(t.li,null,"Possibly replace the cross-entropy-style loss with WGAN or other variants."),"\n",i.createElement(t.li,null,"Add logging for losses, generated sample snapshots, etc."),"\n"),"\n",i.createElement(t.h3,{id:"dataset-preparation-and-preprocessing",style:{position:"relative"}},i.createElement(t.a,{href:"#dataset-preparation-and-preprocessing","aria-label":"dataset preparation and preprocessing permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Dataset preparation and preprocessing"),"\n",i.createElement(t.p,null,"Data preprocessing is crucial to stable GAN training. For image tasks, typical steps include:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,"Resizing or cropping images to a fixed resolution (e.g., 64x64, 128x128)."),"\n",i.createElement(t.li,null,"Normalizing pixel values to ",i.createElement(o.A,{text:"\\([-1,1]\\)"})," if you plan to use Tanh in the generator's output."),"\n",i.createElement(t.li,null,"Optional data augmentation to increase variability and reduce overfitting. For instance, random flips, rotations, color jitter, etc."),"\n"),"\n",i.createElement(t.p,null,"For text data, tokenization, vocabulary building, and handling variable sequence lengths can be tricky. In audio tasks, transforming waveforms into spectrograms or other representations may help."),"\n",i.createElement(t.h3,{id:"experiment-logging-and-version-control",style:{position:"relative"}},i.createElement(t.a,{href:"#experiment-logging-and-version-control","aria-label":"experiment logging and version control permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Experiment logging and version control"),"\n",i.createElement(t.p,null,"I strongly recommend rigorous logging:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"TensorBoard or similar"),": Visualize generator and discriminator losses over time, as well as sample images at different epochs."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Version control"),": Track changes in your model definitions, hyperparameters, and training scripts. Logging hyperparameters (learning rates, batch size, random seeds) can make or break reproducibility."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Automatic checkpointing"),": Frequently save and label model checkpoints so you can roll back if training destabilizes or you want to compare different stages of the learning process."),"\n"),"\n",i.createElement(t.h3,{id:"troubleshooting-common-pitfalls",style:{position:"relative"}},i.createElement(t.a,{href:"#troubleshooting-common-pitfalls","aria-label":"troubleshooting common pitfalls permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Troubleshooting common pitfalls"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Mode collapse"),": If the generator produces highly repetitive outputs, experiment with techniques like minibatch discrimination, more robust divergences (e.g., WGAN-GP), or altering hyperparameters."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Discriminator overpowering"),": A too-powerful discriminator can leave the generator with near-zero gradient updates. Try reducing discriminator capacity (e.g., fewer layers) or lowering the discriminator's learning rate."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Checkerboard artifacts"),": Caused by transposed convolutions. Use kernel sizes that neatly divide upsampling factors, or use sub-pixel convolutions or resize+convolution."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Exploding or vanishing gradients"),": Monitor losses carefully; try gradient clipping or lower learning rates if you see major spikes or collapses."),"\n"),"\n",i.createElement(t.hr),"\n",i.createElement(t.h2,{id:"future-directions-and-research-frontiers",style:{position:"relative"}},i.createElement(t.a,{href:"#future-directions-and-research-frontiers","aria-label":"future directions and research frontiers permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Future directions and research frontiers"),"\n",i.createElement(t.h3,{id:"improving-fidelity-and-diversity-of-generated-data",style:{position:"relative"}},i.createElement(t.a,{href:"#improving-fidelity-and-diversity-of-generated-data","aria-label":"improving fidelity and diversity of generated data permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Improving fidelity and diversity of generated data"),"\n",i.createElement(t.p,null,"Over time, the bar for image fidelity and diversity has risen. Metrics like Frechet Inception Distance (FID) and Inception Score (IS) are widely used to quantify the visual quality and variety of generated samples. Researchers are also exploring new metrics like precision/recall curves for generative models, aiming to measure coverage of the real distribution and avoid illusions of progress that might come from partial coverage."),"\n",i.createElement(t.p,null,"The goal is to generate samples that are both realistic and representative of the entire data distribution. Techniques such as multi-scale discriminators or multi-branch generators show promise. Another direction is ",i.createElement(r.A,null,"multi-modal constraints")," (e.g., text and layout) to further guide and diversify the generation process."),"\n",i.createElement(t.h3,{id:"gans-for-reinforcement-learning-and-robotics",style:{position:"relative"}},i.createElement(t.a,{href:"#gans-for-reinforcement-learning-and-robotics","aria-label":"gans for reinforcement learning and robotics permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"GANs for reinforcement learning and robotics"),"\n",i.createElement(t.p,null,"Adversarial training concepts have begun to appear in reinforcement learning, especially in the context of sim-to-real transfer. A simulated environment can be adapted to better approximate real-world conditions using a discriminator that distinguishes real from simulated experiences. By adjusting the simulation domain so that the discriminator struggles to differentiate it from reality, one can train robust policies that transfer better to real robots."),"\n",i.createElement(t.p,null,"Inverse reinforcement learning can also benefit from adversarial methods: a discriminator can measure how an agent's behavior distribution deviates from expert trajectories, guiding the agent to mimic the expert more closely. This synergy between adversarial ideas and RL is still an area of active research with many open challenges."),"\n",i.createElement(t.h3,{id:"open-problems-and-emerging-trends",style:{position:"relative"}},i.createElement(t.a,{href:"#open-problems-and-emerging-trends","aria-label":"open problems and emerging trends permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Open problems and emerging trends"),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Theoretical convergence"),": GAN training lacks strong theoretical guarantees on convergence. Unlike maximum likelihood-based methods, the game-theoretic aspect of GANs can produce local equilibria or cycle-like behavior. Researchers seek better theoretical frameworks to understand and improve convergence properties."),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Interpretability of latent spaces"),": While models like StyleGAN have shown impressive disentanglement of features, a thorough understanding of how and why certain latent directions correspond to semantic attributes is still incomplete. Improved interpretability can help address issues like unintended biases or spurious correlations learned during training."),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Bias and fairness"),": If the training data exhibits demographic biases, the generator will replicate or even amplify these biases. Addressing fairness in generative modeling is especially critical as synthetically generated media becomes more widespread in areas like facial recognition or content creation."),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Hybrid approaches with diffusion models"),": Diffusion models have emerged as competitive or sometimes superior alternatives to GANs in certain tasks. Researchers have begun experimenting with combining adversarial objectives with diffusion or other likelihood-based approaches. The interplay of different generative paradigms could yield new breakthroughs in sample fidelity, diversity, and control."),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Scalability and big data"),": As datasets grow to tens or hundreds of millions of samples, training large-scale GANs demands cluster-level computation and advanced distributed optimization. Techniques to stabilize and accelerate large-batch or distributed adversarial training are still evolving, but the success of BigGAN in generating high-fidelity ImageNet samples shows the potential for scaling up."),"\n",i.createElement(t.p,null,"Increasingly, the line between purely adversarial methods and other generative paradigms (like autoregressive or latent-variable-based methods) is blurred, as hybrid or combined solutions prove more powerful. Nevertheless, GANs remain at the forefront of synthetic data generation across modalities. By maintaining a thorough understanding of their foundational principles, advanced practitioners can push these methods even further, adapting them to new tasks and constraints in the broader sphere of modern data science and machine learning."),"\n",i.createElement(t.p,null,"Ultimately, mastering GANs opens up unique possibilities in creating, transforming, and understanding complex data — bridging the gap between theoretical insights and practical breakthroughs that shape the future of AI-driven creativity and innovation."))}var d=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,n.RP)(),e.components);return t?i.createElement(t,e,i.createElement(c,e)):c(e)};var m=a(54506),h=a(88864),g=a(58481),p=a.n(g),u=a(5984),f=a(43672),v=a(27042),y=a(72031),b=a(81817),E=a(27105),w=a(17265),x=a(2043),S=a(95751),z=a(94328),A=a(80791),k=a(78137);const N=e=>{let{toc:t}=e;if(!t||!t.items)return null;return i.createElement("nav",{className:A.R},i.createElement("ul",null,t.items.map(((e,t)=>i.createElement("li",{key:t},i.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const a=t.replace("#",""),n=document.getElementById(a);n&&n.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&i.createElement(N,{toc:{items:e.items}}))))))};function G(e){let{data:{mdx:t,allMdx:r,allPostImages:l},children:o}=e;const{frontmatter:s,body:c,tableOfContents:d}=t,h=s.index,g=s.slug.split("/")[1],y=r.nodes.filter((e=>e.frontmatter.slug.includes(`/${g}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),A=y.findIndex((e=>e.frontmatter.index===h)),G=y[A+1],T=y[A-1],M=s.slug.replace(/\/$/,""),C=/[^/]*$/.exec(M)[0],H=`posts/${g}/content/${C}/`,{0:L,1:_}=(0,i.useState)(s.flagWideLayoutByDefault),{0:I,1:j}=(0,i.useState)(!1);var D;(0,i.useEffect)((()=>{j(!0);const e=setTimeout((()=>j(!1)),340);return()=>clearTimeout(e)}),[L]),"adventures"===g?D=w.cb:"research"===g?D=w.Qh:"thoughts"===g&&(D=w.T6);const V=p()(c).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,B=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),a=e%60;return a<=30?`~${t}${a>0?".5":""} h`:`~${t+1} h`}(Math.ceil(V/D)+(s.extraReadTimeMin||0)),P=[{flag:s.flagDraft,component:()=>Promise.all([a.e(5850),a.e(9833)]).then(a.bind(a,49833))},{flag:s.flagMindfuckery,component:()=>Promise.all([a.e(5850),a.e(7805)]).then(a.bind(a,27805))},{flag:s.flagRewrite,component:()=>Promise.all([a.e(5850),a.e(8916)]).then(a.bind(a,78916))},{flag:s.flagOffensive,component:()=>Promise.all([a.e(5850),a.e(6731)]).then(a.bind(a,49112))},{flag:s.flagProfane,component:()=>Promise.all([a.e(5850),a.e(3336)]).then(a.bind(a,83336))},{flag:s.flagMultilingual,component:()=>Promise.all([a.e(5850),a.e(2343)]).then(a.bind(a,62343))},{flag:s.flagUnreliably,component:()=>Promise.all([a.e(5850),a.e(6865)]).then(a.bind(a,11627))},{flag:s.flagPolitical,component:()=>Promise.all([a.e(5850),a.e(4417)]).then(a.bind(a,24417))},{flag:s.flagCognitohazard,component:()=>Promise.all([a.e(5850),a.e(8669)]).then(a.bind(a,18669))},{flag:s.flagHidden,component:()=>Promise.all([a.e(5850),a.e(8124)]).then(a.bind(a,48124))}],{0:O,1:R}=(0,i.useState)([]);return(0,i.useEffect)((()=>{P.forEach((e=>{let{flag:t,component:a}=e;t&&a().then((e=>{R((t=>[].concat((0,m.A)(t),[e.default])))}))}))}),[]),i.createElement(v.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},i.createElement(b.A,{postNumber:s.index,date:s.date,updated:s.updated,readTime:B,difficulty:s.difficultyLevel,title:s.title,desc:s.desc,banner:s.banner,section:g,postKey:C,isMindfuckery:s.flagMindfuckery,mainTag:s.mainTag}),i.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},s.otherTags.map(((e,t)=>i.createElement("span",{key:t,className:`noselect ${k.MW}`,style:{margin:"0 5px 5px 0"}},e)))),i.createElement("div",{className:"postBody"},i.createElement(N,{toc:d})),i.createElement("br",null),i.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},i.createElement(v.P.button,{className:`noselect ${z.pb}`,id:z.xG,onClick:()=>{_(!L)},whileTap:{scale:.93}},i.createElement(v.P.div,{className:S.DJ,key:L,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},L?"Switch to default layout":"Switch to wide layout"))),i.createElement("br",null),i.createElement("div",{className:"postBody",style:{margin:L?"0 -14%":"",maxWidth:L?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},i.createElement("div",{className:`${z.P_} ${I?z.Xn:z.qG}`},O.map(((e,t)=>i.createElement(e,{key:t}))),s.indexCourse?i.createElement(x.A,{index:s.indexCourse,category:s.courseCategoryName}):"",i.createElement(u.Z.Provider,{value:{images:l.nodes,basePath:H.replace(/\/$/,"")+"/"}},i.createElement(n.xA,{components:{Image:f.A}},o)))),i.createElement(E.A,{nextPost:G,lastPost:T,keyCurrent:C,section:g}))}function T(e){return i.createElement(G,e,i.createElement(d,e))}function M(e){var t,a,n,r,l;let{data:o}=e;const{frontmatter:s}=o.mdx,c=s.titleSEO||s.title,d=s.titleOG||c,m=s.titleTwitter||c,g=s.descSEO||s.desc,p=s.descOG||g,u=s.descTwitter||g,f=s.schemaType||"BlogPosting",v=s.keywordsSEO,b=s.date,E=s.updated||b,w=s.imageOG||(null===(t=s.banner)||void 0===t||null===(a=t.childImageSharp)||void 0===a||null===(n=a.gatsbyImageData)||void 0===n||null===(r=n.images)||void 0===r||null===(l=r.fallback)||void 0===l?void 0:l.src),x=s.imageAltOG||p,S=s.imageTwitter||w,z=s.imageAltTwitter||u,A=s.canonicalURL,k=s.flagHidden||!1,N=s.mainTag||"Posts",G=s.slug.split("/")[1]||"posts",{siteUrl:T}=(0,h.Q)(),M={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:T},{"@type":"ListItem",position:2,name:N,item:`${T}/${s.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${T}${s.slug}`}]};return i.createElement(y.A,{title:c+" - avrtt.blog",titleOG:d,titleTwitter:m,description:g,descriptionOG:p,descriptionTwitter:u,schemaType:f,keywords:v,datePublished:b,dateModified:E,imageOG:w,imageAltOG:x,imageTwitter:S,imageAltTwitter:z,canonicalUrl:A,flagHidden:k,mainTag:N,section:G,type:"article"},i.createElement("script",{type:"application/ld+json"},JSON.stringify(M)))}},90548:function(e,t,a){var n=a(96540),i=a(7978);t.A=e=>{let{text:t}=e;return n.createElement(i.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-tsx-content-file-path-src-pages-posts-research-gan-architecture-mdx-d84e609988a49fcccc57.js.map