"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[8201],{16825:function(e,t,a){a.r(t),a.d(t,{Head:function(){return A},PostTemplate:function(){return z},default:function(){return H}});var n=a(54506),i=a(28453),r=a(96540),l=a(16886),s=a(46295),o=a(96098);function c(e){const t=Object.assign({p:"p",hr:"hr",h2:"h2",a:"a",span:"span",h3:"h3",ul:"ul",li:"li",strong:"strong",em:"em",ol:"ol"},(0,i.RP)(),e.components);return r.createElement(r.Fragment,null,"\n",r.createElement("br"),"\n","\n","\n",r.createElement(t.p,null,"Energy-based models (EBMs) have seen a resurgence of interest in modern machine learning research thanks to their capacity for representing complex, high-dimensional probability distributions in a principled yet flexible manner. Although these models date back several decades — with roots in statistical physics and early neural network literature — many of their core ideas have reemerged recently with new theoretical insights and improved training methodologies. The enhanced computational power and deep architectures available today have unlocked possibilities that might have been infeasible in earlier eras."),"\n",r.createElement(t.p,null,'The motivation for energy-based modeling stems from the fundamental need to learn probability distributions over richly structured data, such as images, videos, text, audio signals, and even multimodal combinations. In numerous tasks, what we really desire is a mechanism to assess how "likely" or "plausible" a given data point might be. Consider image generation: we want to sample new images from the same distribution as the training data (e.g., real photographs of faces or digits). Or consider anomaly detection, where we want to know if a novel data point lies outside the typical distribution of observations. Likewise, for tasks such as classification under uncertain conditions, an energy-based perspective can provide a unifying framework that goes beyond purely discriminative approaches.'),"\n",r.createElement(t.p,null,'Why not just learn a direct parametric probability density like a standard Gaussian or a more specialized model? For low-dimensional or structured data, simpler parametric methods might suffice. But for modern, large-scale tasks (such as natural images with tens of thousands or millions of pixels, or language with huge vocabularies), a flexible modeling approach is required. The classical approach is to pick a parametric family of densities with tractable normalization (for instance, a latent-variable model or an autoregressive model), but each approach comes with constraints. Energy-based modeling offers an alternative view: it transforms an arbitrary "energy function" (which can be any scalar output from a neural network) into a distribution by exponentiating and normalizing. This generality provides vast expressive power and unifies many previously distinct models under a single conceptual umbrella.'),"\n",r.createElement(t.p,null,"In certain respects, EBMs can be viewed as bridging the gap between purely discriminative networks (where the model typically outputs scores or logits) and fully generative methods (where the model tries to output or sample from a learned distribution). The hallmark of EBMs is that they do not strictly require the direct computation of a normalized probability density at training or inference time, as they lean heavily on sampling-based approximations. In practice, this means we can incorporate a wide range of neural architectures (feed-forward networks, convolutional or recurrent layers, etc.) without needing special restrictions on how the outputs are formed, other than ensuring that we can backpropagate to obtain gradients with respect to inputs."),"\n",r.createElement(t.p,null,"However, with such flexibility come challenges. Training EBMs is notoriously tricky, often requiring Markov Chain Monte Carlo (MCMC) procedures inside each iteration of gradient descent. Without careful attention to hyperparameters — such as step sizes, number of sampling steps, noise levels, architectural constraints, and additional stability tricks — the training can easily diverge or collapse. Yet, for researchers and practitioners willing to invest in carefully tuning these methods, EBMs offer exciting prospects in generative modeling, anomaly detection, interpretability, compositionality, and more."),"\n",r.createElement(t.p,null,"In this article, I will present a comprehensive overview of energy-based models, from the fundamental equations to advanced applications and stable training strategies. The aim is to thoroughly explore the concepts, math, algorithms, code, and some of the typical pitfalls and solutions one may encounter when deploying EBMs in practice. Throughout, I will highlight connections to canonical examples such as Boltzmann machines and restricted Boltzmann machines, as well as more recent deep architectures that leverage convolutional or residual networks. We will dive into sampling and training procedures like Contrastive Divergence (CD) and Stochastic Gradient Langevin Dynamics (SGLD), and we will walk through an implementation that can learn to generate realistic images (e.g., MNIST digits). By the end, I hope you will see how EBMs can form a powerful and unifying framework in modern machine learning research."),"\n",r.createElement(t.hr),"\n",r.createElement(t.h2,{id:"2-fundamentals-of-energy-based-models",style:{position:"relative"}},r.createElement(t.a,{href:"#2-fundamentals-of-energy-based-models","aria-label":"2 fundamentals of energy based models permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2. Fundamentals of energy-based models"),"\n",r.createElement(t.h3,{id:"21-defining-an-energy-function",style:{position:"relative"}},r.createElement(t.a,{href:"#21-defining-an-energy-function","aria-label":"21 defining an energy function permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.1 defining an energy function"),"\n",r.createElement(t.p,null,"The central object in an energy-based model is the energy function, typically written as ",r.createElement(o.A,{text:"\\(E_\\theta(x)\\)"})," for data point ",r.createElement(o.A,{text:"\\(x\\)"})," (for instance, an image or a vector of features) under parameters ",r.createElement(o.A,{text:"\\(\\theta\\)"}),". This function ",r.createElement(o.A,{text:"\\(E_\\theta(x)\\)"})," maps each possible configuration of ",r.createElement(o.A,{text:"\\(x\\)"})," to a real value (often unconstrained in ",r.createElement(l.A,null,"−∞")," to ",r.createElement(l.A,null,"+∞"),"). Intuitively, if ",r.createElement(o.A,{text:"\\(E_\\theta(x)\\)"})," is small (i.e., negative and large in magnitude), we want to interpret ",r.createElement(o.A,{text:"\\(x\\)"}),' as having high "compatibility" or likelihood. If ',r.createElement(o.A,{text:"\\(E_\\theta(x)\\)"})," is large (positive), we interpret ",r.createElement(o.A,{text:"\\(x\\)"})," as unlikely under the model."),"\n",r.createElement(t.p,null,'In physical systems, energy is a measure of how stable or probable a configuration is. Low-energy states represent stable configurations that the physical system often "prefers". By analogy, an EBM tries to learn an ',r.createElement(l.A,null,"energy landscape")," that assigns low energies to regions of data space observed frequently in the training set, and high energies to regions that are rare or absent. Put another way, the EBM has no inherent restriction on the functional form of ",r.createElement(o.A,{text:"\\(E_\\theta(x)\\)"}),'. It just learns to shape an energy surface so that the training data points end up in low-energy "valleys" while less plausible points are located in high-energy "peaks".'),"\n",r.createElement(t.p,null,"Mathematically, ",r.createElement(o.A,{text:"\\(E_\\theta: \\mathcal{X} \\rightarrow \\mathbb{R}\\)"})," can be realized by any neural network architecture that outputs a single scalar. For example, a feed-forward network with fully connected layers might produce a single logit. Alternatively, for image data, a convolutional network can output one scalar per image; for text, a transformer or RNN encoder might produce an embedding followed by a linear transform that yields a scalar."),"\n",r.createElement(t.p,null,"Because ",r.createElement(o.A,{text:"\\(E_\\theta(x)\\)"})," is so flexible, there is no fundamental structural constraint except that we need to be able to differentiate ",r.createElement(o.A,{text:"\\(E_\\theta(x)\\)"})," with respect to ",r.createElement(o.A,{text:"\\(x\\)"})," (and of course with respect to ",r.createElement(o.A,{text:"\\(\\theta\\)"}),"). This gradient with respect to ",r.createElement(o.A,{text:"\\(x\\)"})," is crucial in the typical sampling-based training process."),"\n",r.createElement(t.h3,{id:"22-turning-energy-into-probability-boltzmann-distribution",style:{position:"relative"}},r.createElement(t.a,{href:"#22-turning-energy-into-probability-boltzmann-distribution","aria-label":"22 turning energy into probability boltzmann distribution permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.2 turning energy into probability (boltzmann distribution)"),"\n",r.createElement(t.p,null,"While an energy function alone does not necessarily define a probability distribution, we can turn it into one by employing the Boltzmann (or Gibbs) distribution from statistical physics. The probability of a configuration ",r.createElement(o.A,{text:"\\(x\\)"})," under parameters ",r.createElement(o.A,{text:"\\(\\theta\\)"})," is given by:"),"\n",r.createElement(o.A,{text:"\\[\np_\\theta(x) = \\frac{\\exp\\left(-E_\\theta(x)\\right)}{Z(\\theta)},\n\\]"}),"\n",r.createElement(t.p,null,"where:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(o.A,{text:"\\(E_\\theta(x)\\)"})," is the energy."),"\n",r.createElement(t.li,null,r.createElement(o.A,{text:"\\(Z(\\theta)\\)"})," is the normalizing constant, also called the partition function:"),"\n"),"\n",r.createElement(o.A,{text:"\\[\nZ(\\theta) = \\int \\exp\\left(-E_\\theta(x)\\right)\\, dx\n\\]"}),"\n",r.createElement(t.p,null,"in the continuous case, or a sum ",r.createElement(o.A,{text:"\\(\\sum_x \\exp(-E_\\theta(x))\\)"})," in the discrete case."),"\n",r.createElement(t.p,null,"The presence of ",r.createElement(o.A,{text:"\\(Z(\\theta)\\)"})," ensures that ",r.createElement(o.A,{text:"\\(p_\\theta(x)\\)"})," integrates or sums to 1 over the entire domain ",r.createElement(o.A,{text:"\\(\\mathcal{X}\\)"}),". Although we have introduced the negative sign inside the exponential for convention (low energy → large exponent → high probability), the exact sign is not crucial if we carefully keep track of how we interpret the energy."),"\n",r.createElement(t.h3,{id:"23-partition-function-and-normalization-challenges",style:{position:"relative"}},r.createElement(t.a,{href:"#23-partition-function-and-normalization-challenges","aria-label":"23 partition function and normalization challenges permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.3 partition function and normalization challenges"),"\n",r.createElement(t.p,null,"The partition function ",r.createElement(o.A,{text:"\\(Z(\\theta)\\)"})," poses one of the principal challenges of energy-based modeling. In high-dimensional spaces, computing ",r.createElement(o.A,{text:"\\(Z(\\theta)\\)"})," exactly is usually intractable because it entails integrating or summing over a vast range of ",r.createElement(o.A,{text:"\\(x\\)"}),". For small discrete systems or low-dimensional cases, one might compute ",r.createElement(o.A,{text:"\\(Z(\\theta)\\)"})," exactly, but in modern deep learning contexts (images, text, audio, etc.), it is effectively impossible."),"\n",r.createElement(t.p,null,"This complication means that we typically cannot directly maximize ",r.createElement(o.A,{text:"\\(p_\\theta(x)\\)"})," in closed form. Instead, we resort to approximate methods, often involving sampling procedures (Markov Chain Monte Carlo is common) that produce samples from ",r.createElement(o.A,{text:"\\(p_\\theta(x)\\)"})," or, more precisely, approximate the gradient of the log-likelihood with respect to ",r.createElement(o.A,{text:"\\(\\theta\\)"}),". Contrastive Divergence (CD), introduced by Hinton, is one of the best-known sampling-based training methods for EBMs. Later sections will dive into the details."),"\n",r.createElement(t.p,null,"When ",r.createElement(o.A,{text:"\\(Z(\\theta)\\)"})," is unknown, we typically aim to shape the energy landscape ",r.createElement(o.A,{text:"\\(E_\\theta(x)\\)"})," so that ",r.createElement(o.A,{text:"\\(x\\)"})," from our dataset land in the low-energy regions while out-of-distribution ",r.createElement(o.A,{text:"\\(x\\)"})," land in high-energy basins. We do this by comparing energies of real data points against energies of points sampled from the model's own distribution. This ensures we do not need the exact value of ",r.createElement(o.A,{text:"\\(Z(\\theta)\\)"})," but only a scheme that manipulates the ",r.createElement("em",null,"relative")," energies of real versus fake points."),"\n",r.createElement(t.h3,{id:"24-canonical-ensemble-learning-cel-and-the-statistical-physics-connection",style:{position:"relative"}},r.createElement(t.a,{href:"#24-canonical-ensemble-learning-cel-and-the-statistical-physics-connection","aria-label":"24 canonical ensemble learning cel and the statistical physics connection permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.4 canonical ensemble learning (cel) and the statistical physics connection"),"\n",r.createElement(t.p,null,'The original perspective on EBMs is closely aligned with the notion of a "canonical ensemble" in statistical physics. In physics, a system in thermal equilibrium at temperature ',r.createElement(o.A,{text:"\\(T\\)"})," follows the Boltzmann distribution:"),"\n",r.createElement(o.A,{text:"\\[\np(x) \\propto \\exp\\left(-\\frac{E(x)}{k_B T}\\right),\n\\]"}),"\n",r.createElement(t.p,null,"where ",r.createElement(o.A,{text:"\\(E(x)\\)"})," is energy, ",r.createElement(o.A,{text:"\\(k_B\\)"})," is the Boltzmann constant, and ",r.createElement(o.A,{text:"\\(T\\)"})," is temperature. Drawing parallels to EBMs, we can see that ",r.createElement(o.A,{text:"\\(E_\\theta(x)\\)"})," is a learned energy function, and the partition function normalizes the resulting exponentiated energy. In the machine learning context, we often set ",r.createElement(o.A,{text:"\\(k_B T=1\\)"})," for simplicity, or treat the temperature as an additional hyperparameter that can modulate the sharpness or smoothness of the energy landscape."),"\n",r.createElement(t.p,null,"This analogy is deeper than it appears: many methods for sampling from EBMs (like Langevin dynamics) are effectively simulating the diffusion or random walk of a physical system in its energy landscape. By carefully introducing noise (analogous to thermal fluctuations) and performing gradient-based steps (analogous to the system relaxing toward low-energy states), we can sample from ",r.createElement(o.A,{text:"\\(p_\\theta(x)\\)"}),"."),"\n",r.createElement(t.hr),"\n",r.createElement(t.h2,{id:"3-core-principles-in-practice",style:{position:"relative"}},r.createElement(t.a,{href:"#3-core-principles-in-practice","aria-label":"3 core principles in practice permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3. core principles in practice"),"\n",r.createElement(t.h3,{id:"31-energy-vs-probability-a-conceptual-comparison",style:{position:"relative"}},r.createElement(t.a,{href:"#31-energy-vs-probability-a-conceptual-comparison","aria-label":"31 energy vs probability a conceptual comparison permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.1 energy vs. probability: a conceptual comparison"),"\n",r.createElement(t.p,null,"It may be useful to compare an EBM's approach to that of purely probabilistic models (e.g., normalizing flows or variational autoencoders). In a normalizing flow, for instance, one tries to construct a bijection that maps a base distribution (like a Gaussian) to the target distribution. The main challenge is to keep track of the log Jacobian determinant so that we can properly normalize. For EBMs, we do not bother with designing invertible transformations; we only define ",r.createElement(o.A,{text:"\\(E_\\theta(x)\\)"}),". The probability is implicitly derived via exponentiation and normalization, which, while powerful, leads to the computational difficulty of not knowing ",r.createElement(o.A,{text:"\\(Z(\\theta)\\)"}),"."),"\n",r.createElement(t.p,null,"A purely probabilistic model also tries to approximate ",r.createElement(o.A,{text:"\\(p(x)\\)"})," by specifying it in a parametric form from the get-go (for instance, a factorized distribution or a certain latent-variable model). In contrast, an EBM can represent highly multimodal distributions or distributions that do not factorize easily, because ",r.createElement(o.A,{text:"\\(E_\\theta(x)\\)"})," can be ",r.createElement("em",null,"any")," neural network. The cost of that generality is the need for specialized sampling and training protocols."),"\n",r.createElement(t.h3,{id:"32-the-role-of-latent-variables-in-ebms",style:{position:"relative"}},r.createElement(t.a,{href:"#32-the-role-of-latent-variables-in-ebms","aria-label":"32 the role of latent variables in ebms permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.2 the role of latent variables in ebms"),"\n",r.createElement(t.p,null,"Another dimension of design is whether the EBM has latent variables ",r.createElement(o.A,{text:"\\(h\\)"})," in addition to visible variables ",r.createElement(o.A,{text:"\\(x\\)"}),". Boltzmann machines, for example, can contain hidden units that can shape the energy of the visible units in ways that capture underlying structure. In a latent-variable EBM, we might define:"),"\n",r.createElement(o.A,{text:"\\[\nE_\\theta(x) = \\min_{h \\in \\mathcal{H}} E_\\theta(x, h),\n\\]"}),"\n",r.createElement(t.p,null,"or we might define:"),"\n",r.createElement(o.A,{text:"\\[\np_\\theta(x) \\propto \\int \\exp\\bigl(-E_\\theta(x, h)\\bigr)\\, dh.\n\\]"}),"\n",r.createElement(t.p,null,"Marginalizing out the hidden variable ",r.createElement(o.A,{text:"\\(h\\)"})," can again be complicated in high-dimensional scenarios. Restricted Boltzmann machines (RBMs) avoid some complexities by adopting a bipartite structure that permits partial factorization. However, for deep EBMs with complex latent spaces, we again rely on approximate sampling or gradient-based methods to handle the latent dimension."),"\n",r.createElement(t.h3,{id:"33-free-energy-and-marginalizing-over-latent-variables",style:{position:"relative"}},r.createElement(t.a,{href:"#33-free-energy-and-marginalizing-over-latent-variables","aria-label":"33 free energy and marginalizing over latent variables permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.3 free energy and marginalizing over latent variables"),"\n",r.createElement(t.p,null,"In EBM literature, one often sees references to ",r.createElement(l.A,null,"free energy"),". The free energy functional can appear when we integrate out latent variables. If ",r.createElement(o.A,{text:"\\(h\\)"})," denotes hidden states, then:"),"\n",r.createElement(o.A,{text:"\\[\nF_\\theta(x) = -\\log \\int \\exp\\bigl(-E_\\theta(x, h)\\bigr)\\, dh.\n\\]"}),"\n",r.createElement(t.p,null,"This ",r.createElement(o.A,{text:"\\(F_\\theta(x)\\)"})," is known as the free energy of ",r.createElement(o.A,{text:"\\(x\\)"}),". Minimizing free energy is effectively encouraging the existence of some hidden representation ",r.createElement(o.A,{text:"\\(h\\)"})," that yields low total energy. Indeed, for models like RBMs, training procedures often revolve around approximating gradients of the free energy. In advanced latent EBMs, free-energy-based training can also be used, but sampling or optimization in ",r.createElement(o.A,{text:"\\(h\\)"})," may again be nontrivial."),"\n",r.createElement(t.h3,{id:"34-advantages-over-purely-probabilistic-approaches",style:{position:"relative"}},r.createElement(t.a,{href:"#34-advantages-over-purely-probabilistic-approaches","aria-label":"34 advantages over purely probabilistic approaches permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.4 advantages over purely probabilistic approaches"),"\n",r.createElement(t.p,null,"EBMs present several advantages:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Flexibility"),": We can parametrize ",r.createElement(o.A,{text:"\\(E_\\theta(x)\\)"})," with almost any neural network, unconstrained by invertibility or closed-form integrals."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Unified view"),": Both discriminative tasks (classification) and generative tasks (sampling) can be formulated under a single framework, often by combining the energy with additional terms or introducing class labels as part of the energy function."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Multimodality"),": Because the energy surface can be arbitrarily shaped, EBMs readily capture complicated, multimodal distributions."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Anomaly detection"),": As we will see, EBMs naturally produce a scalar that can serve as an anomaly score. Data that is truly out of distribution tends to yield higher energies."),"\n"),"\n",r.createElement(t.p,null,"However, these advantages come with the significant computational burden of not having direct access to ",r.createElement(o.A,{text:"\\(Z(\\theta)\\)"}),", leading to the necessity of iterative sampling."),"\n",r.createElement(t.hr),"\n",r.createElement(t.h2,{id:"4-training",style:{position:"relative"}},r.createElement(t.a,{href:"#4-training","aria-label":"4 training permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4. training"),"\n",r.createElement(t.h3,{id:"41-maximum-likelihood-and-why-it-is-difficult-with-ebms",style:{position:"relative"}},r.createElement(t.a,{href:"#41-maximum-likelihood-and-why-it-is-difficult-with-ebms","aria-label":"41 maximum likelihood and why it is difficult with ebms permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.1 maximum likelihood and why it is difficult with ebms"),"\n",r.createElement(t.p,null,"When training a generative model, we typically maximize the log-likelihood of the observed data. For an EBM, the log-likelihood of a single data point ",r.createElement(o.A,{text:"\\(x\\)"})," is:"),"\n",r.createElement(o.A,{text:"\\[\n\\log p_\\theta(x) = - E_\\theta(x) - \\log Z(\\theta).\n\\]"}),"\n",r.createElement(t.p,null,"Taking the gradient with respect to ",r.createElement(o.A,{text:"\\(\\theta\\)"})," gives:"),"\n",r.createElement(o.A,{text:"\\[\n\\nabla_\\theta \\log p_\\theta(x)\n= -\\nabla_\\theta E_\\theta(x)\n- \\nabla_\\theta \\log Z(\\theta).\n\\]"}),"\n",r.createElement(t.p,null,"The gradient of the log partition function ",r.createElement(o.A,{text:"\\(\\log Z(\\theta)\\)"})," is:"),"\n",r.createElement(o.A,{text:"\\[\n\\nabla_\\theta \\log Z(\\theta) \n= \\frac{1}{Z(\\theta)} \\nabla_\\theta Z(\\theta) \n= \\frac{1}{Z(\\theta)} \\nabla_\\theta \\int \\exp\\bigl(- E_\\theta(x')\\bigr)\\, dx'.\n\\]"}),"\n",r.createElement(t.p,null,"We can rewrite it as an expectation under ",r.createElement(o.A,{text:"\\(p_\\theta(x')\\)"}),":"),"\n",r.createElement(o.A,{text:"\\[\n\\nabla_\\theta \\log Z(\\theta)\n= \\mathbb{E}_{x' \\sim p_\\theta(x')}\\bigl[\\nabla_\\theta E_\\theta(x')\\bigr].\n\\]"}),"\n",r.createElement(t.p,null,"Thus,"),"\n",r.createElement(o.A,{text:"\\[\n\\nabla_\\theta \\log p_\\theta(x)\n= -\\nabla_\\theta E_\\theta(x) + \\mathbb{E}_{x' \\sim p_\\theta(x')}\\bigl[\\nabla_\\theta E_\\theta(x')\\bigr].\n\\]"}),"\n",r.createElement(t.p,null,"Implementing this gradient in a naive way would require sampling from ",r.createElement(o.A,{text:"\\(p_\\theta\\)"}),", which itself is unknown unless we run an expensive Markov chain that relies on computing gradients of ",r.createElement(o.A,{text:"\\(E_\\theta(x)\\)"})," with respect to ",r.createElement(o.A,{text:"\\(x\\)"}),". This is why maximum-likelihood training of EBMs is typically done through approximate methods such as Monte Carlo sampling."),"\n",r.createElement(t.h3,{id:"42-contrastive-divergence-cd-the-fundamental-training-objective",style:{position:"relative"}},r.createElement(t.a,{href:"#42-contrastive-divergence-cd-the-fundamental-training-objective","aria-label":"42 contrastive divergence cd the fundamental training objective permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.2 contrastive divergence (cd): the fundamental training objective"),"\n",r.createElement(t.p,null,"Contrastive Divergence (CD), introduced by Hinton, is arguably the most common method for training EBMs (at least historically, especially in the context of restricted Boltzmann machines). The idea is to approximate the gradient of the log-likelihood by short-run MCMC chains initialized at training data or from some buffer of previously generated samples. Specifically, the CD-k algorithm starts from observed data, runs k steps of Gibbs sampling (or another MCMC method) to produce a negative sample, and then computes:"),"\n",r.createElement(o.A,{text:"\\[\n\\nabla_\\theta \\text{CD}_k \\approx \n- \\nabla_\\theta E_\\theta(\\text{data}) \n+ \\nabla_\\theta E_\\theta(\\text{neg sample}),\n\\]"}),"\n",r.createElement(t.p,null,"where the negative sample is drawn from the short-run Markov chain. The hope is that with enough training iterations, the chain states approximate ",r.createElement(o.A,{text:"\\(p_\\theta(x)\\)"})," sufficiently for learning to succeed."),"\n",r.createElement(t.h3,{id:"43-detailed-derivation-of-contrastive-divergence",style:{position:"relative"}},r.createElement(t.a,{href:"#43-detailed-derivation-of-contrastive-divergence","aria-label":"43 detailed derivation of contrastive divergence permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.3 detailed derivation of contrastive divergence"),"\n",r.createElement(t.p,null,"A more formal derivation frames the maximum likelihood gradient as:"),"\n",r.createElement(o.A,{text:"\\[\n-\\nabla_\\theta E_\\theta(x)\n+ \\int p_\\theta(x') \\nabla_\\theta E_\\theta(x')\\, dx'.\n\\]"}),"\n",r.createElement(t.p,null,"Since we cannot compute the integral exactly, we approximate ",r.createElement(o.A,{text:"\\(p_\\theta(x')\\)"})," by a distribution ",r.createElement(o.A,{text:"\\(q(x')\\)"})," that we sample from in a short-run chain. The simplest approach is to start from the data point ",r.createElement(o.A,{text:"\\(x\\)"})," itself and take only a few MCMC steps. This yields a sample ",r.createElement(o.A,{text:"\\(x^{(k)}\\)"})," that hopefully is close to the modes of the distribution but is cheaper to compute than running a long chain. We then replace ",r.createElement(o.A,{text:"\\(\\int p_\\theta(x') \\ldots dx'\\)"})," with the single-sample approximation from ",r.createElement(o.A,{text:"\\(x^{(k)}\\)"}),". The difference ",r.createElement(o.A,{text:"\\(E_\\theta(x^{(k)}) - E_\\theta(x)\\)"})," is the core of the training signal: we want to push down the energy of real data, while pushing up the energy of samples from the model."),"\n",r.createElement(t.h3,{id:"44-intuition-behind-pulling-up-and-pushing-down-energies",style:{position:"relative"}},r.createElement(t.a,{href:"#44-intuition-behind-pulling-up-and-pushing-down-energies","aria-label":"44 intuition behind pulling up and pushing down energies permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),'4.4 intuition behind "pulling up" and "pushing down" energies'),"\n",r.createElement(t.p,null,"One of the most intuitive explanations of the CD objective is that we have two forces:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Pull down")," the energy of real data so these points reside in low-energy basins (i.e., are assigned high probability)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Push up")," the energy of negatively sampled points so that the model does not erroneously assign them too high probability."),"\n"),"\n",r.createElement(t.p,null,'This combination of attractive and repulsive forces sculpts the energy landscape in a way that matches the data distribution. Imagine each real data point creates a "well" around itself, whereas each negative sample pushes up the energy in some region, flattening it out or creating a barrier that prevents the distribution from spreading too widely in unobserved areas.'),"\n",r.createElement(t.h3,{id:"45-stochastic-gradient-langevin-dynamics-sgld",style:{position:"relative"}},r.createElement(t.a,{href:"#45-stochastic-gradient-langevin-dynamics-sgld","aria-label":"45 stochastic gradient langevin dynamics sgld permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.5 stochastic gradient langevin dynamics (sgld)"),"\n",r.createElement(t.p,null,"One popular variant for sampling negative examples during training is Stochastic Gradient Langevin Dynamics (SGLD). Langevin dynamics is an MCMC approach that treats updates in ",r.createElement(o.A,{text:"\\(x\\)"}),"-space as a gradient descent on ",r.createElement(o.A,{text:"\\(E_\\theta(x)\\)"})," plus a noise term that ensures exploration of other modes. Specifically, the update for ",r.createElement(o.A,{text:"\\(x\\)"})," can look like:"),"\n",r.createElement(o.A,{text:"\\[\nx_{t+1} = x_t - \\alpha \\nabla_x E_\\theta(x_t) + \\sqrt{2\\alpha}\\,\\eta_t,\n\\]"}),"\n",r.createElement(t.p,null,"where ",r.createElement(o.A,{text:"\\(\\eta_t\\)"})," is Gaussian noise and ",r.createElement(o.A,{text:"\\(\\alpha\\)"})," is a small step size. In the limit of infinitely small steps and infinite sampling time, this procedure samples exactly from the Boltzmann distribution ",r.createElement(o.A,{text:"\\(p_\\theta(x)\\)"}),". In practice, we only take a finite number of steps (like 10 to 100), so the result is an approximate sample."),"\n",r.createElement(t.p,null,"SGLD is often favored for neural-network-based EBMs because of its relative simplicity to implement and its strong theoretical grounding in the connection to continuous-time diffusion processes."),"\n",r.createElement(t.h3,{id:"46-other-mcmc-approaches-metropolishastings-gibbs-sampling",style:{position:"relative"}},r.createElement(t.a,{href:"#46-other-mcmc-approaches-metropolishastings-gibbs-sampling","aria-label":"46 other mcmc approaches metropolishastings gibbs sampling permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.6 other mcmc approaches (metropolis–hastings, gibbs sampling)"),"\n",r.createElement(t.p,null,"Besides Langevin dynamics, classical MCMC methods like Metropolis–Hastings and Gibbs sampling are also widely used, especially in restricted Boltzmann machines. However, for continuous high-dimensional data like images, Metropolis–Hastings can be cumbersome to tune, and conditional distributions for Gibbs sampling might be intractable. When feasible, partial or block Gibbs updates can help, but in many modern EBM setups, short-run Langevin dynamics or variants of gradient-based MCMC are more common."),"\n",r.createElement(t.hr),"\n",r.createElement(t.h2,{id:"5-implementation-and-tricks",style:{position:"relative"}},r.createElement(t.a,{href:"#5-implementation-and-tricks","aria-label":"5 implementation and tricks permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5. implementation and tricks"),"\n",r.createElement(t.h3,{id:"51-basic-neural-network-parameterizations-of-energy-functions",style:{position:"relative"}},r.createElement(t.a,{href:"#51-basic-neural-network-parameterizations-of-energy-functions","aria-label":"51 basic neural network parameterizations of energy functions permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.1 basic neural network parameterizations of energy functions"),"\n",r.createElement(t.p,null,"Since ",r.createElement(o.A,{text:"\\(E_\\theta(x)\\)"})," is unconstrained, we can choose a straightforward architecture for images, such as a multi-layer convolutional network that reduces an input image to a single scalar:"),"\n",r.createElement(s.A,{text:"\nimport torch\nimport torch.nn as nn\n\nclass EnergyModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, 4, 2, 1),\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(64*8*8, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1)\n        )\n\n    def forward(self, x):\n        return self.features(x).squeeze()  # scalar\n"}),"\n",r.createElement(t.p,null,"You can see that the output is a single scalar for each input ",r.createElement(o.A,{text:"\\(x\\)"}),". We interpret this as ",r.createElement(o.A,{text:"\\(E_\\theta(x)\\)"}),". Of course, this can be extended with deeper residual blocks, skip connections, or attention mechanisms, but the principle remains: an arbitrary function that yields a scalar."),"\n",r.createElement(t.h3,{id:"52-sampling-buffer-or-replay-buffer-for-stabilized-training",style:{position:"relative"}},r.createElement(t.a,{href:"#52-sampling-buffer-or-replay-buffer-for-stabilized-training","aria-label":"52 sampling buffer or replay buffer for stabilized training permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.2 sampling buffer or replay buffer for stabilized training"),"\n",r.createElement(t.p,null,'A widely used trick in modern EBM training is maintaining a "sampling buffer" or "replay buffer" from which we initialize new Markov chains. Rather than starting each chain from random noise, which can require many MCMC steps to yield a decent sample, we re-initialize a portion of the negative samples from a buffer that holds previously generated samples. Because these samples are often somewhat close to the modes, fewer MCMC steps are needed to refine them. This approach speeds up training substantially and can mitigate instability.'),"\n",r.createElement(t.h3,{id:"53-step-sizes-noise-schedules-and-gradient-clipping",style:{position:"relative"}},r.createElement(t.a,{href:"#53-step-sizes-noise-schedules-and-gradient-clipping","aria-label":"53 step sizes noise schedules and gradient clipping permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.3 step sizes, noise schedules, and gradient clipping"),"\n",r.createElement(t.p,null,"Tuning the hyperparameters of the MCMC procedure is critical:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Step size ",r.createElement(o.A,{text:"\\(\\alpha\\)"})),": If the step size is too large, chains may diverge or skip over important modes. If it is too small, we need too many steps for mixing or mode exploration."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Noise variance"),": The random noise we add in Langevin or Metropolis updates is crucial for exploration. It can be constant or gradually decreased. Some advanced schedules gradually reduce noise as we move closer to equilibrium."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Gradient clipping"),": Because neural networks can produce large gradients, it is common to clamp gradients within a certain range (e.g., ",r.createElement(o.A,{text:"\\([-0.03, 0.03]\\)"}),"). This helps avoid explosive updates that push the sample out of the data manifold."),"\n"),"\n",r.createElement(t.h3,{id:"54-regularization-terms-and-bias-constraints",style:{position:"relative"}},r.createElement(t.a,{href:"#54-regularization-terms-and-bias-constraints","aria-label":"54 regularization terms and bias constraints permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.4 regularization terms and bias constraints"),"\n",r.createElement(t.p,null,"Without additional constraints, the output of ",r.createElement(o.A,{text:"\\(E_\\theta(x)\\)"})," can drift arbitrarily. For instance, the model might systematically push energies of ",r.createElement(t.em,null,"all")," points downward by adding a constant negative offset, messing up the normalization. One common remedy is to add a penalty on the magnitude of the network output — for instance, ",r.createElement(o.A,{text:"\\(\\alpha \\|E_\\theta(x)\\|^2\\)"})," — so that energies remain near zero for real samples. Some authors also apply weight decay or other stabilizers that limit the scale of the parameters themselves."),"\n",r.createElement(t.h3,{id:"55-monitoring-training-progress-loss-terms-and-sample-quality",style:{position:"relative"}},r.createElement(t.a,{href:"#55-monitoring-training-progress-loss-terms-and-sample-quality","aria-label":"55 monitoring training progress loss terms and sample quality permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.5 monitoring training progress: loss terms and sample quality"),"\n",r.createElement(t.p,null,"Because typical losses (like CD) can sometimes appear to converge even while the model distribution is not well-formed, it is crucial to track more than just the final loss. Researchers often monitor:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"The average energy assigned to training data."),"\n",r.createElement(t.li,null,"The average energy assigned to generated samples."),"\n",r.createElement(t.li,null,"Metrics on sample quality, such as how visually coherent or diverse they are."),"\n",r.createElement(t.li,null,"Additional metrics like the FID (Fréchet Inception Distance) or Inception Score for image models, or perplexity for text."),"\n"),"\n",r.createElement(t.h3,{id:"56-code-snippets",style:{position:"relative"}},r.createElement(t.a,{href:"#56-code-snippets","aria-label":"56 code snippets permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.6 code snippets"),"\n",r.createElement(t.p,null,"Below is a partial snippet illustrating how one might integrate the MCMC sampling step into a training loop using PyTorch. This snippet is adapted from a more complete code base:"),"\n",r.createElement(s.A,{text:"\ndef langevin_sampling(model, x_init, steps=60, step_size=10, noise_std=0.005):\n    x = x_init.clone().detach().requires_grad_(True)\n    for i in range(steps):\n        # random noise\n        noise = torch.randn_like(x) * noise_std\n        x = x + noise\n        # clamp input range\n        x = x.clamp(-1., 1.)\n\n        # compute energy\n        energy = model(x)\n        # we want to descend the gradient of E, so we do x <- x - grad(E)\n        energy.sum().backward()\n        with torch.no_grad():\n            x = x - step_size * x.grad\n        x.grad.zero_()\n\n    return x.detach()\n\n# In a typical training step:\n# 1. get real data\n# 2. sample from model using replay buffer or random initialization\n# 3. compute E(real) and E(sampled)\n# 4. compute contrastive divergence loss\n# 5. backprop and update\n"}),"\n",r.createElement(t.hr),"\n",r.createElement(t.h2,{id:"6-canonical-examples-and-architectures",style:{position:"relative"}},r.createElement(t.a,{href:"#6-canonical-examples-and-architectures","aria-label":"6 canonical examples and architectures permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6. canonical examples and architectures"),"\n",r.createElement(t.h3,{id:"61-boltzmann-machines-and-restricted-boltzmann-machines",style:{position:"relative"}},r.createElement(t.a,{href:"#61-boltzmann-machines-and-restricted-boltzmann-machines","aria-label":"61 boltzmann machines and restricted boltzmann machines permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.1 boltzmann machines and restricted boltzmann machines"),"\n",r.createElement(t.p,null,"Historically, Boltzmann machines (BMs) played a key role in popularizing EBMs in the neural network community. A BM is an undirected graphical model with visible units ",r.createElement(o.A,{text:"\\(x\\)"})," and possibly hidden units ",r.createElement(o.A,{text:"\\(h\\)"}),". The energy function is often bilinear in ",r.createElement(o.A,{text:"\\(x\\)"})," and ",r.createElement(o.A,{text:"\\(h\\)"}),":"),"\n",r.createElement(o.A,{text:"\\[\nE_\\theta(x, h) = - x^T W h - b^T x - c^T h,\n\\]"}),"\n",r.createElement(t.p,null,"with ",r.createElement(o.A,{text:"\\(\\theta = \\{W, b, c\\}\\)"}),". If we do not impose any restrictions on the connectivity, sampling can be extremely difficult because ",r.createElement(o.A,{text:"\\(x\\)"})," and ",r.createElement(o.A,{text:"\\(h\\)"})," are heavily coupled. A Restricted Boltzmann Machine (RBM) uses bipartite connections between the visible and hidden layers, but no connections among visible units or among hidden units themselves. This structure permits a faster block Gibbs sampling approach and simpler updates for contrastive divergence."),"\n",r.createElement(t.p,null,"RBMs found numerous applications (like pretraining of deep networks in the late 2000s) and remain a classic demonstration of energy-based modeling. However, in modern practice, deep convolutional or residual-based EBMs are typically used when dealing with high-resolution images or other complex data sources."),"\n",r.createElement(t.h3,{id:"62-deep-energy-based-models-using-convolutional-networks",style:{position:"relative"}},r.createElement(t.a,{href:"#62-deep-energy-based-models-using-convolutional-networks","aria-label":"62 deep energy based models using convolutional networks permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.2 deep energy-based models using convolutional networks"),"\n",r.createElement(t.p,null,"Recent years have seen a surge of interest in ",r.createElement(l.A,null,"deep EBMs")," that employ convolutional networks as the backbone for ",r.createElement(o.A,{text:"\\(E_\\theta(x)\\)"}),". This is useful for images or volumetric data (3D). The idea is straightforward: pass ",r.createElement(o.A,{text:"\\(x\\)"})," through multiple convolutional layers, possibly with residual or skip connections, to produce a single scalar. The architecture can be reminiscent of discriminators from Generative Adversarial Networks (GANs), except that we interpret the scalar as an energy instead of a logit for real vs. fake classification."),"\n",r.createElement(t.h3,{id:"63-residual-architectures-and-hybrid-ebms",style:{position:"relative"}},r.createElement(t.a,{href:"#63-residual-architectures-and-hybrid-ebms","aria-label":"63 residual architectures and hybrid ebms permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.3 residual architectures and hybrid ebms"),"\n",r.createElement(t.p,null,"Some advanced EBMs adopt ",r.createElement(l.A,null,"ResNet"),"-like blocks for the energy function to ease gradient flow and stabilize training. Because we rely heavily on gradient-based sampling, the smoothness of the neural network is crucial. Activation choices can matter; for instance, some researchers find that smoother nonlinearities (like softplus or Swish) can lead to more stable MCMC sampling."),"\n",r.createElement(t.p,null,"A ",r.createElement(l.A,null,"hybrid EBM")," might combine an energy-based objective with additional discriminative or generative losses. For example, one could design a multi-task network that outputs both a classification score for ",r.createElement(o.A,{text:"\\(x\\)"})," and an energy. The energy can then be used for anomaly detection, while the classification head is used for supervised learning. This synergy sometimes helps calibrate energies in a more stable way."),"\n",r.createElement(t.h3,{id:"64-overview-of-joint-energy-based-models-jem",style:{position:"relative"}},r.createElement(t.a,{href:"#64-overview-of-joint-energy-based-models-jem","aria-label":"64 overview of joint energy based models jem permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.4 overview of joint energy-based models (jem)"),"\n",r.createElement(t.p,null,"Joint Energy-based Models (JEM), introduced by Grathwohl and gang (ICLR 2020), unify classification and energy-based modeling by letting ",r.createElement(o.A,{text:"\\(E_\\theta(x, y)\\)"})," represent the joint energy of data ",r.createElement(o.A,{text:"\\(x\\)"})," and label ",r.createElement(o.A,{text:"\\(y\\)"}),". Classification amounts to finding ",r.createElement(o.A,{text:"\\(\\hat{y} = \\arg\\min_y E_\\theta(x, y)\\)"}),", while generative modeling arises from sampling ",r.createElement(o.A,{text:"\\(x\\)"})," from ",r.createElement(o.A,{text:"\\(\\exp(-E_\\theta(x, y))\\)"}),". This approach showed that a single model could match the performance of strong discriminative classifiers while also acting as a generative model. However, training stability remains a concern, requiring careful MCMC parameter tuning and additional regularization terms."),"\n",r.createElement(t.hr),"\n",r.createElement(t.h2,{id:"7-generative-modeling-applications",style:{position:"relative"}},r.createElement(t.a,{href:"#7-generative-modeling-applications","aria-label":"7 generative modeling applications permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7. generative modeling applications"),"\n",r.createElement(t.h3,{id:"71-image-generation",style:{position:"relative"}},r.createElement(t.a,{href:"#71-image-generation","aria-label":"71 image generation permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.1 image generation"),"\n",r.createElement(t.p,null,"One of the most common demonstrations of EBMs is image synthesis. By training an EBM on a dataset of images (MNIST, CIFAR-10, etc.), we can eventually sample new images by running MCMC in pixel space. The model attempts to shift random noise toward the modes in the distribution. For instance, if we train on handwritten digits, random initial images gradually morph into something resembling digits (0 through 9)."),"\n",r.createElement(t.p,null,'Unlike certain other generative methods (e.g., VAEs or GANs), EBMs do not necessarily require a separate "generator" network. Instead, we rely on iterative sampling to produce new samples. This iterative approach is computationally heavier, but it can potentially model data more flexibly.'),"\n",r.createElement(t.h3,{id:"72-training-setup-and-hyperparameter-considerations",style:{position:"relative"}},r.createElement(t.a,{href:"#72-training-setup-and-hyperparameter-considerations","aria-label":"72 training setup and hyperparameter considerations permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.2 training setup and hyperparameter considerations"),"\n",r.createElement(t.p,null,"To obtain high-quality samples, you typically need:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,"A properly sized model with enough capacity to represent the complexity of the data distribution."),"\n",r.createElement(t.li,null,"A carefully tuned MCMC procedure (step size, number of steps, noise level)."),"\n",r.createElement(t.li,null,"Possibly a replay buffer to accelerate mixing."),"\n",r.createElement(t.li,null,"Additional regularization (e.g., penalizing large energy values)."),"\n",r.createElement(t.li,null,"Sufficient training time (and perhaps reinitializing from stable checkpoints if divergence occurs)."),"\n"),"\n",r.createElement(t.p,null,"For example, with MNIST (28×28 grayscale images), a small convolutional EBM can suffice. But for larger images (like ImageNet, 128×128 or higher resolution), you may need a deep ResNet or even more advanced architectures, plus careful multi-GPU training protocols."),"\n",r.createElement(t.h3,{id:"73-evaluating-generation-quality-fid-inception-score",style:{position:"relative"}},r.createElement(t.a,{href:"#73-evaluating-generation-quality-fid-inception-score","aria-label":"73 evaluating generation quality fid inception score permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.3 evaluating generation quality (fid, inception score)"),"\n",r.createElement(t.p,null,"As with any generative model, we often rely on statistics like the Frechet Inception Distance (FID) or Inception Score to measure the realism and diversity of generated images. While EBMs can achieve competitive results with enough tuning, they are sometimes outperformed in practice by specialized methods like GANs or normalizing flows on certain benchmarks. Nonetheless, some studies (Du and Mordatch, 2019) have shown that with improved training, deep EBMs can match or surpass strong GAN baselines on image generation tasks."),"\n",r.createElement(t.h3,{id:"74-video-and-3d-data-generation",style:{position:"relative"}},r.createElement(t.a,{href:"#74-video-and-3d-data-generation","aria-label":"74 video and 3d data generation permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.4 video and 3d data generation"),"\n",r.createElement(t.p,null,"In principle, EBMs can handle any dimensional input, including video frames or 3D voxel grids/point clouds, by letting ",r.createElement(o.A,{text:"\\(E_\\theta(x)\\)"})," be defined on these structured inputs. The main difference is that sampling becomes even more computationally intensive due to the higher dimensionality. Researchers have proposed specialized architectures for 3D data or for spatiotemporal correlation in video. Many of these remain active areas of investigation, where advanced EBMs, potentially combined with latent representations, may unlock more tractable training."),"\n",r.createElement(t.h3,{id:"75-comparison-with-gans-vaes-and-normalizing-flows",style:{position:"relative"}},r.createElement(t.a,{href:"#75-comparison-with-gans-vaes-and-normalizing-flows","aria-label":"75 comparison with gans vaes and normalizing flows permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.5 comparison with gans, vaes, and normalizing flows"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"GANs"),": Provide fast sampling at test time (just a forward pass of the generator) but can suffer from mode collapse. EBMs do iterative sampling (slower) but can represent multiple modes more naturally. Training EBMs can be more stable in some respects (no discriminator–generator two-player game) but can diverge in others if MCMC hyperparameters are not set carefully."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"VAEs"),": Provide a lower-bound optimization approach, often yield smooth latent spaces, and have straightforward sampling. However, the approximate posterior might be restrictive or cause issues like posterior collapse. EBMs skip the explicit latent posterior in the simplest formulation."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Normalizing flows"),": Guarantee exact log-likelihood evaluation with direct sampling. But they require invertible transformations that can limit architecture design or memory usage. EBMs are unconstrained in that sense but pay the price with approximate partition functions."),"\n"),"\n",r.createElement(t.h3,{id:"76-mode-collapse-vs-multi-modal-distributions",style:{position:"relative"}},r.createElement(t.a,{href:"#76-mode-collapse-vs-multi-modal-distributions","aria-label":"76 mode collapse vs multi modal distributions permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.6 mode collapse vs. multi-modal distributions"),"\n",r.createElement(t.p,null,"GANs often face the infamous problem of mode collapse, where the generator only learns a subset of all possible modes. While EBMs ",r.createElement("em",null,"can")," also fail to discover all modes if the MCMC does not explore thoroughly, the approach is, in principle, less prone to strict collapse. Because we are pushing energies up or down in the entire space, multiple modes can remain. Nonetheless, in practice, short-run MCMC or poor hyperparameters can inadvertently fail to capture modes, so in that sense, partial collapses or slow mixing can occur."),"\n",r.createElement(t.hr),"\n",r.createElement(t.h2,{id:"8-other-applications-of-energy-based-models",style:{position:"relative"}},r.createElement(t.a,{href:"#8-other-applications-of-energy-based-models","aria-label":"8 other applications of energy based models permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8. other applications of energy-based models"),"\n",r.createElement(t.h3,{id:"81-out-of-distribution-detection-anomaly-detection",style:{position:"relative"}},r.createElement(t.a,{href:"#81-out-of-distribution-detection-anomaly-detection","aria-label":"81 out of distribution detection anomaly detection permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.1 out-of-distribution detection (anomaly detection)"),"\n",r.createElement(t.p,null,"One of the most promising applications for EBMs is ",r.createElement(l.A,null,"out-of-distribution (OOD) detection"),". Because ",r.createElement(o.A,{text:"\\(E_\\theta(x)\\)"})," is smaller (more negative) for typical in-distribution samples and larger (positive) for anomalies, a simple threshold on energy can serve as an OOD detector. In contrast, purely discriminative methods might produce high confidence for OOD inputs."),"\n",r.createElement(t.h3,{id:"82-classification-and-object-recognition-reinterpreted-as-ebms",style:{position:"relative"}},r.createElement(t.a,{href:"#82-classification-and-object-recognition-reinterpreted-as-ebms","aria-label":"82 classification and object recognition reinterpreted as ebms permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.2 classification and object recognition reinterpreted as ebms"),"\n",r.createElement(t.p,null,"As mentioned, a classifier can be recast as an EBM on ",r.createElement(o.A,{text:"\\((x, y)\\)"}),". The probability of label ",r.createElement(o.A,{text:"\\(y\\)"})," given ",r.createElement(o.A,{text:"\\(x\\)"})," might be:"),"\n",r.createElement(o.A,{text:"\\[\np_\\theta(y|x) = \\frac{\\exp\\left(-E_\\theta(x, y)\\right)}\n{\\sum_{y'} \\exp\\left(-E_\\theta(x, y')\\right)}.\n\\]"}),"\n",r.createElement(t.p,null,"By training ",r.createElement(o.A,{text:"\\(E_\\theta(x, y)\\)"})," with maximum likelihood or with an approximate scheme, we effectively do supervised learning. The advantage is that we can then also try to sample from ",r.createElement(o.A,{text:"\\(p_\\theta(x, y)\\)"})," to generate synthetic ",r.createElement(o.A,{text:"\\((x, y)\\)"})," pairs or do novelty detection for label–input mismatches."),"\n",r.createElement(t.h3,{id:"83-denoising-and-image-reconstruction-tasks",style:{position:"relative"}},r.createElement(t.a,{href:"#83-denoising-and-image-reconstruction-tasks","aria-label":"83 denoising and image reconstruction tasks permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.3 denoising and image reconstruction tasks"),"\n",r.createElement(t.p,null,"In tasks like image denoising or inpainting, we want to find ",r.createElement(o.A,{text:"\\(x\\)"})," close to a noisy observation ",r.createElement(o.A,{text:"\\(x_{\\text{noisy}}\\)"})," but also lying in the manifold of possible clean images. An EBM can solve:"),"\n",r.createElement(o.A,{text:"\\[\n\\min_x \\, E_\\theta(x) + \\lambda \\, d\\bigl(x, x_{\\text{noisy}}\\bigr),\n\\]"}),"\n",r.createElement(t.p,null,"where ",r.createElement(o.A,{text:"\\(d\\)"})," is some distance metric (e.g., ",r.createElement(o.A,{text:"\\(L_2\\)"}),"). In practice, a gradient-based routine can do an iterative refinement, pushing ",r.createElement(o.A,{text:"\\(x\\)"})," to low energy while remaining close to the observed data."),"\n",r.createElement(t.h3,{id:"84-natural-language-processing-perspectives",style:{position:"relative"}},r.createElement(t.a,{href:"#84-natural-language-processing-perspectives","aria-label":"84 natural language processing perspectives permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.4 natural language processing perspectives"),"\n",r.createElement(t.p,null,"Text data is discrete, which complicates gradient-based MCMC. Nonetheless, if we define an EBM in the latent continuous space of a neural text encoder, we can attempt to do sampling in that representation. Another approach is to apply Gumbel-softmax reparameterizations to handle discrete tokens. Although less common than, say, transformers for language modeling, EBM-based text generation has been explored in specialized research. It remains a challenging domain, partially because of how to incorporate language structure into the energy function and sampling procedure."),"\n",r.createElement(t.hr),"\n",r.createElement(t.h2,{id:"9-handling-training-instabilities",style:{position:"relative"}},r.createElement(t.a,{href:"#9-handling-training-instabilities","aria-label":"9 handling training instabilities permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9. handling training instabilities"),"\n",r.createElement(t.h3,{id:"91-common-pitfalls-divergence-scenarios-and-local-maxima",style:{position:"relative"}},r.createElement(t.a,{href:"#91-common-pitfalls-divergence-scenarios-and-local-maxima","aria-label":"91 common pitfalls divergence scenarios and local maxima permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9.1 common pitfalls: divergence scenarios and local maxima"),"\n",r.createElement(t.p,null,"A hallmark of EBM training is that it can diverge if hyperparameters are poorly tuned or if the network can trivially push down energies for both real and negative samples. Divergence may manifest as the sampling chain producing meaningless noise while the model's energies saturate at negative values. Once diverged, the model can be difficult to recover without reverting to an earlier checkpoint."),"\n",r.createElement(t.p,null,"Another potential pitfall is that the MCMC chain gets trapped in local minima or spurious modes, never exploring the true distribution. This results in inaccurate negative samples that do not provide the correct push-back in the CD training objective."),"\n",r.createElement(t.h3,{id:"92-hyperparameter-sensitivities-learning-rate-noise-step-count",style:{position:"relative"}},r.createElement(t.a,{href:"#92-hyperparameter-sensitivities-learning-rate-noise-step-count","aria-label":"92 hyperparameter sensitivities learning rate noise step count permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9.2 hyperparameter sensitivities (learning rate, noise, step count)"),"\n",r.createElement(t.p,null,"Practitioners typically note that EBM training is more sensitive to hyperparameters than other deep models. The learning rate for model updates, the number of MCMC steps per iteration, the step size inside each MCMC step, and the noise level are all crucial. A small mismatch can lead to either an overly smoothed or an extremely rugged energy surface."),"\n",r.createElement(t.p,null,"Tactics include:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"Start with smaller step sizes for MCMC and gradually increase."),"\n",r.createElement(t.li,null,"Introduce progressive noise schedules or cyclical approaches where the noise is varied in each epoch."),"\n",r.createElement(t.li,null,"Carefully tune the ratio of real data to negative data or the frequency with which the replay buffer is updated."),"\n"),"\n",r.createElement(t.h3,{id:"93-checkpoint-reloading-strategies-for-recovering-from-divergence",style:{position:"relative"}},r.createElement(t.a,{href:"#93-checkpoint-reloading-strategies-for-recovering-from-divergence","aria-label":"93 checkpoint reloading strategies for recovering from divergence permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9.3 checkpoint reloading strategies for recovering from divergence"),"\n",r.createElement(t.p,null,"When your EBM starts to diverge, you might see a sudden spike in energies or the negative sample energies saturating. As a fallback, one can revert to a stable checkpoint from a few epochs earlier. This strategy can salvage training runs that are otherwise irrecoverable, though it is essentially a manual fix. In some pipelines, automated heuristics watch for divergence signals and revert the model automatically to a stable checkpoint, perhaps with adjusted hyperparameters."),"\n",r.createElement(t.h3,{id:"94-regularization-and-architectural-constraints-for-stability",style:{position:"relative"}},r.createElement(t.a,{href:"#94-regularization-and-architectural-constraints-for-stability","aria-label":"94 regularization and architectural constraints for stability permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9.4 regularization and architectural constraints for stability"),"\n",r.createElement(t.p,null,"Adding an explicit penalty on ",r.createElement(o.A,{text:"\\(\\|E_\\theta(x)\\|\\)"})," or bounding the network's last layer can help limit runaways. Certain architectures that produce smoother energy surfaces might also be more stable in practice. For instance, networks that avoid ReLU dead zones and rely on activations that are differentiable at zero (like Swish or softplus) can yield more stable sampling updates. Additionally, spectral normalization of the convolution weights, commonly used in GAN discriminators, can help keep the EBM's gradients within a bounded range."),"\n",r.createElement(t.hr),"\n",r.createElement(t.h2,{id:"10-advanced-topics-and-extensions",style:{position:"relative"}},r.createElement(t.a,{href:"#10-advanced-topics-and-extensions","aria-label":"10 advanced topics and extensions permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"10. advanced topics and extensions"),"\n",r.createElement(t.h3,{id:"101-compositional-ebms-product-of-experts",style:{position:"relative"}},r.createElement(t.a,{href:"#101-compositional-ebms-product-of-experts","aria-label":"101 compositional ebms product of experts permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"10.1 compositional ebms (product of experts)"),"\n",r.createElement(t.p,null,"EBMs are naturally compositional because energies can be added. The product of experts framework (e.g., multiple RBMs combined) states that:"),"\n",r.createElement(o.A,{text:"\\[\nE_{\\text{combined}}(x) = E_{\\theta_1}(x) + E_{\\theta_2}(x),\n\\]"}),"\n",r.createElement(t.p,null,"thus leading to ",r.createElement(o.A,{text:"\\(p_{\\text{combined}}(x) = p_{\\theta_1}(x) \\, p_{\\theta_2}(x) / Z\\)"}),". This can incorporate multiple knowledge sources or constraints into a single EBM, each of which shapes the final distribution."),"\n",r.createElement(t.h3,{id:"102-hybrid-monte-carlo-and-advanced-sampling-methods",style:{position:"relative"}},r.createElement(t.a,{href:"#102-hybrid-monte-carlo-and-advanced-sampling-methods","aria-label":"102 hybrid monte carlo and advanced sampling methods permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"10.2 hybrid monte carlo and advanced sampling methods"),"\n",r.createElement(t.p,null,"Hybrid/Hamiltonian Monte Carlo (HMC) uses Hamiltonian dynamics to propose new states more efficiently in high dimensions, often resulting in better mixing than simpler methods. For EBMs with well-behaved gradients, HMC can be beneficial, though it is more complex to implement (requiring leapfrog steps, momentum variables, acceptance–rejection, etc.). Another direction is ",r.createElement(l.A,null,"tempered transitions"),", in which intermediate distributions bridge from a broad distribution (high temperature) to the target distribution (temperature 1), mitigating the risk of being stuck in a single mode."),"\n",r.createElement(t.h3,{id:"103-energy-based-interpretation-of-classifiers-jem-revisited",style:{position:"relative"}},r.createElement(t.a,{href:"#103-energy-based-interpretation-of-classifiers-jem-revisited","aria-label":"103 energy based interpretation of classifiers jem revisited permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"10.3 energy-based interpretation of classifiers (jem revisited)"),"\n",r.createElement(t.p,null,"JEM's approach can be generalized to many classification tasks: each label ",r.createElement(o.A,{text:"\\(y\\)"})," is associated with a sub-landscape in ",r.createElement(o.A,{text:"\\(x\\)"}),"-space, and the relative energies define ",r.createElement(o.A,{text:"\\(p_\\theta(y|x)\\)"}),". One can also attempt open-set recognition by checking the absolute energy ",r.createElement(o.A,{text:"\\(E_\\theta(x)\\)"}),". If it is too large, then ",r.createElement(o.A,{text:"\\(x\\)"})," might be out of distribution, even if ",r.createElement(o.A,{text:"\\(\\arg\\min_y E_\\theta(x,y)\\)"})," yields some label. This gives classifiers a built-in OOD rejection feature, at least in theory."),"\n",r.createElement(t.h3,{id:"104-incorporating-domain-knowledge-or-symbolic-constraints",style:{position:"relative"}},r.createElement(t.a,{href:"#104-incorporating-domain-knowledge-or-symbolic-constraints","aria-label":"104 incorporating domain knowledge or symbolic constraints permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"10.4 incorporating domain knowledge or symbolic constraints"),"\n",r.createElement(t.p,null,"Because EBMs revolve around shaping an energy function, domain knowledge can be injected as an additional term in the energy. For example, if we know certain constraints on ",r.createElement(o.A,{text:"\\(x\\)"})," (like geometric constraints, or physical constraints for robotics states), we can add a penalty to ",r.createElement(o.A,{text:"\\(E_\\theta(x)\\)"})," that enforces or encourages them. This can be done by a hand-designed potential or an auxiliary network that encodes domain-specific knowledge. The synergy between learned features and hand-crafted constraints can be quite powerful."),"\n",r.createElement(t.hr),"\n",r.createElement(t.h2,{id:"11-implementation-walkthrough-example-code",style:{position:"relative"}},r.createElement(t.a,{href:"#11-implementation-walkthrough-example-code","aria-label":"11 implementation walkthrough example code permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"11. implementation walkthrough (example code)"),"\n",r.createElement(t.p,null,"In this section, let's go step by step through a code example of training an EBM for MNIST digit generation. While we focus on MNIST for demonstration, similar principles extend to other image datasets or even other modalities, with changes mainly in the network architecture and hyperparameters."),"\n",r.createElement(t.h3,{id:"111-data-loading-and-normalization-steps",style:{position:"relative"}},r.createElement(t.a,{href:"#111-data-loading-and-normalization-steps","aria-label":"111 data loading and normalization steps permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"11.1 data loading and normalization steps"),"\n",r.createElement(t.p,null,"Below is a typical snippet (PyTorch-based) for loading MNIST, normalizing pixel values from -1 to 1 for convenience:"),"\n",r.createElement(s.A,{text:'\nfrom torchvision.datasets import MNIST\nfrom torchvision import transforms\nimport torch.utils.data as data\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))  # scale to [-1, 1]\n])\n\ntrain_set = MNIST(root="data", train=True, transform=transform, download=True)\ntest_set = MNIST(root="data", train=False, transform=transform, download=True)\n\ntrain_loader = data.DataLoader(train_set, batch_size=128, shuffle=True, num_workers=4)\ntest_loader = data.DataLoader(test_set, batch_size=128, shuffle=False, num_workers=4)\n'}),"\n",r.createElement(t.h3,{id:"112-model-architecture-simple-cnn-or-resnet",style:{position:"relative"}},r.createElement(t.a,{href:"#112-model-architecture-simple-cnn-or-resnet","aria-label":"112 model architecture simple cnn or resnet permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"11.2 model architecture (simple cnn or resnet)"),"\n",r.createElement(t.p,null,"We define a small CNN to output a single scalar for each 28×28 input image:"),"\n",r.createElement(s.A,{text:"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\nclass CNNModel(nn.Module):\n    def __init__(self, hidden_features=32):\n        super().__init__()\n        c_hid1 = hidden_features // 2\n        c_hid2 = hidden_features\n        c_hid3 = hidden_features * 2\n        self.cnn_layers = nn.Sequential(\n            nn.Conv2d(1, c_hid1, kernel_size=5, stride=2, padding=4),\n            Swish(),\n            nn.Conv2d(c_hid1, c_hid2, kernel_size=3, stride=2, padding=1),\n            Swish(),\n            nn.Conv2d(c_hid2, c_hid3, kernel_size=3, stride=2, padding=1),\n            Swish(),\n            nn.Conv2d(c_hid3, c_hid3, kernel_size=3, stride=2, padding=1),\n            Swish(),\n            nn.Flatten(),\n            nn.Linear(c_hid3*4, c_hid3),\n            Swish(),\n            nn.Linear(c_hid3, 1)\n        )\n\n    def forward(self, x):\n        return self.cnn_layers(x).squeeze(dim=-1)\n"}),"\n",r.createElement(t.p,null,"Here, ",r.createElement(o.A,{text:"\\(E_\\theta(x)\\)"})," is effectively ",r.createElement(o.A,{text:"\\(-\\text{output}\\)"})," if we follow the negative sign convention. In practice, you can store ",r.createElement(o.A,{text:"\\(E_\\theta(x)\\)"})," as the direct output and interpret the sign carefully in the training objective."),"\n",r.createElement(t.h3,{id:"113-contrastive-divergence-training-loop-real-vs-fake-images-buffer-sampling-strategy-loss-function-details",style:{position:"relative"}},r.createElement(t.a,{href:"#113-contrastive-divergence-training-loop-real-vs-fake-images-buffer-sampling-strategy-loss-function-details","aria-label":"113 contrastive divergence training loop real vs fake images buffer sampling strategy loss function details permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"11.3 contrastive divergence training loop: real vs. fake images, buffer sampling strategy, loss function details"),"\n",r.createElement(t.p,null,"We now define a replay buffer for negative samples:"),"\n",r.createElement(s.A,{text:"\nimport random\nimport numpy as np\n\nclass Sampler:\n    def __init__(self, model, img_shape, sample_size=128, max_len=8192):\n        self.model = model\n        self.img_shape = img_shape\n        self.sample_size = sample_size\n        self.max_len = max_len\n        self.examples = [(torch.rand((1,) + img_shape) * 2 - 1) for _ in range(self.sample_size)]\n\n    def sample_new_exmps(self, steps=60, step_size=10):\n        n_new = np.random.binomial(self.sample_size, 0.05)\n        rand_imgs = torch.rand((n_new,) + self.img_shape) * 2 - 1\n        old_imgs = torch.cat(random.choices(self.examples, k=self.sample_size-n_new), dim=0)\n        inp_imgs = torch.cat([rand_imgs, old_imgs], dim=0).detach()\n        inp_imgs = inp_imgs.to(next(self.model.parameters()).device)\n\n        inp_imgs = self.generate_samples(self.model, inp_imgs, steps=steps, step_size=step_size)\n\n        self.examples = list(inp_imgs.cpu().chunk(self.sample_size, dim=0)) + self.examples\n        self.examples = self.examples[:self.max_len]\n        return inp_imgs\n\n    @staticmethod\n    def generate_samples(model, inp_imgs, steps=60, step_size=10, return_img_per_step=False):\n        is_training = model.training\n        model.eval()\n        for p in model.parameters():\n            p.requires_grad = False\n        inp_imgs.requires_grad = True\n\n        had_gradients_enabled = torch.is_grad_enabled()\n        torch.set_grad_enabled(True)\n\n        noise = torch.randn(inp_imgs.shape, device=inp_imgs.device)\n        imgs_per_step = []\n\n        for _ in range(steps):\n            noise.normal_(0, 0.005)\n            inp_imgs.data.add_(noise.data)\n            inp_imgs.data.clamp_(-1.0, 1.0)\n\n            out_imgs = -model(inp_imgs)  # negative sign for convenience\n            out_imgs.sum().backward()\n            inp_imgs.grad.data.clamp_(-0.03, 0.03)\n            inp_imgs.data.add_(-step_size * inp_imgs.grad.data)\n            inp_imgs.grad.detach_()\n            inp_imgs.grad.zero_()\n            inp_imgs.data.clamp_(-1.0, 1.0)\n\n            if return_img_per_step:\n                imgs_per_step.append(inp_imgs.clone().detach())\n\n        for p in model.parameters():\n            p.requires_grad = True\n        model.train(is_training)\n        torch.set_grad_enabled(had_gradients_enabled)\n\n        if return_img_per_step:\n            return torch.stack(imgs_per_step, dim=0)\n        else:\n            return inp_imgs\n"}),"\n",r.createElement(t.p,null,"Next, we combine everything into a training loop using PyTorch Lightning for convenience:"),"\n",r.createElement(s.A,{text:"\nimport pytorch_lightning as pl\nimport torch.optim as optim\n\nclass DeepEnergyModel(pl.LightningModule):\n    def __init__(self, img_shape, batch_size, alpha=0.1, lr=1e-4, beta1=0.0):\n        super().__init__()\n        self.save_hyperparameters()\n        self.cnn = CNNModel()\n        self.sampler = Sampler(self.cnn, img_shape=img_shape, sample_size=batch_size)\n        self.example_input_array = torch.zeros(1, *img_shape)\n\n    def forward(self, x):\n        return self.cnn(x)\n\n    def configure_optimizers(self):\n        optimizer = optim.Adam(self.parameters(), lr=self.hparams.lr, betas=(self.hparams.beta1, 0.999))\n        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.97)\n        return [optimizer], [scheduler]\n\n    def training_step(self, batch, batch_idx):\n        real_imgs, _ = batch\n        noise = torch.randn_like(real_imgs) * 0.005\n        real_imgs = (real_imgs + noise).clamp(-1., 1.)\n\n        fake_imgs = self.sampler.sample_new_exmps(steps=60, step_size=10)\n        inp_imgs = torch.cat([real_imgs, fake_imgs], dim=0)\n        real_out, fake_out = self.cnn(inp_imgs).chunk(2, dim=0)\n\n        # alpha * (real_out^2 + fake_out^2) is the regularization\n        reg_loss = self.hparams.alpha * (real_out**2 + fake_out**2).mean()\n        cdiv_loss = fake_out.mean() - real_out.mean()\n        loss = reg_loss + cdiv_loss\n\n        self.log('train_loss', loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        real_imgs, _ = batch\n        rand_imgs = torch.rand_like(real_imgs) * 2 - 1\n        inp_imgs = torch.cat([real_imgs, rand_imgs], dim=0)\n        real_out, fake_out = self.cnn(inp_imgs).chunk(2, dim=0)\n        cdiv = fake_out.mean() - real_out.mean()\n        self.log('val_cdiv', cdiv)\n"}),"\n",r.createElement(t.h3,{id:"114-logging-and-visualization-tensorboard-metrics",style:{position:"relative"}},r.createElement(t.a,{href:"#114-logging-and-visualization-tensorboard-metrics","aria-label":"114 logging and visualization tensorboard metrics permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"11.4 logging and visualization (tensorboard, metrics)"),"\n",r.createElement(t.p,null,"We might add callbacks to log images or negative samples from the replay buffer at each epoch, visualize their progression over MCMC steps, or track how out-of-distribution inputs are scored. These details are typically handled via standard PyTorch Lightning callback mechanisms or custom logging code."),"\n",r.createElement(t.h3,{id:"115-example-results-mnist-digit-generation",style:{position:"relative"}},r.createElement(t.a,{href:"#115-example-results-mnist-digit-generation","aria-label":"115 example results mnist digit generation permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"11.5 example results: mnist digit generation"),"\n",r.createElement(t.p,null,"Trained with appropriate hyperparameters (e.g., 60 epochs, ",r.createElement(o.A,{text:"\\(lr = 1e-4\\)"}),", etc.), this model begins to generate digits that look like MNIST samples. Below is a conceptual example of how one might generate images from scratch:"),"\n",r.createElement(s.A,{text:"\ndef generate_digits(model, num_steps=256, batch_size=8):\n    with torch.no_grad():\n        start_imgs = torch.rand((batch_size, 1, 28, 28)) * 2 - 1\n    imgs = Sampler.generate_samples(model.cnn, start_imgs, steps=num_steps, step_size=10, return_img_per_step=True)\n    return imgs\n\nmodel = DeepEnergyModel((1,28,28), batch_size=128)\n# Suppose the model is trained or loaded from checkpoint\n\ngen_imgs_per_step = generate_digits(model)\n# We can now visualize gen_imgs_per_step to see the evolution from random noise to digit-like patterns.\n"}),"\n",r.createElement(t.hr),"\n",r.createElement(t.h2,{id:"12-conclusion",style:{position:"relative"}},r.createElement(t.a,{href:"#12-conclusion","aria-label":"12 conclusion permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"12. conclusion"),"\n",r.createElement(t.p,null,"Energy-based models provide a powerful and elegant framework for learning flexible probability distributions by defining a scalar energy function ",r.createElement(o.A,{text:"\\(E_\\theta(x)\\)"}),". Through exponentiation and normalization, these models describe a probability distribution that is, in principle, capable of capturing complex, multimodal phenomena. The ability to unify discriminative and generative modeling is attractive, as is the natural anomaly detection capability derived from the scalar energy."),"\n",r.createElement(t.p,null,"However, EBMs also bring computational challenges. The partition function is intractable, forcing us to rely on approximate gradient-based sampling. Training loops generally incorporate techniques such as Contrastive Divergence, replay buffers, and carefully tuned hyperparameters for MCMC steps (step sizes, noise levels, gradient clipping, etc.). Without these stabilizations, EBMs can diverge."),"\n",r.createElement(t.p,null,"Despite these difficulties, a growing body of research (Du and Mordatch, 2019; Nijkamp and gang, 2019; Grathwohl and gang, 2020, among others) demonstrates that modern deep networks used as energy functions can rival or exceed performance of more widely used generative methods under certain conditions. They also open interesting avenues for tasks like out-of-distribution detection, compositional modeling (e.g., product of experts), and integrated classifier–generator systems (JEM)."),"\n",r.createElement(t.p,null,"In practice, learning to harness EBMs can be a rewarding endeavor: the conceptual clarity of shaping an energy landscape is appealing, and the direct interpretability of energies (especially for anomaly detection and reconstructions) can be enlightening. If you are willing to invest in hyperparameter tuning, advanced sampling procedures, and possibly fallback checkpoint reloading, energy-based deep models can prove to be an invaluable addition to a modern machine learning toolkit."),"\n",r.createElement(t.p,null,"The next time you face a problem where you need both generative modeling and a robust measure of sample plausibility (and you are not entirely satisfied with typical VAEs or GANs), remember that energy-based models are a strong contender. As research continues to develop improved training and inference methods, it is likely that EBMs will remain an influential pillar in both theoretical and applied machine learning for years to come."))}var h=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,i.RP)(),e.components);return t?r.createElement(t,e,r.createElement(c,e)):c(e)},m=a(36710),d=a(58481),p=a.n(d),g=a(36310),u=a(87245),f=a(27042),v=a(59849),b=a(5591),y=a(61122),E=a(9219),x=a(33203),w=a(95751),_=a(94328),S=a(80791),k=a(78137);const M=e=>{let{toc:t}=e;if(!t||!t.items)return null;return r.createElement("nav",{className:S.R},r.createElement("ul",null,t.items.map(((e,t)=>r.createElement("li",{key:t},r.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const a=t.replace("#",""),n=document.getElementById(a);n&&n.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&r.createElement(M,{toc:{items:e.items}}))))))};function z(e){let{data:{mdx:t,allMdx:l,allPostImages:s},children:o}=e;const{frontmatter:c,body:h,tableOfContents:m}=t,d=c.index,v=c.slug.split("/")[1],S=l.nodes.filter((e=>e.frontmatter.slug.includes(`/${v}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),z=S.findIndex((e=>e.frontmatter.index===d)),H=S[z+1],A=S[z-1],C=c.slug.replace(/\/$/,""),T=/[^/]*$/.exec(C)[0],B=`posts/${v}/content/${T}/`,{0:I,1:V}=(0,r.useState)(c.flagWideLayoutByDefault),{0:N,1:L}=(0,r.useState)(!1);var D;(0,r.useEffect)((()=>{L(!0);const e=setTimeout((()=>L(!1)),340);return()=>clearTimeout(e)}),[I]),"adventures"===v?D=E.cb:"research"===v?D=E.Qh:"thoughts"===v&&(D=E.T6);const j=p()(h).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,G=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),a=e%60;return a<=30?`~${t}${a>0?".5":""} h`:`~${t+1} h`}(Math.ceil(j/D)+(c.extraReadTimeMin||0)),P=[{flag:c.flagDraft,component:()=>Promise.all([a.e(3231),a.e(8809)]).then(a.bind(a,28809))},{flag:c.flagMindfuckery,component:()=>Promise.all([a.e(3231),a.e(2471)]).then(a.bind(a,67709))},{flag:c.flagRewrite,component:()=>Promise.all([a.e(3231),a.e(6764)]).then(a.bind(a,62002))},{flag:c.flagOffensive,component:()=>Promise.all([a.e(3231),a.e(2443)]).then(a.bind(a,17681))},{flag:c.flagProfane,component:()=>Promise.all([a.e(3231),a.e(8048)]).then(a.bind(a,53286))},{flag:c.flagMultilingual,component:()=>Promise.all([a.e(3231),a.e(4069)]).then(a.bind(a,78831))},{flag:c.flagUnreliably,component:()=>Promise.all([a.e(3231),a.e(3417)]).then(a.bind(a,8179))},{flag:c.flagPolitical,component:()=>Promise.all([a.e(3231),a.e(5195)]).then(a.bind(a,30433))},{flag:c.flagCognitohazard,component:()=>Promise.all([a.e(3231),a.e(3175)]).then(a.bind(a,8413))},{flag:c.flagHidden,component:()=>Promise.all([a.e(3231),a.e(9556)]).then(a.bind(a,14794))}],{0:q,1:O}=(0,r.useState)([]);return(0,r.useEffect)((()=>{P.forEach((e=>{let{flag:t,component:a}=e;t&&a().then((e=>{O((t=>[].concat((0,n.A)(t),[e.default])))}))}))}),[]),r.createElement(f.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},r.createElement(b.A,{postNumber:c.index,date:c.date,updated:c.updated,readTime:G,difficulty:c.difficultyLevel,title:c.title,desc:c.desc,banner:c.banner,section:v,postKey:T,isMindfuckery:c.flagMindfuckery,mainTag:c.mainTag}),r.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},c.otherTags.map(((e,t)=>r.createElement("span",{key:t,className:`noselect ${k.MW}`,style:{margin:"0 5px 5px 0"}},e)))),r.createElement("div",{className:"postBody"},r.createElement(M,{toc:m})),r.createElement("br"),r.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},r.createElement(f.P.button,{className:`noselect ${_.pb}`,id:_.xG,onClick:()=>{V(!I)},whileTap:{scale:.93}},r.createElement(f.P.div,{className:w.DJ,key:I,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},I?"Switch to default layout":"Switch to wide layout"))),r.createElement("br"),r.createElement("div",{className:"postBody",style:{margin:I?"0 -14%":"",maxWidth:I?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},r.createElement("div",{className:`${_.P_} ${N?_.Xn:_.qG}`},q.map(((e,t)=>r.createElement(e,{key:t}))),c.indexCourse?r.createElement(x.A,{index:c.indexCourse,category:c.courseCategoryName}):"",r.createElement(g.Z.Provider,{value:{images:s.nodes,basePath:B.replace(/\/$/,"")+"/"}},r.createElement(i.xA,{components:{Image:u.A}},o)))),r.createElement(y.A,{nextPost:H,lastPost:A,keyCurrent:T,section:v}))}function H(e){return r.createElement(z,e,r.createElement(h,e))}function A(e){var t,a,n,i,l;let{data:s}=e;const{frontmatter:o}=s.mdx,c=o.titleSEO||o.title,h=o.titleOG||c,d=o.titleTwitter||c,p=o.descSEO||o.desc,g=o.descOG||p,u=o.descTwitter||p,f=o.schemaType||"BlogPosting",b=o.keywordsSEO,y=o.date,E=o.updated||y,x=o.imageOG||(null===(t=o.banner)||void 0===t||null===(a=t.childImageSharp)||void 0===a||null===(n=a.gatsbyImageData)||void 0===n||null===(i=n.images)||void 0===i||null===(l=i.fallback)||void 0===l?void 0:l.src),w=o.imageAltOG||g,_=o.imageTwitter||x,S=o.imageAltTwitter||u,k=o.canonicalURL,M=o.flagHidden||!1,z=o.mainTag||"Posts",H=o.slug.split("/")[1]||"posts",{siteUrl:A}=(0,m.Q)(),C={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:A},{"@type":"ListItem",position:2,name:z,item:`${A}/${o.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${A}${o.slug}`}]};return r.createElement(v.A,{title:c+" - avrtt.blog",titleOG:h,titleTwitter:d,description:p,descriptionOG:g,descriptionTwitter:u,schemaType:f,keywords:b,datePublished:y,dateModified:E,imageOG:x,imageAltOG:w,imageTwitter:_,imageAltTwitter:S,canonicalUrl:k,flagHidden:M,mainTag:z,section:H,type:"article"},r.createElement("script",{type:"application/ld+json"},JSON.stringify(C)))}},96098:function(e,t,a){var n=a(96540),i=a(7978);t.A=e=>{let{text:t}=e;return n.createElement(i.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-js-content-file-path-src-pages-posts-research-energy-based-models-mdx-a14968f7a2a724c91ffe.js.map