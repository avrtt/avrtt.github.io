"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[6889],{3962:function(e,t){t.A="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iaXNvLTg4NTktMSI/Pgo8IS0tIEdlbmVyYXRvcjogQWRvYmUgSWxsdXN0cmF0b3IgMTYuMC4wLCBTVkcgRXhwb3J0IFBsdWctSW4gLiBTVkcgVmVyc2lvbjogNi4wMCBCdWlsZCAwKSAgLS0+CjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+CjxzdmcgdmVyc2lvbj0iMS4xIiBpZD0iQ2FwYV8xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB4PSIwcHgiIHk9IjBweCIKCSB3aWR0aD0iNDE2Ljk3OXB4IiBoZWlnaHQ9IjQxNi45NzlweCIgdmlld0JveD0iMCAwIDQxNi45NzkgNDE2Ljk3OSIgc3R5bGU9ImVuYWJsZS1iYWNrZ3JvdW5kOm5ldyAwIDAgNDE2Ljk3OSA0MTYuOTc5OyIKCSB4bWw6c3BhY2U9InByZXNlcnZlIj4KPGc+Cgk8cGF0aCBkPSJNMzU2LjAwNCw2MS4xNTZjLTgxLjM3LTgxLjQ3LTIxMy4zNzctODEuNTUxLTI5NC44NDgtMC4xODJjLTgxLjQ3LDgxLjM3MS04MS41NTIsMjEzLjM3OS0wLjE4MSwyOTQuODUKCQljODEuMzY5LDgxLjQ3LDIxMy4zNzgsODEuNTUxLDI5NC44NDksMC4xODFDNDM3LjI5MywyNzQuNjM2LDQzNy4zNzUsMTQyLjYyNiwzNTYuMDA0LDYxLjE1NnogTTIzNy42LDM0MC43ODYKCQljMCwzLjIxNy0yLjYwNyw1LjgyMi01LjgyMiw1LjgyMmgtNDYuNTc2Yy0zLjIxNSwwLTUuODIyLTIuNjA1LTUuODIyLTUuODIyVjE2Ny44ODVjMC0zLjIxNywyLjYwNy01LjgyMiw1LjgyMi01LjgyMmg0Ni41NzYKCQljMy4yMTUsMCw1LjgyMiwyLjYwNCw1LjgyMiw1LjgyMlYzNDAuNzg2eiBNMjA4LjQ5LDEzNy45MDFjLTE4LjYxOCwwLTMzLjc2Ni0xNS4xNDYtMzMuNzY2LTMzLjc2NQoJCWMwLTE4LjYxNywxNS4xNDctMzMuNzY2LDMzLjc2Ni0zMy43NjZjMTguNjE5LDAsMzMuNzY2LDE1LjE0OCwzMy43NjYsMzMuNzY2QzI0Mi4yNTYsMTIyLjc1NSwyMjcuMTA3LDEzNy45MDEsMjA4LjQ5LDEzNy45MDF6Ii8+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPC9zdmc+Cg=="},9360:function(e,t,n){n.d(t,{A:function(){return r}});var a=n(96540),l=n(3962),i="styles-module--tooltiptext--a263b";var r=e=>{let{text:t,isBadge:n=!1}=e;const{0:r,1:s}=(0,a.useState)(!1),o=(0,a.useRef)(null);return(0,a.useEffect)((()=>{function e(e){o.current&&e.target instanceof Node&&!o.current.contains(e.target)&&s(!1)}return document.addEventListener("click",e),()=>{document.removeEventListener("click",e)}}),[]),a.createElement("span",{className:"styles-module--tooltipWrapper--75ebf",ref:o},a.createElement("img",{id:n?"styles-module--infoBadge--e3d66":"styles-module--info--26c1f",src:l.A,alt:"info",onClick:e=>{e.stopPropagation(),s((e=>!e))}}),a.createElement("span",{className:r?`${i} styles-module--visible--c063c`:i},t))}},90548:function(e,t,n){var a=n(96540),l=n(7978);t.A=e=>{let{text:t}=e;return a.createElement(l.A,null,t)}},90769:function(e,t,n){n.r(t),n.d(t,{Head:function(){return _},PostTemplate:function(){return z},default:function(){return C}});var a=n(28453),l=n(96540),i=n(9360),r=n(61992),s=n(62087),o=n(90548);function c(e){const t=Object.assign({h2:"h2",a:"a",span:"span",h3:"h3",p:"p",ul:"ul",li:"li",strong:"strong",hr:"hr",h4:"h4"},(0,a.RP)(),e.components),{Image:n}=t;return n||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),l.createElement(l.Fragment,null,"\n",l.createElement("br"),"\n","\n",l.createElement(t.h2,{id:"discrete-distributions",style:{position:"relative"}},l.createElement(t.a,{href:"#discrete-distributions","aria-label":"discrete distributions permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Discrete distributions"),"\n",l.createElement(t.h3,{id:"bernoulli-distribution",style:{position:"relative"}},l.createElement(t.a,{href:"#bernoulli-distribution","aria-label":"bernoulli distribution permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Bernoulli distribution"),"\n",l.createElement(t.p,null,"The ",l.createElement(r.A,null,"Bernoulli distribution"),' is the simplest discrete distribution, describing a single binary trial that can yield either "success" (',l.createElement(o.A,{text:"\\( x = 1 \\)"}),") with probability ",l.createElement(o.A,{text:"\\( p \\)"}),' or "failure" (',l.createElement(o.A,{text:"\\( x = 0 \\)"}),") with probability ",l.createElement(o.A,{text:"\\( 1 - p \\)"}),". The probability mass function (pmf) is:"),"\n",l.createElement(o.A,{text:"\\[\nP(X = x) = p^x (1 - p)^{1 - x}, \\quad x \\in \\{0, 1\\},\\ p \\in [0,1].\n\\]"}),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Support"),": ",l.createElement(o.A,{text:"\\( x \\in \\{0, 1\\} \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameters"),": success probability ",l.createElement(o.A,{text:"\\( p \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Skewness & kurtosis"),": for ",l.createElement(o.A,{text:"\\( p \\neq 0.5 \\)"}),", the distribution is skewed; it becomes symmetric at ",l.createElement(o.A,{text:"\\( p = 0.5 \\)"}),". Kurtosis can be relatively high because the distribution is concentrated on two points."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Use cases"),': The Bernoulli distribution is at the core of binary classification labels in machine learning. It is often used to model success/failure processes such as "clicked/not clicked" in online advertising.'),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameter estimation"),": The maximum likelihood estimator (MLE) for ",l.createElement(o.A,{text:"\\( p \\)"})," is the sample mean of the observed successes. For instance, if you observe ",l.createElement(o.A,{text:"\\( n \\)"})," trials with ",l.createElement(o.A,{text:"\\( \\sum x_i = s \\)"})," successes, the MLE is ",l.createElement(o.A,{text:"\\( \\hat{p} = s / n \\)"}),"."),"\n"),"\n",l.createElement(n,{alt:"Bernoulli pmf",path:"",caption:"A simple Bernoulli pmf with different values of p.",zoom:"false"}),"\n",l.createElement(t.h3,{id:"binomial-distribution",style:{position:"relative"}},l.createElement(t.a,{href:"#binomial-distribution","aria-label":"binomial distribution permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Binomial distribution"),"\n",l.createElement(t.p,null,"A ",l.createElement(r.A,null,"Binomial distribution")," describes the number of successes in a fixed number ",l.createElement(o.A,{text:"\\( n \\)"})," of independent Bernoulli trials, each with probability of success ",l.createElement(o.A,{text:"\\( p \\)"}),". Its pmf is:"),"\n",l.createElement(o.A,{text:"\\[\nP(X = k) = \\binom{n}{k} p^k (1 - p)^{n - k}, \\quad k \\in \\{0, 1, \\dots, n\\}.\n\\]"}),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Support"),": ",l.createElement(o.A,{text:"\\( k \\in \\{0, 1, \\ldots, n\\} \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameters"),": number of trials ",l.createElement(o.A,{text:"\\( n \\)"}),", success probability ",l.createElement(o.A,{text:"\\( p \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Mean & variance"),": ",l.createElement(o.A,{text:"\\( \\mathbb{E}[X] = np \\)"})," and ",l.createElement(o.A,{text:"\\( \\mathrm{Var}[X] = np(1-p) \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Skewness"),": ",l.createElement(o.A,{text:"\\( \\frac{1-2p}{\\sqrt{np(1-p)}} \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Use cases"),": Frequently used in data science for modeling counts of occurrences across multiple trials, such as the number of customers who respond to a campaign out of ",l.createElement(o.A,{text:"\\( n \\)"})," recipients."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameter estimation"),": The MLE approach again yields ",l.createElement(o.A,{text:"\\( \\hat{p} = \\frac{\\sum_k k}{n\\,m} \\)"}),", where ",l.createElement(o.A,{text:"\\( m \\)"})," is the total number of binomial experiments (if data come in aggregated form). If you have separate sequences of trials, you typically estimate ",l.createElement(o.A,{text:"\\( p \\)"})," as the overall fraction of successes."),"\n"),"\n",l.createElement(n,{alt:"Binomial pmf",path:"",caption:"Example binomial distribution with n=10, p=0.3 and n=10, p=0.7.",zoom:"false"}),"\n",l.createElement(t.h3,{id:"poisson-distribution",style:{position:"relative"}},l.createElement(t.a,{href:"#poisson-distribution","aria-label":"poisson distribution permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Poisson distribution"),"\n",l.createElement(t.p,null,"The ",l.createElement(r.A,null,"Poisson distribution")," is used for modeling the number of events occurring in a fixed interval, assuming events happen with a known constant mean rate ",l.createElement(o.A,{text:"\\( \\lambda \\)"})," and independently of the time since the last event. Its pmf is:"),"\n",l.createElement(o.A,{text:"\\[\nP(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}, \\quad k \\in \\{0, 1, 2, \\dots\\}.\n\\]"}),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Support"),": ",l.createElement(o.A,{text:"\\( k = 0, 1, 2, \\dots \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameter"),": rate ",l.createElement(o.A,{text:"\\( \\lambda \\)"})," (> 0)"),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Mean & variance"),": ",l.createElement(o.A,{text:"\\( \\mathbb{E}[X] = \\mathrm{Var}[X] = \\lambda \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Skewness & kurtosis"),": Skewness decreases as ",l.createElement(o.A,{text:"\\( \\lambda \\)"})," grows, but it remains right-skewed."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Use cases"),": Modeling counts of rare or random events, e.g., number of arrivals in a queue, number of network requests in a given time frame."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameter estimation"),": The MLE for ",l.createElement(o.A,{text:"\\( \\lambda \\)"})," is the sample mean of observed counts."),"\n"),"\n",l.createElement(t.p,null,"One interesting property is that if ",l.createElement(o.A,{text:"\\( X \\sim \\text{Poisson}(\\lambda) \\)"})," and ",l.createElement(o.A,{text:"\\( Y \\sim \\text{Poisson}(\\mu) \\)"})," are independent, then ",l.createElement(o.A,{text:"\\( X + Y \\)"})," is ",l.createElement(o.A,{text:"\\( \\text{Poisson}(\\lambda + \\mu) \\)"}),"."),"\n",l.createElement(t.h3,{id:"geometric-distribution",style:{position:"relative"}},l.createElement(t.a,{href:"#geometric-distribution","aria-label":"geometric distribution permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Geometric distribution"),"\n",l.createElement(t.p,null,"A ",l.createElement(r.A,null,"Geometric distribution"),' models the number of trials needed to get the first success (or, in another convention, the number of failures before the first success). We\'ll use the "number of trials" version. Its pmf is:'),"\n",l.createElement(o.A,{text:"\\[\nP(X = k) = (1-p)^{k-1} p, \\quad k \\in \\{1,2,\\ldots\\}, \\, p \\in [0,1].\n\\]"}),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Support"),": ",l.createElement(o.A,{text:"\\( k = 1, 2, 3, \\dots \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameter"),": success probability ",l.createElement(o.A,{text:"\\( p \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Mean & variance"),": ",l.createElement(o.A,{text:"\\( \\mathbb{E}[X] = \\frac{1}{p} \\)"}),", ",l.createElement(o.A,{text:"\\( \\mathrm{Var}[X] = \\frac{1-p}{p^2} \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Memoryless property"),": The probability of success in future trials is independent of the number of failures so far."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Use cases"),': Modeling discrete "waiting time" until an event occurs, such as how many times you must roll a die before seeing a certain face.'),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameter estimation"),": The MLE for ",l.createElement(o.A,{text:"\\( p \\)"})," is ",l.createElement(o.A,{text:"\\( \\hat{p} = \\frac{1}{\\overline{X}} \\)"}),", where ",l.createElement(o.A,{text:"\\( \\overline{X} \\)"})," is the sample mean of the observed counts until first success."),"\n"),"\n",l.createElement(t.h3,{id:"negative-binomial-distribution",style:{position:"relative"}},l.createElement(t.a,{href:"#negative-binomial-distribution","aria-label":"negative binomial distribution permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Negative binomial distribution"),"\n",l.createElement(t.p,null,"The ",l.createElement(r.A,null,"Negative binomial distribution")," can be viewed as the number of successes before a specified number of failures is reached, or as a sum of a fixed number of geometric random variables. One popular parametrization (the number of successes before ",l.createElement(o.A,{text:"\\( r \\)"})," failures) has the pmf:"),"\n",l.createElement(o.A,{text:"\\[\nP(X = k) = \\binom{k + r - 1}{k} (1-p)^r p^k, \\quad k \\in \\{0, 1, \\dots\\}.\n\\]"}),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameters"),": ",l.createElement(o.A,{text:"\\( r \\)"})," (the number of failures) and ",l.createElement(o.A,{text:"\\( p \\)"})," (success probability)"),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Mean & variance"),": ",l.createElement(o.A,{text:"\\( \\mathbb{E}[X] = \\frac{rp}{1-p} \\)"}),", ",l.createElement(o.A,{text:"\\( \\mathrm{Var}[X] = \\frac{rp}{(1-p)^2} \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Use cases"),": Particularly useful when the variance in the data exceeds the mean (overdispersion), unlike the Poisson distribution. Common in modeling number of successes in real-world count processes, e.g., number of insurance claims before a threshold of losses is reached."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameter estimation"),": Methods include MLE and method of moments. Overdispersion can make direct estimation trickier than Poisson. Specialized software functions are often used."),"\n"),"\n",l.createElement(t.h3,{id:"multinomial-distribution",style:{position:"relative"}},l.createElement(t.a,{href:"#multinomial-distribution","aria-label":"multinomial distribution permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Multinomial distribution"),"\n",l.createElement(t.p,null,"A ",l.createElement(r.A,null,"Multinomial distribution")," generalizes the binomial to more than two possible outcomes. If each trial can result in one of ",l.createElement(o.A,{text:"\\( k \\)"})," categories, with probabilities ",l.createElement(o.A,{text:"\\( p_1, p_2, \\dots, p_k \\)"})," (summing to 1), and you perform ",l.createElement(o.A,{text:"\\( n \\)"})," independent trials, the probability of observing counts ",l.createElement(o.A,{text:"\\( x_1, \\dots, x_k \\)"})," is:"),"\n",l.createElement(o.A,{text:"\\[\nP(X_1 = x_1, \\ldots, X_k = x_k) = \\frac{n!}{x_1! \\cdots x_k!} p_1^{x_1} \\cdots p_k^{x_k},\n\\]"}),"\n",l.createElement(t.p,null,"subject to ",l.createElement(o.A,{text:"\\( \\sum_{i=1}^k x_i = n \\)"}),"."),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Support"),": Nonnegative integer vectors of dimension ",l.createElement(o.A,{text:"\\( k \\)"})," summing to ",l.createElement(o.A,{text:"\\( n \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameters"),": number of trials ",l.createElement(o.A,{text:"\\( n \\)"}),", probability vector ",l.createElement(o.A,{text:"\\( (p_1, \\ldots, p_k) \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Mean & covariance"),": ",l.createElement(o.A,{text:"\\( \\mathbb{E}[X_i] = np_i \\)"}),"; covariances depend on both ",l.createElement(o.A,{text:"\\( p_i \\)"})," and ",l.createElement(o.A,{text:"\\( p_j \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Use cases"),": Modeling the counts of outcomes across multiple categories, e.g., how many times each side of a die shows up in ",l.createElement(o.A,{text:"\\( n \\)"})," rolls."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameter estimation"),": The MLE for ",l.createElement(o.A,{text:"\\( p_i \\)"})," is the fraction of observations falling into category ",l.createElement(o.A,{text:"\\( i \\)"}),": ",l.createElement(o.A,{text:"\\( \\hat{p}_i = \\frac{x_i}{n} \\)"}),"."),"\n"),"\n",l.createElement(t.h3,{id:"categorical-distribution",style:{position:"relative"}},l.createElement(t.a,{href:"#categorical-distribution","aria-label":"categorical distribution permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Categorical distribution"),"\n",l.createElement(t.p,null,"The ",l.createElement(r.A,null,"Categorical distribution")," is to the multinomial what the Bernoulli is to the binomial. It's a single trial that results in exactly one of ",l.createElement(o.A,{text:"\\( k \\)"})," categories, with probabilities ",l.createElement(o.A,{text:"\\( (p_1, \\dots, p_k) \\)"}),". The pmf for outcome ",l.createElement(o.A,{text:"\\( x \\in \\{1, \\dots, k\\} \\)"})," is:"),"\n",l.createElement(t.p,null,l.createElement(o.A,{text:"\\( P(X = x) = p_x \\)"}),", where ",l.createElement(o.A,{text:"\\(\\sum_{i=1}^{k} p_i = 1 \\)"}),"."),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Support"),": ",l.createElement(o.A,{text:"\\( x \\in \\{1, 2, \\dots, k\\} \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameters"),": probability vector ",l.createElement(o.A,{text:"\\( (p_1, \\ldots, p_k) \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Use cases"),": Class labels in multi-class classification, or any scenario with a single trial and multiple possible outcomes."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameter estimation"),": The MLE is again the empirical relative frequency for each category."),"\n"),"\n",l.createElement(t.hr),"\n",l.createElement(t.h2,{id:"continuous-distributions",style:{position:"relative"}},l.createElement(t.a,{href:"#continuous-distributions","aria-label":"continuous distributions permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Continuous distributions"),"\n",l.createElement(t.h3,{id:"the-gaussian-normal-distribution-in-depth",style:{position:"relative"}},l.createElement(t.a,{href:"#the-gaussian-normal-distribution-in-depth","aria-label":"the gaussian normal distribution in depth permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"The gaussian (normal) distribution in-depth"),"\n",l.createElement(t.p,null,"The ",l.createElement(r.A,null,"Gaussian distribution")," (also known as the Normal distribution) is one of the most important continuous distributions in statistics and machine learning due to the central limit theorem and its convenient analytical properties. Its probability density function (pdf) for a variable ",l.createElement(o.A,{text:"\\( X \\)"})," is:"),"\n",l.createElement(o.A,{text:"\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi} \\,\\sigma} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right),\n\\]"}),"\n",l.createElement(t.p,null,"where:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(o.A,{text:"\\( \\mu \\)"})," is the mean (location parameter)."),"\n",l.createElement(t.li,null,l.createElement(o.A,{text:"\\( \\sigma \\)"})," is the standard deviation (scale parameter)."),"\n"),"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Key properties"),":"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Symmetry & shape"),": Perfectly symmetrical around ",l.createElement(o.A,{text:"\\( \\mu \\)"}),"."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Support"),": ",l.createElement(o.A,{text:"\\( x \\in (-\\infty,\\infty) \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Mean & variance"),": ",l.createElement(o.A,{text:"\\( \\mathbb{E}[X] = \\mu \\)"})," and ",l.createElement(o.A,{text:"\\( \\mathrm{Var}[X] = \\sigma^2 \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Conditional and marginal distributions"),": Any linear combination (or conditional distribution) of jointly normal variables is also normally distributed."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Bayes' theorem for Gaussian variables"),": Gaussian distributions serve as conjugate priors in many Bayesian inference scenarios, making posterior distributions also Gaussian."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Maximum likelihood"),": The MLE estimates are the sample mean and sample variance for ",l.createElement(o.A,{text:"\\( \\mu \\)"})," and ",l.createElement(o.A,{text:"\\( \\sigma^2 \\)"})," respectively."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Sequential estimation"),": With each new observation, one can update the sample mean and variance incrementally. In a Bayesian context with a Gaussian prior, the posterior is still Gaussian."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Periodic variables"),": If data are truly periodic (like angles), simply using a normal distribution may lead to inaccuracies, because normal distributions ignore wraparound effects."),"\n"),"\n",l.createElement(n,{alt:"Gaussian pdf",path:"",caption:"Normal distributions with different means and variances.",zoom:"false"}),"\n",l.createElement(t.h3,{id:"the-exponential-distribution-and-the-exponential-family",style:{position:"relative"}},l.createElement(t.a,{href:"#the-exponential-distribution-and-the-exponential-family","aria-label":"the exponential distribution and the exponential family permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"The exponential distribution and the exponential family"),"\n",l.createElement(t.p,null,"The ",l.createElement(r.A,null,"exponential distribution")," is a core member of the broader ",l.createElement(t.strong,null,"exponential family")," of distributions (which also includes Bernoulli, Poisson, Gamma, and more). Focusing on the exponential distribution itself:"),"\n",l.createElement(o.A,{text:"\\[\nf(x) = \\lambda \\exp(-\\lambda x), \\quad x \\ge 0,\\ \\lambda > 0.\n\\]"}),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Support"),": ",l.createElement(o.A,{text:"\\( x \\in [0,\\infty) \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameter"),": rate ",l.createElement(o.A,{text:"\\( \\lambda \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Mean & variance"),": ",l.createElement(o.A,{text:"\\( \\mathbb{E}[X] = \\frac{1}{\\lambda}, \\mathrm{Var}[X] = \\frac{1}{\\lambda^2} \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Memoryless property"),": The exponential distribution is continuous-time analog of the geometric distribution's memorylessness."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Use cases"),": Time between events in a Poisson process, reliability analysis, survival analysis."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameter estimation"),": The MLE for ",l.createElement(o.A,{text:"\\( \\lambda \\)"})," is ",l.createElement(o.A,{text:"\\( \\hat{\\lambda} = \\frac{n}{\\sum x_i} \\)"}),", where ",l.createElement(o.A,{text:"\\( x_i \\)"})," are the observed waiting times."),"\n"),"\n",l.createElement(t.h4,{id:"conjugate-and-noninformative-priors",style:{position:"relative"}},l.createElement(t.a,{href:"#conjugate-and-noninformative-priors","aria-label":"conjugate and noninformative priors permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Conjugate and noninformative priors"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Conjugate priors"),": In Bayesian inference, the gamma distribution is a conjugate prior for ",l.createElement(o.A,{text:"\\( \\lambda \\)"})," of an exponential distribution. This means the posterior remains gamma-distributed after observing data."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Noninformative priors"),": Sometimes, a Jeffreys prior (",l.createElement(i.A,{text:"A Jeffreys prior is derived to be invariant under reparameterization and often used when we have little prior knowledge."}),"), which is proportional to the square root of the Fisher information, is used when no strong prior beliefs are held."),"\n"),"\n",l.createElement(t.h3,{id:"gamma-distribution",style:{position:"relative"}},l.createElement(t.a,{href:"#gamma-distribution","aria-label":"gamma distribution permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Gamma distribution"),"\n",l.createElement(t.p,null,"The ",l.createElement(r.A,null,"Gamma distribution")," generalizes the exponential by allowing an additional shape parameter ",l.createElement(o.A,{text:"\\( k \\)"}),". One parametrization is:"),"\n",l.createElement(o.A,{text:"\\[\nf(x) = \\frac{\\lambda^k}{\\Gamma(k)}\\, x^{k-1} e^{-\\lambda x}, \\quad x \\ge 0.\n\\]"}),"\n",l.createElement(t.p,null,"(Here ",l.createElement(o.A,{text:"\\( \\Gamma(\\cdot) \\)"})," is the gamma function, not to be confused with a random variable.)"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Support"),": ",l.createElement(o.A,{text:"\\( x \\in [0,\\infty) \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameters"),": shape ",l.createElement(o.A,{text:"\\( k \\)"}),", rate ",l.createElement(o.A,{text:"\\( \\lambda \\)"})," (sometimes scale ",l.createElement(o.A,{text:"\\( \\theta = 1/\\lambda \\)"})," is used)"),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Mean & variance"),": ",l.createElement(o.A,{text:"\\( \\mathbb{E}[X] = \\frac{k}{\\lambda}, \\mathrm{Var}[X] = \\frac{k}{\\lambda^2} \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Relation to exponential"),": The exponential is a special gamma with ",l.createElement(o.A,{text:"\\( k = 1 \\)"}),"."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Use cases"),": Modeling waiting times, reliability analysis, sum of exponential variables. If events follow a Poisson process, then the waiting time for the ",l.createElement(o.A,{text:"\\( k \\)"}),"-th event is gamma-distributed."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameter estimation"),": Common methods include MLE or method of moments. MLE typically requires numerical optimization (e.g., using Newton-Raphson)."),"\n"),"\n",l.createElement(t.h3,{id:"beta-distribution",style:{position:"relative"}},l.createElement(t.a,{href:"#beta-distribution","aria-label":"beta distribution permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Beta distribution"),"\n",l.createElement(t.p,null,"Defined on ",l.createElement(o.A,{text:"\\( [0,1] \\)"}),", the ",l.createElement(r.A,null,"Beta distribution")," is a versatile choice for modeling probabilities or proportions. Its pdf is:"),"\n",l.createElement(o.A,{text:"\\[\nf(x) = \\frac{x^{\\alpha - 1} (1 - x)^{\\beta - 1}}{B(\\alpha,\\beta)}, \\quad x \\in [0,1],\n\\]"}),"\n",l.createElement(t.p,null,"where ",l.createElement(o.A,{text:"\\( B(\\alpha,\\beta) \\)"})," is the beta function (normalizing constant)."),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Support"),": ",l.createElement(o.A,{text:"\\( x \\in [0,1] \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameters"),": shape parameters ",l.createElement(o.A,{text:"\\( \\alpha, \\beta > 0 \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Mean & variance"),": ",l.createElement(o.A,{text:"\\( \\mathbb{E}[X] = \\frac{\\alpha}{\\alpha+\\beta} \\)"}),", ",l.createElement(o.A,{text:"\\( \\mathrm{Var}[X] = \\frac{\\alpha \\beta}{(\\alpha+\\beta)^2 (\\alpha+\\beta+1)} \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Use cases"),": Particularly important in Bayesian inference as conjugate priors for the Bernoulli/binomial parameters; also used in modeling distributions of proportions in a population."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameter estimation"),": The MLE or method of moments can be used. Bayesian updates for ",l.createElement(o.A,{text:"\\( \\alpha \\)"})," and ",l.createElement(o.A,{text:"\\( \\beta \\)"})," are very common in posterior inference for unknown probabilities."),"\n"),"\n",l.createElement(t.h3,{id:"chi-square-distribution",style:{position:"relative"}},l.createElement(t.a,{href:"#chi-square-distribution","aria-label":"chi square distribution permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Chi-square distribution"),"\n",l.createElement(t.p,null,"A ",l.createElement(r.A,null,"chi-square distribution")," with ",l.createElement(o.A,{text:"\\( \\nu \\)"})," degrees of freedom is the distribution of a sum of squared independent standard normal variables. Its pdf is:"),"\n",l.createElement(o.A,{text:"\\[\nf(x) = \\frac{1}{2^{\\nu/2} \\Gamma(\\nu/2)} x^{\\frac{\\nu}{2} - 1} e^{-x/2}, \\quad x \\ge 0.\n\\]"}),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Support"),": ",l.createElement(o.A,{text:"\\( x \\in [0,\\infty) \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameter"),": degrees of freedom ",l.createElement(o.A,{text:"\\( \\nu \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Mean & variance"),": ",l.createElement(o.A,{text:"\\( \\mathbb{E}[X] = \\nu \\)"}),", ",l.createElement(o.A,{text:"\\( \\mathrm{Var}[X] = 2\\nu \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Use cases"),": Commonly used in hypothesis testing (chi-square tests), confidence intervals for variance, and other inferential procedures in statistics."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameter estimation"),": Usually arises as a derived distribution from normal assumptions, rather than from direct fitting. However, method-of-moments can be applied if needed."),"\n"),"\n",l.createElement(t.h3,{id:"students-t-distribution",style:{position:"relative"}},l.createElement(t.a,{href:"#students-t-distribution","aria-label":"students t distribution permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Student's t-distribution"),"\n",l.createElement(t.p,null,"The ",l.createElement(r.A,null,"Student's t-distribution")," arises when estimating the mean of a normally distributed population in situations where sample size is small and population variance is unknown. Its pdf is:"),"\n",l.createElement(o.A,{text:"\\[\nf(x) = \\frac{\\Gamma\\!\\big(\\tfrac{\\nu+1}{2}\\big)}{\\sqrt{\\nu\\pi}\\,\\Gamma\\!\\big(\\tfrac{\\nu}{2}\\big)} \\left(1 + \\frac{x^2}{\\nu}\\right)^{-\\frac{\\nu+1}{2}},\n\\]"}),"\n",l.createElement(t.p,null,"where ",l.createElement(o.A,{text:"\\( \\nu \\)"})," is the degrees of freedom."),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Support"),": ",l.createElement(o.A,{text:"\\( x \\in (-\\infty,\\infty) \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameter"),": degrees of freedom ",l.createElement(o.A,{text:"\\( \\nu \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Mean & variance"),": The mean is 0 for ",l.createElement(o.A,{text:"\\( \\nu > 1 \\)"}),". The variance is ",l.createElement(o.A,{text:"\\( \\frac{\\nu}{\\nu-2} \\)"})," for ",l.createElement(o.A,{text:"\\( \\nu > 2 \\)"}),". For lower ",l.createElement(o.A,{text:"\\( \\nu \\)"}),", these moments may be undefined."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Heavier tails"),": More prone to outliers than the normal. As ",l.createElement(o.A,{text:"\\( \\nu \\to \\infty \\)"}),", it converges to the normal distribution."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Use cases"),": In regression models, the t-distribution is favored in small-sample scenarios or when outliers are present."),"\n"),"\n",l.createElement(t.h3,{id:"f-distribution",style:{position:"relative"}},l.createElement(t.a,{href:"#f-distribution","aria-label":"f distribution permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"F-distribution"),"\n",l.createElement(t.p,null,"An ",l.createElement(r.A,null,"F-distribution")," is the distribution of a ratio of two chi-square variables, each divided by their respective degrees of freedom. Specifically, if ",l.createElement(o.A,{text:"\\( U \\sim \\chi^2_{\\nu_1} \\)"})," and ",l.createElement(o.A,{text:"\\( V \\sim \\chi^2_{\\nu_2} \\)"}),", then:"),"\n",l.createElement(o.A,{text:"\\[\nF = \\frac{U/\\nu_1}{V/\\nu_2}\n\\]"}),"\n",l.createElement(t.p,null,"follows an F-distribution with ",l.createElement(o.A,{text:"\\( \\nu_1, \\nu_2 \\)"})," degrees of freedom."),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Support"),": ",l.createElement(o.A,{text:"\\( x \\in [0,\\infty) \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameters"),": ",l.createElement(o.A,{text:"\\( \\nu_1, \\nu_2 \\)"})," (degrees of freedom)"),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Use cases"),": Widely used in ANOVA, comparing variances in different populations, or model comparison in regression."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameter estimation"),': Typically arises in test statistics. Direct "fitting" is less common in routine ML pipelines, but occasionally used in specialized modeling.'),"\n"),"\n",l.createElement(t.h3,{id:"weibull-distribution",style:{position:"relative"}},l.createElement(t.a,{href:"#weibull-distribution","aria-label":"weibull distribution permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Weibull distribution"),"\n",l.createElement(t.p,null,"A ",l.createElement(r.A,null,"Weibull distribution")," has the pdf:"),"\n",l.createElement(o.A,{text:"\\[\nf(x) = \\frac{k}{\\lambda} \\left(\\frac{x}{\\lambda}\\right)^{k-1} \\exp\\left(-\\left(\\frac{x}{\\lambda}\\right)^k\\right), \\quad x \\ge 0.\n\\]"}),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Support"),": ",l.createElement(o.A,{text:"\\( x \\in [0,\\infty) \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameters"),": shape ",l.createElement(o.A,{text:"\\( k \\)"}),", scale ",l.createElement(o.A,{text:"\\( \\lambda \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Use cases"),": Time-to-failure or survival analysis, reliability engineering, wind speed distributions."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Special cases"),": If ",l.createElement(o.A,{text:"\\( k=1 \\)"}),", it becomes the exponential distribution."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameter estimation"),": Methods include MLE and method of moments. Often used in engineering to estimate lifetimes of components."),"\n"),"\n",l.createElement(t.h3,{id:"log-normal-distribution",style:{position:"relative"}},l.createElement(t.a,{href:"#log-normal-distribution","aria-label":"log normal distribution permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Log-normal distribution"),"\n",l.createElement(t.p,null,"A random variable ",l.createElement(o.A,{text:"\\( X \\)"})," is ",l.createElement(r.A,null,"log-normally distributed")," if ",l.createElement(o.A,{text:"\\( \\log(X) \\)"})," is normally distributed. The pdf is:"),"\n",l.createElement(o.A,{text:"\\[\nf(x) = \\frac{1}{x \\sigma \\sqrt{2\\pi}} \\exp\\!\\Bigl(-\\frac{(\\ln x - \\mu)^2}{2\\sigma^2}\\Bigr), \\quad x > 0.\n\\]"}),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Support"),": ",l.createElement(o.A,{text:"\\( x \\in (0,\\infty) \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameters"),": ",l.createElement(o.A,{text:"\\( \\mu, \\sigma \\)"})," are the mean and standard deviation of the log-transformed variable"),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Use cases"),": Modeling skewed positive data (e.g., certain economic or biological variables)."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameter estimation"),": Typically done by fitting a normal distribution to the log of the data. MLE is straightforward: estimate the mean and variance of ",l.createElement(o.A,{text:"\\( \\ln(x) \\)"}),"."),"\n"),"\n",l.createElement(t.hr),"\n",l.createElement(t.h2,{id:"special-distributions-in-machine-learning",style:{position:"relative"}},l.createElement(t.a,{href:"#special-distributions-in-machine-learning","aria-label":"special distributions in machine learning permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Special distributions in machine learning"),"\n",l.createElement(t.h3,{id:"pareto-distribution",style:{position:"relative"}},l.createElement(t.a,{href:"#pareto-distribution","aria-label":"pareto distribution permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Pareto distribution"),"\n",l.createElement(t.p,null,"The ",l.createElement(r.A,null,"Pareto distribution")," is a heavy-tailed distribution often used in scenarios exhibiting power-law characteristics, such as wealth distribution or file size distributions on the internet. Its pdf is:"),"\n",l.createElement(o.A,{text:"\\[\nf(x) = \\alpha \\frac{x_m^\\alpha}{x^{\\alpha+1}}, \\quad x \\ge x_m,\\ \\alpha > 0.\n\\]"}),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Support"),": ",l.createElement(o.A,{text:"\\( x \\in [x_m,\\infty) \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameters"),": scale ",l.createElement(o.A,{text:"\\( x_m > 0 \\)"}),", shape ",l.createElement(o.A,{text:"\\( \\alpha \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Heavy tails"),": As ",l.createElement(o.A,{text:"\\( x \\)"})," grows, the distribution decays slowly compared to exponential or Gaussian, making large outliers more probable."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Use cases"),': Modeling phenomena with "the rich get richer" dynamics or extreme outliers. In ML, can inform heavy-tailed priors or anomaly detection in big data.'),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameter estimation"),": MLE is ",l.createElement(o.A,{text:"\\( \\hat{\\alpha} = \\frac{n}{\\sum_{i=1}^n \\ln(x_i/x_m)} \\)"}),". Because of the heavy tail, robust estimation techniques are sometimes used to mitigate the impact of outliers."),"\n"),"\n",l.createElement(t.h3,{id:"laplace-distribution",style:{position:"relative"}},l.createElement(t.a,{href:"#laplace-distribution","aria-label":"laplace distribution permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Laplace distribution"),"\n",l.createElement(t.p,null,"Also called the ",l.createElement(r.A,null,"double exponential distribution"),", the ",l.createElement(r.A,null,"Laplace distribution")," has pdf:"),"\n",l.createElement(o.A,{text:"\\[\nf(x) = \\frac{1}{2b}\\,\\exp\\left(-\\frac{|x-\\mu|}{b}\\right).\n\\]"}),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameters"),": location ",l.createElement(o.A,{text:"\\( \\mu \\)"}),", diversity ",l.createElement(o.A,{text:"\\( b \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Use cases"),": In machine learning, the Laplace distribution is central in L1 regularization (Lasso). It has heavier tails compared to the normal distribution, yielding robust behavior against outliers."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameter estimation"),": The MLE for ",l.createElement(o.A,{text:"\\( \\mu \\)"})," is the median of the data (rather than the mean), and ",l.createElement(o.A,{text:"\\( b \\)"})," is the average absolute deviation from the median."),"\n"),"\n",l.createElement(t.h3,{id:"gumbel-distribution",style:{position:"relative"}},l.createElement(t.a,{href:"#gumbel-distribution","aria-label":"gumbel distribution permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Gumbel distribution"),"\n",l.createElement(t.p,null,"A ",l.createElement(r.A,null,"Gumbel distribution")," is often used in extreme value theory for modeling maxima of samples. One form of the pdf is:"),"\n",l.createElement(o.A,{text:"\\[\nf(x) = \\frac{1}{\\beta} \\exp \\Bigl(-z - e^{-z}\\Bigr), \\quad z = \\frac{x - \\mu}{\\beta}.\n\\]"}),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameters"),": location ",l.createElement(o.A,{text:"\\( \\mu \\)"}),", scale ",l.createElement(o.A,{text:"\\( \\beta > 0 \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Use cases"),': Modeling extreme events, such as maximum rainfall in a year or worst-case loads on servers. Also arises in logistic regression link functions (the "logit" can be related to a Gumbel distribution difference).'),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameter estimation"),": MLE or method of moments are standard. In practice, specialized libraries implement these fittings."),"\n"),"\n",l.createElement(t.h3,{id:"cauchy-distribution",style:{position:"relative"}},l.createElement(t.a,{href:"#cauchy-distribution","aria-label":"cauchy distribution permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Cauchy distribution"),"\n",l.createElement(t.p,null,"A ",l.createElement(r.A,null,"Cauchy distribution")," has the pdf:"),"\n",l.createElement(o.A,{text:"\\[\nf(x) = \\frac{1}{\\pi \\gamma \\bigl[1 + \\bigl(\\frac{x - x_0}{\\gamma}\\bigr)^2\\bigr]},\n\\]"}),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameters"),": location ",l.createElement(o.A,{text:"\\( x_0 \\)"}),", scale ",l.createElement(o.A,{text:"\\( \\gamma \\)"})),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Undefined mean & variance"),": Unlike most common distributions, the Cauchy has undefined (infinite) mean and variance. This makes parameter estimation challenging."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Heavily heavy-tailed"),": Rare but extremely large values occur with non-negligible probability."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Use cases"),": Often a cautionary example of distributions that defy certain classical statistical intuitions. Rarely used in direct modeling but important conceptually."),"\n"),"\n",l.createElement(t.h3,{id:"dirichlet-distribution",style:{position:"relative"}},l.createElement(t.a,{href:"#dirichlet-distribution","aria-label":"dirichlet distribution permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Dirichlet distribution"),"\n",l.createElement(t.p,null,"The ",l.createElement(r.A,null,"Dirichlet distribution")," generalizes the Beta distribution to higher dimensions, producing a probability vector in a ",l.createElement(o.A,{text:"\\( k \\)"}),"-simplex. Its pdf, for ",l.createElement(o.A,{text:"\\( x_i \\ge 0 \\)"})," and ",l.createElement(o.A,{text:"\\( \\sum x_i = 1 \\)"}),", is:"),"\n",l.createElement(o.A,{text:"\\[\nf(x_1,\\dots,x_k) = \\frac{1}{B(\\alpha_1,\\dots,\\alpha_k)} \\prod_{i=1}^k x_i^{\\alpha_i - 1},\n\\]"}),"\n",l.createElement(t.p,null,"where ",l.createElement(o.A,{text:"\\( B(\\cdot) \\)"})," is now the multinomial beta function, and ",l.createElement(o.A,{text:"\\( \\alpha_i > 0 \\)"})," are the parameters."),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Support"),": The ",l.createElement(o.A,{text:"\\( (k-1) \\)"}),"-dimensional simplex"),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Use cases"),": Bayesian modeling of categorical or multinomial parameters, topic modeling (LDA uses Dirichlet priors), mixture models, and any problem requiring a distribution over distributions."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Parameter estimation"),": Typically done in a Bayesian context. If data are in the form of multiple categorical draws, the posterior distribution of probabilities is also Dirichlet when using a Dirichlet prior."),"\n"),"\n",l.createElement(t.hr),"\n",l.createElement(t.h2,{id:"conclusion",style:{position:"relative"}},l.createElement(t.a,{href:"#conclusion","aria-label":"conclusion permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Conclusion"),"\n",l.createElement(t.p,null,"Choosing the right distribution is crucial for accurate modeling and inference. Discrete distributions like Bernoulli, Binomial, and Poisson help in scenarios with count data or binary outcomes, while continuous ones like Gaussian, Exponential, and Gamma cater to a wide range of phenomena  time-to-event analyses, large-sample approximations, or uncertain probability estimates. Heavy-tailed or specialized distributions (Pareto, Laplace, Cauchy, Gumbel, Dirichlet) are invaluable in certain extreme or specialized ML contexts."),"\n",l.createElement(t.p,null,"It's important to:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Check assumptions")," (e.g., independence, identical distribution, shape properties)."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Estimate parameters")," reliably using MLE, method of moments, or Bayesian methods."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Understand relationships")," (e.g., exponential is a special Gamma, Beta is a prior for Bernoulli/binomial, etc.)."),"\n"),"\n",l.createElement(t.p,null,"In practice, data scientists should explore multiple distributions, compare goodness-of-fit, and always remember to validate model assumptions with domain knowledge and empirical tests."),"\n",l.createElement(n,{alt:"Distribution selection",path:"",caption:"Selecting an appropriate distribution depends on the nature of the data, assumptions, and analysis goals.",zoom:"false"}),"\n",l.createElement(t.p,null,"Below is a brief Python snippet illustrating how to sample from and plot some of these distributions using ",l.createElement(i.A,{text:"SciPy is a Python library providing many numerical routines including probability distributions."}),":"),"\n",l.createElement(s.A,{text:"\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import bernoulli, binom, norm, gamma\n\n# Example: generate random data and plot histograms for different distributions\nnp.random.seed(42)\n\nbern_data = bernoulli.rvs(p=0.3, size=1000)\nbinom_data = binom.rvs(n=10, p=0.3, size=1000)\nnorm_data = norm.rvs(loc=0, scale=1, size=1000)\ngamma_data = gamma.rvs(a=2, scale=1/0.5, size=1000)  # shape=2, rate=0.5\n\nfig, axes = plt.subplots(2, 2, figsize=(8, 6))\n\naxes[0,0].hist(bern_data, bins=[-0.5, 0.5, 1.5], density=True, alpha=0.7, color='blue')\naxes[0,0].set_title(\"Bernoulli(p=0.3)\")\n\naxes[0,1].hist(binom_data, bins=np.arange(12)-0.5, density=True, alpha=0.7, color='green')\naxes[0,1].set_title(\"Binomial(n=10, p=0.3)\")\n\naxes[1,0].hist(norm_data, bins=30, density=True, alpha=0.7, color='red')\naxes[1,0].set_title(\"Normal(0,1)\")\n\naxes[1,1].hist(gamma_data, bins=30, density=True, alpha=0.7, color='purple')\naxes[1,1].set_title(\"Gamma(k=2, rate=0.5)\")\n\nplt.tight_layout()\nplt.show()\n"}),"\n",l.createElement(t.p,null,"Through real-world applications and further chapters in this course, you'll see how choosing appropriate distributions impacts inference, hypothesis testing, and the performance of machine learning models."))}var m=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,a.RP)(),e.components);return t?l.createElement(t,e,l.createElement(c,e)):c(e)};var u=n(54506),d=n(88864),h=n(58481),p=n.n(h),E=n(5984),f=n(43672),g=n(27042),b=n(72031),x=n(81817),v=n(27105),y=n(17265),A=n(2043),w=n(95751),S=n(94328),M=n(80791),k=n(78137);const L=e=>{let{toc:t}=e;if(!t||!t.items)return null;return l.createElement("nav",{className:M.R},l.createElement("ul",null,t.items.map(((e,t)=>l.createElement("li",{key:t},l.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const n=t.replace("#",""),a=document.getElementById(n);a&&a.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&l.createElement(L,{toc:{items:e.items}}))))))};function z(e){let{data:{mdx:t,allMdx:i,allPostImages:r},children:s}=e;const{frontmatter:o,body:c,tableOfContents:m}=t,d=o.index,h=o.slug.split("/")[1],b=i.nodes.filter((e=>e.frontmatter.slug.includes(`/${h}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),M=b.findIndex((e=>e.frontmatter.index===d)),z=b[M+1],C=b[M-1],_=o.slug.replace(/\/$/,""),T=/[^/]*$/.exec(_)[0],H=`posts/${h}/content/${T}/`,{0:N,1:P}=(0,l.useState)(o.flagWideLayoutByDefault),{0:I,1:j}=(0,l.useState)(!1);var B;(0,l.useEffect)((()=>{j(!0);const e=setTimeout((()=>j(!1)),340);return()=>clearTimeout(e)}),[N]),"adventures"===h?B=y.cb:"research"===h?B=y.Qh:"thoughts"===h&&(B=y.T6);const V=p()(c).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,D=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),n=e%60;return n<=30?`~${t}${n>0?".5":""} h`:`~${t+1} h`}(Math.ceil(V/B)+(o.extraReadTimeMin||0)),G=[{flag:o.flagDraft,component:()=>Promise.all([n.e(5850),n.e(9833)]).then(n.bind(n,49833))},{flag:o.flagMindfuckery,component:()=>Promise.all([n.e(5850),n.e(7805)]).then(n.bind(n,27805))},{flag:o.flagRewrite,component:()=>Promise.all([n.e(5850),n.e(8916)]).then(n.bind(n,78916))},{flag:o.flagOffensive,component:()=>Promise.all([n.e(5850),n.e(6731)]).then(n.bind(n,49112))},{flag:o.flagProfane,component:()=>Promise.all([n.e(5850),n.e(3336)]).then(n.bind(n,83336))},{flag:o.flagMultilingual,component:()=>Promise.all([n.e(5850),n.e(2343)]).then(n.bind(n,62343))},{flag:o.flagUnreliably,component:()=>Promise.all([n.e(5850),n.e(6865)]).then(n.bind(n,11627))},{flag:o.flagPolitical,component:()=>Promise.all([n.e(5850),n.e(4417)]).then(n.bind(n,24417))},{flag:o.flagCognitohazard,component:()=>Promise.all([n.e(5850),n.e(8669)]).then(n.bind(n,18669))},{flag:o.flagHidden,component:()=>Promise.all([n.e(5850),n.e(8124)]).then(n.bind(n,48124))}],{0:O,1:U}=(0,l.useState)([]);return(0,l.useEffect)((()=>{G.forEach((e=>{let{flag:t,component:n}=e;t&&n().then((e=>{U((t=>[].concat((0,u.A)(t),[e.default])))}))}))}),[]),l.createElement(g.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},l.createElement(x.A,{postNumber:o.index,date:o.date,updated:o.updated,readTime:D,difficulty:o.difficultyLevel,title:o.title,desc:o.desc,banner:o.banner,section:h,postKey:T,isMindfuckery:o.flagMindfuckery,mainTag:o.mainTag}),l.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},o.otherTags.map(((e,t)=>l.createElement("span",{key:t,className:`noselect ${k.MW}`,style:{margin:"0 5px 5px 0"}},e)))),l.createElement("div",{className:"postBody"},l.createElement(L,{toc:m})),l.createElement("br",null),l.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},l.createElement(g.P.button,{className:`noselect ${S.pb}`,id:S.xG,onClick:()=>{P(!N)},whileTap:{scale:.93}},l.createElement(g.P.div,{className:w.DJ,key:N,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},N?"Switch to default layout":"Switch to wide layout"))),l.createElement("br",null),l.createElement("div",{className:"postBody",style:{margin:N?"0 -14%":"",maxWidth:N?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},l.createElement("div",{className:`${S.P_} ${I?S.Xn:S.qG}`},O.map(((e,t)=>l.createElement(e,{key:t}))),o.indexCourse?l.createElement(A.A,{index:o.indexCourse,category:o.courseCategoryName}):"",l.createElement(E.Z.Provider,{value:{images:r.nodes,basePath:H.replace(/\/$/,"")+"/"}},l.createElement(a.xA,{components:{Image:f.A}},s)))),l.createElement(v.A,{nextPost:z,lastPost:C,keyCurrent:T,section:h}))}function C(e){return l.createElement(z,e,l.createElement(m,e))}function _(e){var t,n,a,i,r;let{data:s}=e;const{frontmatter:o}=s.mdx,c=o.titleSEO||o.title,m=o.titleOG||c,u=o.titleTwitter||c,h=o.descSEO||o.desc,p=o.descOG||h,E=o.descTwitter||h,f=o.schemaType||"BlogPosting",g=o.keywordsSEO,x=o.date,v=o.updated||x,y=o.imageOG||(null===(t=o.banner)||void 0===t||null===(n=t.childImageSharp)||void 0===n||null===(a=n.gatsbyImageData)||void 0===a||null===(i=a.images)||void 0===i||null===(r=i.fallback)||void 0===r?void 0:r.src),A=o.imageAltOG||p,w=o.imageTwitter||y,S=o.imageAltTwitter||E,M=o.canonicalURL,k=o.flagHidden||!1,L=o.mainTag||"Posts",z=o.slug.split("/")[1]||"posts",{siteUrl:C}=(0,d.Q)(),_={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:C},{"@type":"ListItem",position:2,name:L,item:`${C}/${o.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${C}${o.slug}`}]};return l.createElement(b.A,{title:c+" - avrtt.blog",titleOG:m,titleTwitter:u,description:h,descriptionOG:p,descriptionTwitter:E,schemaType:f,keywords:g,datePublished:x,dateModified:v,imageOG:y,imageAltOG:A,imageTwitter:w,imageAltTwitter:S,canonicalUrl:M,flagHidden:k,mainTag:L,section:z,type:"article"},l.createElement("script",{type:"application/ld+json"},JSON.stringify(_)))}}}]);
//# sourceMappingURL=component---src-templates-post-tsx-content-file-path-src-pages-posts-research-statistical-distributions-mdx-915e75e654c4765c19e7.js.map