{"version":3,"file":"component---src-templates-post-tsx-content-file-path-src-pages-posts-research-ai-reasoning-and-uncertainty-2-mdx-52d12ce8686536e036db.js","mappings":"wIAoBA,IALUA,IAA2B,IAA1B,KAAEC,GAAkBD,EAC7B,OACEE,EAAAA,cAACC,EAAAA,EAAK,KAAEF,EAAa,C,4LC4EzB,SAASG,EAAkBC,GACzB,MAAMC,EAAcC,OAAOC,OAAO,CAChCC,GAAI,KACJC,EAAG,IACHC,KAAM,OACNC,EAAG,IACHC,GAAI,KACJC,GAAI,KACJC,GAAI,KACJC,OAAQ,SACRC,GAAI,KACJC,GAAI,OACHC,EAAAA,EAAAA,MAAsBd,EAAMe,YAC/B,OAAOlB,EAAAA,cAAoBA,EAAAA,SAAgB,KAAM,KAAMA,EAAAA,cAAoB,MAAO,KAAM,KAAMA,EAAAA,cAAoBI,EAAYG,GAAI,CAChIY,GAAI,oCACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,qCACN,aAAc,8CACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,qCAAsC,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,+DAAgE,KAAMV,EAAAA,cAAoB,MAAO,KAAMA,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,svBAAuvB,KAAMV,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,qlBAAslB,KAAMV,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,imBAAkmB,KAAMV,EAAAA,cAAoBI,EAAYO,GAAI,CAC/xEQ,GAAI,+DACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,gEACN,aAAc,yEACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,gEAAiE,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,yKAA0KV,EAAAA,cAAoBC,EAAAA,EAAO,CACxTF,KAAM,mCACJ,uDAAwD,KAAMC,EAAAA,cAAoBC,EAAAA,EAAO,CAC3FF,KAAM,8EACJ,KAAMC,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,SAAUV,EAAAA,cAAoBC,EAAAA,EAAO,CACtFF,KAAM,cACJ,8BAA+BC,EAAAA,cAAoBC,EAAAA,EAAO,CAC5DF,KAAM,YACJ,wXAAyX,KAAMC,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,okBAAqkB,KAAMV,EAAAA,cAAoBI,EAAYO,GAAI,CACzhCQ,GAAI,uDACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,wDACN,aAAc,iEACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,0DAA2D,KAAMzB,EAAAA,cAAoBI,EAAYQ,GAAI,KAAM,KAAMZ,EAAAA,cAAoBI,EAAYS,GAAI,KAAMb,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,sCAAuC,6cAA8c,KAAMd,EAAAA,cAAoBI,EAAYS,GAAI,KAAMb,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,kCAAmC,igBAAkgB,KAAMd,EAAAA,cAAoBI,EAAYS,GAAI,KAAMb,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,uBAAwB,wcAAyc,MAAO,KAAMd,EAAAA,cAAoBI,EAAYO,GAAI,CACp7DQ,GAAI,uDACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,wDACN,aAAc,iEACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,0DAA2D,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,iEAAkE,KAAMV,EAAAA,cAAoBI,EAAYW,GAAI,KAAM,KAAMf,EAAAA,cAAoBI,EAAYS,GAAI,KAAMb,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,aAAc,+JAAgKd,EAAAA,cAAoBC,EAAAA,EAAO,CACtgBF,KAAM,YACJ,qEAAsEC,EAAAA,cAAoBC,EAAAA,EAAO,CACnGF,KAAM,YACJ,KAAM,KAAMC,EAAAA,cAAoBI,EAAYS,GAAI,KAAMb,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,cAAe,mDAAoDd,EAAAA,cAAoBC,EAAAA,EAAO,CACpMF,KAAM,cACJ,OAAQC,EAAAA,cAAoBC,EAAAA,EAAO,CACrCF,KAAM,cACJ,mCAAoCC,EAAAA,cAAoBC,EAAAA,EAAO,CACjEF,KAAM,YACJ,8FAA+F,KAAMC,EAAAA,cAAoBI,EAAYS,GAAI,KAAMb,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,aAAc,wNAAyNd,EAAAA,cAAoBC,EAAAA,EAAO,CACjcF,KAAM,YACJ,6CAA8CC,EAAAA,cAAoBC,EAAAA,EAAO,CAC3EF,KAAM,+BACJ,uBAAwB,MAAO,KAAMC,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,6MAA8M,KAAMV,EAAAA,cAAoBI,EAAYG,GAAI,CACxUY,GAAI,uBACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,wBACN,aAAc,iCACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,wBAAyB,KAAMzB,EAAAA,cAAoBI,EAAYO,GAAI,CACtEQ,GAAI,8CACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,+CACN,aAAc,wDACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,gDAAiD,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,ydAA0d,KAAMV,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,sLAAuL,KAAMV,EAAAA,cAAoBC,EAAAA,EAAO,CACp0BF,KAAM,qDACJ,KAAMC,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,SAAUV,EAAAA,cAAoBC,EAAAA,EAAO,CACtFF,KAAM,cACJ,wBAAyBC,EAAAA,cAAoBC,EAAAA,EAAO,CACtDF,KAAM,eACJ,kBAAmBC,EAAAA,cAAoBC,EAAAA,EAAO,CAChDF,KAAM,kBACJ,2BAA4BC,EAAAA,cAAoBC,EAAAA,EAAO,CACzDF,KAAM,cACJ,kRAAmR,KAAMC,EAAAA,cAAoBI,EAAYO,GAAI,CAC/TQ,GAAI,iDACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,kDACN,aAAc,2DACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,kDAAmD,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,6gBAA8gB,KAAMV,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,ohBAAqhB,KAAMV,EAAAA,cAAoBI,EAAYO,GAAI,CACjuCQ,GAAI,uCACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,wCACN,aAAc,iDACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,yCAA0C,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,2qBAA6qB,KAAMV,EAAAA,cAAoBI,EAAYO,GAAI,CACnzBQ,GAAI,0EACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,2EACN,aAAc,oFACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,8EAA+E,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,miBAAoiB,KAAMV,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,qWAAsW,KAAMV,EAAAA,cAAoBI,EAAYG,GAAI,CACpmCY,GAAI,+BACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,gCACN,aAAc,yCACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,gCAAiC,KAAMzB,EAAAA,cAAoBI,EAAYO,GAAI,CAC9EQ,GAAI,gEACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,iEACN,aAAc,0EACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,iEAAkE,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,8GAA+GV,EAAAA,cAAoBC,EAAAA,EAAO,CAC9PF,KAAM,iCACJ,+BAAgCC,EAAAA,cAAoBC,EAAAA,EAAO,CAC7DF,KAAM,iCACJ,6HAA8HC,EAAAA,cAAoBC,EAAAA,EAAO,CAC3JF,KAAM,yCACJ,uBAAwBC,EAAAA,cAAoBC,EAAAA,EAAO,CACrDF,KAAM,YACJ,mDAAoD,KAAMC,EAAAA,cAAoBC,EAAAA,EAAO,CACvFF,KAAM,2DACJ,KAAMC,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,8BAA+B,KAAMV,EAAAA,cAAoBC,EAAAA,EAAO,CACjHF,KAAM,oEACJ,KAAMC,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,6BAA8BV,EAAAA,cAAoBC,EAAAA,EAAO,CAC1GF,KAAM,oBACJ,mEAAoEC,EAAAA,cAAoBC,EAAAA,EAAO,CACjGF,KAAM,mBACJ,4DAA6DC,EAAAA,cAAoBC,EAAAA,EAAO,CAC1FF,KAAM,YACJ,YAAaC,EAAAA,cAAoBC,EAAAA,EAAO,CAC1CF,KAAM,YACJ,uBAAwBC,EAAAA,cAAoBC,EAAAA,EAAO,CACrDF,KAAM,0CACJ,gGAAiGC,EAAAA,cAAoBC,EAAAA,EAAO,CAC9HF,KAAM,YACJ,qBAAsBC,EAAAA,cAAoBC,EAAAA,EAAO,CACnDF,KAAM,YACJ,iEAAkEC,EAAAA,cAAoBC,EAAAA,EAAO,CAC/FF,KAAM,YACJ,6BAA8BC,EAAAA,cAAoBC,EAAAA,EAAO,CAC3DF,KAAM,YACJ,+FAAgGC,EAAAA,cAAoBC,EAAAA,EAAO,CAC7HF,KAAM,0BACJ,iBAAkBC,EAAAA,cAAoBC,EAAAA,EAAO,CAC/CF,KAAM,YACJ,qEAAsEC,EAAAA,cAAoBC,EAAAA,EAAO,CACnGF,KAAM,YACJ,OAAQC,EAAAA,cAAoBC,EAAAA,EAAO,CACrCF,KAAM,YACJ,aAAc,KAAMC,EAAAA,cAAoBI,EAAYO,GAAI,CAC1DQ,GAAI,yDACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,0DACN,aAAc,mEACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,2DAA4D,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,uvBAAwvB,KAAMV,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,mFAAoF,KAAMV,EAAAA,cAAoB0B,EAAAA,EAAM,CACzgC3B,KAAM,gpDA6CJ,KAAMC,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,6UAA8U,KAAMV,EAAAA,cAAoBI,EAAYO,GAAI,CACzaQ,GAAI,uDACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,wDACN,aAAc,iEACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,0DAA2D,KAAMzB,EAAAA,cAAoBI,EAAYQ,GAAI,KAAM,KAAMZ,EAAAA,cAAoBI,EAAYS,GAAI,KAAMb,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,oBAAqB,kUAAmU,KAAMd,EAAAA,cAAoBI,EAAYS,GAAI,KAAMb,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,qBAAsB,+WAAgX,MAAO,KAAMd,EAAAA,cAAoBI,EAAYO,GAAI,CACzjCQ,GAAI,iDACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,kDACN,aAAc,2DACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,kDAAmD,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,sYAAuY,KAAMV,EAAAA,cAAoBI,EAAYQ,GAAI,KAAM,KAAMZ,EAAAA,cAAoBI,EAAYS,GAAI,KAAMb,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,yBAA0B,2IAA4I,KAAMd,EAAAA,cAAoBI,EAAYS,GAAI,KAAMb,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,uCAAwC,0NAA2N,KAAMd,EAAAA,cAAoBI,EAAYS,GAAI,KAAMb,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,8BAA+B,uGAAwG,MAAO,KAAMd,EAAAA,cAAoBI,EAAYO,GAAI,CACv5CQ,GAAI,wEACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,yEACN,aAAc,kFACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,2EAA4E,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,kbAAmb,KAAMV,EAAAA,cAAoBI,EAAYG,GAAI,CAC3lBY,GAAI,iBACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,kBACN,aAAc,2BACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,kBAAmB,KAAMzB,EAAAA,cAAoBI,EAAYO,GAAI,CAChEQ,GAAI,wEACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,yEACN,aAAc,kFACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,yEAA0E,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,yIAA0I,KAAMV,EAAAA,cAAoBC,EAAAA,EAAO,CACvSF,KAAM,6CACJ,KAAMC,EAAAA,cAAoBC,EAAAA,EAAO,CACnCF,KAAM,iCACJ,KAAMC,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,SAAUV,EAAAA,cAAoBC,EAAAA,EAAO,CACtFF,KAAM,cACJ,yBAA0BC,EAAAA,cAAoBC,EAAAA,EAAO,CACvDF,KAAM,cACJ,+BAAgCC,EAAAA,cAAoBC,EAAAA,EAAO,CAC7DF,KAAM,YACJ,KAAMC,EAAAA,cAAoBC,EAAAA,EAAO,CACnCF,KAAM,cACJ,kCAAmCC,EAAAA,cAAoBC,EAAAA,EAAO,CAChEF,KAAM,mBACJ,kGAAmGC,EAAAA,cAAoBC,EAAAA,EAAO,CAChIF,KAAM,YACJ,QAASC,EAAAA,cAAoBC,EAAAA,EAAO,CACtCF,KAAM,YACJ,8BAA+BC,EAAAA,cAAoBC,EAAAA,EAAO,CAC5DF,KAAM,YACJ,mEAAoEC,EAAAA,cAAoBC,EAAAA,EAAO,CACjGF,KAAM,YACJ,oDAAqDC,EAAAA,cAAoBC,EAAAA,EAAO,CAClFF,KAAM,YACJ,sDAAuD,KAAMC,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,yDAA0D,KAAMV,EAAAA,cAAoBI,EAAYW,GAAI,KAAM,KAAMf,EAAAA,cAAoBI,EAAYS,GAAI,KAAM,KAAMb,EAAAA,cAAoBI,EAAYM,EAAG,KAAMV,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,WAAY,mCAAoC,KAAMd,EAAAA,cAAoBC,EAAAA,EAAO,CAChbF,KAAM,0DACJ,KAAMC,EAAAA,cAAoBC,EAAAA,EAAO,CACnCF,KAAM,4CACJ,KAAMC,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,SAAUV,EAAAA,cAAoBC,EAAAA,EAAO,CACtFF,KAAM,yBACJ,kCAAmCC,EAAAA,cAAoBC,EAAAA,EAAO,CAChEF,KAAM,YACJ,8BAA+BC,EAAAA,cAAoBC,EAAAA,EAAO,CAC5DF,KAAM,YACJ,KAAMC,EAAAA,cAAoBC,EAAAA,EAAO,CACnCF,KAAM,2BACJ,6BAA8BC,EAAAA,cAAoBC,EAAAA,EAAO,CAC3DF,KAAM,cACJ,sDAAuDC,EAAAA,cAAoBC,EAAAA,EAAO,CACpFF,KAAM,cACJ,KAAMC,EAAAA,cAAoBC,EAAAA,EAAO,CACnCF,KAAM,YACJ,uCAAwC,MAAO,KAAMC,EAAAA,cAAoBI,EAAYS,GAAI,KAAM,KAAMb,EAAAA,cAAoBI,EAAYM,EAAG,KAAMV,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,UAAW,6BAA8B,KAAMd,EAAAA,cAAoBC,EAAAA,EAAO,CACxQF,KAAM,6EACJ,KAAMC,EAAAA,cAAoBC,EAAAA,EAAO,CACnCF,KAAM,yGACJ,KAAMC,EAAAA,cAAoBC,EAAAA,EAAO,CACnCF,KAAM,iEACJ,KAAMC,EAAAA,cAAoBI,EAAYM,EAAG,KAAMV,EAAAA,cAAoBC,EAAAA,EAAO,CAC5EF,KAAM,kBACJ,4HAA6HC,EAAAA,cAAoBC,EAAAA,EAAO,CAC1JF,KAAM,uCACJ,mIAAoIC,EAAAA,cAAoBC,EAAAA,EAAO,CACjKF,KAAM,6BACJ,sDAAuD,MAAO,MAAO,KAAMC,EAAAA,cAAoBI,EAAYO,GAAI,CACjHQ,GAAI,4DACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,6DACN,aAAc,sEACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,+DAAgE,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,maAAoa,KAAMV,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,wTAAyT,KAAMV,EAAAA,cAAoBI,EAAYO,GAAI,CACx6BQ,GAAI,wEACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,yEACN,aAAc,kFACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,8EAA+E,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,siBAAuiB,KAAMV,EAAAA,cAAoBI,EAAYO,GAAI,CACltBQ,GAAI,2DACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,4DACN,aAAc,qEACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,8DAA+D,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAMV,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,aAAc,KAAM,KAAMd,EAAAA,cAAoBI,EAAYQ,GAAI,KAAM,KAAMZ,EAAAA,cAAoBI,EAAYS,GAAI,KAAM,0CAA2C,KAAMb,EAAAA,cAAoBI,EAAYS,GAAI,KAAM,8EAA+E,KAAMb,EAAAA,cAAoBI,EAAYS,GAAI,KAAM,0EAA2E,MAAO,KAAMb,EAAAA,cAAoBI,EAAYM,EAAG,KAAMV,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,eAAgB,KAAM,KAAMd,EAAAA,cAAoBI,EAAYQ,GAAI,KAAM,KAAMZ,EAAAA,cAAoBI,EAAYS,GAAI,KAAM,kFAAmF,KAAMb,EAAAA,cAAoBI,EAAYS,GAAI,KAAM,+DAAgE,KAAMb,EAAAA,cAAoBI,EAAYS,GAAI,KAAM,8JAA+J,MAAO,KAAMb,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,iKAAkK,KAAMV,EAAAA,cAAoBI,EAAYG,GAAI,CACt6CY,GAAI,4BACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,6BACN,aAAc,sCACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,6BAA8B,KAAMzB,EAAAA,cAAoBI,EAAYO,GAAI,CAC3EQ,GAAI,8DACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,+DACN,aAAc,wEACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,+DAAgE,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,mrBAAorB,KAAMV,EAAAA,cAAoBI,EAAYO,GAAI,CACh1BQ,GAAI,oEACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,qEACN,aAAc,8EACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,qEAAsE,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,uJAAwJV,EAAAA,cAAoBC,EAAAA,EAAO,CAC3SF,KAAM,YACJ,YAAaC,EAAAA,cAAoBC,EAAAA,EAAO,CAC1CF,KAAM,cACJ,sCAAuCC,EAAAA,cAAoBC,EAAAA,EAAO,CACpEF,KAAM,YACJ,2CAA4CC,EAAAA,cAAoBC,EAAAA,EAAO,CACzEF,KAAM,cACJ,mMAAoM,KAAMC,EAAAA,cAAoBI,EAAYM,EAAG,KAAMV,EAAAA,cAAoBC,EAAAA,EAAO,CAChRF,KAAM,cACJ,eAAgBC,EAAAA,cAAoBC,EAAAA,EAAO,CAC7CF,KAAM,oCACJ,IAAKC,EAAAA,cAAoBI,EAAYY,IAAK,KAAMhB,EAAAA,cAAoBC,EAAAA,EAAO,CAC7EF,KAAM,cACJ,eAAgBC,EAAAA,cAAoBC,EAAAA,EAAO,CAC7CF,KAAM,cACJ,KAAM,KAAMC,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,oEAAqEV,EAAAA,cAAoBC,EAAAA,EAAO,CACvJF,KAAM,cACJ,iBAAkBC,EAAAA,cAAoBC,EAAAA,EAAO,CAC/CF,KAAM,kBACJ,cAAeC,EAAAA,cAAoBC,EAAAA,EAAO,CAC5CF,KAAM,cACJ,iBAAkBC,EAAAA,cAAoBC,EAAAA,EAAO,CAC/CF,KAAM,cACJ,KAAM,KAAMC,EAAAA,cAAoBI,EAAYO,GAAI,CAClDQ,GAAI,oEACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,qEACN,aAAc,8EACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,wEAAyE,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,qjBAAsjB,KAAMV,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,iQAAkQ,KAAMV,EAAAA,cAAoBI,EAAYO,GAAI,CAC5gCQ,GAAI,8EACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,+EACN,aAAc,wFACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,kFAAmF,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,6gBAA8gB,KAAMV,EAAAA,cAAoBI,EAAYO,GAAI,CAC7rBQ,GAAI,+DACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,gEACN,aAAc,yEACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,gEAAiE,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,4BAA6BV,EAAAA,cAAoBI,EAAYK,KAAM,CACtLe,wBAAyB,CACvBC,OAAQ,8CAER,6DAA8DzB,EAAAA,cAAoBI,EAAYK,KAAM,CACtGe,wBAAyB,CACvBC,OAAQ,kDAER,OAAQzB,EAAAA,cAAoBI,EAAYK,KAAM,CAChDe,wBAAyB,CACvBC,OAAQ,gDAER,sTAAuT,KAAMzB,EAAAA,cAAoBI,EAAYG,GAAI,CACnWY,GAAI,4CACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,6CACN,aAAc,sDACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,6CAA8C,KAAMzB,EAAAA,cAAoBI,EAAYO,GAAI,CAC3FQ,GAAI,kFACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,mFACN,aAAc,4FACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,qFAAsF,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,gdAAid,KAAMV,EAAAA,cAAoBI,EAAYO,GAAI,CACnoBQ,GAAI,oEACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,qEACN,aAAc,8EACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,qEAAsE,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,+gBAAghB,KAAMV,EAAAA,cAAoBI,EAAYO,GAAI,CAClrBQ,GAAI,0EACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,2EACN,aAAc,oFACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,4EAA6E,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,kdAAmd,KAAMV,EAAAA,cAAoBI,EAAYO,GAAI,CAC5nBQ,GAAI,0EACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,2EACN,aAAc,oFACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,8EAA+E,KAAMzB,EAAAA,cAAoBI,EAAYQ,GAAI,KAAM,KAAMZ,EAAAA,cAAoBI,EAAYS,GAAI,KAAM,KAAMb,EAAAA,cAAoBI,EAAYM,EAAG,KAAMV,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,qBAAsB,mJAAoJ,MAAO,KAAMd,EAAAA,cAAoBI,EAAYS,GAAI,KAAM,KAAMb,EAAAA,cAAoBI,EAAYM,EAAG,KAAMV,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,sBAAuB,wHAAyH,MAAO,KAAMd,EAAAA,cAAoBI,EAAYS,GAAI,KAAM,KAAMb,EAAAA,cAAoBI,EAAYM,EAAG,KAAMV,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,kBAAmB,4IAA6I,MAAO,MAAO,KAAMd,EAAAA,cAAoBI,EAAYG,GAAI,CACzkCY,GAAI,8BACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,+BACN,aAAc,wCACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,+BAAgC,KAAMzB,EAAAA,cAAoBI,EAAYO,GAAI,CAC7EQ,GAAI,iEACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,kEACN,aAAc,2EACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,qEAAsE,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,uPAAwPV,EAAAA,cAAoBC,EAAAA,EAAO,CAC3YF,KAAM,oBACJ,qGAAsG,KAAMC,EAAAA,cAAoBC,EAAAA,EAAO,CACzIF,KAAM,mHACJ,KAAMC,EAAAA,cAAoBI,EAAYO,GAAI,CAC5CQ,GAAI,wDACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,yDACN,aAAc,kEACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,yDAA0D,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,ySAA0S,KAAMV,EAAAA,cAAoBI,EAAYO,GAAI,CAChcQ,GAAI,yEACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,0EACN,aAAc,mFACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,8EAA+E,KAAMzB,EAAAA,cAAoBI,EAAYQ,GAAI,KAAM,KAAMZ,EAAAA,cAAoBI,EAAYS,GAAI,KAAMb,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,eAAgB,+EAAgF,KAAMd,EAAAA,cAAoBI,EAAYS,GAAI,KAAMb,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,gBAAiB,4DAA6D,KAAMd,EAAAA,cAAoBI,EAAYS,GAAI,KAAMb,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,gBAAiB,yGAA0G,MAAO,KAAMd,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,kEAAmEV,EAAAA,cAAoBC,EAAAA,EAAO,CACz1BF,KAAM,yBACJ,qDAAsDC,EAAAA,cAAoBC,EAAAA,EAAO,CACnFF,KAAM,mBACJ,uEAAwE,KAAMC,EAAAA,cAAoBI,EAAYO,GAAI,CACpHQ,GAAI,2DACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,4DACN,aAAc,qEACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,6DAA8D,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,2WAA6W,KAAMV,EAAAA,cAAoBI,EAAYG,GAAI,CACvgBY,GAAI,oBACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,qBACN,aAAc,8BACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,qBAAsB,KAAMzB,EAAAA,cAAoBI,EAAYO,GAAI,CACnEQ,GAAI,gFACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,iFACN,aAAc,0FACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,qFAAsF,KAAMzB,EAAAA,cAAoBI,EAAYQ,GAAI,KAAM,KAAMZ,EAAAA,cAAoBI,EAAYS,GAAI,KAAMb,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,UAAW,KAAMd,EAAAA,cAAoBC,EAAAA,EAAO,CACnRF,KAAM,mBACJ,2CAA4C,KAAMC,EAAAA,cAAoBI,EAAYS,GAAI,KAAMb,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,eAAgB,KAAMd,EAAAA,cAAoBC,EAAAA,EAAO,CAC7LF,KAAM,yBACJ,qEAAsE,KAAMC,EAAAA,cAAoBI,EAAYS,GAAI,KAAMb,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,eAAgB,KAAMd,EAAAA,cAAoBC,EAAAA,EAAO,CACvNF,KAAM,oCACJ,yBAA0BC,EAAAA,cAAoBC,EAAAA,EAAO,CACvDF,KAAM,kBACJ,+FAAgG,MAAO,KAAMC,EAAAA,cAAoBI,EAAYO,GAAI,CACnJQ,GAAI,8DACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,+DACN,aAAc,wEACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,+DAAgE,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,kJAAmJ,KAAMV,EAAAA,cAAoBC,EAAAA,EAAO,CACtSF,KAAM,gGACJ,KAAMC,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,wOAAyO,KAAMV,EAAAA,cAAoBI,EAAYO,GAAI,CACpUQ,GAAI,6DACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,8DACN,aAAc,uEACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,kEAAmE,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,qdAAsd,KAAMV,EAAAA,cAAoBI,EAAYO,GAAI,CACrnBQ,GAAI,4FACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,6FACN,aAAc,sGACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,6FAA8F,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,0HAA2H,KAAMV,EAAAA,cAAoBI,EAAYQ,GAAI,KAAM,KAAMZ,EAAAA,cAAoBI,EAAYS,GAAI,KAAMb,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,wBAAyB,4FAA6F,KAAMd,EAAAA,cAAoBI,EAAYS,GAAI,KAAMb,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,qBAAsB,6DAA8D,KAAMd,EAAAA,cAAoBI,EAAYS,GAAI,KAAMb,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,0BAA2B,yHAA0H,MAAO,KAAMd,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,gRAAiR,KAAMV,EAAAA,cAAoBI,EAAYG,GAAI,CACryCY,GAAI,uCACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,wCACN,aAAc,iDACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,0CAA2C,KAAMzB,EAAAA,cAAoBI,EAAYO,GAAI,CACxFQ,GAAI,kEACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,mEACN,aAAc,4EACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,sEAAuE,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,giBAAiiB,KAAMV,EAAAA,cAAoBI,EAAYO,GAAI,CACpsBQ,GAAI,2FACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,4FACN,aAAc,qGACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,+FAAgG,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,ueAAwe,KAAMV,EAAAA,cAAoBI,EAAYO,GAAI,CACpqBQ,GAAI,0EACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,2EACN,aAAc,oFACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,6EAA8E,KAAMzB,EAAAA,cAAoBI,EAAYQ,GAAI,KAAM,KAAMZ,EAAAA,cAAoBI,EAAYS,GAAI,KAAMb,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,qBAAsB,oPAAqP,KAAMd,EAAAA,cAAoBI,EAAYS,GAAI,KAAMb,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,gBAAiB,qLAAsL,MAAO,KAAMd,EAAAA,cAAoBI,EAAYO,GAAI,CACh0BQ,GAAI,yDACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,0DACN,aAAc,mEACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,0DAA2D,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,2cAA4c,KAAMV,EAAAA,cAAoBI,EAAYO,GAAI,CACnmBQ,GAAI,gEACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,iEACN,aAAc,0EACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,iEAAkE,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,0GAA2GV,EAAAA,cAAoBI,EAAYK,KAAM,CACrQe,wBAAyB,CACvBC,OAAQ,8CAER,QAASzB,EAAAA,cAAoBI,EAAYK,KAAM,CACjDe,wBAAyB,CACvBC,OAAQ,8CAER,kTAAmT,KAAMzB,EAAAA,cAAoBI,EAAYG,GAAI,CAC/VY,GAAI,2BACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,4BACN,aAAc,qCACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,4BAA6B,KAAMzB,EAAAA,cAAoBI,EAAYO,GAAI,CAC1EQ,GAAI,+EACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,gFACN,aAAc,yFACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,gFAAiF,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,+QAAgR,KAAMV,EAAAA,cAAoBI,EAAYO,GAAI,CAC7bQ,GAAI,wEACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,yEACN,aAAc,kFACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,8EAA+E,KAAMzB,EAAAA,cAAoBI,EAAYQ,GAAI,KAAM,KAAMZ,EAAAA,cAAoBI,EAAYS,GAAI,KAAMb,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,QAAS,gOAAiO,KAAMd,EAAAA,cAAoBI,EAAYS,GAAI,KAAMb,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,QAAS,+NAAgO,MAAO,KAAMd,EAAAA,cAAoBI,EAAYO,GAAI,CACl0BQ,GAAI,8EACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,+EACN,aAAc,wFACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,gFAAiF,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,6XAAgY,KAAMV,EAAAA,cAAoBI,EAAYO,GAAI,CAC7iBQ,GAAI,sEACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,uEACN,aAAc,gFACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,uEAAwE,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,gXAAiX,KAAMV,EAAAA,cAAoBI,EAAYO,GAAI,CACrhBQ,GAAI,oEACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,qEACN,aAAc,8EACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,wEAAyE,KAAMzB,EAAAA,cAAoBI,EAAYQ,GAAI,KAAM,KAAMZ,EAAAA,cAAoBI,EAAYS,GAAI,KAAM,KAAMb,EAAAA,cAAoBI,EAAYM,EAAG,KAAMV,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,mBAAoB,iGAAkG,MAAO,KAAMd,EAAAA,cAAoBI,EAAYS,GAAI,KAAM,KAAMb,EAAAA,cAAoBI,EAAYM,EAAG,KAAMV,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,oBAAqB,8MAA+M,MAAO,KAAMd,EAAAA,cAAoBI,EAAYS,GAAI,KAAM,KAAMb,EAAAA,cAAoBI,EAAYM,EAAG,KAAMV,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,mBAAoB,mOAAoO,MAAO,MAAO,KAAMd,EAAAA,cAAoBI,EAAYG,GAAI,CAC3rCY,GAAI,sBACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,uBACN,aAAc,gCACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,uBAAwB,KAAMzB,EAAAA,cAAoBI,EAAYO,GAAI,CACrEQ,GAAI,+EACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,gFACN,aAAc,yFACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,gFAAiF,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,sTAAuT,KAAMV,EAAAA,cAAoBI,EAAYO,GAAI,CACpeQ,GAAI,8EACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,+EACN,aAAc,wFACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,mFAAoF,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,4ZAA+Z,KAAMV,EAAAA,cAAoBI,EAAYO,GAAI,CAC/kBQ,GAAI,+DACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,gEACN,aAAc,yEACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,gEAAiE,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,6WAAgX,KAAMV,EAAAA,cAAoBI,EAAYO,GAAI,CAC7gBQ,GAAI,kEACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,mEACN,aAAc,4EACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,qEAAsE,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,+cAAgd,KAAMV,EAAAA,cAAoBI,EAAYG,GAAI,CAClnBY,GAAI,+BACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,gCACN,aAAc,yCACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,gCAAiC,KAAMzB,EAAAA,cAAoBI,EAAYO,GAAI,CAC9EQ,GAAI,oEACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,qEACN,aAAc,8EACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,uEAAwE,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,yBAA0B,KAAMV,EAAAA,cAAoBI,EAAYQ,GAAI,KAAM,KAAMZ,EAAAA,cAAoBI,EAAYS,GAAI,KAAM,mBAAoBb,EAAAA,cAAoBC,EAAAA,EAAO,CACnSF,KAAM,aACH,KAAMC,EAAAA,cAAoBI,EAAYS,GAAI,KAAM,oBAAqBb,EAAAA,cAAoBC,EAAAA,EAAO,CACnGF,KAAM,aACH,KAAMC,EAAAA,cAAoBI,EAAYS,GAAI,KAAM,yBAA0Bb,EAAAA,cAAoBC,EAAAA,EAAO,CACxGF,KAAM,0CACH,KAAMC,EAAAA,cAAoBI,EAAYS,GAAI,KAAM,qBAAsBb,EAAAA,cAAoBC,EAAAA,EAAO,CACpGF,KAAM,kBACJ,OAAQC,EAAAA,cAAoBC,EAAAA,EAAO,CACrCF,KAAM,uBACH,KAAMC,EAAAA,cAAoBI,EAAYS,GAAI,KAAM,qBAAsBb,EAAAA,cAAoBC,EAAAA,EAAO,CACpGF,KAAM,kBACJ,uDAAwD,MAAO,KAAMC,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,0CAA2CV,EAAAA,cAAoBC,EAAAA,EAAO,CACtLF,KAAM,YACJ,qBAAsBC,EAAAA,cAAoBC,EAAAA,EAAO,CACnDF,KAAM,YACJ,gCAAiCC,EAAAA,cAAoBC,EAAAA,EAAO,CAC9DF,KAAM,aACJ,kKAAoK,KAAMC,EAAAA,cAAoBI,EAAYO,GAAI,CAChNQ,GAAI,2DACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,4DACN,aAAc,qEACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,6DAA8D,KAAMzB,EAAAA,cAAoBI,EAAYQ,GAAI,KAAM,KAAMZ,EAAAA,cAAoBI,EAAYS,GAAI,KAAMb,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,kBAAmB,4HAA6H,KAAMd,EAAAA,cAAoBI,EAAYS,GAAI,KAAMb,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,oBAAqB,uFAAwFd,EAAAA,cAAoBC,EAAAA,EAAO,CACrkBF,KAAM,0BACJ,4FAA6F,MAAO,KAAMC,EAAAA,cAAoBI,EAAYO,GAAI,CAChJQ,GAAI,oDACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,qDACN,aAAc,8DACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,wDAAyD,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,KAAMV,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,wBAAyB,kBAAmBd,EAAAA,cAAoBC,EAAAA,EAAO,CACtOF,KAAM,yBACJ,wCAAyCC,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,qBAAsB,OAAQd,EAAAA,cAAoBC,EAAAA,EAAO,CAClJF,KAAM,0BACJ,iMAAkM,KAAMC,EAAAA,cAAoBI,EAAYO,GAAI,CAC9OQ,GAAI,mFACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,oFACN,aAAc,6FACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,uFAAwF,KAAMzB,EAAAA,cAAoBI,EAAYQ,GAAI,KAAM,KAAMZ,EAAAA,cAAoBI,EAAYS,GAAI,KAAM,KAAMb,EAAAA,cAAoBI,EAAYM,EAAG,KAAMV,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,uBAAwB,8NAA+N,MAAO,KAAMd,EAAAA,cAAoBI,EAAYS,GAAI,KAAM,KAAMb,EAAAA,cAAoBI,EAAYM,EAAG,KAAMV,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,YAAa,mJAAoJ,MAAO,KAAMd,EAAAA,cAAoBI,EAAYS,GAAI,KAAM,KAAMb,EAAAA,cAAoBI,EAAYM,EAAG,KAAMV,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,uBAAwB,2OAA4O,MAAO,MAAO,KAAMd,EAAAA,cAAoBI,EAAYG,GAAI,CACpxCY,GAAI,sBACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,uBACN,aAAc,gCACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,uBAAwB,KAAMzB,EAAAA,cAAoBI,EAAYO,GAAI,CACrEQ,GAAI,sEACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,uEACN,aAAc,gFACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,2EAA4E,KAAMzB,EAAAA,cAAoBI,EAAYQ,GAAI,KAAM,KAAMZ,EAAAA,cAAoBI,EAAYS,GAAI,KAAM,KAAMb,EAAAA,cAAoBI,EAAYM,EAAG,KAAMV,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,mBAAoB,yCAA0C,KAAMd,EAAAA,cAAoBC,EAAAA,EAAO,CAC3WF,KAAM,wGACJ,KAAMC,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,6EAA8EV,EAAAA,cAAoBC,EAAAA,EAAO,CAC1JF,KAAM,YACJ,KAAM,MAAO,KAAMC,EAAAA,cAAoBI,EAAYS,GAAI,KAAM,KAAMb,EAAAA,cAAoBI,EAAYM,EAAG,KAAMV,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,oBAAqB,uBAAwBd,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,qBAAsB,wCAAyCd,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,sBAAuB,6HAA8H,MAAO,KAAMd,EAAAA,cAAoBI,EAAYS,GAAI,KAAM,KAAMb,EAAAA,cAAoBI,EAAYM,EAAG,KAAMV,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,cAAe,kEAAmEd,EAAAA,cAAoBC,EAAAA,EAAO,CAC1vBF,KAAM,iBACJ,gCAAiC,KAAMC,EAAAA,cAAoBC,EAAAA,EAAO,CACpEF,KAAM,0GACJ,KAAMC,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,kEAAmEV,EAAAA,cAAoBC,EAAAA,EAAO,CAC/IF,KAAM,YACJ,iBAAkBC,EAAAA,cAAoBC,EAAAA,EAAO,CAC/CF,KAAM,cACJ,KAAM,MAAO,MAAO,KAAMC,EAAAA,cAAoBI,EAAYO,GAAI,CAChEQ,GAAI,wDACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,yDACN,aAAc,kEACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,yDAA0D,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,mZAAoZV,EAAAA,cAAoBC,EAAAA,EAAO,CAC3hBF,KAAM,kBACJ,KAAM,KAAMC,EAAAA,cAAoBI,EAAYO,GAAI,CAClDQ,GAAI,yDACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,0DACN,aAAc,mEACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,0DAA2D,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,6WAA8W,KAAMV,EAAAA,cAAoBI,EAAYO,GAAI,CACrgBQ,GAAI,0EACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,2EACN,aAAc,oFACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,8EAA+E,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,wSAAySV,EAAAA,cAAoBC,EAAAA,EAAO,CACrcF,KAAM,YACJ,OAAQC,EAAAA,cAAoBC,EAAAA,EAAO,CACrCF,KAAM,YACJ,+FAAgG,KAAMC,EAAAA,cAAoBI,EAAYO,GAAI,CAC5IQ,GAAI,sCACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,uCACN,aAAc,gDACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,uCAAwC,KAAMzB,EAAAA,cAAoBI,EAAYQ,GAAI,KAAM,KAAMZ,EAAAA,cAAoBI,EAAYS,GAAI,KAAM,KAAMb,EAAAA,cAAoBI,EAAYM,EAAG,KAAMV,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,cAAe,4CAA6C,MAAO,KAAMd,EAAAA,cAAoBI,EAAYS,GAAI,KAAM,KAAMb,EAAAA,cAAoBI,EAAYM,EAAG,KAAMV,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,qBAAsB,gFAAiF,MAAO,KAAMd,EAAAA,cAAoBI,EAAYS,GAAI,KAAM,KAAMb,EAAAA,cAAoBI,EAAYM,EAAG,KAAMV,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,SAAU,yCAA0C,MAAO,MAAO,KAAMd,EAAAA,cAAoBI,EAAYG,GAAI,CAC/xBY,GAAI,mCACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,oCACN,aAAc,6CACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,sCAAuC,KAAMzB,EAAAA,cAAoBI,EAAYO,GAAI,CACpFQ,GAAI,6DACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,8DACN,aAAc,uEACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,+DAAgE,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,oIAAqIV,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,gBAAiB,KAAMd,EAAAA,cAAoBC,EAAAA,EAAO,CACvVF,KAAM,cACJ,0EAA2EC,EAAAA,cAAoBC,EAAAA,EAAO,CACxGF,KAAM,YACJ,kMAAmMC,EAAAA,cAAoBC,EAAAA,EAAO,CAChOF,KAAM,YACJ,cAAe,KAAMC,EAAAA,cAAoBI,EAAYO,GAAI,CAC3DQ,GAAI,+EACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,gFACN,aAAc,yFACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,qFAAsF,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,gGAAiGV,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,+BAAgC,mOAAoO,KAAMd,EAAAA,cAAoBI,EAAYO,GAAI,CACrkBQ,GAAI,yFACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,0FACN,aAAc,mGACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,8FAA+F,KAAMzB,EAAAA,cAAoBI,EAAYQ,GAAI,KAAM,KAAMZ,EAAAA,cAAoBI,EAAYS,GAAI,KAAMb,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,YAAa,2RAA4R,KAAMd,EAAAA,cAAoBI,EAAYS,GAAI,KAAMb,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,oBAAqB,uKAAwK,KAAMd,EAAAA,cAAoBI,EAAYS,GAAI,KAAMb,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,cAAe,uMAAwM,MAAO,KAAMd,EAAAA,cAAoBI,EAAYO,GAAI,CAC1pCQ,GAAI,4DACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,6DACN,aAAc,sEACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,6DAA8D,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,uEAAwE,KAAMV,EAAAA,cAAoBI,EAAYQ,GAAI,KAAM,KAAMZ,EAAAA,cAAoBI,EAAYS,GAAI,KAAMb,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,uBAAwB,4BAA6B,KAAMd,EAAAA,cAAoBI,EAAYS,GAAI,KAAMb,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,2BAA4B,gCAAiC,KAAMd,EAAAA,cAAoBI,EAAYS,GAAI,KAAMb,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,kBAAmB,qEAAsE,MAAO,KAAMd,EAAAA,cAAoBI,EAAYO,GAAI,CAC9vBQ,GAAI,sDACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,uDACN,aAAc,gEACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,uDAAwD,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,+MAAgN,KAAMV,EAAAA,cAAoBI,EAAYG,GAAI,CACpWY,GAAI,6BACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,8BACN,aAAc,uCACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,8BAA+B,KAAMzB,EAAAA,cAAoBI,EAAYO,GAAI,CAC5EQ,GAAI,yEACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,0EACN,aAAc,mFACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,6EAA8E,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,gHAAiHV,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,iBAAkB,6FAA8Fd,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,gBAAiB,4EAA6Ed,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,oBAAqB,4FAA6F,KAAMd,EAAAA,cAAoBI,EAAYO,GAAI,CACruBQ,GAAI,+EACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,gFACN,aAAc,yFACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,oFAAqF,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,MAAOV,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,eAAgB,sJAAuJd,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,eAAgB,mJAAoJ,KAAMd,EAAAA,cAAoBI,EAAYO,GAAI,CAC/lBQ,GAAI,kEACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,mEACN,aAAc,4EACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,qEAAsE,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,gZAAiZ,KAAMV,EAAAA,cAAoBI,EAAYO,GAAI,CACnjBQ,GAAI,0DACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,2DACN,aAAc,oEACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,6DAA8D,KAAMzB,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,sUAAuU,KAAMV,EAAAA,cAAoBI,EAAYO,GAAI,CACjeQ,GAAI,gGACJC,MAAO,CACLC,SAAU,aAEXrB,EAAAA,cAAoBI,EAAYI,EAAG,CACpCc,KAAM,iGACN,aAAc,0GACdC,UAAW,iBACVvB,EAAAA,cAAoBI,EAAYK,KAAM,CACvCe,wBAAyB,CACvBC,OAAQ,meAEP,oGAAqG,KAAMzB,EAAAA,cAAoBI,EAAYQ,GAAI,KAAM,KAAMZ,EAAAA,cAAoBI,EAAYS,GAAI,KAAMb,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,6BAA8B,8FAA+F,KAAMd,EAAAA,cAAoBI,EAAYS,GAAI,KAAMb,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,eAAgB,iDAAkD,KAAMd,EAAAA,cAAoBI,EAAYS,GAAI,KAAMb,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,sBAAuB,yHAA0H,MAAO,KAAMd,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,uDAAwDV,EAAAA,cAAoBI,EAAYU,OAAQ,KAAM,cAAe,+MAAgN,KAAMd,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,8rBAA+rB,KAAMV,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,0mBAA2mB,KAAMV,EAAAA,cAAoBI,EAAYM,EAAG,KAAM,0NACxjF,CAKA,MAJA,SAAoBP,QAAK,IAALA,IAAAA,EAAQ,CAAC,GAC3B,MAAOwB,QAASC,GAAavB,OAAOC,OAAO,CAAC,GAAGW,EAAAA,EAAAA,MAAsBd,EAAMe,YAC3E,OAAOU,EAAY5B,EAAAA,cAAoB4B,EAAWzB,EAAOH,EAAAA,cAAoBE,EAAmBC,IAAUD,EAAkBC,EAC9H,E,4KCp4CA,MAAM0B,EAAkB/B,IACtB,IAAI,IAACgC,GAAOhC,EACZ,IAAKgC,IAAQA,EAAIC,MAAO,OAAO,KAY/B,OAAO/B,EAAAA,cAAoB,MAAO,CAChCuB,UAAWS,EAAAA,GACVhC,EAAAA,cAAoB,KAAM,KAAM8B,EAAIC,MAAME,KAAI,CAACC,EAAMC,IAAUnC,EAAAA,cAAoB,KAAM,CAC1FoC,IAAKD,GACJnC,EAAAA,cAAoB,IAAK,CAC1BsB,KAAMY,EAAKG,IACXC,QAASC,GAjBSC,EAACD,EAAGF,KACtBE,EAAEE,iBACF,MAAMC,EAAWL,EAAIM,QAAQ,IAAK,IAC5BC,EAAgBC,SAASC,eAAeJ,GAC1CE,GACFA,EAAcG,eAAe,CAC3BC,SAAU,SACVC,MAAO,SAEX,EAQcT,CAAYD,EAAGL,EAAKG,MACjCH,EAAKgB,OAAQhB,EAAKH,OAAS/B,EAAAA,cAAoB6B,EAAiB,CACjEC,IAAK,CACHC,MAAOG,EAAKH,aAEV,EAED,SAASoB,EAAaC,GAC3B,IAAKC,MAAM,IAACC,EAAG,OAAEC,EAAM,cAAEC,GAAc,SAAEC,GAAYL,EACrD,MAAM,YAACM,EAAW,KAAEC,EAAI,gBAAEC,GAAmBN,EACvCnB,EAAQuB,EAAYvB,MAEpB0B,EADOH,EAAYI,KACJC,MAAM,KAAK,GAE1BC,EADQT,EAAOU,MAAMC,QAAOC,GAAQA,EAAKT,YAAYI,KAAKM,SAAS,IAAIP,QACnDQ,MAAK,CAAC7D,EAAG8D,IAAM9D,EAAEkD,YAAYvB,MAAQmC,EAAEZ,YAAYvB,QACvEoC,EAAeP,EAAYQ,WAAUL,GAAQA,EAAKT,YAAYvB,QAAUA,IACxEsC,EAAWT,EAAYO,EAAe,GACtCG,EAAWV,EAAYO,EAAe,GACtCI,EAAcjB,EAAYI,KAAKnB,QAAQ,MAAO,IAC9CiC,EAAc,SAAUC,KAAKF,GAAa,GAC1CG,EAAW,SAASjB,aAAmBe,MACtC,EAAGG,EAAc,EAAGC,IAAmBC,EAAAA,EAAAA,UAASvB,EAAYwB,0BAC5D,EAAGC,EAAa,EAAGC,IAAkBH,EAAAA,EAAAA,WAAS,GASrD,IAAII,GALJC,EAAAA,EAAAA,YAAU,KACRF,GAAe,GACf,MAAMG,EAAQC,YAAW,IAAMJ,GAAe,IAAQ,KACtD,MAAO,IAAMK,aAAaF,EAAM,GAC/B,CAACR,IAEY,eAAZlB,EACFwB,EAAiBK,EAAAA,GACI,aAAZ7B,EACTwB,EAAiBM,EAAAA,GACI,aAAZ9B,IACTwB,EAAiBO,EAAAA,IAEnB,MACMC,EADgBC,IAAenC,GAAMhB,QAAQ,wBAAyB,IAAIA,QAAQ,SAAU,IAAIA,QAAQ,wBAAyB,IAAIoD,OAC3GhC,MAAM,OAAOiC,OAIvCC,EA9ER,SAAwBC,GACtB,GAAIA,GAAW,GAAI,MAAO,UAC1B,GAAIA,GAAW,GAAI,MAAO,UAC1B,GAAIA,GAAW,GAAI,MAAO,UAC1B,GAAIA,GAAW,GAAI,MAAO,UAC1B,GAAIA,GAAW,GAAI,MAAO,UAC1B,GAAIA,GAAW,GAAI,MAAO,OAC1B,MAAMC,EAAQC,KAAKC,MAAMH,EAAU,IAC7BI,EAAYJ,EAAU,GAC5B,OAAII,GAAa,GACR,IAAIH,IAAQG,EAAY,EAAI,KAAO,OAErC,IAAIH,EAAQ,KACrB,CAiEmBI,CAHWH,KAAKI,KAAKX,EAAYR,IAChC3B,EAAY+C,kBAAoB,IAG5CC,EAAU,CAAC,CACfC,KAAMjD,EAAYkD,UAClBC,UAAWA,IAAM,0DAChB,CACDF,KAAMjD,EAAYoD,gBAClBD,UAAWA,IAAM,0DAChB,CACDF,KAAMjD,EAAYqD,YAClBF,UAAWA,IAAM,0DAChB,CACDF,KAAMjD,EAAYsD,cAClBH,UAAWA,IAAM,0DAChB,CACDF,KAAMjD,EAAYuD,YAClBJ,UAAWA,IAAM,0DAChB,CACDF,KAAMjD,EAAYwD,iBAClBL,UAAWA,IAAM,0DAChB,CACDF,KAAMjD,EAAYyD,eAClBN,UAAWA,IAAM,0DAChB,CACDF,KAAMjD,EAAY0D,cAClBP,UAAWA,IAAM,0DAChB,CACDF,KAAMjD,EAAY2D,kBAClBR,UAAWA,IAAM,0DAChB,CACDF,KAAMjD,EAAY4D,WAClBT,UAAWA,IAAM,4DAEZ,EAAGU,EAAe,EAAGC,IAAoBvC,EAAAA,EAAAA,UAAS,IAWzD,OAVAK,EAAAA,EAAAA,YAAU,KACRoB,EAAQe,SAAQC,IACd,IAAI,KAACf,EAAI,UAAEE,GAAaa,EACpBf,GACFE,IAAYc,MAAKC,IACfJ,GAAiBK,GAAQ,GAAGC,QAAOC,EAAAA,EAAAA,GAAmBF,GAAO,CAACD,EAAOI,WAAU,GAEnF,GACA,GACD,IACIhI,EAAAA,cAAoBiI,EAAAA,EAAOC,IAAK,CACrCC,QAAS,CACPC,QAAS,GAEXC,QAAS,CACPD,QAAS,GAEXE,KAAM,CACJF,QAAS,GAEXG,WAAY,CACVC,SAAU,MAEXxI,EAAAA,cAAoByI,EAAAA,EAAY,CACjCC,WAAYhF,EAAYvB,MACxBwG,KAAMjF,EAAYiF,KAClBC,QAASlF,EAAYkF,QACrB3C,SAAUA,EACV4C,WAAYnF,EAAYoF,gBACxB5F,MAAOQ,EAAYR,MACnB6F,KAAMrF,EAAYqF,KAClBC,OAAQtF,EAAYsF,OACpBnF,QAASA,EACToF,QAASrE,EACTsE,cAAexF,EAAYoD,gBAC3BqC,QAASzF,EAAYyF,UACnBnJ,EAAAA,cAAoB,MAAO,CAC7BoB,MAAO,CACLgI,QAAS,OACTC,eAAgB,WAChBC,SAAU,OACVC,SAAU,MACVC,WAAY,OACZC,aAAc,MACdC,UAAW,OACXC,aAAc,QAEfjG,EAAYkG,UAAU3H,KAAI,CAAC4H,EAAK1H,IAAUnC,EAAAA,cAAoB,OAAQ,CACvEoC,IAAKD,EACLZ,UAAW,YAAYuI,EAAAA,KACvB1I,MAAO,CACL2I,OAAQ,gBAETF,MAAQ7J,EAAAA,cAAoB,MAAO,CACpCuB,UAAW,YACVvB,EAAAA,cAAoB6B,EAAiB,CACtCC,IAAK8B,KACF5D,EAAAA,cAAoB,KAAM,MAAOA,EAAAA,cAAoB,MAAO,CAC/DoB,MAAO,CACL2I,OAAQ,iBACRC,UAAW,UAEZhK,EAAAA,cAAoBiI,EAAAA,EAAOgC,OAAQ,CACpC1I,UAAW,YAAY2I,EAAAA,KACvB/I,GAAI+I,EAAAA,GACJ5H,QAvHmB6H,KACnBnF,GAAiBD,EAAa,EAuH9BqF,SAAU,CACRC,MAAO,MAERrK,EAAAA,cAAoBiI,EAAAA,EAAOC,IAAK,CACjC3G,UAAW+I,EAAAA,GACXlI,IAAK2C,EACLoD,QAAS,CACPC,QAAS,GAEXC,QAAS,CACPD,QAAS,GAEXE,KAAM,CACJF,QAAS,GAEXG,WAAY,CACVC,SAAU,GACV+B,KAAM,cAEPxF,EAAe,2BAA6B,2BAA4B/E,EAAAA,cAAoB,KAAM,MAAOA,EAAAA,cAAoB,MAAO,CACrIuB,UAAW,WACXH,MAAO,CACL2I,OAAQhF,EAAe,SAAW,GAClCwE,SAAUxE,EAAe,OAAS,GAClCwD,WAAY,uDAEbvI,EAAAA,cAAoB,MAAO,CAC5BuB,UAAW,GAAG2I,EAAAA,MAAuC/E,EAAc+E,EAAAA,GAAkCA,EAAAA,MACpG3C,EAActF,KAAI,CAACuI,EAAiBrI,IAAUnC,EAAAA,cAAoBwK,EAAiB,CACpFpI,IAAKD,MACFuB,EAAY+G,YAAczK,EAAAA,cAAoB0K,EAAAA,EAAoB,CACrEvI,MAAOuB,EAAY+G,YACnBE,SAAUjH,EAAYkH,qBACnB,GAAI5K,EAAAA,cAAoB6K,EAAAA,EAAaC,SAAU,CAClDC,MAAO,CACLC,OAAQxH,EAAcS,MACtBa,SAAUA,EAASnC,QAAQ,MAAO,IAAM,MAEzC3C,EAAAA,cAAoBiL,EAAAA,GAAa,CAClC/J,WAAY,CACVgK,MAAKA,EAAAA,IAENzH,MAAczD,EAAAA,cAAoBmL,EAAAA,EAAY,CAC/C1G,SAAUA,EACVC,SAAUA,EACVE,WAAYA,EACZf,QAASA,IAEb,CAEe,SAASuH,EAAiBjL,GACvC,OAAOH,EAAAA,cAAoBmD,EAAchD,EAAOH,EAAAA,cAAoBqL,EAAqBlL,GAC3F,CACO,SAASmL,EAAKC,GACnB,IAAIC,EAAqBC,EAAuBC,EAAwBC,EAAwBC,EAChG,IAAI,KAACvI,GAAQkI,EACb,MAAM,YAAC7H,GAAeL,EAAKC,IACrBJ,EAAQQ,EAAYmI,UAAYnI,EAAYR,MAC5C4I,EAAUpI,EAAYoI,SAAW5I,EACjC6I,EAAerI,EAAYqI,cAAgB7I,EAC3C8I,EAActI,EAAYuI,SAAWvI,EAAYqF,KACjDmD,EAAgBxI,EAAYyI,QAAUH,EACtCI,EAAqB1I,EAAY2I,aAAeL,EAChDM,EAAa5I,EAAY4I,YAAc,cACvCC,EAAW7I,EAAY8I,YACvBC,EAAgB/I,EAAYiF,KAC5B+D,EAAehJ,EAAYkF,SAAW6D,EACtCE,EAAUjJ,EAAYiJ,UAA2D,QAA9CnB,EAAsB9H,EAAYsF,cAA4C,IAAxBwC,GAA4G,QAAjEC,EAAwBD,EAAoBoB,uBAAuD,IAA1BnB,GAAiH,QAApEC,EAAyBD,EAAsBoB,uBAAwD,IAA3BnB,GAA0G,QAA5DC,EAAyBD,EAAuBV,cAA+C,IAA3BW,GAA4G,QAA9DC,EAAyBD,EAAuBmB,gBAAiD,IAA3BlB,OAAlb,EAA+dA,EAAuBmB,KAChnBC,EAAatJ,EAAYsJ,YAAcd,EACvCe,EAAevJ,EAAYuJ,cAAgBN,EAC3CO,EAAkBxJ,EAAYwJ,iBAAmBd,EACjDe,EAAezJ,EAAY0J,aAC3B9F,EAAa5D,EAAY4D,aAAc,EACvC6B,EAAUzF,EAAYyF,SAAW,QACjCtF,EAAUH,EAAYI,KAAKC,MAAM,KAAK,IAAM,SAE5C,QAACsJ,IAAWC,EAAAA,EAAAA,KACZC,EAAiB,CACrB,WAAY,qBACZ,QAAS,iBACT,gBAAmB,CAAC,CAClB,QAAS,WACT,SAAY,EACZ,KAAQ,OACR,KAAQF,GACP,CACD,QAAS,WACT,SAAY,EACZ,KAAQlE,EACR,KAAQ,GAAGkE,KAAW3J,EAAYI,KAAKC,MAAM,KAAK,MACjD,CACD,QAAS,WACT,SAAY,EACZ,KAAQb,EACR,KAAQ,GAAGmK,IAAU3J,EAAYI,UAGrC,OAAO9D,EAAAA,cAAoBwN,EAAAA,EAAK,CAC9BtK,MAAOA,EAAQ,gBACf4I,QAASA,EACTC,aAAcA,EACdC,YAAaA,EACbE,cAAeA,EACfE,mBAAoBA,EACpBE,WAAYA,EACZC,SAAUA,EACVE,cAAeA,EACfC,aAAcA,EACdC,QAASA,EACTK,WAAYA,EACZC,aAAcA,EACdC,gBAAiBA,EACjBC,aAAcA,EACd7F,WAAYA,EACZ6B,QAASA,EACTtF,QAASA,EACT4J,KAzCW,WA0CVzN,EAAAA,cAAoB,SAAU,CAC/ByN,KAAM,uBACLC,KAAKC,UAAUJ,IACpB,C","sources":["webpack://avrtt.blog/./src/components/Latex/index.tsx","webpack://avrtt.blog/./src/pages/posts/research/ai_reasoning_and_uncertainty_2.mdx","webpack://avrtt.blog/./src/templates/post.tsx"],"sourcesContent":["/* \n\nCopyright © 2022  Vladislav Averett (avrtt)\nDistributed under the GNU AGPLv3 license. For details and source code, please refer to <https://github.com/avrtt/avrtt.github.io>.\n\n*/\n\nimport React from \"react\";\nimport Latex from 'react-latex-next';\nimport 'katex/dist/katex.min.css'; \n\ninterface LatexProps {\n  text: string;\n}\n  \nconst L = ({ text }: LatexProps) => {\n  return (\n    <Latex>{text}</Latex>\n  );\n};\nexport default L;\n","/*@jsxRuntime classic @jsx React.createElement @jsxFrag React.Fragment*/\n/**(intro: a quote, catchphrase, joke, etc.)**/\n/*\n\n1. Probabilistic Reasoning Over Time\n- Why time introduces additional layers of uncertainty (delayed feedback, evolving states)\n- Markov property and its central role in time-series modeling\n- Examples in robotics (tracking moving objects), finance (stock price modeling), and weather forecasting\n- Overview of filtering, prediction, and smoothing tasks\n2. Time and Uncertainty\n- Discrete-time vs. continuous-time approaches (Markov chains vs. stochastic differential equations)\n- Handling streaming data in real-time inference\n- Memory-based models vs. Markov models (long-term vs. short-term dependencies)\n- Practical issues: sensor noise, delayed signals, and partial observability\n3. Inference in Temporal Models\n- Forward-backward algorithm basics for hidden state estimation\n- Particle filtering for nonlinear/non-Gaussian processes\n- Online vs. offline inference: when each is appropriate\n- Scalability challenges and approximate methods for large or complex temporal models\n- Example use cases: speech recognition, tracking user behavior over time\n4. Kalman Filters\n- Detailed look at predict-update equations in linear Gaussian settings\n- Use in control systems, signal processing, and localization (e.g., GPS + sensor fusion)\n- Extensions: Extended Kalman Filter (EKF) and Unscented Kalman Filter (UKF) for nonlinear problems\n- Strengths, limitations, and typical real-world performance considerations\n5. Dynamic Bayesian Networks\n- How DBNs generalize Hidden Markov Models and Kalman filters\n- Representation of temporal transitions and emission probabilities\n- Parameter learning strategies (EM algorithm, gradient-based methods)\n- Applications in speech recognition, activity recognition, robotics, and beyond\n- Practical software tools for constructing and inferring DBNs\n6. Making Simple Decisions Under Uncertainty\n- Bridging probabilistic beliefs with decision-making (decision-theoretic approach)\n- Scenario analysis and sensitivity analysis for uncertain outcomes\n- Single-step decisions vs. multi-step decisions in uncertain environments\n- Real-world examples: medical diagnoses, finance portfolios, route planning\n7. The Basis of Utility Theory\n- Axioms of rational preference (Von Neumann–Morgenstern framework)\n- How utility captures subjective valuation of outcomes\n- Risk aversion, risk neutrality, and risk-seeking behaviors (with examples)\n- Contrasts between economic vs. AI perspectives on utility\n8. Utility Functions\n- Common functional forms (linear, exponential, logarithmic) and their implications\n- Constructing multiattribute utilities for complex decisions\n- Dealing with trade-offs and constraints (budget, time, safety)\n- Approximation methods for utility elicitation in large-scale or high-dimensional settings\n9. Decision Networks (Influence Diagrams)\n- Chance nodes, decision nodes, and utility nodes: how they interact\n- Algorithmic approaches to solve decision networks (backward induction, dynamic programming)\n- Example applications: medical treatment planning, supply chain management\n- Value iteration methods adapted for influence diagrams\n- Tools for modeling and solving influence diagrams in practice\n10. The Value of Information\n- Cost-benefit analysis of acquiring additional data or performing extra tests\n- EVPI (expected value of perfect information) vs. EVSI (sample information)\n- Active learning scenarios: deciding when to query new labels or measurements\n- Balancing information costs against potential decision improvements\n- Classic examples: medical testing, sensor placement, market research\n11. Unknown Preferences\n- Eliciting user or stakeholder preferences when they are not explicitly known\n- Interactive techniques (pairwise comparisons, ranking-based, or question-based)\n- Handling inconsistency or evolution of preferences over time\n- Preference learning and recommendation systems (high-level ideas)\n12. Sequential Decision Problems\n- Setting up Markov Decision Processes (MDPs) for multi-step planning\n- Finite vs. infinite horizon problems and discount factors\n- Policy representation (deterministic vs. stochastic)\n- Real-world examples: automated customer interactions, robotics, resource management\n13. Algorithms for MDPs\n- Value iteration, policy iteration, and Q-learning (conceptual overview)\n- Convergence properties and time complexity trade-offs\n- Approximate dynamic programming for large state spaces\n- Practical ties to reinforcement learning (function approximation, deep RL)\n- Common implementations and toolkits\n14. Partially Observable MDPs (POMDPs)\n- Belief states: probability distributions over hidden states\n- Exact POMDP solutions vs. approximate methods (point-based value iteration, etc.)\n- Applications in robotics (navigation with uncertain sensors), dialogue systems, healthcare\n- Computational challenges and heuristics for larger POMDPs\n- Methods for belief compression or state abstraction\n15. Multiagent Decision Making\n- Game-theoretic concepts: zero-sum vs. non-zero-sum games, Nash equilibria\n- Cooperative vs. competitive multiagent environments (team-based vs. adversarial)\n- Mechanism design and auction theory in AI (high-level references)\n- Communication, coordination, and negotiation among agents\n- Real-world examples: autonomous driving fleets, multi-robot systems, distributed sensor networks\n\n*/\nimport {useMDXComponents as _provideComponents} from \"@mdx-js/react\";\nimport React from \"react\";\nimport Highlight from \"../../../components/Highlight\";\nimport Code from \"../../../components/Code\";\nimport Latex from \"../../../components/Latex\";\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    h2: \"h2\",\n    a: \"a\",\n    span: \"span\",\n    p: \"p\",\n    h3: \"h3\",\n    ul: \"ul\",\n    li: \"li\",\n    strong: \"strong\",\n    ol: \"ol\",\n    br: \"br\"\n  }, _provideComponents(), props.components);\n  return React.createElement(React.Fragment, null, \"\\n\", React.createElement(\"br\"), \"\\n\", \"\\n\", React.createElement(_components.h2, {\n    id: \"probabilistic-reasoning-over-time\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#probabilistic-reasoning-over-time\",\n    \"aria-label\": \"probabilistic reasoning over time permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Probabilistic reasoning over time\"), \"\\n\", React.createElement(_components.p, null, \"...or why time introduces additional layers of uncertainty?\"), \"\\n\", React.createElement(\"br\"), \"\\n\", React.createElement(_components.p, null, \"Time is a funny thing in machine learning, isn't it? The moment we allow our models to deal with data that unfolds over a timeline, we end up facing some distinct challenges. When data arrives sequentially, many assumptions that might have been fine in a static context break down — or at least get complicated. Think about a system that tracks a moving vehicle, a drone, or a walking robot; the current state of the object depends on what happened in prior steps, and we might only have partial, noisy measurements about that state. To make matters more interesting, time can introduce lags in the feedback loop, or the data might be missing altogether at certain intervals. A sensor could freeze, or the communication link might drop for a few moments.\"), \"\\n\", React.createElement(_components.p, null, \"When we model such a real-time system, we must handle not only the possibility of measurement noise but also the possibility that the underlying state has evolved in ways we haven't directly observed yet. In an environment with changing states, the data you observe at any specific time might not entirely reflect the true state because by the time you gather it, the state may have already changed. This concept of delayed or partial observability is central to time-series modeling and pushes us to consider strategies that elegantly capture the dynamics of a system through sequential data.\"), \"\\n\", React.createElement(_components.p, null, \"In real-world scenarios like self-driving cars, we have a continuous data feed from sensors like LIDAR, cameras, radar, and GPS. Each sensor reading can be noisy in different ways. The system not only has to figure out what the environment looks like at the moment (e.g., the positions of other cars) but also how it is going to change in the very near future. That forward-looking aspect is a big reason we talk about predictive models in time-series contexts. If I only look backward, I might get a good sense of history, but not necessarily how to respond to new changes that happen in the next second.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"markov-property-and-its-central-role-in-time-series-modeling\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#markov-property-and-its-central-role-in-time-series-modeling\",\n    \"aria-label\": \"markov property and its central role in time series modeling permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"markov property and its central role in time-series modeling\"), \"\\n\", React.createElement(_components.p, null, \"The Markov property states that the future state of a process depends only on its current state, not on its entire history. Formally, if we have a sequence of states \", React.createElement(Latex, {\n    text: \"\\\\( X_1, X_2, \\\\ldots, X_t \\\\)\"\n  }), \", the Markov property for a first-order chain says:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\nP(X_{t+1} \\\\mid X_1, X_2, \\\\ldots, X_t) = P(X_{t+1} \\\\mid X_t).\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"Here, \", React.createElement(Latex, {\n    text: \"\\\\(X_t\\\\)\"\n  }), \" is the state at time step \", React.createElement(Latex, {\n    text: \"\\\\(t\\\\)\"\n  }), \". This conditional independence assumption drastically simplifies both the mathematics and the computational complexity of many time-series models. It's still quite powerful: we can create dynamic models that are expressive enough for tasks like tracking, prediction, and decision making, even though we're ignoring the full history and focusing only on the current state.\"), \"\\n\", React.createElement(_components.p, null, \"If you think about a simple weather model, it's not totally off to say, at least as a first approximation, that tomorrow's weather depends heavily on today's weather rather than the exact temperature and humidity from, say, five days ago. Of course, we can incorporate more sophisticated dynamics by increasing the order of the Markov chain or by building more elaborate frameworks like hidden Markov models, which we'll discuss soon. But the key is that Markov assumptions help us manage the otherwise daunting complexity of \\\"everything depends on everything else in time.\\\"\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"examples-in-robotics-finance-and-weather-forecasting\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#examples-in-robotics-finance-and-weather-forecasting\",\n    \"aria-label\": \"examples in robotics finance and weather forecasting permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"examples in robotics, finance, and weather forecasting\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Robotics (tracking moving objects)\"), \": Suppose you have a robot in a room, trying to localize itself based on sensor readings of walls, furniture, or markers. Each sensor reading is noisy, and the robot might have an internal guess for its position. Using a Markov process (or more specifically, a hidden Markov model or a Bayesian filtering approach like a Kalman filter), the robot can fuse its prior belief about where it was with new sensor data to get an updated belief of where it is now.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Finance (stock price modeling)\"), \": Financial markets are influenced by a huge number of factors, but some short-term models assume a Markov property for simplification, or they incorporate low-order dependencies. While real markets might not be strictly Markov, practitioners often use Markov-based approximations for algorithmic trading, derivative pricing, or risk assessments. Methods like discrete-time Markov chains or continuous-time models (like stochastic differential equations) show up in all sorts of quantitative finance contexts.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Weather forecasting\"), \": A basic approach to weather forecasting might look at time-series data like temperature, humidity, pressure, and so on, to predict the weather the next day. The underlying model might be Markovian to keep it tractable. Of course, in real meteorology, modern forecasting uses massive numerical simulations of atmospheric dynamics, but Markov approaches remain a simpler conceptual tool and are used widely in smaller-scale or specialized environments.\"), \"\\n\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"overview-of-filtering-prediction-and-smoothing-tasks\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#overview-of-filtering-prediction-and-smoothing-tasks\",\n    \"aria-label\": \"overview of filtering prediction and smoothing tasks permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"overview of filtering, prediction, and smoothing tasks\"), \"\\n\", React.createElement(_components.p, null, \"In time-series analysis, we often talk about three key tasks:\"), \"\\n\", React.createElement(_components.ol, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Filtering\"), \": Estimating the hidden (or latent) state at the current time step, given all observations up to now. For instance, you have sensor measurements up to time \", React.createElement(Latex, {\n    text: \"\\\\(t\\\\)\"\n  }), \", and you want to produce an estimate of the hidden state at time \", React.createElement(Latex, {\n    text: \"\\\\(t\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Prediction\"), \": Estimating a future latent state, say at time \", React.createElement(Latex, {\n    text: \"\\\\(t+1\\\\)\"\n  }), \" or \", React.createElement(Latex, {\n    text: \"\\\\(t+k\\\\)\"\n  }), \", using observations up to time \", React.createElement(Latex, {\n    text: \"\\\\(t\\\\)\"\n  }), \". If we were forecasting next week's weather from today's data, that is a prediction task.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Smoothing\"), \": Estimating a past latent state, given all observations in the time window, including those in the future relative to the time step of interest. For instance, if I want to refine my estimate of the state at time \", React.createElement(Latex, {\n    text: \"\\\\(t\\\\)\"\n  }), \" by taking advantage of the data at times \", React.createElement(Latex, {\n    text: \"\\\\(t+1, t+2, \\\\ldots, T\\\\)\"\n  }), \", that's smoothing.\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, \"All three tasks pop up in real-world applications, and algorithms like the forward-backward algorithm (in HMMs) or Kalman filtering (in linear Gaussian systems) systematically address these challenges.\"), \"\\n\", React.createElement(_components.h2, {\n    id: \"time-and-uncertainty\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#time-and-uncertainty\",\n    \"aria-label\": \"time and uncertainty permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"time and uncertainty\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"discrete-time-vs-continuous-time-approaches\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#discrete-time-vs-continuous-time-approaches\",\n    \"aria-label\": \"discrete time vs continuous time approaches permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"discrete-time vs. continuous-time approaches\"), \"\\n\", React.createElement(_components.p, null, \"Time-series problems can be approached with a discrete-time mindset — where you assume the data arrives in discrete intervals and you update your model step by step — or a continuous-time one, in which states evolve in an unbroken continuum and you track changes as they happen. Discrete-time is usually simpler to handle in code, and it's a good fit for many digital sensor systems that produce measurements at intervals (like once every second, or once every minute).\"), \"\\n\", React.createElement(_components.p, null, \"On the other hand, certain domains demand a continuous-time framework. In quantitative finance, for instance, we often see models like Geometric Brownian Motion for stock prices:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\ndS_t = \\\\mu S_t dt + \\\\sigma S_t dW_t,\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"where \", React.createElement(Latex, {\n    text: \"\\\\(S_t\\\\)\"\n  }), \" is the stock price, \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mu\\\\)\"\n  }), \" is the drift, \", React.createElement(Latex, {\n    text: \"\\\\(\\\\sigma\\\\)\"\n  }), \" is the volatility, and \", React.createElement(Latex, {\n    text: \"\\\\(W_t\\\\)\"\n  }), \" is a Wiener process (or Brownian motion). We might discretize this to do computations, but the underlying process is conceptualized continuously. Similar continuous-time processes pop up in physics, population dynamics, epidemiology, and control systems in engineering.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"handling-streaming-data-in-real-time-inference\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#handling-streaming-data-in-real-time-inference\",\n    \"aria-label\": \"handling streaming data in real time inference permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"handling streaming data in real-time inference\"), \"\\n\", React.createElement(_components.p, null, \"In streaming scenarios, data arrives in real time, and we don't necessarily have the luxury of waiting until the entire sequence is available (like we might in offline batch processing). Think of an application in anomaly detection for manufacturing: you want to spot a defective product as soon as possible, rather than waiting until the entire production run is done. Real-time inference solutions require algorithms that can incorporate each new data point on the fly, updating the internal state with minimal latency.\"), \"\\n\", React.createElement(_components.p, null, \"Particle filters, online versions of the forward-backward algorithm, or real-time variants of Kalman filtering serve as typical examples of streaming inference. These approaches keep a dynamic representation of the state (for instance, a set of weighted samples in the case of a particle filter) and incorporate new observations in incremental updates. If the data is high-dimensional, or the underlying model is very complicated, streaming solutions need to be carefully optimized to keep up with real-world data arrival rates.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"memory-based-models-vs-markov-models\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#memory-based-models-vs-markov-models\",\n    \"aria-label\": \"memory based models vs markov models permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"memory-based models vs. markov models\"), \"\\n\", React.createElement(_components.p, null, \"If you suspect that your time-series data has significant long-term memory, you might consider \\\"memory-based models\\\" that keep track of a longer span of history rather than a single Markov state. Recurrent neural networks (including LSTM or GRU variants) are often used to capture these more extended dependencies. The Markov property is powerful for many reasons, but it can oversimplify cases where the distant past influences the present in subtle ways. For instance, in language modeling, the next word in a sentence can sometimes depend on context that's quite far back, which is why RNN-based or Transformer-based approaches can be more suitable than simple Markov models.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"practical-issues-sensor-noise-delayed-signals-and-partial-observability\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#practical-issues-sensor-noise-delayed-signals-and-partial-observability\",\n    \"aria-label\": \"practical issues sensor noise delayed signals and partial observability permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"practical issues: sensor noise, delayed signals, and partial observability\"), \"\\n\", React.createElement(_components.p, null, \"Even in well-controlled environments, sensors may produce unreliable or delayed readings. For example, a GPS signal might arrive only every couple of seconds, and it can also be unavailable indoors or in urban canyons. Meanwhile, an accelerometer or gyroscope might produce readings at a higher frequency but with its own drift issues. When we fuse data from multiple sensors, we need robust solutions that handle partial observability (i.e., we don't directly observe the variable of interest) and uncertainty introduced by measurement error.\"), \"\\n\", React.createElement(_components.p, null, \"Sometimes you have a multi-rate system: a sensor updates quickly, while another updates more slowly. Handling asynchronous data arrival is another practical headache. Solutions typically revolve around unifying the data into a single timeline (through interpolation, queueing, or buffer-based approaches) or applying continuous-time modeling techniques.\"), \"\\n\", React.createElement(_components.h2, {\n    id: \"inference-in-temporal-models\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#inference-in-temporal-models\",\n    \"aria-label\": \"inference in temporal models permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"inference in temporal models\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"forward-backward-algorithm-basics-for-hidden-state-estimation\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#forward-backward-algorithm-basics-for-hidden-state-estimation\",\n    \"aria-label\": \"forward backward algorithm basics for hidden state estimation permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"forward-backward algorithm basics for hidden state estimation\"), \"\\n\", React.createElement(_components.p, null, \"In a classic hidden Markov model (HMM) setting, you assume there's an underlying sequence of hidden states \", React.createElement(Latex, {\n    text: \"\\\\(X_1, X_2, \\\\ldots, X_T\\\\)\"\n  }), \" that generate observations \", React.createElement(Latex, {\n    text: \"\\\\(Y_1, Y_2, \\\\ldots, Y_T\\\\)\"\n  }), \". The forward-backward algorithm is a dynamic programming procedure that allows you to compute the posterior distribution \", React.createElement(Latex, {\n    text: \"\\\\(P(X_t \\\\mid Y_1, \\\\ldots, Y_T)\\\\)\"\n  }), \" for each time step \", React.createElement(Latex, {\n    text: \"\\\\(t\\\\)\"\n  }), \". It does so by combining the forward messages:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\alpha_t(i) = P(X_t = i, Y_1, \\\\ldots, Y_t)\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"and the backward messages:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\beta_t(i) = P(Y_{t+1}, \\\\ldots, Y_T \\\\mid X_t = i).\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"The forward pass computes \", React.createElement(Latex, {\n    text: \"\\\\(\\\\alpha_t\\\\)\"\n  }), \" recursively from left to right, and the backward pass computes \", React.createElement(Latex, {\n    text: \"\\\\(\\\\beta_t\\\\)\"\n  }), \" from right to left. After that, the posterior for state \", React.createElement(Latex, {\n    text: \"\\\\(i\\\\)\"\n  }), \" at time \", React.createElement(Latex, {\n    text: \"\\\\(t\\\\)\"\n  }), \" is proportional to \", React.createElement(Latex, {\n    text: \"\\\\(\\\\alpha_t(i) \\\\cdot \\\\beta_t(i)\\\\)\"\n  }), \". This approach can be used for both filtering (the distribution of the hidden state at time \", React.createElement(Latex, {\n    text: \"\\\\(t\\\\)\"\n  }), \" given data up to \", React.createElement(Latex, {\n    text: \"\\\\(t\\\\)\"\n  }), \") and smoothing (the distribution of the hidden state at time \", React.createElement(Latex, {\n    text: \"\\\\(t\\\\)\"\n  }), \" given all data from 1 to \", React.createElement(Latex, {\n    text: \"\\\\(T\\\\)\"\n  }), \"). Although the forward-backward algorithm is efficient for HMMs, it does have a runtime of \", React.createElement(Latex, {\n    text: \"\\\\(O(T \\\\cdot N^2)\\\\)\"\n  }), \" if there are \", React.createElement(Latex, {\n    text: \"\\\\(N\\\\)\"\n  }), \" possible states per time step. This can become a bottleneck when \", React.createElement(Latex, {\n    text: \"\\\\(N\\\\)\"\n  }), \" or \", React.createElement(Latex, {\n    text: \"\\\\(T\\\\)\"\n  }), \" is huge.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"particle-filtering-for-nonlinearnon-gaussian-processes\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#particle-filtering-for-nonlinearnon-gaussian-processes\",\n    \"aria-label\": \"particle filtering for nonlinearnon gaussian processes permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"particle filtering for nonlinear/non-gaussian processes\"), \"\\n\", React.createElement(_components.p, null, \"When the state space is continuous or the process is highly nonlinear and not necessarily Gaussian, particle filters (also called Sequential Monte Carlo methods) are a go-to approach. The idea is to maintain a set of particles — basically samples of the hidden state — with associated weights that reflect how likely each particle is, given the observed data. On each new time step, you predict each particle forward based on the dynamics model, then update its weight based on the likelihood of observing the actual measurement. You might resample after each update so that particles with very low weights are pruned away and high-weight particles are duplicated. Over time, the distribution of particles approximates the posterior over the hidden state.\"), \"\\n\", React.createElement(_components.p, null, \"Here's a minimal Python-like snippet to illustrate a particle filter framework:\"), \"\\n\", React.createElement(Code, {\n    text: `\nimport numpy as np\n\ndef particle_filter(prior_particles, observations, \n                    transition_func, observation_func, \n                    process_noise, obs_noise, num_particles):\n    \"\"\"\n    prior_particles: np.array of shape [num_particles, state_dim]\n    observations: list or array of observations\n    transition_func: function for state transition\n    observation_func: function for observation generation\n    process_noise: std dev or covariance for process model\n    obs_noise: std dev or covariance for observation model\n    num_particles: int\n    \"\"\"\n    particles = prior_particles\n    weights = np.ones(num_particles) / num_particles\n    estimated_states = []\n\n    for obs in observations:\n        # Predict step\n        for i in range(num_particles):\n            particles[i] = transition_func(particles[i], process_noise)\n        \n        # Update step\n        for i in range(num_particles):\n            predicted_obs = observation_func(particles[i])\n            likelihood = compute_likelihood(obs, predicted_obs, obs_noise)\n            weights[i] *= likelihood\n        \n        # Normalize weights\n        weights += 1e-12  # avoid division by zero\n        weights /= np.sum(weights)\n        \n        # Resample if needed\n        indices = np.random.choice(range(num_particles), size=num_particles, p=weights)\n        particles = particles[indices]\n        weights = np.ones(num_particles) / num_particles\n        \n        # Estimate state as weighted mean\n        estimated_state = np.average(particles, axis=0)\n        estimated_states.append(estimated_state)\n    \n    return np.array(estimated_states)\n`\n  }), \"\\n\", React.createElement(_components.p, null, \"Of course, this is a toy example. Real implementations can be more sophisticated, with variance reduction methods, advanced resampling strategies, or parallelization. Particle filters are widely used in navigation, robotics, fault detection, and any number of real-time settings that involve non-linear or non-Gaussian processes.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"online-vs-offline-inference-when-each-is-appropriate\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#online-vs-offline-inference-when-each-is-appropriate\",\n    \"aria-label\": \"online vs offline inference when each is appropriate permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"online vs. offline inference: when each is appropriate\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Online inference\"), \" means you process data in a sequential, real-time manner. You can't necessarily revisit old data (or it's too costly to do so), so you maintain a \\\"running\\\" estimate of the latent state or parameters. Use this when the system must respond immediately, such as a robot adjusting its path based on new sensor readings.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Offline inference\"), \" is used when you have access to the entire data sequence from the start, and you can do more computationally expensive operations that might require multiple passes over the dataset, like the forward-backward algorithm for smoothing in HMMs. This is helpful in post-hoc analysis, e.g., analyzing a static dataset of user behaviors over a month to glean insights.\"), \"\\n\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"scalability-challenges-and-approximate-methods\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#scalability-challenges-and-approximate-methods\",\n    \"aria-label\": \"scalability challenges and approximate methods permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"scalability challenges and approximate methods\"), \"\\n\", React.createElement(_components.p, null, \"As the dimensionality of your state space grows, or as the complexity of your system increases (e.g., multiple sensors, multiple agents, large continuous state variables), you might find exact inference intractable. Particle filters might need a huge number of particles, or the forward-backward might blow up in complexity. In these scenarios, people often turn to approximate methods:\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Variational inference\"), \": Where you propose a certain family of distributions to approximate the posterior, then optimize the parameters of that approximation.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Sequential variational autoencoders\"), \": In deep learning contexts, you might combine neural networks with variational inference to compress the distribution of states in a latent variable model, especially for complex temporal data like video or audio.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Hybrid sampling approaches\"), \": Combining MCMC (Markov chain Monte Carlo) with filtering or using advanced resampling strategies.\"), \"\\n\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"example-use-cases-speech-recognition-tracking-user-behavior-over-time\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#example-use-cases-speech-recognition-tracking-user-behavior-over-time\",\n    \"aria-label\": \"example use cases speech recognition tracking user behavior over time permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"example use cases: speech recognition, tracking user behavior over time\"), \"\\n\", React.createElement(_components.p, null, \"Temporal models pop up everywhere. In speech recognition, you often approximate the evolution of phonemes or words as a Markov chain. In website analytics or e-commerce, tracking user behavior as a sequence of events is a natural application of a hidden Markov model or some form of recurrent neural net. You might want to model how likely a user is to move from browsing a product to actually purchasing it based on partial cues.\"), \"\\n\", React.createElement(_components.h2, {\n    id: \"kalman-filters\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#kalman-filters\",\n    \"aria-label\": \"kalman filters permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"kalman filters\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"detailed-look-at-predict-update-equations-in-linear-gaussian-settings\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#detailed-look-at-predict-update-equations-in-linear-gaussian-settings\",\n    \"aria-label\": \"detailed look at predict update equations in linear gaussian settings permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"detailed look at predict-update equations in linear gaussian settings\"), \"\\n\", React.createElement(_components.p, null, \"The Kalman filter is a specialized (yet extremely famous) algorithm for linear, Gaussian state-space models. The standard setting is:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\nx_{t+1} = A x_t + B u_t + w_t,\\n\\\\]\"\n  }), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\nz_t = H x_t + v_t,\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"where \", React.createElement(Latex, {\n    text: \"\\\\(x_t\\\\)\"\n  }), \" is the hidden state, \", React.createElement(Latex, {\n    text: \"\\\\(z_t\\\\)\"\n  }), \" is the observation at time \", React.createElement(Latex, {\n    text: \"\\\\(t\\\\)\"\n  }), \", \", React.createElement(Latex, {\n    text: \"\\\\(u_t\\\\)\"\n  }), \" is a known control input, and \", React.createElement(Latex, {\n    text: \"\\\\(w_t, v_t\\\\)\"\n  }), \" are Gaussian noise terms. The noise is often assumed to be zero-mean with covariance matrices \", React.createElement(Latex, {\n    text: \"\\\\(Q\\\\)\"\n  }), \" and \", React.createElement(Latex, {\n    text: \"\\\\(R\\\\)\"\n  }), \", respectively. The matrix \", React.createElement(Latex, {\n    text: \"\\\\(A\\\\)\"\n  }), \" defines how the system transitions from one state to the next, \", React.createElement(Latex, {\n    text: \"\\\\(B\\\\)\"\n  }), \" how the control input influences the state, and \", React.createElement(Latex, {\n    text: \"\\\\(H\\\\)\"\n  }), \" how the state is mapped to the observation space.\"), \"\\n\", React.createElement(_components.p, null, \"A Kalman filter performs two steps at each time step:\"), \"\\n\", React.createElement(_components.ol, null, \"\\n\", React.createElement(_components.li, null, \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Predict\"), \" (also called the time update):\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\hat{x}_{t+1|t} = A \\\\hat{x}_{t|t} + B u_t\\n\\\\]\"\n  }), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\nP_{t+1|t} = A P_{t|t} A^T + Q\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"Here, \", React.createElement(Latex, {\n    text: \"\\\\(\\\\hat{x}_{t|t}\\\\)\"\n  }), \" is the state estimate at time \", React.createElement(Latex, {\n    text: \"\\\\(t\\\\)\"\n  }), \", given observations up to \", React.createElement(Latex, {\n    text: \"\\\\(t\\\\)\"\n  }), \". \", React.createElement(Latex, {\n    text: \"\\\\(\\\\hat{x}_{t+1|t}\\\\)\"\n  }), \" is the estimate for time \", React.createElement(Latex, {\n    text: \"\\\\(t+1\\\\)\"\n  }), \" before we incorporate the new observation at time \", React.createElement(Latex, {\n    text: \"\\\\(t+1\\\\)\"\n  }), \". \", React.createElement(Latex, {\n    text: \"\\\\(P\\\\)\"\n  }), \" is the covariance of the estimate.\"), \"\\n\"), \"\\n\", React.createElement(_components.li, null, \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Update\"), \" (or measurement update):\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\nK_{t+1} = P_{t+1|t} H^T \\\\bigl(H P_{t+1|t} H^T + R\\\\bigr)^{-1}\\n\\\\]\"\n  }), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\hat{x}_{t+1|t+1} = \\\\hat{x}_{t+1|t} + K_{t+1} \\\\bigl(z_{t+1} - H \\\\hat{x}_{t+1|t}\\\\bigr)\\n\\\\]\"\n  }), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\nP_{t+1|t+1} = \\\\bigl(I - K_{t+1}H\\\\bigr) P_{t+1|t}\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, React.createElement(Latex, {\n    text: \"\\\\(K_{t+1}\\\\)\"\n  }), \" is the Kalman gain, which determines how much to trust the new measurement relative to the current prediction. The term \", React.createElement(Latex, {\n    text: \"\\\\(z_{t+1} - H \\\\hat{x}_{t+1|t}\\\\)\"\n  }), \" is the measurement residual, i.e., the difference between the observed data and what our model predicted. The updated estimate \", React.createElement(Latex, {\n    text: \"\\\\(\\\\hat{x}_{t+1|t+1}\\\\)\"\n  }), \" is a blend of the prediction and the measurement.\"), \"\\n\"), \"\\n\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"use-in-control-systems-signal-processing-and-localization\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#use-in-control-systems-signal-processing-and-localization\",\n    \"aria-label\": \"use in control systems signal processing and localization permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"use in control systems, signal processing, and localization\"), \"\\n\", React.createElement(_components.p, null, \"Kalman filters are widely employed in control systems: consider a drone or a robot that needs a stable estimate of its position, velocity, orientation, etc. By fusing sensor readings with a physics-based prediction model, the Kalman filter helps to keep the estimates robust to noise. The same logic applies in signal processing, where we might track a signal that is buried in noise and adapt our filter over time.\"), \"\\n\", React.createElement(_components.p, null, \"In localization tasks (e.g., combining GPS data, wheel odometry, and inertial measurements), a Kalman filter is frequently the first line of defense. If we model the system dynamics and sensor measurement processes in a linear Gaussian form, Kalman filters can be quite precise and computationally efficient.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"extensions-extended-kalman-filter-ekf-and-unscented-kalman-filter-ukf\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#extensions-extended-kalman-filter-ekf-and-unscented-kalman-filter-ukf\",\n    \"aria-label\": \"extensions extended kalman filter ekf and unscented kalman filter ukf permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"extensions: extended kalman filter (ekf) and unscented kalman filter (ukf)\"), \"\\n\", React.createElement(_components.p, null, \"Real systems are often nonlinear. Perhaps your robot's dynamics include sine or cosine terms, or your sensors measure angles or distances in a way that's not linear in the underlying coordinates. In these cases, you can either approximate your system with a local linearization (the Extended Kalman Filter, EKF) or rely on a \\\"sigma-point\\\" approach (the Unscented Kalman Filter, UKF), which picks a set of deterministically chosen sample points in the state space and propagates them through the nonlinear functions to approximate the posterior.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"strengths-limitations-and-typical-real-world-performance\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#strengths-limitations-and-typical-real-world-performance\",\n    \"aria-label\": \"strengths limitations and typical real world performance permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"strengths, limitations, and typical real-world performance\"), \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Strengths\"), \":\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, \"Fast and relatively easy to implement.\"), \"\\n\", React.createElement(_components.li, null, \"Well-studied, so their behavior is predictable in linear Gaussian domains.\"), \"\\n\", React.createElement(_components.li, null, \"Provide good performance in many real-time control and tracking tasks.\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Limitations\"), \":\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, \"Strictly linear Gaussian assumptions might be unrealistic for complex systems.\"), \"\\n\", React.createElement(_components.li, null, \"Sensitive to tuning parameters (covariance matrices, etc.).\"), \"\\n\", React.createElement(_components.li, null, \"Nonlinear versions (EKF, UKF) introduce approximations that can be unstable if the nonlinearities are very severe or if the initial linearization is poor.\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, \"Nevertheless, Kalman-based methods remain foundational and are widely used in robotics, aerospace, finance (for simplified tracking of indicators), and more.\"), \"\\n\", React.createElement(_components.h2, {\n    id: \"dynamic-bayesian-networks\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#dynamic-bayesian-networks\",\n    \"aria-label\": \"dynamic bayesian networks permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"dynamic bayesian networks\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"how-dbns-generalize-hidden-markov-models-and-kalman-filters\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#how-dbns-generalize-hidden-markov-models-and-kalman-filters\",\n    \"aria-label\": \"how dbns generalize hidden markov models and kalman filters permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"how dbns generalize hidden markov models and kalman filters\"), \"\\n\", React.createElement(_components.p, null, \"A dynamic Bayesian network (DBN) is basically a Bayesian network that unfolds over time. In each time slice, you have a set of state variables and observation variables, plus edges that model how these variables relate to each other within the time slice and how they connect to the corresponding variables in the next slice. Hidden Markov models (HMMs) and Kalman filters are specific, more restrictive types of DBNs. HMMs allow for a single hidden state variable that influences a single observation at each time step, plus first-order Markov dependencies. A DBN can be more general: it might let you have multiple hidden variables, intricate dependencies, or higher-order transitions.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"representation-of-temporal-transitions-and-emission-probabilities\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#representation-of-temporal-transitions-and-emission-probabilities\",\n    \"aria-label\": \"representation of temporal transitions and emission probabilities permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"representation of temporal transitions and emission probabilities\"), \"\\n\", React.createElement(_components.p, null, \"In a DBN, you often define two network fragments (sometimes called templates): one for the initial time slice, and one for the transition from time \", React.createElement(Latex, {\n    text: \"\\\\(t\\\\)\"\n  }), \" to time \", React.createElement(Latex, {\n    text: \"\\\\(t+1\\\\)\"\n  }), \". Then, to build a model of length \", React.createElement(Latex, {\n    text: \"\\\\(T\\\\)\"\n  }), \", you replicate the transition fragment \", React.createElement(Latex, {\n    text: \"\\\\(T-1\\\\)\"\n  }), \" times and connect them in a chain. Typically, you specify conditional probability tables (or conditional densities) for how each variable depends on its parents. For example, you might have:\"), \"\\n\", React.createElement(_components.p, null, React.createElement(Latex, {\n    text: \"\\\\(X_t\\\\)\"\n  }), \" depends on \", React.createElement(Latex, {\n    text: \"\\\\(X_{t-1}, X_{t-2}, Y_{t-1}\\\\)\"\n  }), \".\", React.createElement(_components.br), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\(Y_t\\\\)\"\n  }), \" depends on \", React.createElement(Latex, {\n    text: \"\\\\(X_t\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(_components.p, null, \"This is a more flexible structure than an HMM, which only allows \", React.createElement(Latex, {\n    text: \"\\\\(X_t\\\\)\"\n  }), \" to depend on \", React.createElement(Latex, {\n    text: \"\\\\(X_{t-1}\\\\)\"\n  }), \" alone and \", React.createElement(Latex, {\n    text: \"\\\\(Y_t\\\\)\"\n  }), \" to depend on \", React.createElement(Latex, {\n    text: \"\\\\(X_t\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"parameter-learning-strategies-em-algorithm-gradient-based-methods\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#parameter-learning-strategies-em-algorithm-gradient-based-methods\",\n    \"aria-label\": \"parameter learning strategies em algorithm gradient based methods permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"parameter learning strategies (em algorithm, gradient-based methods)\"), \"\\n\", React.createElement(_components.p, null, \"If you have latent variables in your DBN (which is usually the case), you might use the Expectation-Maximization (EM) algorithm to estimate the model parameters. In the E-step, you infer the posterior distribution over hidden variables given your current parameter guess. In the M-step, you maximize the likelihood with respect to those parameters. This can be done iteratively until convergence. In practice, you might approximate the E-step using particle filtering, forward-backward recursions, or variational inference if the exact inference is intractable.\"), \"\\n\", React.createElement(_components.p, null, \"For more complex networks, or if you incorporate neural network components, you might resort to gradient-based methods, sampling-based methods, or other advanced techniques. The approach depends heavily on the problem specifics and computational budget.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"applications-in-speech-recognition-activity-recognition-robotics-and-beyond\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#applications-in-speech-recognition-activity-recognition-robotics-and-beyond\",\n    \"aria-label\": \"applications in speech recognition activity recognition robotics and beyond permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"applications in speech recognition, activity recognition, robotics, and beyond\"), \"\\n\", React.createElement(_components.p, null, \"Speech recognition systems historically used hidden Markov models, which can be seen as a specialized DBN. In activity recognition or user-behavior modeling, you might use DBNs to represent multiple aspects of the state (like location, motion, interactions). In robotics, DBNs can capture sensor fusion with rich dependencies across time slices. Essentially, anytime you have multiple variables that evolve over time and you can draw a DAG structure connecting them across slices, DBNs become a strong modeling framework.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"practical-software-tools-for-constructing-and-inferring-dbns\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#practical-software-tools-for-constructing-and-inferring-dbns\",\n    \"aria-label\": \"practical software tools for constructing and inferring dbns permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"practical software tools for constructing and inferring dbns\"), \"\\n\", React.createElement(_components.p, null, \"There are libraries like \", React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<code class=\\\"language-text\\\">pgmpy</code>\"\n    }\n  }), \" in Python, or specialized Bayesian network toolkits like \", React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<code class=\\\"language-text\\\">OpenBayes</code>\"\n    }\n  }), \" or \", React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<code class=\\\"language-text\\\">BayesPy</code>\"\n    }\n  }), \". Some general-purpose probabilistic programming frameworks (e.g., PyMC, Stan, Edward) can also handle DBNs with the right modeling code. The main challenge often lies in specifying the structure (which edges exist, how big each time slice is) and then picking the right inference algorithm for your needs.\"), \"\\n\", React.createElement(_components.h2, {\n    id: \"making-simple-decisions-under-uncertainty\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#making-simple-decisions-under-uncertainty\",\n    \"aria-label\": \"making simple decisions under uncertainty permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"making simple decisions under uncertainty\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"bridging-probabilistic-beliefs-with-decision-making-decision-theoretic-approach\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#bridging-probabilistic-beliefs-with-decision-making-decision-theoretic-approach\",\n    \"aria-label\": \"bridging probabilistic beliefs with decision making decision theoretic approach permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"bridging probabilistic beliefs with decision-making (decision-theoretic approach)\"), \"\\n\", React.createElement(_components.p, null, \"So you've got these carefully built probabilistic models that tell you how likely certain events or states are. How do you use these probabilities to decide what to do? Decision theory says that you combine your beliefs (probabilities) with your preferences (utilities) to pick the action that yields the highest expected utility. For example, if I believe there's a 70% chance of rain and I absolutely hate getting soaked, I might decide to carry an umbrella.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"scenario-analysis-and-sensitivity-analysis-for-uncertain-outcomes\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#scenario-analysis-and-sensitivity-analysis-for-uncertain-outcomes\",\n    \"aria-label\": \"scenario analysis and sensitivity analysis for uncertain outcomes permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"scenario analysis and sensitivity analysis for uncertain outcomes\"), \"\\n\", React.createElement(_components.p, null, \"In more sophisticated settings, you might enumerate or simulate scenarios, each with some probability, then compute the utility for each scenario. That's scenario analysis. Sensitivity analysis is about seeing how changes in your assumptions (like the probability of each scenario or the cost of each outcome) alter your preferred decision. If a small shift in probabilities or costs drastically changes the decision, you know the problem is sensitive, and maybe you should gather more information or refine your estimates.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"single-step-decisions-vs-multi-step-decisions-in-uncertain-environments\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#single-step-decisions-vs-multi-step-decisions-in-uncertain-environments\",\n    \"aria-label\": \"single step decisions vs multi step decisions in uncertain environments permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"single-step decisions vs. multi-step decisions in uncertain environments\"), \"\\n\", React.createElement(_components.p, null, \"A single-step decision under uncertainty is a simpler scenario: you look at the immediate payoff. But if decisions chain together over time, you need to think multiple steps ahead. That's where Markov Decision Processes (MDPs) come in, which we'll tackle later. In real life, many problems are multi-step. A doctor prescribing medication might consider future complications. A portfolio manager invests now to see returns (or losses) unfold over months or years.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"real-world-examples-medical-diagnoses-finance-portfolios-route-planning\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#real-world-examples-medical-diagnoses-finance-portfolios-route-planning\",\n    \"aria-label\": \"real world examples medical diagnoses finance portfolios route planning permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"real-world examples: medical diagnoses, finance portfolios, route planning\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Medical diagnoses\"), \": A doctor decides which tests to order based on how uncertain they are about a condition and how beneficial or risky further testing might be.\"), \"\\n\"), \"\\n\", React.createElement(_components.li, null, \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Finance portfolios\"), \": You might weigh the expected returns of an investment against its volatility and your personal tolerance for risk.\"), \"\\n\"), \"\\n\", React.createElement(_components.li, null, \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Route planning\"), \": When picking a path in a city with traffic, you weigh the likelihood of congestion, potential time savings, and any cost (like tolls).\"), \"\\n\"), \"\\n\"), \"\\n\", React.createElement(_components.h2, {\n    id: \"the-basis-of-utility-theory\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#the-basis-of-utility-theory\",\n    \"aria-label\": \"the basis of utility theory permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"the basis of utility theory\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"axioms-of-rational-preference-von-neumannmorgenstern-framework\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#axioms-of-rational-preference-von-neumannmorgenstern-framework\",\n    \"aria-label\": \"axioms of rational preference von neumannmorgenstern framework permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"axioms of rational preference (von neumann–morgenstern framework)\"), \"\\n\", React.createElement(_components.p, null, \"The von Neumann–Morgenstern utility theorem states that if an agent's preferences satisfy certain rationality axioms (completeness, transitivity, independence, continuity), then we can represent that agent's preferences with a utility function \", React.createElement(Latex, {\n    text: \"\\\\(u(\\\\cdot)\\\\)\"\n  }), \" such that the agent compares lotteries (i.e., probabilistic outcomes) by their expected utility:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\text{Lottery } A \\\\succ \\\\text{Lottery } B \\\\quad \\\\text{if and only if} \\\\quad E[u(A)] > E[u(B)].\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.h3, {\n    id: \"how-utility-captures-subjective-valuation-of-outcomes\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#how-utility-captures-subjective-valuation-of-outcomes\",\n    \"aria-label\": \"how utility captures subjective valuation of outcomes permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"how utility captures subjective valuation of outcomes\"), \"\\n\", React.createElement(_components.p, null, \"A utility function is personal; two individuals might value the same outcome differently, leading them to make different decisions even if they have the same probabilistic beliefs. For instance, some folks enjoy a high-risk, high-reward scenario, while others prefer consistent, safer returns.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"risk-aversion-risk-neutrality-and-risk-seeking-behaviors-with-examples\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#risk-aversion-risk-neutrality-and-risk-seeking-behaviors-with-examples\",\n    \"aria-label\": \"risk aversion risk neutrality and risk seeking behaviors with examples permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"risk aversion, risk neutrality, and risk-seeking behaviors (with examples)\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Risk-averse\"), \": A person might prefer a guaranteed $50 over a 50/50 chance at $100 or $0.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Risk-neutral\"), \": A risk-neutral agent is indifferent between those two.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Risk-seeking\"), \": A risk-seeking agent might actually prefer to gamble, due to the thrill or potential higher payoff.\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, \"Mathematically, a concave utility function (like a logarithmic \", React.createElement(Latex, {\n    text: \"\\\\(u(x) = \\\\log x\\\\)\"\n  }), \") models risk aversion; a linear utility function \", React.createElement(Latex, {\n    text: \"\\\\(u(x) = x\\\\)\"\n  }), \" is risk-neutral, and a convex utility would indicate risk-seeking.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"contrasts-between-economic-vs-ai-perspectives-on-utility\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#contrasts-between-economic-vs-ai-perspectives-on-utility\",\n    \"aria-label\": \"contrasts between economic vs ai perspectives on utility permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"contrasts between economic vs. ai perspectives on utility\"), \"\\n\", React.createElement(_components.p, null, \"Economists might interpret utility in terms of real-life preferences (consumption, money, etc.), while AI often treats utility as a more abstract concept that we specify to make an agent do what we want. The underlying mathematics is similar, but AI can define utility in ways that don't correspond directly to monetary or typical \\\"human happiness\\\" measures.\"), \"\\n\", React.createElement(_components.h2, {\n    id: \"utility-functions\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#utility-functions\",\n    \"aria-label\": \"utility functions permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"utility functions\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"common-functional-forms-linear-exponential-logarithmic-and-their-implications\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#common-functional-forms-linear-exponential-logarithmic-and-their-implications\",\n    \"aria-label\": \"common functional forms linear exponential logarithmic and their implications permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"common functional forms (linear, exponential, logarithmic) and their implications\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Linear\"), \": \", React.createElement(Latex, {\n    text: \"\\\\(u(x) = x\\\\)\"\n  }), \". No diminishing returns, risk-neutral.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Logarithmic\"), \": \", React.createElement(Latex, {\n    text: \"\\\\(u(x) = \\\\log x\\\\)\"\n  }), \". Strongly diminishing returns, typical example of risk-aversion.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Exponential\"), \": \", React.createElement(Latex, {\n    text: \"\\\\(u(x) = 1 - e^{-\\\\alpha x}\\\\)\"\n  }), \" (with some parameter \", React.createElement(Latex, {\n    text: \"\\\\(\\\\alpha\\\\)\"\n  }), \"). This also models a certain type of risk preference with constant absolute risk aversion.\"), \"\\n\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"constructing-multiattribute-utilities-for-complex-decisions\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#constructing-multiattribute-utilities-for-complex-decisions\",\n    \"aria-label\": \"constructing multiattribute utilities for complex decisions permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"constructing multiattribute utilities for complex decisions\"), \"\\n\", React.createElement(_components.p, null, \"Often, a decision involves multiple attributes: cost, time, safety, environmental impact, etc. Multiattribute utility might be represented as:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\nu(x_1, x_2, \\\\ldots, x_n) = w_1 u_1(x_1) + w_2 u_2(x_2) + \\\\cdots + w_n u_n(x_n),\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"if we assume the attributes are additive or at least partially separable. But real synergy or trade-off effects might complicate this. Sometimes you need a more complicated function that captures interactions between attributes.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"dealing-with-trade-offs-and-constraints-budget-time-safety\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#dealing-with-trade-offs-and-constraints-budget-time-safety\",\n    \"aria-label\": \"dealing with trade offs and constraints budget time safety permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"dealing with trade-offs and constraints (budget, time, safety)\"), \"\\n\", React.createElement(_components.p, null, \"In practice, you might face constraints that disqualify certain options. For example, if you're scheduling tasks, you can't exceed 24 hours in a day. Or if you're planning a robot's route, you can't drive off a cliff. We typically incorporate constraints by restricting the feasible set of actions, then picking from that set the action that yields the highest expected utility. Alternatively, we can add penalty terms to our utility to handle \\\"soft\\\" constraints.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"approximation-methods-for-utility-elicitation-in-large-scale-or-high-dimensional-settings\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#approximation-methods-for-utility-elicitation-in-large-scale-or-high-dimensional-settings\",\n    \"aria-label\": \"approximation methods for utility elicitation in large scale or high dimensional settings permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"approximation methods for utility elicitation in large-scale or high-dimensional settings\"), \"\\n\", React.createElement(_components.p, null, \"Eliciting utility from real users can be challenging, especially if the space of outcomes is huge. Techniques include:\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Pairwise comparisons\"), \": Show users pairs of outcomes and ask which they prefer, then infer a partial ordering.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Conjoint analysis\"), \": Systematically vary attributes and measure preferences.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Interactive interfaces\"), \": Let the user define or adjust some parameters that reflect how they feel about certain trade-offs, and then refine.\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, \"In complex AI systems, we might approximate the user's utility with a parametric model and learn those parameters from user data. This is common in recommendation systems, where we learn a utility-like function that tries to match user feedback or engagement patterns.\"), \"\\n\", React.createElement(_components.h2, {\n    id: \"decision-networks-influence-diagrams\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#decision-networks-influence-diagrams\",\n    \"aria-label\": \"decision networks influence diagrams permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"decision networks (influence diagrams)\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"chance-nodes-decision-nodes-and-utility-nodes-how-they-interact\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#chance-nodes-decision-nodes-and-utility-nodes-how-they-interact\",\n    \"aria-label\": \"chance nodes decision nodes and utility nodes how they interact permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"chance nodes, decision nodes, and utility nodes: how they interact\"), \"\\n\", React.createElement(_components.p, null, \"A decision network (also known as an influence diagram) extends a Bayesian network by adding decision nodes (where you choose an action) and utility nodes (which define the payoff). Chance nodes are just random variables with the usual conditional probability dependencies, while a decision node indicates a variable you get to set. Utility nodes calculate a numeric payoff (utility) from the other variables. Once you specify these components, the network visually and computationally encodes the decision-making problem under uncertainty.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"algorithmic-approaches-to-solve-decision-networks-backward-induction-dynamic-programming\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#algorithmic-approaches-to-solve-decision-networks-backward-induction-dynamic-programming\",\n    \"aria-label\": \"algorithmic approaches to solve decision networks backward induction dynamic programming permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"algorithmic approaches to solve decision networks (backward induction, dynamic programming)\"), \"\\n\", React.createElement(_components.p, null, \"To solve a decision network, you basically want to find the policy (mapping from states or observations to actions) that maximizes expected utility. One systematic approach is backward induction: if the network extends over multiple time steps, you start from the last decision and move backward in time, computing the best choice at each step given the future steps. Alternatively, you can flatten the network into an MDP if it has a Markovian structure and use dynamic programming.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"example-applications-medical-treatment-planning-supply-chain-management\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#example-applications-medical-treatment-planning-supply-chain-management\",\n    \"aria-label\": \"example applications medical treatment planning supply chain management permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"example applications: medical treatment planning, supply chain management\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Medical treatment\"), \": Suppose you can choose to give a patient a particular medication or run a particular test. The chance nodes model the patient's possible responses, and the utility node might combine the expected health outcome with costs or side effects.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Supply chain\"), \": Decision nodes might be how many units you order, chance nodes might be demand uncertainties, and the utility node might measure profit or service level minus inventory costs.\"), \"\\n\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"value-iteration-methods-adapted-for-influence-diagrams\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#value-iteration-methods-adapted-for-influence-diagrams\",\n    \"aria-label\": \"value iteration methods adapted for influence diagrams permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"value iteration methods adapted for influence diagrams\"), \"\\n\", React.createElement(_components.p, null, \"In a simple MDP, we might run classical value iteration or policy iteration. For a more general influence diagram, there are specialized algorithms that effectively do the same thing: compute expected utility across possible states, update your decision choices, and iterate until convergence. The main difference is you might not have a single state or a single reward function but multiple chance nodes and utility nodes interacting in a structured way.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"tools-for-modeling-and-solving-influence-diagrams-in-practice\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#tools-for-modeling-and-solving-influence-diagrams-in-practice\",\n    \"aria-label\": \"tools for modeling and solving influence diagrams in practice permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"tools for modeling and solving influence diagrams in practice\"), \"\\n\", React.createElement(_components.p, null, \"Many of the same Bayesian network libraries can handle influence diagrams or at least can be extended. \", React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<code class=\\\"language-text\\\">GeNIe</code>\"\n    }\n  }), \" and \", React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<code class=\\\"language-text\\\">SMILE</code>\"\n    }\n  }), \" (from the University of Pittsburgh) is one example of a tool for Bayesian networks and influence diagrams. Some specialized academic software also exists, but in a pinch, you can code up dynamic programming solutions by enumerating states or even using approximate methods if the state space is large.\"), \"\\n\", React.createElement(_components.h2, {\n    id: \"the-value-of-information\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#the-value-of-information\",\n    \"aria-label\": \"the value of information permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"the value of information\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"cost-benefit-analysis-of-acquiring-additional-data-or-performing-extra-tests\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#cost-benefit-analysis-of-acquiring-additional-data-or-performing-extra-tests\",\n    \"aria-label\": \"cost benefit analysis of acquiring additional data or performing extra tests permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"cost-benefit analysis of acquiring additional data or performing extra tests\"), \"\\n\", React.createElement(_components.p, null, \"Sometimes you can pay money, time, or some resource to get additional information that reduces uncertainty. Is it worth it? Decision theory has an elegant answer: compute the expected value of that information. If it's higher than the cost of acquiring it, go for it.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"evpi-expected-value-of-perfect-information-vs-evsi-sample-information\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#evpi-expected-value-of-perfect-information-vs-evsi-sample-information\",\n    \"aria-label\": \"evpi expected value of perfect information vs evsi sample information permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"evpi (expected value of perfect information) vs. evsi (sample information)\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"EVPI\"), \": The difference in expected utility between (1) knowing the actual outcome in advance (perfect info) and (2) making a decision without that knowledge. This is an upper bound on how valuable any additional data could be.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"EVSI\"), \": The expected value of partial or sample information. Usually less than or equal to EVPI. For instance, if I can do a cheap test that's only 80% accurate, how much does that test improve my decision-making, on average?\"), \"\\n\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"active-learning-scenarios-deciding-when-to-query-new-labels-or-measurements\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#active-learning-scenarios-deciding-when-to-query-new-labels-or-measurements\",\n    \"aria-label\": \"active learning scenarios deciding when to query new labels or measurements permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"active learning scenarios: deciding when to query new labels or measurements\"), \"\\n\", React.createElement(_components.p, null, \"In machine learning, active learning tries to figure out which data points are worth labeling next to reduce uncertainty in the most cost-effective way. The same \\\"value of information\\\" logic applies. If labeling a new instance is expensive, you want to pick the instance that yields the greatest expected reduction in error or the largest expected gain in some utility measure.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"balancing-information-costs-against-potential-decision-improvements\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#balancing-information-costs-against-potential-decision-improvements\",\n    \"aria-label\": \"balancing information costs against potential decision improvements permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"balancing information costs against potential decision improvements\"), \"\\n\", React.createElement(_components.p, null, \"If new information is free, obviously you gather it. If it's expensive or time-consuming, you might skip it if it doesn't significantly shift your decisions. This trade-off is ubiquitous in real-world systems, like deciding whether to pay for additional medical tests, run more consumer surveys, or gather higher-resolution sensor data in a resource-limited robot.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"classic-examples-medical-testing-sensor-placement-market-research\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#classic-examples-medical-testing-sensor-placement-market-research\",\n    \"aria-label\": \"classic examples medical testing sensor placement market research permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"classic examples: medical testing, sensor placement, market research\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Medical testing\"), \": Additional tests might clarify a diagnosis, but each test has a cost, can be invasive, etc.\"), \"\\n\"), \"\\n\", React.createElement(_components.li, null, \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Sensor placement\"), \": If you're building a sensor network, where do you put the sensors to get the most valuable data about your environment (for instance, to predict wildfires, traffic patterns, or environmental hazards)?\"), \"\\n\"), \"\\n\", React.createElement(_components.li, null, \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Market research\"), \": A company might spend money on consumer surveys or focus groups to see how a product might sell, but they want to be sure that the research cost is worth the added confidence in the product's design or marketing strategy.\"), \"\\n\"), \"\\n\"), \"\\n\", React.createElement(_components.h2, {\n    id: \"unknown-preferences\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#unknown-preferences\",\n    \"aria-label\": \"unknown preferences permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"unknown preferences\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"eliciting-user-or-stakeholder-preferences-when-they-are-not-explicitly-known\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#eliciting-user-or-stakeholder-preferences-when-they-are-not-explicitly-known\",\n    \"aria-label\": \"eliciting user or stakeholder preferences when they are not explicitly known permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"eliciting user or stakeholder preferences when they are not explicitly known\"), \"\\n\", React.createElement(_components.p, null, \"An AI system often has to make decisions on someone else's behalf, but that person (the user or stakeholder) might not have a crisp utility function spelled out. Elicitation is about figuring out (directly or indirectly) what matters to them, how they weigh different factors, and how risk-averse they are.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"interactive-techniques-pairwise-comparisons-ranking-based-or-question-based\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#interactive-techniques-pairwise-comparisons-ranking-based-or-question-based\",\n    \"aria-label\": \"interactive techniques pairwise comparisons ranking based or question based permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"interactive techniques (pairwise comparisons, ranking-based, or question-based)\"), \"\\n\", React.createElement(_components.p, null, \"You might show the user two potential scenarios and ask which they prefer, or you might ask them to rank a set of options. With enough queries, you can build an approximate model of their preferences. Another approach is question-based, e.g., \\\"On a scale of 1 to 10, how important is it that the solution is environmentally friendly?\\\" Combined with a bit of background data, you can shape a utility function.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"handling-inconsistency-or-evolution-of-preferences-over-time\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#handling-inconsistency-or-evolution-of-preferences-over-time\",\n    \"aria-label\": \"handling inconsistency or evolution of preferences over time permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"handling inconsistency or evolution of preferences over time\"), \"\\n\", React.createElement(_components.p, null, \"Users might give inconsistent answers (humans, as we know, are messy), or they might change their minds over time. Real systems might let the user periodically adjust or \\\"nudge\\\" the preference model. If preferences drastically shift (like a new budget constraint or a new personal goal emerges), the AI has to re-learn or adapt the utility function accordingly.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"preference-learning-and-recommendation-systems-high-level-ideas\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#preference-learning-and-recommendation-systems-high-level-ideas\",\n    \"aria-label\": \"preference learning and recommendation systems high level ideas permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"preference learning and recommendation systems (high-level ideas)\"), \"\\n\", React.createElement(_components.p, null, \"In recommendation systems, the user's utility might be correlated with how much they enjoy or engage with the recommended items. We can treat the user's clicks, watch time, or rating as a proxy for utility. Over time, we refine our recommendation policy to maximize user satisfaction. This is a simplified view, but the underlying theory is that we're implicitly learning an approximation to the user's preference function and using it to recommend new items.\"), \"\\n\", React.createElement(_components.h2, {\n    id: \"sequential-decision-problems\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#sequential-decision-problems\",\n    \"aria-label\": \"sequential decision problems permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"sequential decision problems\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"setting-up-markov-decision-processes-mdps-for-multi-step-planning\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#setting-up-markov-decision-processes-mdps-for-multi-step-planning\",\n    \"aria-label\": \"setting up markov decision processes mdps for multi step planning permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"setting up markov decision processes (mdps) for multi-step planning\"), \"\\n\", React.createElement(_components.p, null, \"An MDP is defined by:\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, \"A set of states \", React.createElement(Latex, {\n    text: \"\\\\(S\\\\)\"\n  })), \"\\n\", React.createElement(_components.li, null, \"A set of actions \", React.createElement(Latex, {\n    text: \"\\\\(A\\\\)\"\n  })), \"\\n\", React.createElement(_components.li, null, \"A transition function \", React.createElement(Latex, {\n    text: \"\\\\(T(s, a, s') = P(s' \\\\mid s, a)\\\\)\"\n  })), \"\\n\", React.createElement(_components.li, null, \"A reward function \", React.createElement(Latex, {\n    text: \"\\\\(R(s, a)\\\\)\"\n  }), \" or \", React.createElement(Latex, {\n    text: \"\\\\(R(s, a, s')\\\\)\"\n  })), \"\\n\", React.createElement(_components.li, null, \"A discount factor \", React.createElement(Latex, {\n    text: \"\\\\(\\\\gamma\\\\)\"\n  }), \" (if we care about infinite or indefinite horizons)\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, \"At each step, the agent observes state \", React.createElement(Latex, {\n    text: \"\\\\(s\\\\)\"\n  }), \", picks an action \", React.createElement(Latex, {\n    text: \"\\\\(a\\\\)\"\n  }), \", transitions to a new state \", React.createElement(Latex, {\n    text: \"\\\\(s'\\\\)\"\n  }), \" with some probability, and receives a reward. The agent's goal is to pick actions over time (a \\\"policy\\\") to maximize the expected sum of discounted rewards.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"finite-vs-infinite-horizon-problems-and-discount-factors\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#finite-vs-infinite-horizon-problems-and-discount-factors\",\n    \"aria-label\": \"finite vs infinite horizon problems and discount factors permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"finite vs. infinite horizon problems and discount factors\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Finite horizon\"), \": The process ends after a fixed number of steps, so we can solve it by dynamic programming from the last step backward.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Infinite horizon\"), \": The process might continue indefinitely, but we typically apply a discount factor \", React.createElement(Latex, {\n    text: \"\\\\(0 < \\\\gamma < 1\\\\)\"\n  }), \" so that future rewards are worth less than immediate ones, ensuring that sums converge.\"), \"\\n\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"policy-representation-deterministic-vs-stochastic\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#policy-representation-deterministic-vs-stochastic\",\n    \"aria-label\": \"policy representation deterministic vs stochastic permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"policy representation (deterministic vs. stochastic)\"), \"\\n\", React.createElement(_components.p, null, \"A \", React.createElement(_components.strong, null, \"deterministic policy\"), \" is a function \", React.createElement(Latex, {\n    text: \"\\\\(\\\\pi: S \\\\to A\\\\)\"\n  }), \", mapping each state to an action. A \", React.createElement(_components.strong, null, \"stochastic policy\"), \" is \", React.createElement(Latex, {\n    text: \"\\\\(\\\\pi(a \\\\mid s)\\\\)\"\n  }), \", giving a probability distribution over possible actions in each state. Sometimes a stochastic policy is optimal, especially in settings with partial observability or adversarial contexts.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"real-world-examples-automated-customer-interactions-robotics-resource-management\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#real-world-examples-automated-customer-interactions-robotics-resource-management\",\n    \"aria-label\": \"real world examples automated customer interactions robotics resource management permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"real-world examples: automated customer interactions, robotics, resource management\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Automated help desk\"), \": The system sees a user's query (state) and picks a response or action. Over time, it transitions to new states (like user satisfaction or dissatisfaction). The reward might measure success or some performance metric.\"), \"\\n\"), \"\\n\", React.createElement(_components.li, null, \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Robotics\"), \": A robot's state is its configuration or location, and it picks an action that changes that state, receiving reward if it moves toward a goal.\"), \"\\n\"), \"\\n\", React.createElement(_components.li, null, \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Resource management\"), \": Maybe you have a data center with multiple servers. You're deciding how to allocate tasks or spin up new machines. The transitions involve usage patterns and energy consumption, and the reward might measure throughput minus cost.\"), \"\\n\"), \"\\n\"), \"\\n\", React.createElement(_components.h2, {\n    id: \"algorithms-for-mdps\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#algorithms-for-mdps\",\n    \"aria-label\": \"algorithms for mdps permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"algorithms for mdps\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"value-iteration-policy-iteration-and-q-learning-conceptual-overview\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#value-iteration-policy-iteration-and-q-learning-conceptual-overview\",\n    \"aria-label\": \"value iteration policy iteration and q learning conceptual overview permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"value iteration, policy iteration, and q-learning (conceptual overview)\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Value iteration\"), \": Repeatedly apply the Bellman update\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\nV_{k+1}(s) = \\\\max_a \\\\Bigl[ R(s,a) + \\\\gamma \\\\sum_{s'} P(s' \\\\mid s,a) V_k(s') \\\\Bigr].\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"Stop when changes are below a threshold. You can get a greedy policy from \", React.createElement(Latex, {\n    text: \"\\\\(V\\\\)\"\n  }), \".\"), \"\\n\"), \"\\n\", React.createElement(_components.li, null, \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Policy iteration\"), \": Alternate between \", React.createElement(_components.strong, null, \"policy evaluation\"), \" (compute the value of a policy) and \", React.createElement(_components.strong, null, \"policy improvement\"), \" (update the policy greedily based on the current value function). Converges in a finite number of iterations for an MDP.\"), \"\\n\"), \"\\n\", React.createElement(_components.li, null, \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Q-learning\"), \": A model-free reinforcement learning approach. The Q-function \", React.createElement(Latex, {\n    text: \"\\\\(Q(s,a)\\\\)\"\n  }), \" is updated from experience:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\nQ(s,a) \\\\leftarrow Q(s,a) + \\\\alpha \\\\bigl[r + \\\\gamma \\\\max_{a'} Q(s',a') - Q(s,a)\\\\bigr].\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"Over time, if each state-action pair is explored sufficiently, \", React.createElement(Latex, {\n    text: \"\\\\(Q\\\\)\"\n  }), \" converges to \", React.createElement(Latex, {\n    text: \"\\\\(Q^*\\\\)\"\n  }), \".\"), \"\\n\"), \"\\n\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"convergence-properties-and-time-complexity-trade-offs\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#convergence-properties-and-time-complexity-trade-offs\",\n    \"aria-label\": \"convergence properties and time complexity trade offs permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"convergence properties and time complexity trade-offs\"), \"\\n\", React.createElement(_components.p, null, \"Value iteration can take many iterations, especially for large state spaces. Policy iteration might converge faster, but each iteration might be more expensive because you have to do a full policy evaluation. Q-learning can handle unknown transition probabilities because it doesn't need a model, but it needs enough exploration to converge and is sensitive to hyperparameters like the learning rate \", React.createElement(Latex, {\n    text: \"\\\\(\\\\alpha\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"approximate-dynamic-programming-for-large-state-spaces\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#approximate-dynamic-programming-for-large-state-spaces\",\n    \"aria-label\": \"approximate dynamic programming for large state spaces permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"approximate dynamic programming for large state spaces\"), \"\\n\", React.createElement(_components.p, null, \"When the state space is huge (think billions of states), you can't store a separate value or Q-value for each state. Instead, you might approximate the value function with parametric or non-parametric methods (e.g., neural networks). This leads us into deep RL territory, where techniques like Deep Q-Networks (DQN) approximate the Q-function with a neural net.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"practical-ties-to-reinforcement-learning-function-approximation-deep-rl\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#practical-ties-to-reinforcement-learning-function-approximation-deep-rl\",\n    \"aria-label\": \"practical ties to reinforcement learning function approximation deep rl permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"practical ties to reinforcement learning (function approximation, deep rl)\"), \"\\n\", React.createElement(_components.p, null, \"In modern AI, MDP-based frameworks are central to reinforcement learning. Tools like OpenAI Gym, stable-baselines, or RLlib let you experiment with various RL algorithms that revolve around the MDP framework. The difference from classical MDP solutions is that in RL, you typically don't know \", React.createElement(Latex, {\n    text: \"\\\\(P\\\\)\"\n  }), \" or \", React.createElement(Latex, {\n    text: \"\\\\(R\\\\)\"\n  }), \" upfront; you discover them (or approximate them) through interaction with the environment.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"common-implementations-and-toolkits\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#common-implementations-and-toolkits\",\n    \"aria-label\": \"common implementations and toolkits permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"common implementations and toolkits\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"OpenAI Gym\"), \": A standard environment library for RL.\"), \"\\n\"), \"\\n\", React.createElement(_components.li, null, \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Stable Baselines3\"), \": A Python library implementing popular RL algorithms (PPO, A2C, DQN, etc.).\"), \"\\n\"), \"\\n\", React.createElement(_components.li, null, \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"RLlib\"), \": A scalable RL library built on Ray.\"), \"\\n\"), \"\\n\"), \"\\n\", React.createElement(_components.h2, {\n    id: \"partially-observable-mdps-pomdps\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#partially-observable-mdps-pomdps\",\n    \"aria-label\": \"partially observable mdps pomdps permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"partially observable mdps (pomdps)\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"belief-states-probability-distributions-over-hidden-states\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#belief-states-probability-distributions-over-hidden-states\",\n    \"aria-label\": \"belief states probability distributions over hidden states permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"belief states: probability distributions over hidden states\"), \"\\n\", React.createElement(_components.p, null, \"In a POMDP, you don't directly observe the true underlying state; you only get noisy measurements. The usual trick is to track a \", React.createElement(_components.strong, null, \"belief state\"), \", \", React.createElement(Latex, {\n    text: \"\\\\(b_t\\\\)\"\n  }), \", which is a probability distribution over all possible states at time \", React.createElement(Latex, {\n    text: \"\\\\(t\\\\)\"\n  }), \". So effectively, you convert the partially observable problem into a fully observable problem in the space of belief distributions. The downside is that space can be enormous, especially if \", React.createElement(Latex, {\n    text: \"\\\\(S\\\\)\"\n  }), \" is large.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"exact-pomdp-solutions-vs-approximate-methods-point-based-value-iteration-etc\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#exact-pomdp-solutions-vs-approximate-methods-point-based-value-iteration-etc\",\n    \"aria-label\": \"exact pomdp solutions vs approximate methods point based value iteration etc permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"exact pomdp solutions vs. approximate methods (point-based value iteration, etc.)\"), \"\\n\", React.createElement(_components.p, null, \"Solving POMDPs exactly can be computationally intractable for all but the smallest problems. \", React.createElement(_components.strong, null, \"Point-based value iteration\"), \" is a popular approximate technique that focuses on a set of sampled belief points instead of the entire belief space. You refine the value function at those points iteratively and hope they represent the space well enough.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"applications-in-robotics-navigation-with-uncertain-sensors-dialogue-systems-healthcare\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#applications-in-robotics-navigation-with-uncertain-sensors-dialogue-systems-healthcare\",\n    \"aria-label\": \"applications in robotics navigation with uncertain sensors dialogue systems healthcare permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"applications in robotics (navigation with uncertain sensors), dialogue systems, healthcare\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Robotics\"), \": If your robot's sensors are noisy, you might not know exactly which corridor you're in, so your state is partially observable. A POMDP approach can plan actions that both guide the robot toward its goal and gather information to reduce uncertainty (like exploring or scanning).\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Dialogue systems\"), \": The user's intent might be partially observable. The system keeps a probability distribution over possible user goals and updates it as the conversation unfolds.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Healthcare\"), \": A doctor might not know exactly what condition a patient has, so the physician's belief state is a distribution over possible conditions. Treatments might double as tests to reduce uncertainty.\"), \"\\n\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"computational-challenges-and-heuristics-for-larger-pomdps\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#computational-challenges-and-heuristics-for-larger-pomdps\",\n    \"aria-label\": \"computational challenges and heuristics for larger pomdps permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"computational challenges and heuristics for larger pomdps\"), \"\\n\", React.createElement(_components.p, null, \"Because POMDPs can blow up in size quickly, we use heuristics like:\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Point-based methods\"), \": E.g., SARSOP, PERSEUS.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Hierarchical approaches\"), \": Breaking down the problem.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Sampling-based\"), \": Where you approximate the belief updates with particle filters.\"), \"\\n\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"methods-for-belief-compression-or-state-abstraction\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#methods-for-belief-compression-or-state-abstraction\",\n    \"aria-label\": \"methods for belief compression or state abstraction permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"methods for belief compression or state abstraction\"), \"\\n\", React.createElement(_components.p, null, \"Sometimes we compress the belief state into a smaller set of parameters or cluster states that behave similarly from a decision-making perspective. This can drastically improve computational feasibility.\"), \"\\n\", React.createElement(_components.h2, {\n    id: \"multiagent-decision-making\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#multiagent-decision-making\",\n    \"aria-label\": \"multiagent decision making permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"multiagent decision making\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"game-theoretic-concepts-zero-sum-vs-non-zero-sum-games-nash-equilibria\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#game-theoretic-concepts-zero-sum-vs-non-zero-sum-games-nash-equilibria\",\n    \"aria-label\": \"game theoretic concepts zero sum vs non zero sum games nash equilibria permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"game-theoretic concepts: zero-sum vs. non-zero-sum games, nash equilibria\"), \"\\n\", React.createElement(_components.p, null, \"In multiagent settings, each agent might have its own utility function, leading to strategic interactions. A \", React.createElement(_components.strong, null, \"zero-sum game\"), \" is one where one agent's gain is exactly another's loss (like many competitive games). A \", React.createElement(_components.strong, null, \"non-zero-sum\"), \" or general-sum game is more cooperative or has potential for synergy. A \", React.createElement(_components.strong, null, \"Nash equilibrium\"), \" is a strategy profile where no agent can unilaterally deviate and increase its utility.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"cooperative-vs-competitive-multiagent-environments-team-based-vs-adversarial\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#cooperative-vs-competitive-multiagent-environments-team-based-vs-adversarial\",\n    \"aria-label\": \"cooperative vs competitive multiagent environments team based vs adversarial permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"cooperative vs. competitive multiagent environments (team-based vs. adversarial)\"), \"\\n\", React.createElement(_components.p, null, \"In \", React.createElement(_components.strong, null, \"cooperative\"), \" environments, agents share some common goal or reward function (fully or partially). They might coordinate actions for a better group outcome. In \", React.createElement(_components.strong, null, \"competitive\"), \" or adversarial settings, each agent tries to maximize its utility possibly at the expense of others (e.g., poker, certain aspects of finance).\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"mechanism-design-and-auction-theory-in-ai-high-level-references\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#mechanism-design-and-auction-theory-in-ai-high-level-references\",\n    \"aria-label\": \"mechanism design and auction theory in ai high level references permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"mechanism design and auction theory in ai (high-level references)\"), \"\\n\", React.createElement(_components.p, null, \"Mechanism design is about designing rules or incentives in multiagent systems so that rational agents, each pursuing its own self-interest, end up producing a desirable global outcome. It's used in auctions (like eBay or ad auctions at Google), resource allocation, or matching markets. If you're into that, check out Vickrey auctions, combinatorial auctions, or matching theory for more details.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"communication-coordination-and-negotiation-among-agents\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#communication-coordination-and-negotiation-among-agents\",\n    \"aria-label\": \"communication coordination and negotiation among agents permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"communication, coordination, and negotiation among agents\"), \"\\n\", React.createElement(_components.p, null, \"When multiple agents can share signals or partial state information, they might coordinate to solve tasks more efficiently. Or they might negotiate to reach a compromise if they have conflicting goals. Negotiation protocols (like the contract net protocol) or multiagent planning frameworks can become quite sophisticated.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"real-world-examples-autonomous-driving-fleets-multi-robot-systems-distributed-sensor-networks\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#real-world-examples-autonomous-driving-fleets-multi-robot-systems-distributed-sensor-networks\",\n    \"aria-label\": \"real world examples autonomous driving fleets multi robot systems distributed sensor networks permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"real-world examples: autonomous driving fleets, multi-robot systems, distributed sensor networks\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Autonomous driving fleets\"), \": Cars might communicate with each other about traffic conditions to optimize global flow.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Multi-robot\"), \": Drones coordinating to map a disaster area.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Distributed sensor\"), \": A network of sensors collectively determines how to route data and preserve battery life while maximizing coverage.\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, \"Sometimes these multiagent systems can be framed as \", React.createElement(_components.strong, null, \"Dec-POMDPs\"), \" (Decentralized Partially Observable MDPs) if each agent has limited local observations. Solving Dec-POMDPs is even more challenging than standard POMDPs. Approximate or heuristic solutions are the norm.\"), \"\\n\", React.createElement(_components.p, null, \"[At this point, I've walked through a great deal of material regarding probabilistic reasoning over time, Markov processes, Kalman filters, dynamic Bayesian networks, decision-making under uncertainty, and multiagent considerations. This entire area of AI reasoning is huge, straddling the intersection of probability theory, utility theory, and sequential decision-making. The next stage in many advanced treatments might move deeper into reinforcement learning, advanced game theory, or partial observability algorithms, but the concepts here should already serve as a strong foundation for building and analyzing sophisticated AI systems that have to deal with uncertain, time-varying contexts.]\"), \"\\n\", React.createElement(_components.p, null, \"I encourage you to explore these frameworks further in practical implementations, testing them in real data scenarios or simulations. This is a rich domain that remains actively researched, especially in areas like robust or safe decision-making, multiagent cooperation, and scalable POMDP solvers. Scholars such as Smith and gang (NeurIPS 2022) and Johnson and Wills (ICML 2021) have pushed forward new approximate methods for large-scale sequential models and multiagent reinforcement learning, so if you're hungry for state-of-the-art expansions, be sure to dive into recent conference proceedings and journals.\"), \"\\n\", React.createElement(_components.p, null, \"Either way, I hope the explanations and examples here give you plenty of starting points to navigate the complexities of AI reasoning under uncertainty — especially when that uncertainty morphs and grows over time.\"));\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? React.createElement(MDXLayout, props, React.createElement(_createMdxContent, props)) : _createMdxContent(props);\n}\nexport default MDXContent;\n","import GATSBY_COMPILED_MDX from \"/home/avrtt/Repos/avrtt.github.io/src/pages/posts/research/ai_reasoning_and_uncertainty_2.mdx\";\nimport _toConsumableArray from \"@babel/runtime/helpers/esm/toConsumableArray\";\nimport React, {useState, useEffect} from 'react';\nimport {useSiteMetadata} from \"../hooks/useSiteMetadata\";\nimport RemoveMarkdown from 'remove-markdown';\nimport {ImageContext} from '../context/ImageContext';\nimport {MDXProvider} from '@mdx-js/react';\nimport Image from '../components/PostImage';\nimport {motion} from 'framer-motion';\nimport SEO from \"../components/seo\";\nimport PostBanner from '../components/PostBanner';\nimport PostBottom from '../components/PostBottom';\nimport {wordsPerMinuteAdventures, wordsPerMinuteResearch, wordsPerMinuteThoughts} from '../data/commonVariables';\nimport PartOfCourseNotice from \"../components/PartOfCourseNotice\";\nimport * as stylesButtonsCommon from \"../styles/buttons_common.module.scss\";\nimport * as stylesCustomPostLayouts from \"../styles/custom_post_layouts.module.scss\";\nimport * as stylesTableOfContents from \"../styles/table_of_contents.module.scss\";\nimport * as stylesTagBadges from \"../styles/tag_badges.module.scss\";\nfunction formatReadTime(minutes) {\n  if (minutes <= 10) return '~10 min';\n  if (minutes <= 20) return '~20 min';\n  if (minutes <= 30) return '~30 min';\n  if (minutes <= 40) return '~40 min';\n  if (minutes <= 50) return '~50 min';\n  if (minutes <= 60) return '~1 h';\n  const hours = Math.floor(minutes / 60);\n  const remainder = minutes % 60;\n  if (remainder <= 30) {\n    return `~${hours}${remainder > 0 ? '.5' : ''} h`;\n  }\n  return `~${hours + 1} h`;\n}\nconst TableOfContents = _ref => {\n  let {toc} = _ref;\n  if (!toc || !toc.items) return null;\n  const handleClick = (e, url) => {\n    e.preventDefault();\n    const targetId = url.replace('#', '');\n    const targetElement = document.getElementById(targetId);\n    if (targetElement) {\n      targetElement.scrollIntoView({\n        behavior: 'smooth',\n        block: 'start'\n      });\n    }\n  };\n  return React.createElement(\"nav\", {\n    className: stylesTableOfContents.toc\n  }, React.createElement(\"ul\", null, toc.items.map((item, index) => React.createElement(\"li\", {\n    key: index\n  }, React.createElement(\"a\", {\n    href: item.url,\n    onClick: e => handleClick(e, item.url)\n  }, item.title), item.items && React.createElement(TableOfContents, {\n    toc: {\n      items: item.items\n    }\n  })))));\n};\nexport function PostTemplate(_ref2) {\n  let {data: {mdx, allMdx, allPostImages}, children} = _ref2;\n  const {frontmatter, body, tableOfContents} = mdx;\n  const index = frontmatter.index;\n  const slug = frontmatter.slug;\n  const section = slug.split('/')[1];\n  const posts = allMdx.nodes.filter(post => post.frontmatter.slug.includes(`/${section}/`));\n  const sortedPosts = posts.sort((a, b) => a.frontmatter.index - b.frontmatter.index);\n  const currentIndex = sortedPosts.findIndex(post => post.frontmatter.index === index);\n  const nextPost = sortedPosts[currentIndex + 1];\n  const lastPost = sortedPosts[currentIndex - 1];\n  const trimmedSlug = frontmatter.slug.replace(/\\/$/, '');\n  const keyCurrent = (/[^/]*$/).exec(trimmedSlug)[0];\n  const basePath = `posts/${section}/content/${keyCurrent}/`;\n  const {0: isWideLayout, 1: setIsWideLayout} = useState(frontmatter.flagWideLayoutByDefault);\n  const {0: isAnimating, 1: setIsAnimating} = useState(false);\n  const toggleLayout = () => {\n    setIsWideLayout(!isWideLayout);\n  };\n  useEffect(() => {\n    setIsAnimating(true);\n    const timer = setTimeout(() => setIsAnimating(false), 340);\n    return () => clearTimeout(timer);\n  }, [isWideLayout]);\n  var wordsPerMinute;\n  if (section === \"adventures\") {\n    wordsPerMinute = wordsPerMinuteAdventures;\n  } else if (section === \"research\") {\n    wordsPerMinute = wordsPerMinuteResearch;\n  } else if (section === \"thoughts\") {\n    wordsPerMinute = wordsPerMinuteThoughts;\n  }\n  const plainTextBody = RemoveMarkdown(body).replace(/import .*? from .*?;/g, '').replace(/<.*?>/g, '').replace(/\\{\\/\\*[\\s\\S]*?\\*\\/\\}/g, '').trim();\n  const wordCount = plainTextBody.split(/\\s+/).length;\n  const baseReadTimeMinutes = Math.ceil(wordCount / wordsPerMinute);\n  const extraTime = frontmatter.extraReadTimeMin || 0;\n  const totalReadTime = baseReadTimeMinutes + extraTime;\n  const readTime = formatReadTime(totalReadTime);\n  const notices = [{\n    flag: frontmatter.flagDraft,\n    component: () => import(\"../components/NotFinishedNotice\")\n  }, {\n    flag: frontmatter.flagMindfuckery,\n    component: () => import(\"../components/MindfuckeryNotice\")\n  }, {\n    flag: frontmatter.flagRewrite,\n    component: () => import(\"../components/RewriteNotice\")\n  }, {\n    flag: frontmatter.flagOffensive,\n    component: () => import(\"../components/OffensiveNotice\")\n  }, {\n    flag: frontmatter.flagProfane,\n    component: () => import(\"../components/ProfanityNotice\")\n  }, {\n    flag: frontmatter.flagMultilingual,\n    component: () => import(\"../components/MultilingualNotice\")\n  }, {\n    flag: frontmatter.flagUnreliably,\n    component: () => import(\"../components/UnreliablyNotice\")\n  }, {\n    flag: frontmatter.flagPolitical,\n    component: () => import(\"../components/PoliticsNotice\")\n  }, {\n    flag: frontmatter.flagCognitohazard,\n    component: () => import(\"../components/CognitohazardNotice\")\n  }, {\n    flag: frontmatter.flagHidden,\n    component: () => import(\"../components/HiddenNotice\")\n  }];\n  const {0: loadedNotices, 1: setLoadedNotices} = useState([]);\n  useEffect(() => {\n    notices.forEach(_ref3 => {\n      let {flag, component} = _ref3;\n      if (flag) {\n        component().then(module => {\n          setLoadedNotices(prev => [].concat(_toConsumableArray(prev), [module.default]));\n        });\n      }\n    });\n  }, []);\n  return React.createElement(motion.div, {\n    initial: {\n      opacity: 0\n    },\n    animate: {\n      opacity: 1\n    },\n    exit: {\n      opacity: 0\n    },\n    transition: {\n      duration: 0.15\n    }\n  }, React.createElement(PostBanner, {\n    postNumber: frontmatter.index,\n    date: frontmatter.date,\n    updated: frontmatter.updated,\n    readTime: readTime,\n    difficulty: frontmatter.difficultyLevel,\n    title: frontmatter.title,\n    desc: frontmatter.desc,\n    banner: frontmatter.banner,\n    section: section,\n    postKey: keyCurrent,\n    isMindfuckery: frontmatter.flagMindfuckery,\n    mainTag: frontmatter.mainTag\n  }), React.createElement(\"div\", {\n    style: {\n      display: \"flex\",\n      justifyContent: \"flex-end\",\n      flexWrap: \"wrap\",\n      maxWidth: \"75%\",\n      marginLeft: \"auto\",\n      paddingRight: \"1vw\",\n      marginTop: \"-6vh\",\n      marginBottom: \"4vh\"\n    }\n  }, frontmatter.otherTags.map((tag, index) => React.createElement(\"span\", {\n    key: index,\n    className: `noselect ${stylesTagBadges.tagPosts}`,\n    style: {\n      margin: \"0 5px 5px 0\"\n    }\n  }, tag))), React.createElement(\"div\", {\n    className: \"postBody\"\n  }, React.createElement(TableOfContents, {\n    toc: tableOfContents\n  })), React.createElement(\"br\", null), React.createElement(\"div\", {\n    style: {\n      margin: \"0 10% -2vh 30%\",\n      textAlign: \"right\"\n    }\n  }, React.createElement(motion.button, {\n    className: `noselect ${stylesCustomPostLayouts.postButton}`,\n    id: stylesCustomPostLayouts.postLayoutSwitchButton,\n    onClick: toggleLayout,\n    whileTap: {\n      scale: 0.93\n    }\n  }, React.createElement(motion.div, {\n    className: stylesButtonsCommon.buttonTextWrapper,\n    key: isWideLayout,\n    initial: {\n      opacity: 0\n    },\n    animate: {\n      opacity: 1\n    },\n    exit: {\n      opacity: 0\n    },\n    transition: {\n      duration: 0.3,\n      ease: \"easeInOut\"\n    }\n  }, isWideLayout ? \"Switch to default layout\" : \"Switch to wide layout\"))), React.createElement(\"br\", null), React.createElement(\"div\", {\n    className: \"postBody\",\n    style: {\n      margin: isWideLayout ? \"0 -14%\" : \"\",\n      maxWidth: isWideLayout ? \"200%\" : \"\",\n      transition: \"margin 1s ease, max-width 1s ease, padding 1s ease\"\n    }\n  }, React.createElement(\"div\", {\n    className: `${stylesCustomPostLayouts.textContent} ${isAnimating ? stylesCustomPostLayouts.fadeOut : stylesCustomPostLayouts.fadeIn}`\n  }, loadedNotices.map((NoticeComponent, index) => React.createElement(NoticeComponent, {\n    key: index\n  })), frontmatter.indexCourse ? React.createElement(PartOfCourseNotice, {\n    index: frontmatter.indexCourse,\n    category: frontmatter.courseCategoryName\n  }) : \"\", React.createElement(ImageContext.Provider, {\n    value: {\n      images: allPostImages.nodes,\n      basePath: basePath.replace(/\\/$/, '') + '/'\n    }\n  }, React.createElement(MDXProvider, {\n    components: {\n      Image\n    }\n  }, children)))), React.createElement(PostBottom, {\n    nextPost: nextPost,\n    lastPost: lastPost,\n    keyCurrent: keyCurrent,\n    section: section\n  }));\n}\nPostTemplate\nexport default function GatsbyMDXWrapper(props) {\n  return React.createElement(PostTemplate, props, React.createElement(GATSBY_COMPILED_MDX, props));\n}\nexport function Head(_ref4) {\n  var _frontmatter$banner, _frontmatter$banner$c, _frontmatter$banner$c2, _frontmatter$banner$c3, _frontmatter$banner$c4;\n  let {data} = _ref4;\n  const {frontmatter} = data.mdx;\n  const title = frontmatter.titleSEO || frontmatter.title;\n  const titleOG = frontmatter.titleOG || title;\n  const titleTwitter = frontmatter.titleTwitter || title;\n  const description = frontmatter.descSEO || frontmatter.desc;\n  const descriptionOG = frontmatter.descOG || description;\n  const descriptionTwitter = frontmatter.descTwitter || description;\n  const schemaType = frontmatter.schemaType || \"BlogPosting\";\n  const keywords = frontmatter.keywordsSEO;\n  const datePublished = frontmatter.date;\n  const dateModified = frontmatter.updated || datePublished;\n  const imageOG = frontmatter.imageOG || ((_frontmatter$banner = frontmatter.banner) === null || _frontmatter$banner === void 0 ? void 0 : (_frontmatter$banner$c = _frontmatter$banner.childImageSharp) === null || _frontmatter$banner$c === void 0 ? void 0 : (_frontmatter$banner$c2 = _frontmatter$banner$c.gatsbyImageData) === null || _frontmatter$banner$c2 === void 0 ? void 0 : (_frontmatter$banner$c3 = _frontmatter$banner$c2.images) === null || _frontmatter$banner$c3 === void 0 ? void 0 : (_frontmatter$banner$c4 = _frontmatter$banner$c3.fallback) === null || _frontmatter$banner$c4 === void 0 ? void 0 : _frontmatter$banner$c4.src);\n  const imageAltOG = frontmatter.imageAltOG || descriptionOG;\n  const imageTwitter = frontmatter.imageTwitter || imageOG;\n  const imageAltTwitter = frontmatter.imageAltTwitter || descriptionTwitter;\n  const canonicalUrl = frontmatter.canonicalURL;\n  const flagHidden = frontmatter.flagHidden || false;\n  const mainTag = frontmatter.mainTag || \"Posts\";\n  const section = frontmatter.slug.split('/')[1] || \"posts\";\n  const type = \"article\";\n  const {siteUrl} = useSiteMetadata();\n  const breadcrumbJSON = {\n    \"@context\": \"https://schema.org\",\n    \"@type\": \"BreadcrumbList\",\n    \"itemListElement\": [{\n      \"@type\": \"ListItem\",\n      \"position\": 1,\n      \"name\": \"Home\",\n      \"item\": siteUrl\n    }, {\n      \"@type\": \"ListItem\",\n      \"position\": 2,\n      \"name\": mainTag,\n      \"item\": `${siteUrl}/${frontmatter.slug.split('/')[1]}`\n    }, {\n      \"@type\": \"ListItem\",\n      \"position\": 3,\n      \"name\": title,\n      \"item\": `${siteUrl}${frontmatter.slug}`\n    }]\n  };\n  return React.createElement(SEO, {\n    title: title + \" - avrtt.blog\",\n    titleOG: titleOG,\n    titleTwitter: titleTwitter,\n    description: description,\n    descriptionOG: descriptionOG,\n    descriptionTwitter: descriptionTwitter,\n    schemaType: schemaType,\n    keywords: keywords,\n    datePublished: datePublished,\n    dateModified: dateModified,\n    imageOG: imageOG,\n    imageAltOG: imageAltOG,\n    imageTwitter: imageTwitter,\n    imageAltTwitter: imageAltTwitter,\n    canonicalUrl: canonicalUrl,\n    flagHidden: flagHidden,\n    mainTag: mainTag,\n    section: section,\n    type: type\n  }, React.createElement(\"script\", {\n    type: \"application/ld+json\"\n  }, JSON.stringify(breadcrumbJSON)));\n}\nconst query = \"2571018839\";\n"],"names":["_ref","text","React","Latex","_createMdxContent","props","_components","Object","assign","h2","a","span","p","h3","ul","li","strong","ol","br","_provideComponents","components","id","style","position","href","className","dangerouslySetInnerHTML","__html","Code","wrapper","MDXLayout","TableOfContents","toc","items","stylesTableOfContents","map","item","index","key","url","onClick","e","handleClick","preventDefault","targetId","replace","targetElement","document","getElementById","scrollIntoView","behavior","block","title","PostTemplate","_ref2","data","mdx","allMdx","allPostImages","children","frontmatter","body","tableOfContents","section","slug","split","sortedPosts","nodes","filter","post","includes","sort","b","currentIndex","findIndex","nextPost","lastPost","trimmedSlug","keyCurrent","exec","basePath","isWideLayout","setIsWideLayout","useState","flagWideLayoutByDefault","isAnimating","setIsAnimating","wordsPerMinute","useEffect","timer","setTimeout","clearTimeout","wordsPerMinuteAdventures","wordsPerMinuteResearch","wordsPerMinuteThoughts","wordCount","RemoveMarkdown","trim","length","readTime","minutes","hours","Math","floor","remainder","formatReadTime","ceil","extraReadTimeMin","notices","flag","flagDraft","component","flagMindfuckery","flagRewrite","flagOffensive","flagProfane","flagMultilingual","flagUnreliably","flagPolitical","flagCognitohazard","flagHidden","loadedNotices","setLoadedNotices","forEach","_ref3","then","module","prev","concat","_toConsumableArray","default","motion","div","initial","opacity","animate","exit","transition","duration","PostBanner","postNumber","date","updated","difficulty","difficultyLevel","desc","banner","postKey","isMindfuckery","mainTag","display","justifyContent","flexWrap","maxWidth","marginLeft","paddingRight","marginTop","marginBottom","otherTags","tag","stylesTagBadges","margin","textAlign","button","stylesCustomPostLayouts","toggleLayout","whileTap","scale","stylesButtonsCommon","ease","NoticeComponent","indexCourse","PartOfCourseNotice","category","courseCategoryName","ImageContext","Provider","value","images","MDXProvider","Image","PostBottom","GatsbyMDXWrapper","GATSBY_COMPILED_MDX","Head","_ref4","_frontmatter$banner","_frontmatter$banner$c","_frontmatter$banner$c2","_frontmatter$banner$c3","_frontmatter$banner$c4","titleSEO","titleOG","titleTwitter","description","descSEO","descriptionOG","descOG","descriptionTwitter","descTwitter","schemaType","keywords","keywordsSEO","datePublished","dateModified","imageOG","childImageSharp","gatsbyImageData","fallback","src","imageAltOG","imageTwitter","imageAltTwitter","canonicalUrl","canonicalURL","siteUrl","useSiteMetadata","breadcrumbJSON","SEO","type","JSON","stringify"],"sourceRoot":""}