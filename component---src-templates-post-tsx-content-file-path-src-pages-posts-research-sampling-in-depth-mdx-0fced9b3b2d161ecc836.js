"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[383],{84016:function(e,t,a){a.r(t),a.d(t,{Head:function(){return I},PostTemplate:function(){return z},default:function(){return H}});var n=a(28453),r=a(96540),i=a(61992),l=a(62087),o=a(90548);function s(e){const t=Object.assign({p:"p",h3:"h3",a:"a",span:"span",br:"br",ol:"ol",li:"li",strong:"strong",h2:"h2",ul:"ul"},(0,n.RP)(),e.components),{Image:a}=t;return a||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),r.createElement(r.Fragment,null,"\n",r.createElement("br"),"\n","\n","\n",r.createElement(t.p,null,"Sampling lies at the heart of countless machine learning and statistical methods, providing a pathway for exploring stochastic latent variables and enabling many key techniques for training large-scale models. Within the realm of deep learning, discrete structures have traditionally presented challenges due to their inherently non-differentiable nature. For example, if I want to sample a discrete random variable (like a categorical variable or a subset) inside a neural network and then backpropagate a gradient signal through that sampling procedure, I immediately run into the dilemma that the argmax or discrete selection steps are non-differentiable. However, this barrier has been partially overcome by new families of continuous relaxations and reparameterization methods, which allow for approximate or exact gradient flow through random sampling steps."),"\n",r.createElement(t.p,null,"These methods are particularly relevant when dealing with discrete latent variables in a deep network — for instance, when I use a ",r.createElement(i.A,null,"variational autoencoder (VAE)")," with a discrete latent space. Sampling from discrete random variables used to involve non-differentiable draws, forcing the use of high-variance gradient estimators like REINFORCE or policy gradients. But with the introduction of the ",r.createElement(i.A,null,"Gumbel-softmax reparameterization trick")," (Jang and gang, ICLR 2017; Maddison and gang, ICLR 2017), along with other creative continuous approximations of discrete operations, we now have an expanded toolkit that can drastically improve training stability and reduce estimator variance."),"\n",r.createElement(t.p,null,"The broader theme of this article is the family of methods that revolve around ",r.createElement(o.A,{text:"\\( \\text{Gumbel noise} \\)"})," and its close cousins, which facilitate reparameterizing discrete draws in a continuous fashion. We will explore the fundamental ideas of the Gumbel distribution, the ",r.createElement(i.A,null,"Gumbel-max trick"),", the ",r.createElement(i.A,null,"Gumbel-softmax distribution"),", ",r.createElement(i.A,null,"Gumbel top-k sampling"),", and the ",r.createElement(i.A,null,"Gumbel-Sinkhorn operator"),". These techniques allow for differentiable approximations of argmax, top-k, subsets, permutations, and even adjacency matrices for graphs. By carefully tuning temperature parameters or employing advanced gradient estimators, we can move seamlessly between discrete and continuous realms."),"\n",r.createElement(t.h3,{id:"motivation-for-sampling-in-deep-learning",style:{position:"relative"}},r.createElement(t.a,{href:"#motivation-for-sampling-in-deep-learning","aria-label":"motivation for sampling in deep learning permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"motivation for sampling in deep learning"),"\n",r.createElement(t.p,null,"Deep neural networks have opened new possibilities for generative modeling, reinforcement learning, and structured prediction. In many of these problems, I might want to represent a variable as discrete — for instance, a categorical latent variable indicating which cluster an observation belongs to, a subset of features in a combinatorial selection problem, or the ordering of items in a ranking or matching problem. Such discrete structures bring interpretability and can lead to simpler or more efficient solutions, but they also introduce non-differentiable operations."),"\n",r.createElement(t.p,null,"The question then becomes: how can I train large neural networks that contain these discrete components end-to-end with backpropagation? The direct approach, involving discrete sampling, breaks the chain rule of calculus. This leads to gradient estimators like REINFORCE that can exhibit high variance, especially in high-dimensional settings."),"\n",r.createElement(t.p,null,"Hence the need for ",r.createElement(i.A,null,"reparameterization tricks"),". The general principle behind reparameterization is that I express a random variable ",r.createElement(o.A,{text:"\\( X \\)"})," with a distribution parameterized by ",r.createElement(o.A,{text:"\\( \\theta \\)"})," as a deterministic function of a parameter-free noise variable ",r.createElement(o.A,{text:"\\( \\epsilon \\)"}),". Formally, instead of"),"\n",r.createElement(o.A,{text:"\\( X \\sim p_\\theta(x) \\)"}),"\n",r.createElement(t.p,null,"I try to write",r.createElement(t.br),"\n",r.createElement(o.A,{text:"\\( X = g(\\theta, \\epsilon) \\)"}),",",r.createElement(t.br),"\n","where ",r.createElement(o.A,{text:"\\( \\epsilon \\sim p(\\epsilon) \\)"})," has a distribution that does not depend on ",r.createElement(o.A,{text:"\\( \\theta \\)"}),". This style of reparameterization is easy to implement for continuous distributions like the Gaussian, where ",r.createElement(o.A,{text:"\\( X = \\mu + \\sigma \\cdot \\epsilon \\)"})," and ",r.createElement(o.A,{text:"\\( \\epsilon \\sim \\mathcal{N}(0,1) \\)"}),". But for discrete distributions, we can't so simply separate out the random part from the parameters — at least not without some creativity."),"\n",r.createElement(t.p,null,"This is where the ",r.createElement(i.A,null,"Gumbel distribution")," steps in. The Gumbel-max trick shows that if I have a categorical distribution with class probabilities ",r.createElement(o.A,{text:"\\( \\pi_1, \\ldots, \\pi_k \\)"}),", I can sample exactly from that distribution by sampling i.i.d. ",r.createElement(o.A,{text:"\\( G_i \\sim \\text{Gumbel}(0,1) \\)"})," and taking ",r.createElement(o.A,{text:"\\( \\arg\\max_i (\\log \\pi_i + G_i) \\)"}),". That's an exact sampler, but the ",r.createElement(o.A,{text:"\\( \\arg\\max \\)"})," is not differentiable. So the Gumbel-softmax relaxation replaces the ",r.createElement(o.A,{text:"\\( \\arg\\max \\)"})," with a ",r.createElement(o.A,{text:"\\( \\text{softmax} \\)"})," operation, thereby making the entire process differentiable."),"\n",r.createElement(t.h3,{id:"discrete-structures-in-machine-learning-models",style:{position:"relative"}},r.createElement(t.a,{href:"#discrete-structures-in-machine-learning-models","aria-label":"discrete structures in machine learning models permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"discrete structures in machine learning models"),"\n",r.createElement(t.p,null,"Some common examples of discrete structures in ML:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"Categorical variables (unstructured vectors).")," A straightforward scenario is a K-class latent variable ",r.createElement(o.A,{text:"\\( z \\in \\{1, \\ldots, K\\} \\)"}),". This arises in VAEs that impose a discrete latent space or in generative models that must pick from discrete sets of states."),"\n"),"\n",r.createElement(t.li,null,"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"Subsets.")," I might want to pick a subset of items (features, data samples, or other sets). This is a combinatorial selection problem, often associated with the top-k or top-p selection, or used in resource-constrained optimization."),"\n"),"\n",r.createElement(t.li,null,"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"Permutations.")," Another step up in complexity is a scenario where I want to sample permutations, for instance in ranking tasks, matching problems, or route planning. A permutation matrix is discrete and combinatorial, so direct backprop is not feasible."),"\n"),"\n",r.createElement(t.li,null,"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"Graph-based latent variables.")," If the structure of a graph (which edges exist) is unknown, sampling from a distribution over graphs is a discrete selection problem over edges."),"\n"),"\n"),"\n",r.createElement(t.p,null,"Each of these discrete structures can be tackled with Gumbel-based methods or other advanced gradient estimators."),"\n",r.createElement(t.h3,{id:"continuous-relaxations-and-reparameterization-tricks",style:{position:"relative"}},r.createElement(t.a,{href:"#continuous-relaxations-and-reparameterization-tricks","aria-label":"continuous relaxations and reparameterization tricks permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"continuous relaxations and reparameterization tricks"),"\n",r.createElement(t.p,null,"The overarching idea is to turn each discrete sample or operation (argmax, top-k, constructing adjacency, etc.) into a smooth approximation so that the forward pass yields values that, at high temperature, approximate continuous distributions, and at low temperature, approach discrete distributions. Then I can backprop through these continuous approximations."),"\n",r.createElement(t.p,null,"We'll dig deep into the Gumbel distribution and the softmax relaxation, paying attention to how temperature scheduling can gradually push the model from a continuous representation (easy to train but less discrete) toward a more discrete representation (closer to the final goal but potentially harder to optimize). I'll also highlight alternative methods (like REINFORCE, RELAX, REBAR) that do not require continuous relaxations but come with their own trade-offs in variance and bias."),"\n",r.createElement(t.p,null,"This article aims to give you an in-depth look at how these sampling approaches work, how to implement them, and how they can be used in real-world models. Along the way, I'll provide references to relevant papers and highlight recent research trends."),"\n",r.createElement(t.h2,{id:"gumbel-softmax-reparameterization",style:{position:"relative"}},r.createElement(t.a,{href:"#gumbel-softmax-reparameterization","aria-label":"gumbel softmax reparameterization permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"gumbel-softmax reparameterization"),"\n",r.createElement(t.h3,{id:"the-gumbel-max-trick-and-categorical-distributions",style:{position:"relative"}},r.createElement(t.a,{href:"#the-gumbel-max-trick-and-categorical-distributions","aria-label":"the gumbel max trick and categorical distributions permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"the gumbel-max trick and categorical distributions"),"\n",r.createElement(t.p,null,"Suppose I have a categorical distribution over ",r.createElement(o.A,{text:"\\( K \\)"})," classes with probabilities ",r.createElement(o.A,{text:"\\( \\pi = (\\pi_1, \\ldots, \\pi_K) \\)"}),", meaning ",r.createElement(o.A,{text:"\\( \\sum_{i=1}^K \\pi_i = 1 \\)"}),". I want to sample ",r.createElement(o.A,{text:"\\( z \\in \\{1, \\ldots, K\\} \\)"})," from that distribution. One way is to do a typical discrete sampling approach: pick class ",r.createElement(o.A,{text:"\\( i \\)"})," with probability ",r.createElement(o.A,{text:"\\( \\pi_i \\)"}),". But the Gumbel-max trick redefines the sampling procedure as follows:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,"Sample ",r.createElement(o.A,{text:"\\( G_1, \\ldots, G_K \\)"})," i.i.d. from a ",r.createElement(i.A,null,"Gumbel(0,1)")," distribution. A Gumbel(0,1) random variable can be sampled as ",r.createElement(o.A,{text:"\\( -\\log(-\\log(U)) \\)"})," where ",r.createElement(o.A,{text:"\\( U \\sim \\text{Uniform}(0, 1) \\)"}),"."),"\n",r.createElement(t.li,null,"Compute ",r.createElement(o.A,{text:"\\( i^* = \\arg\\max_i \\bigl(\\log \\pi_i + G_i\\bigr) \\)"}),"."),"\n"),"\n",r.createElement(t.p,null,"The random index ",r.createElement(o.A,{text:"\\( i^* \\)"})," follows the original categorical distribution with probability ",r.createElement(o.A,{text:"\\( \\pi \\)"}),". Thus, ",r.createElement(o.A,{text:"\\( z = i^* \\)"})," is a discrete sample from the correct distribution."),"\n",r.createElement(t.p,null,"However, ",r.createElement(o.A,{text:"\\( i^* \\)"})," is not differentiable with respect to ",r.createElement(o.A,{text:"\\( \\pi \\)"}),", because ",r.createElement(o.A,{text:"\\( \\arg\\max(\\cdots) \\)"})," is not a smooth function."),"\n",r.createElement(t.h3,{id:"the-argmax-operation-vs-a-softmax-relaxation",style:{position:"relative"}},r.createElement(t.a,{href:"#the-argmax-operation-vs-a-softmax-relaxation","aria-label":"the argmax operation vs a softmax relaxation permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"the argmax operation vs. a softmax relaxation"),"\n",r.createElement(t.p,null,"The ",r.createElement(i.A,null,"Gumbel-softmax trick"),", also called the ",r.createElement(i.A,null,"Concrete distribution")," by Jang and gang (ICLR 2017) and Maddison and gang (ICLR 2017), replaces ",r.createElement(o.A,{text:"\\( \\arg\\max_i \\bigl(\\log \\pi_i + G_i\\bigr) \\)"})," with a ",r.createElement(o.A,{text:"\\( \\softmax \\)"})," operation:"),"\n",r.createElement(o.A,{text:"\\[\ny_i = \\frac{\\exp\\bigl((\\log \\pi_i + G_i)/\\tau\\bigr)}{\\sum_{j=1}^K \\exp\\bigl((\\log \\pi_j + G_j)/\\tau\\bigr)},\n\\]"}),"\n",r.createElement(t.p,null,"where ",r.createElement(o.A,{text:"\\( \\tau \\)"})," is a temperature parameter. If ",r.createElement(o.A,{text:"\\( \\tau \\)"})," is very low, the ",r.createElement(o.A,{text:"\\( \\softmax \\)"})," is sharply peaked, closely approximating an ",r.createElement(o.A,{text:"\\( \\arg\\max \\)"}),". If ",r.createElement(o.A,{text:"\\( \\tau \\)"})," is very high, the vector ",r.createElement(o.A,{text:"\\( y = (y_1, \\ldots, y_K) \\)"})," spreads out, approaching a uniform distribution."),"\n",r.createElement(t.p,null,"Crucially, ",r.createElement(o.A,{text:"\\( y_i \\)"})," is differentiable with respect to the logits ",r.createElement(o.A,{text:"\\( \\log \\pi_i \\)"}),". Hence if ",r.createElement(o.A,{text:"\\( \\pi \\)"})," is produced by some neural network (e.g., the encoder of a VAE), I can backpropagate from ",r.createElement(o.A,{text:"\\( y_i \\)"})," to ",r.createElement(o.A,{text:"\\( \\pi \\)"}),"."),"\n",r.createElement(t.p,null,"I note that the vector ",r.createElement(o.A,{text:"\\( y \\)"})," is still a ",r.createElement("em",null,"continuous")," vector in ",r.createElement(o.A,{text:"\\( \\mathbb{R}^K \\)"}),". I can interpret ",r.createElement(o.A,{text:"\\( y \\)"}),' as a "soft" one-hot vector, with each component in ',r.createElement(o.A,{text:"\\( (0, 1) \\)"})," but summing to 1."),"\n",r.createElement(t.h3,{id:"temperature-hyperparameter-and-annealing-strategies",style:{position:"relative"}},r.createElement(t.a,{href:"#temperature-hyperparameter-and-annealing-strategies","aria-label":"temperature hyperparameter and annealing strategies permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"temperature hyperparameter and annealing strategies"),"\n",r.createElement(t.p,null,"A central hyperparameter in Gumbel-softmax is the temperature ",r.createElement(o.A,{text:"\\( \\tau \\)"}),". If ",r.createElement(o.A,{text:"\\( \\tau \\)"})," is close to 0, the ",r.createElement(o.A,{text:"\\( \\softmax \\)"})," becomes extremely peaked, approaching a one-hot vector, but the gradients can become large (or vanish in certain regimes). If ",r.createElement(o.A,{text:"\\( \\tau \\)"})," is large, the vector ",r.createElement(o.A,{text:"\\( y \\)"})," is more uniform, and the training signals flow more stably."),"\n",r.createElement(t.p,null,"A common trick is to start with a relatively high temperature ",r.createElement(o.A,{text:"\\( \\tau_\\text{start} \\)"})," and then gradually reduce it during training toward a smaller value ",r.createElement(o.A,{text:"\\( \\tau_\\text{end} \\)"}),". This is called ",r.createElement(i.A,null,"temperature annealing"),". It helps the model begin training with smoother approximations, and later it can hone in on more discrete solutions."),"\n",r.createElement(t.h3,{id:"training-categorical-vaes-with-gumbel-softmax",style:{position:"relative"}},r.createElement(t.a,{href:"#training-categorical-vaes-with-gumbel-softmax","aria-label":"training categorical vaes with gumbel softmax permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"training categorical vaes with gumbel-softmax"),"\n",r.createElement(t.p,null,"Variational autoencoders with continuous latent variables rely on the reparameterization trick for Gaussians. But if I want to do a ",r.createElement(i.A,null,"categorical VAE"),", I can't easily reparameterize a discrete distribution. The Gumbel-softmax trick offers a solution."),"\n",r.createElement(t.p,null,"In a categorical VAE, the encoder outputs ",r.createElement(o.A,{text:"\\( \\alpha_1, \\ldots, \\alpha_K \\)"})," (logits), from which I derive ",r.createElement(o.A,{text:"\\( \\pi \\)"})," by ",r.createElement(o.A,{text:"\\( \\pi_i = \\exp(\\alpha_i)/\\sum_{j}\\exp(\\alpha_j) \\)"}),". Then to sample ",r.createElement(o.A,{text:"\\( z \\)"}),", I do:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,"Sample ",r.createElement(o.A,{text:"\\( G_i \\sim \\text{Gumbel}(0,1) \\)"})," for each class ",r.createElement(o.A,{text:"\\( i \\)"}),"."),"\n",r.createElement(t.li,null,"Compute ",r.createElement(o.A,{text:"\\( y = \\softmax\\bigl((\\log \\pi + G)/\\tau\\bigr) \\)"}),"."),"\n"),"\n",r.createElement(t.p,null,"Now ",r.createElement(o.A,{text:"\\( y \\)"})," is the ",r.createElement(i.A,null,"relaxed discrete latent variable"),". The reconstruction function can condition on ",r.createElement(o.A,{text:"\\( y \\)"}),". At the same time, I can compute a KL divergence term that approximates the difference between the approximate posterior ",r.createElement(o.A,{text:"\\( q_\\phi(z|x) \\)"})," and a prior ",r.createElement(o.A,{text:"\\( p(z) \\)"}),", which might be a uniform categorical distribution."),"\n",r.createElement(t.h3,{id:"the-straight-through-estimator",style:{position:"relative"}},r.createElement(t.a,{href:"#the-straight-through-estimator","aria-label":"the straight through estimator permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"the straight-through estimator"),"\n",r.createElement(t.p,null,"Another variant is the ",r.createElement(i.A,null,"straight-through estimator"),". In this approach, I compute the argmax in the forward pass to get an actual discrete sample ",r.createElement(o.A,{text:"\\( \\hat{z} \\)"}),", but in the backward pass, I treat that argmax as if it was a ",r.createElement(o.A,{text:"\\( \\softmax \\)"})," of the same logits. This means I get a discrete sample for the forward pass (which is beneficial if I want strict category decisions), but I still propagate useful gradients back."),"\n",r.createElement(t.p,null,"Concretely, if ",r.createElement(o.A,{text:"\\( y_{\\text{soft}} \\)"})," is the Gumbel-softmax output, I can create ",r.createElement(o.A,{text:"\\( y_{\\text{hard}} \\)"})," by taking ",r.createElement(o.A,{text:"\\( \\arg\\max \\)"})," across the classes, i.e. a true one-hot vector. Then the forward pass uses ",r.createElement(o.A,{text:"\\( y_{\\text{hard}} \\)"})," as input to the next layer, but for the backward pass, I manually replace the gradient with the gradient from ",r.createElement(o.A,{text:"\\( y_{\\text{soft}} \\)"}),". In code, this is often achieved by adding and subtracting ",r.createElement(o.A,{text:"\\( y_{\\text{soft}} \\)"})," from ",r.createElement(o.A,{text:"\\( y_{\\text{hard}} \\)"})," inside a ",r.createElement(i.A,null,"stop-gradient")," operation."),"\n",r.createElement(t.h3,{id:"comparisons-to-alternative-gradient-estimators-reinforce-rebar-relax",style:{position:"relative"}},r.createElement(t.a,{href:"#comparisons-to-alternative-gradient-estimators-reinforce-rebar-relax","aria-label":"comparisons to alternative gradient estimators reinforce rebar relax permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"comparisons to alternative gradient estimators (reinforce, rebar, relax)"),"\n",r.createElement(t.p,null,"If I do not use a continuous relaxation, I might resort to ",r.createElement(i.A,null,"REINFORCE")," (Williams, 1992), which treats the discrete sample as a random node in the computation graph and uses the log-derivative trick ",r.createElement(o.A,{text:"\\( \\nabla_\\theta E_{z \\sim p_\\theta} [f(z)] = E_{z \\sim p_\\theta} [f(z) \\nabla_\\theta \\log p_\\theta(z)] \\)"}),". But REINFORCE can suffer from high variance, so it's common to incorporate ",r.createElement(i.A,null,"baselines")," to reduce variance."),"\n",r.createElement(t.p,null,r.createElement(i.A,null,"REBAR")," (Tucker and gang, ICLR 2017) and ",r.createElement(i.A,null,"RELAX")," (Grathwohl and gang, ICLR 2018) combine a continuous relaxation with a control variate technique to reduce variance further. These methods can provide unbiased (or low-bias) gradient estimates that have lower variance than straightforward REINFORCE. However, they are also more complex to implement, requiring additional neural networks to approximate baseline functions."),"\n",r.createElement(t.p,null,"Overall, the Gumbel-softmax approach is conceptually simpler to implement. It does introduce a small bias, because the sample is no longer strictly discrete, but in practice, it can work well and scale to large problems more straightforwardly."),"\n",r.createElement(t.h2,{id:"categorical-vae-example",style:{position:"relative"}},r.createElement(t.a,{href:"#categorical-vae-example","aria-label":"categorical vae example permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"categorical vae example"),"\n",r.createElement(t.h3,{id:"model-overview-and-latent-variable-structure",style:{position:"relative"}},r.createElement(t.a,{href:"#model-overview-and-latent-variable-structure","aria-label":"model overview and latent variable structure permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"model overview and latent variable structure"),"\n",r.createElement(t.p,null,"Let's consider an example of a ",r.createElement(i.A,null,"Variational Autoencoder")," with one or more ",r.createElement(i.A,null,"categorical latent variables"),". For simplicity, imagine I have a single latent variable ",r.createElement(o.A,{text:"\\( z \\)"})," with ",r.createElement(o.A,{text:"\\( K \\)"})," categories. The generative process is:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"Sample ",r.createElement(o.A,{text:"\\( z \\in \\{1, \\ldots, K\\} \\)"})," from some prior ",r.createElement(o.A,{text:"\\( p(z) \\)"}),"."),"\n",r.createElement(t.li,null,"Generate observation ",r.createElement(o.A,{text:"\\( x \\)"})," from a conditional distribution ",r.createElement(o.A,{text:"\\( p_\\theta(x|z) \\)"})," (e.g., a neural network that outputs parameters of a Bernoulli or Gaussian distribution over ",r.createElement(o.A,{text:"\\( x \\)"}),", given a one-hot encoding of ",r.createElement(o.A,{text:"\\( z \\)"}),")."),"\n"),"\n",r.createElement(t.p,null,"In a normal continuous VAE, ",r.createElement(o.A,{text:"\\( z \\)"})," might be a vector from a Gaussian. Here, it's a single categorical variable. The goal is to learn ",r.createElement(o.A,{text:"\\( p_\\theta(x|z) \\)"})," and also an approximate posterior ",r.createElement(o.A,{text:"\\( q_\\phi(z|x) \\)"}),"."),"\n",r.createElement(t.h3,{id:"gumbel-softmax-sampling-within-the-encoder",style:{position:"relative"}},r.createElement(t.a,{href:"#gumbel-softmax-sampling-within-the-encoder","aria-label":"gumbel softmax sampling within the encoder permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"gumbel-softmax sampling within the encoder"),"\n",r.createElement(t.p,null,"In the encoder, I produce logits ",r.createElement(o.A,{text:"\\( \\alpha(x) = (\\alpha_1, \\ldots, \\alpha_K) \\)"}),", from which I define ",r.createElement(o.A,{text:"\\( \\pi_i = \\exp(\\alpha_i) / \\sum_{j=1}^K \\exp(\\alpha_j) \\)"}),". To sample ",r.createElement(o.A,{text:"\\( z \\)"})," in a differentiable manner, I do:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,"\n",r.createElement(o.A,{text:"\\( G_i \\sim \\text{Gumbel}(0,1) \\)"}),"\n"),"\n",r.createElement(t.li,null,"\n",r.createElement(o.A,{text:"\\( y_i = \\frac{\\exp((\\log \\pi_i + G_i)/\\tau)}{\\sum_{j=1}^K \\exp((\\log \\pi_j + G_j)/\\tau)} \\)"}),"\n"),"\n"),"\n",r.createElement(t.p,null,"The vector ",r.createElement(o.A,{text:"\\( y = (y_1, \\ldots, y_K) \\)"})," is a relaxed discrete sample. Then the decoder takes ",r.createElement(o.A,{text:"\\( y \\)"})," as input, e.g. ",r.createElement(o.A,{text:"\\( p_\\theta(x|z = y) \\)"}),"."),"\n",r.createElement(t.h3,{id:"kl-divergence-and-reconstruction-loss",style:{position:"relative"}},r.createElement(t.a,{href:"#kl-divergence-and-reconstruction-loss","aria-label":"kl divergence and reconstruction loss permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"kl divergence and reconstruction loss"),"\n",r.createElement(t.p,null,"The training objective for a VAE is the ",r.createElement(i.A,null,"Evidence Lower BOund (ELBO)"),":"),"\n",r.createElement(o.A,{text:"\\[\n\\mathcal{L}(\\theta, \\phi) = E_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_\\text{KL}(q_\\phi(z|x) \\| p(z)).\n\\]"}),"\n",r.createElement(t.p,null,"When ",r.createElement(o.A,{text:"\\( z \\)"})," is discrete, ",r.createElement(o.A,{text:"\\( q_\\phi(z|x) \\)"})," is a categorical distribution. If the prior ",r.createElement(o.A,{text:"\\( p(z) \\)"})," is uniform, ",r.createElement(o.A,{text:"\\( D_\\text{KL}(q_\\phi(z|x) \\| p(z)) \\)"})," has a closed-form expression:"),"\n",r.createElement(o.A,{text:"\\( D_\\text{KL}(q_\\phi(z|x) \\| p(z)) = \\sum_{i=1}^K \\pi_i \\log \\bigl(\\pi_i \\cdot K\\bigr). \\)"}),"\n",r.createElement(t.p,null,"But note, I never directly sample ",r.createElement(o.A,{text:"\\( z \\)"})," from the discrete distribution. Instead, I sample the continuous relaxation ",r.createElement(o.A,{text:"\\( y \\)"}),". The reparameterization with Gumbel-softmax is used for the expectation term ",r.createElement(o.A,{text:"\\( E_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] \\)"}),". The KL term is computed in closed form using the distribution ",r.createElement(o.A,{text:"\\( \\pi \\)"}),"."),"\n",r.createElement(t.h3,{id:"code-walk-through-and-implementation-details",style:{position:"relative"}},r.createElement(t.a,{href:"#code-walk-through-and-implementation-details","aria-label":"code walk through and implementation details permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"code walk-through and implementation details"),"\n",r.createElement(t.p,null,"Below is a simplified code snippet illustrating how to implement a categorical VAE with Gumbel-softmax in Python (PyTorch-like pseudocode). The relevant steps are in the encoder, where I produce logits and sample them with the reparameterization trick."),"\n",r.createElement(l.A,{text:"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass GumbelSoftmaxVAE(nn.Module):\n    def __init__(self, input_dim, hidden_dim, latent_k, temp_init=1.0):\n        super().__init__()\n        self.latent_k = latent_k\n        self.temp = temp_init\n        \n        # Encoder\n        self.encoder_fc = nn.Linear(input_dim, hidden_dim)\n        self.encoder_logits = nn.Linear(hidden_dim, latent_k)\n        \n        # Decoder\n        self.decoder_fc = nn.Linear(latent_k, hidden_dim)\n        self.decoder_out = nn.Linear(hidden_dim, input_dim)\n    \n    def encode(self, x):\n        h = F.relu(self.encoder_fc(x))\n        logits = self.encoder_logits(h)\n        # Return logits for the categorical distribution\n        return logits\n    \n    def gumbel_softmax_sample(self, logits, temperature):\n        # sample gumbel\n        gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits) + 1e-20) + 1e-20)\n        # add to logits and apply softmax\n        y = F.softmax((logits + gumbel_noise) / temperature, dim=-1)\n        return y\n    \n    def decode(self, y):\n        h = F.relu(self.decoder_fc(y))\n        out = torch.sigmoid(self.decoder_out(h))\n        return out\n    \n    def forward(self, x):\n        logits = self.encode(x)\n        y = self.gumbel_softmax_sample(logits, self.temp)\n        reconstruction = self.decode(y)\n        return reconstruction, y, logits\n"}),"\n",r.createElement(t.p,null,"Here, ",r.createElement(o.A,{text:"\\( \\texttt{gumbel\\_softmax\\_sample} \\)"})," implements the Gumbel-softmax reparameterization. The temperature ",r.createElement(o.A,{text:"\\( \\texttt{self.temp} \\)"})," can be annealed over epochs by doing something like:"),"\n",r.createElement(l.A,{text:"\n# Suppose we have an update rule\ndef update_temperature(model, epoch, start_temp=1.0, end_temp=0.1, total_epochs=100):\n    # Simple linear or exponential scheduling\n    # For example, exponential\n    ratio = min(float(epoch) / total_epochs, 1.0)\n    new_temp = start_temp * (end_temp / start_temp) ** ratio\n    model.temp = new_temp\n"}),"\n",r.createElement(t.h3,{id:"practical-considerations-and-tuning-the-temperature",style:{position:"relative"}},r.createElement(t.a,{href:"#practical-considerations-and-tuning-the-temperature","aria-label":"practical considerations and tuning the temperature permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"practical considerations and tuning the temperature"),"\n",r.createElement(t.p,null,"In practice, selecting a starting temperature in ",r.createElement(o.A,{text:"\\( [1, 2] \\)"})," and gradually decaying to ",r.createElement(o.A,{text:"\\( 0.5 \\)"})," or ",r.createElement(o.A,{text:"\\( 0.1 \\)"})," can be a good starting point. If the temperature becomes too low too fast, training can destabilize or get stuck. If it remains too high, the latent variables never become discrete, and the model might not leverage the categorical structure."),"\n",r.createElement(t.p,null,"Additionally, watch out for ",r.createElement(i.A,null,"gradient magnitudes"),". With extremely small ",r.createElement(o.A,{text:"\\( \\tau \\)"}),", the softmax can saturate, leading to vanishing or exploding gradients. Some frameworks also provide built-in Gumbel-softmax or ",r.createElement(i.A,null,"straight-through")," functionalities that can handle these details."),"\n",r.createElement(t.h3,{id:"evaluation-metrics-for-generative-quality-and-latent-utilization",style:{position:"relative"}},r.createElement(t.a,{href:"#evaluation-metrics-for-generative-quality-and-latent-utilization","aria-label":"evaluation metrics for generative quality and latent utilization permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"evaluation metrics for generative quality and latent utilization"),"\n",r.createElement(t.p,null,"Once trained, we can evaluate the generative quality by sampling from the learned model:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,"Sample ",r.createElement(o.A,{text:"\\( z \\sim \\text{Cat}(p(z)) \\)"})," (or from the approximate posterior if evaluating reconstruction)."),"\n",r.createElement(t.li,null,"Pass the corresponding one-hot or relaxed vector to the decoder."),"\n"),"\n",r.createElement(t.p,null,"We can measure how well the model reconstructs test data (reconstruction error), how well the latent classes are used (e.g., how many categories remain near zero probability in the prior or encoder distribution), and if the learned categories are semantically meaningful."),"\n",r.createElement(t.h2,{id:"gumbel-top-k-sampling-and-subset-selection",style:{position:"relative"}},r.createElement(t.a,{href:"#gumbel-top-k-sampling-and-subset-selection","aria-label":"gumbel top k sampling and subset selection permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"gumbel top-k sampling and subset selection"),"\n",r.createElement(t.h3,{id:"from-gumbel-argmax-to-gumbel-top-k-for-subsets",style:{position:"relative"}},r.createElement(t.a,{href:"#from-gumbel-argmax-to-gumbel-top-k-for-subsets","aria-label":"from gumbel argmax to gumbel top k for subsets permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"from gumbel-argmax to gumbel top-k for subsets"),"\n",r.createElement(t.p,null,"In many tasks, I don't just want to pick one element with ",r.createElement(o.A,{text:"\\( \\arg\\max \\)"}),"; I might want the top-",r.createElement(o.A,{text:"\\( k \\)"})," elements out of ",r.createElement(o.A,{text:"\\( K \\)"}),". The Gumbel-max trick extends naturally to the top-k scenario:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,"Sample ",r.createElement(o.A,{text:"\\( G_i \\sim \\text{Gumbel}(0,1) \\)"})," for ",r.createElement(o.A,{text:"\\( i = 1, \\ldots, K \\)"}),"."),"\n",r.createElement(t.li,null,"Compute ",r.createElement(o.A,{text:"\\( i_1, \\ldots, i_k \\)"})," as the indices of the ",r.createElement(o.A,{text:"\\( k \\)"})," largest values of ",r.createElement(o.A,{text:"\\( \\log \\pi_i + G_i \\)"}),"."),"\n"),"\n",r.createElement(t.p,null,"This yields an exact sample of a ",r.createElement(o.A,{text:"\\( k \\)"}),"-subset from the distribution that picks subsets with probabilities proportional to the product of their ",r.createElement(o.A,{text:"\\( \\pi \\)"})," parameters (under certain assumptions). But for differentiability, I can do an approximate version."),"\n",r.createElement(t.h3,{id:"sampling-without-replacement-and-top-k-relaxation",style:{position:"relative"}},r.createElement(t.a,{href:"#sampling-without-replacement-and-top-k-relaxation","aria-label":"sampling without replacement and top k relaxation permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"sampling without replacement and top-k relaxation"),"\n",r.createElement(t.p,null,"The top-k version of Gumbel sampling can be relaxed in multiple ways. One approach is to produce a ",r.createElement(o.A,{text:"\\( K \\)"}),"-dimensional vector whose largest ",r.createElement(o.A,{text:"\\( k \\)"})," entries are significantly higher than the rest, but in a continuous manner. Another approach is iterative, where I pick the largest logit, remove it, and pick the next largest from the remaining subset, etc."),"\n",r.createElement(t.p,null,"A well-known approach is to do something akin to:"),"\n",r.createElement(o.A,{text:"\\[\ny = \\softmax\\Bigl( ( \\log \\pi + G ) / \\tau \\Bigr),\n\\]"}),"\n",r.createElement(t.p,null,"then only keep the top-k entries in ",r.createElement(o.A,{text:"\\( y \\)"})," by zeroing out others or applying some continuous threshold. This approach can be more complex, but the principle is to preserve as much differentiability as possible."),"\n",r.createElement(t.h3,{id:"subsetoperator-class-iterative-softmax-and-temperature-tuning",style:{position:"relative"}},r.createElement(t.a,{href:"#subsetoperator-class-iterative-softmax-and-temperature-tuning","aria-label":"subsetoperator class iterative softmax and temperature tuning permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"subsetoperator class: iterative softmax and temperature tuning"),"\n",r.createElement(t.p,null,"A pseudo-implementation might look like this:"),"\n",r.createElement(l.A,{text:'\nimport torch\nimport torch.nn.functional as F\n\ndef sample_topk_gumbel(logits, k, temperature):\n    # logits shape: (batch_size, K)\n    # we want to get a "soft" top-k selection\n    gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits) + 1e-20) + 1e-20)\n    y = (logits + gumbel_noise) / temperature\n    # softmax\n    y_soft = F.softmax(y, dim=-1)\n    # potentially we can approximate top-k via a sharper distribution or partial threshold\n    # for demonstration, let\'s keep it simple\n    # we can also return the sorted indices or partial top-k distribution\n    return y_soft\n'}),"\n",r.createElement(t.p,null,"One might refine that to an iterative or thresholding approach that ensures exactly ",r.createElement(o.A,{text:"\\( k \\)"})," items are chosen."),"\n",r.createElement(t.h3,{id:"differentiable-subset-selection-in-k-nearest-neighbor-classification",style:{position:"relative"}},r.createElement(t.a,{href:"#differentiable-subset-selection-in-k-nearest-neighbor-classification","aria-label":"differentiable subset selection in k nearest neighbor classification permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"differentiable subset selection in k-nearest neighbor classification"),"\n",r.createElement(t.p,null,"An interesting application is making the choice of which neighbors are considered in a k-NN model differentiable. For example, I might embed points in a latent space and then pick the top-",r.createElement(o.A,{text:"\\( k \\)"})," nearest neighbors as a ",r.createElement(i.A,null,"soft subset")," based on Gumbel-based distances. Then backpropagation can adjust the embedding to improve classification or regression performance."),"\n",r.createElement(t.p,null,"If I do a standard k-NN approach, the selection of neighbors is non-differentiable. But if I approximate it with a Gumbel top-k scheme, the entire pipeline can, in principle, be made end-to-end trainable, though this is still an area of active research."),"\n",r.createElement(t.h3,{id:"empirical-distribution-checks-and-histogram-comparison",style:{position:"relative"}},r.createElement(t.a,{href:"#empirical-distribution-checks-and-histogram-comparison","aria-label":"empirical distribution checks and histogram comparison permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"empirical distribution checks and histogram comparison"),"\n",r.createElement(t.p,null,"When implementing Gumbel top-k or other discrete sampling approximations, I should always check that the empirical distribution of the selected subsets matches my intended distribution. For instance, if ",r.createElement(o.A,{text:"\\( \\pi \\)"})," is uniform, I want each k-subset to be equally likely on average. In practice, numerical issues, finite sample sizes, or extreme temperature values may skew the distribution."),"\n",r.createElement(t.p,null,"To verify correctness, one can:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,"Sample a large number of subsets."),"\n",r.createElement(t.li,null,"Estimate the distribution of subsets."),"\n",r.createElement(t.li,null,"Compare it against the theoretically expected distribution."),"\n"),"\n",r.createElement(t.p,null,"For moderate ",r.createElement(o.A,{text:"\\( K \\)"})," and ",r.createElement(o.A,{text:"\\( k \\)"}),", a histogram can confirm that the sampling is unbiased or only mildly biased."),"\n",r.createElement(t.h3,{id:"extensions-to-resource-constrained-subset-selection-and-combinatorial-optimization",style:{position:"relative"}},r.createElement(t.a,{href:"#extensions-to-resource-constrained-subset-selection-and-combinatorial-optimization","aria-label":"extensions to resource constrained subset selection and combinatorial optimization permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"extensions to resource-constrained subset selection and combinatorial optimization"),"\n",r.createElement(t.p,null,"Beyond picking neighbors, I might have a large set of items from which I need to pick a subset that optimizes some cost function, subject to resource constraints. Traditional combinatorial optimization algorithms are not easily integrated into deep networks, but with Gumbel-based subset sampling, I can incorporate these constraints as a differentiable approximation and optimize end-to-end with gradient-based methods. Research on ",r.createElement(i.A,null,"differentiable subset selection")," and ",r.createElement(i.A,null,"deep combinatorial optimization")," is growing rapidly (see Mena and gang, 2018 and Kool and gang, 2019 for example approaches)."),"\n",r.createElement(t.h2,{id:"gumbel-sinkhorn-networks-for-permutations",style:{position:"relative"}},r.createElement(t.a,{href:"#gumbel-sinkhorn-networks-for-permutations","aria-label":"gumbel sinkhorn networks for permutations permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"gumbel-sinkhorn networks for permutations"),"\n",r.createElement(t.h3,{id:"doubly-stochastic-matrices-and-linear-assignment",style:{position:"relative"}},r.createElement(t.a,{href:"#doubly-stochastic-matrices-and-linear-assignment","aria-label":"doubly stochastic matrices and linear assignment permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"doubly stochastic matrices and linear assignment"),"\n",r.createElement(t.p,null,"A ",r.createElement(i.A,null,"permutation matrix"),' is a binary matrix with exactly one "1" in each row and each column, and zeros elsewhere. If I want to sample a permutation ',r.createElement(o.A,{text:"\\( P \\)"})," from some distribution, that is effectively a discrete structure. But I can approximate permutations with ",r.createElement(i.A,null,"doubly stochastic matrices"),", which have nonnegative entries and each row and column sums to 1."),"\n",r.createElement(t.p,null,"The ",r.createElement(i.A,null,"Sinkhorn operator")," is a function that projects or normalizes any positive matrix into a doubly stochastic matrix via iterative row and column normalization."),"\n",r.createElement(t.h3,{id:"relaxing-permutations-with-the-sinkhorn-operator",style:{position:"relative"}},r.createElement(t.a,{href:"#relaxing-permutations-with-the-sinkhorn-operator","aria-label":"relaxing permutations with the sinkhorn operator permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"relaxing permutations with the sinkhorn operator"),"\n",r.createElement(t.p,null,"The ",r.createElement(i.A,null,"Gumbel-Sinkhorn trick")," (Mena and gang, ICLR 2018) extends the Gumbel-max trick to permutations. Instead of sampling ",r.createElement(o.A,{text:"\\( \\arg\\max \\)"})," across rows, I inject Gumbel noise into a matrix and then apply the Sinkhorn normalization repeatedly:"),"\n",r.createElement(o.A,{text:"\\[\nS = \\text{Sinkhorn}\\Bigl(\\frac{Z}{\\tau}\\Bigr),\n\\]"}),"\n",r.createElement(t.p,null,"where ",r.createElement(o.A,{text:"\\( Z \\)"})," is a ",r.createElement(o.A,{text:"\\( n \\times n \\)"})," matrix built as ",r.createElement(o.A,{text:"\\( Z_{ij} = \\log \\Pi_{ij} + G_{ij} \\)"})," (for a distribution ",r.createElement(o.A,{text:"\\( \\Pi \\)"})," over permutations). The result ",r.createElement(o.A,{text:"\\( S \\)"})," is close to a permutation matrix but is differentiable with respect to ",r.createElement(o.A,{text:"\\( Z \\)"}),"."),"\n",r.createElement(t.h3,{id:"gumbel-matching-vs-gumbel-sinkhorn-distributions",style:{position:"relative"}},r.createElement(t.a,{href:"#gumbel-matching-vs-gumbel-sinkhorn-distributions","aria-label":"gumbel matching vs gumbel sinkhorn distributions permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"gumbel-matching vs. gumbel-sinkhorn distributions"),"\n",r.createElement(t.p,null,"The original ",r.createElement(i.A,null,"Gumbel-matching")," approach picks the single best matching with ",r.createElement(o.A,{text:"\\( \\arg\\max \\)"})," operations. Gumbel-Sinkhorn relaxes that to produce a ",r.createElement(i.A,null,"doubly stochastic matrix"),". As ",r.createElement(o.A,{text:"\\( \\tau \\rightarrow 0 \\)"}),", ",r.createElement(o.A,{text:"\\( S \\)"})," becomes closer to a true permutation matrix. For moderate or high temperatures, ",r.createElement(o.A,{text:"\\( S \\)"})," is a fuzzy or partial assignment."),"\n",r.createElement(t.h3,{id:"implementation-of-sinkhorn-and-the-matching-function",style:{position:"relative"}},r.createElement(t.a,{href:"#implementation-of-sinkhorn-and-the-matching-function","aria-label":"implementation of sinkhorn and the matching function permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"implementation of sinkhorn and the matching function"),"\n",r.createElement(t.p,null,"The ",r.createElement(i.A,null,"Sinkhorn operator")," can be implemented as:"),"\n",r.createElement(l.A,{text:"\nimport torch\n\ndef sinkhorn(log_alpha, n_iters=10):\n    # log_alpha: (batch_size, n, n)\n    # returns a (batch_size, n, n) doubly stochastic matrix\n    # exponentiate\n    alpha = torch.exp(log_alpha)\n    for _ in range(n_iters):\n        # row normalization\n        alpha = alpha / (torch.sum(alpha, dim=2, keepdim=True) + 1e-9)\n        # column normalization\n        alpha = alpha / (torch.sum(alpha, dim=1, keepdim=True) + 1e-9)\n    return alpha\n"}),"\n",r.createElement(t.p,null,"Then, if I want to incorporate Gumbel noise, I can do:"),"\n",r.createElement(l.A,{text:"\ndef gumbel_sinkhorn_sample(logits, tau, n_iters=10):\n    # logits: (batch_size, n, n)\n    gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits) + 1e-20) + 1e-20)\n    log_alpha = (logits + gumbel_noise) / tau\n    # apply Sinkhorn\n    S = sinkhorn(log_alpha, n_iters=n_iters)\n    return S\n"}),"\n",r.createElement(t.p,null,"The matrix ",r.createElement(o.A,{text:"\\( S \\)"})," is a relaxed permutation. If I want to get a hard permutation, I can apply an ",r.createElement(o.A,{text:"\\( \\arg\\max \\)"})," row/column selection on ",r.createElement(o.A,{text:"\\( S \\)"}),"."),"\n",r.createElement(t.h3,{id:"sampling-permutation-matrices-in-a-differentiable-way",style:{position:"relative"}},r.createElement(t.a,{href:"#sampling-permutation-matrices-in-a-differentiable-way","aria-label":"sampling permutation matrices in a differentiable way permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"sampling permutation matrices in a differentiable way"),"\n",r.createElement(t.p,null,"Since ",r.createElement(o.A,{text:"\\( S \\)"})," is continuous in the forward pass, I can backpropagate through it. As ",r.createElement(o.A,{text:"\\( \\tau \\)"})," decreases, ",r.createElement(o.A,{text:"\\( S \\)"})," approaches a permutation matrix, but might cause sharper gradients. One must tune or anneal ",r.createElement(o.A,{text:"\\( \\tau \\)"}),"."),"\n",r.createElement(t.h3,{id:"applications-to-sorting-ranking-and-structured-prediction-tasks",style:{position:"relative"}},r.createElement(t.a,{href:"#applications-to-sorting-ranking-and-structured-prediction-tasks","aria-label":"applications to sorting ranking and structured prediction tasks permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"applications to sorting, ranking, and structured prediction tasks"),"\n",r.createElement(t.p,null,"The Gumbel-Sinkhorn approach has been used in tasks like:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Neural sorting and ranking"),". Approximating the top positions in a ranking as a continuous operation."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Linear assignment problems"),". If I want to solve a matching problem in a deep learning context, the Gumbel-Sinkhorn approach can embed that optimization in the network."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Structured prediction"),". Many structured outputs (like permutations of tokens) can be handled with differentiable approximation methods."),"\n"),"\n",r.createElement(t.h2,{id:"learning-latent-permutations-in-practice",style:{position:"relative"}},r.createElement(t.a,{href:"#learning-latent-permutations-in-practice","aria-label":"learning latent permutations in practice permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"learning latent permutations in practice"),"\n",r.createElement(t.h3,{id:"sinkhorn-based-convolutional-networks",style:{position:"relative"}},r.createElement(t.a,{href:"#sinkhorn-based-convolutional-networks","aria-label":"sinkhorn based convolutional networks permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"sinkhorn-based convolutional networks"),"\n",r.createElement(t.p,null,"Imagine a scenario where I have a convolutional neural network that's supposed to reorder or unscramble an input. For instance, I might take an image, split it into patches, shuffle the patches, and feed that scrambled image into a network that must unscramble it by predicting the correct permutation of patches."),"\n",r.createElement(t.h3,{id:"the-unscrambling-mnist-digits-example",style:{position:"relative"}},r.createElement(t.a,{href:"#the-unscrambling-mnist-digits-example","aria-label":"the unscrambling mnist digits example permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"the unscrambling mnist digits example"),"\n",r.createElement(t.p,null,"A canonical example is unscrambling an MNIST digit that has been cut into ",r.createElement(o.A,{text:"\\( n \\times n \\)"})," patches and permuted randomly. The network attempts to find the correct permutation so that the patches line up to reconstruct the original digit."),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Data"),": For each MNIST digit, generate random permutations of the ",r.createElement(o.A,{text:"\\( n^2 \\)"})," patches."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Model"),": Takes the scrambled patches as input, produces ",r.createElement(o.A,{text:"\\( \\log \\Pi \\)"})," of shape ",r.createElement(o.A,{text:"\\( n^2 \\times n^2 \\)"}),"."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Apply Gumbel-Sinkhorn"),": Yields a doubly stochastic matrix ",r.createElement(o.A,{text:"\\( S \\)"}),"."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Reconstruct")," the unscrambled image using ",r.createElement(o.A,{text:"\\( S \\)"}),"."),"\n"),"\n",r.createElement(t.h3,{id:"network-architecture-and-training-procedure",style:{position:"relative"}},r.createElement(t.a,{href:"#network-architecture-and-training-procedure","aria-label":"network architecture and training procedure permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"network architecture and training procedure"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Feature extractor"),": A small CNN or MLP that processes each patch or the entire scrambled image."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Permutation predictor"),": Outputs a matrix of shape ",r.createElement(o.A,{text:"\\( n^2 \\times n^2 \\)"})," representing the cost/logits for assigning patch ",r.createElement(o.A,{text:"\\( i \\)"})," to location ",r.createElement(o.A,{text:"\\( j \\)"}),"."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Gumbel-Sinkhorn"),": Produces the continuous approximation ",r.createElement(o.A,{text:"\\( S \\)"}),"."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Reconstruction loss"),": After applying the (relaxed) permutation to the patches, measure the reconstruction error against the original unscrambled image."),"\n"),"\n",r.createElement(t.p,null,"In practice, you might do an MSE or cross-entropy reconstruction loss on the unscrambled image."),"\n",r.createElement(t.h3,{id:"evaluating-permutation-accuracy-kendall-tau",style:{position:"relative"}},r.createElement(t.a,{href:"#evaluating-permutation-accuracy-kendall-tau","aria-label":"evaluating permutation accuracy kendall tau permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"evaluating permutation accuracy (kendall-tau)"),"\n",r.createElement(t.p,null,"A typical metric for permutations is ",r.createElement(i.A,null,"Kendall-Tau"),", which measures how many pairwise inversions differ from the ground truth permutation. Alternatively, we can measure the fraction of patches correctly placed."),"\n",r.createElement(t.p,null,"During inference, if needed, we can take ",r.createElement(o.A,{text:"\\( \\arg\\max \\)"})," row-wise or column-wise on ",r.createElement(o.A,{text:"\\( S \\)"})," to obtain a discrete permutation. Then we measure how many patches match their correct positions."),"\n",r.createElement(t.h3,{id:"sample-results-and-visualization",style:{position:"relative"}},r.createElement(t.a,{href:"#sample-results-and-visualization","aria-label":"sample results and visualization permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"sample results and visualization"),"\n",r.createElement(t.p,null,"One might visualize the scrambled images and their unscrambled counterparts to see if the model is truly learning a good ordering. This can be done by generating plots of the input, the predicted unscrambled output, and a difference overlay."),"\n",r.createElement(a,{alt:"Example of unscrambled MNIST digits",path:"",caption:"An illustration of applying Gumbel-Sinkhorn to unscramble image patches.",zoom:"false"}),"\n",r.createElement(t.h2,{id:"graph-sampling-and-neural-relational-inference",style:{position:"relative"}},r.createElement(t.a,{href:"#graph-sampling-and-neural-relational-inference","aria-label":"graph sampling and neural relational inference permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"graph sampling and neural relational inference"),"\n",r.createElement(t.h3,{id:"overview-of-graph-based-latent-variable-models",style:{position:"relative"}},r.createElement(t.a,{href:"#overview-of-graph-based-latent-variable-models","aria-label":"overview of graph based latent variable models permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"overview of graph-based latent variable models"),"\n",r.createElement(t.p,null,"Many systems, from social networks to physical systems, revolve around interactions among entities represented as graphs. In some settings, the graph is not known a priori; we want to learn the connectivity structure. For instance, in ",r.createElement(i.A,null,"Neural Relational Inference")," (Kipf and gang, ICML 2018), we observe the motions of particles that might be connected by unseen springs, and we want to discover the underlying interaction graph."),"\n",r.createElement(t.h3,{id:"encoding-and-decoding-graph-structures-in-vaes",style:{position:"relative"}},r.createElement(t.a,{href:"#encoding-and-decoding-graph-structures-in-vaes","aria-label":"encoding and decoding graph structures in vaes permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"encoding and decoding graph structures in vaes"),"\n",r.createElement(t.p,null,"One approach is to treat the adjacency matrix ",r.createElement(o.A,{text:"\\( A \\)"})," of the graph as a latent variable. If there are ",r.createElement(o.A,{text:"\\( n \\)"})," nodes, ",r.createElement(o.A,{text:"\\( A \\)"})," is an ",r.createElement(o.A,{text:"\\( n \\times n \\)"})," matrix of 0/1 entries. We can place a Bernoulli distribution on each edge, or a categorical distribution if there are multiple edge types."),"\n",r.createElement(t.p,null,"We'd then define:"),"\n",r.createElement(o.A,{text:"\\( p_\\theta(\\text{observations} | A) \\)"}),"\n",r.createElement(t.p,null,"which might be a physics simulation or a message-passing network that uses the adjacency to define interactions. Then the posterior ",r.createElement(o.A,{text:"\\( q_\\phi(A|\\text{observations}) \\)"})," can be approximated by a neural network. But sampling discrete adjacency leads to the same differentiability issue."),"\n",r.createElement(t.h3,{id:"using-gumbel-softmax-for-edge-sampling",style:{position:"relative"}},r.createElement(t.a,{href:"#using-gumbel-softmax-for-edge-sampling","aria-label":"using gumbel softmax for edge sampling permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"using gumbel-softmax for edge sampling"),"\n",r.createElement(t.p,null,"By applying a Gumbel-softmax approach to each pair of nodes, we can produce a ",r.createElement(i.A,null,"continuous relaxation")," of the adjacency matrix. For an ",r.createElement(o.A,{text:"\\( n \\times n \\)"})," adjacency, each entry is a Bernoulli or multi-class variable. Then the Gumbel-softmax reparameterization can be used to generate a smoothed adjacency matrix."),"\n",r.createElement(t.h3,{id:"learning-interaction-networks-springs-example",style:{position:"relative"}},r.createElement(t.a,{href:"#learning-interaction-networks-springs-example","aria-label":"learning interaction networks springs example permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"learning interaction networks (springs example)"),"\n",r.createElement(t.p,null,"In the ",r.createElement(i.A,null,"springs example"),", we have ",r.createElement(o.A,{text:"\\( n \\)"})," particles connected by unknown springs. We see their positions over time. The adjacency matrix tells us which pairs of particles are connected. By using the Gumbel-softmax adjacency, the neural network can infer which pairs are connected (or strongly interacting) in a fully differentiable manner. This forms the basis of a ",r.createElement(i.A,null,"Neural Relational Inference")," VAE-like system."),"\n",r.createElement(t.p,null,"Kipf and gang (ICML 2018) show that this approach can learn the correct adjacency in many synthetic physics simulations and also generalize to unseen conditions."),"\n",r.createElement(t.h3,{id:"visualizing-discovered-graphs-and-analyzing-results",style:{position:"relative"}},r.createElement(t.a,{href:"#visualizing-discovered-graphs-and-analyzing-results","aria-label":"visualizing discovered graphs and analyzing results permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"visualizing discovered graphs and analyzing results"),"\n",r.createElement(t.p,null,"A valuable step is to visualize the learned adjacency matrix, especially in a small example with a handful of particles. You can compare it to the ground truth adjacency to see if the model picks out the correct structure. Sometimes the model might produce soft adjacency values that reflect partial confidence in certain edges."),"\n",r.createElement(t.h2,{id:"beyond-gumbel-other-discrete-reparameterization-approaches",style:{position:"relative"}},r.createElement(t.a,{href:"#beyond-gumbel-other-discrete-reparameterization-approaches","aria-label":"beyond gumbel other discrete reparameterization approaches permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"beyond gumbel: other discrete reparameterization approaches"),"\n",r.createElement(t.h3,{id:"binary-concrete-continuous-bernoulli-distributions-for-binary-latent-variables",style:{position:"relative"}},r.createElement(t.a,{href:"#binary-concrete-continuous-bernoulli-distributions-for-binary-latent-variables","aria-label":"binary concrete continuous bernoulli distributions for binary latent variables permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"binary concrete (continuous bernoulli) distributions for binary latent variables"),"\n",r.createElement(t.p,null,"If the discrete variable is binary (",r.createElement(o.A,{text:"\\( 0 \\)"})," or ",r.createElement(o.A,{text:"\\( 1 \\)"}),"), the ",r.createElement(i.A,null,"binary concrete")," distribution is a specialized version of Gumbel-softmax for ",r.createElement(o.A,{text:"\\( K = 2 \\)"}),". Another option is ",r.createElement(i.A,null,"sigmoid-based relaxations"),", which can also work but might have different gradient properties."),"\n",r.createElement(t.h3,{id:"reinforce-style-estimators-with-variance-reduction-baseline-input-dependent-etc",style:{position:"relative"}},r.createElement(t.a,{href:"#reinforce-style-estimators-with-variance-reduction-baseline-input-dependent-etc","aria-label":"reinforce style estimators with variance reduction baseline input dependent etc permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"reinforce-style estimators with variance reduction (baseline, input-dependent, etc.)"),"\n",r.createElement(t.p,null,r.createElement(i.A,null,"REINFORCE")," is a classic alternative that does not rely on relaxing the distribution. Instead, we compute ",r.createElement(o.A,{text:"\\( \\nabla \\log p_\\theta(z) \\)"})," for discrete ",r.createElement(o.A,{text:"\\( z \\)"}),". It's an unbiased estimator but can have large variance. Adding a baseline or a learned value function can help."),"\n",r.createElement(t.h3,{id:"relax-and-rebar-advanced-variance-reduced-gradient-estimators",style:{position:"relative"}},r.createElement(t.a,{href:"#relax-and-rebar-advanced-variance-reduced-gradient-estimators","aria-label":"relax and rebar advanced variance reduced gradient estimators permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"relax and rebar: advanced variance-reduced gradient estimators"),"\n",r.createElement(t.p,null,r.createElement(i.A,null,"REBAR")," (Tucker and gang, ICLR 2017) and ",r.createElement(i.A,null,"RELAX")," (Grathwohl and gang, ICLR 2018) attempt to combine the best of both worlds: they use the continuous Gumbel-softmax variable as a control variate and an additional function to correct for bias, yielding a gradient estimator with lower variance than pure REINFORCE while still being theoretically grounded."),"\n",r.createElement(t.h3,{id:"trade-offs-in-bias-vs-variance-and-model-complexity",style:{position:"relative"}},r.createElement(t.a,{href:"#trade-offs-in-bias-vs-variance-and-model-complexity","aria-label":"trade offs in bias vs variance and model complexity permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"trade-offs in bias vs. variance and model complexity"),"\n",r.createElement(t.p,null,"When choosing among Gumbel-softmax, REINFORCE, RELAX, or other gradient estimators, you weigh:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Bias"),": Gumbel-softmax introduces a bias since the sample is never truly discrete. But in the limit of ",r.createElement(o.A,{text:"\\( \\tau \\to 0 \\)"}),", the bias becomes negligible."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Variance"),": REINFORCE can be high-variance, but sophisticated control variates can reduce it. Gumbel-softmax typically has moderate variance."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Implementation complexity"),": Gumbel-softmax is straightforward to implement. RELAX or REBAR are more advanced."),"\n"),"\n",r.createElement(t.h3,{id:"practical-guidelines-on-choosing-an-estimator",style:{position:"relative"}},r.createElement(t.a,{href:"#practical-guidelines-on-choosing-an-estimator","aria-label":"practical guidelines on choosing an estimator permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"practical guidelines on choosing an estimator"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Scale of the problem"),": Gumbel-softmax can handle moderate or large-scale discrete problems easily, but might degrade in extremely large combinatorial spaces."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Exactness vs. efficiency"),": If a small bias is acceptable, Gumbel-based relaxations are often simpler. If you need unbiased estimates, consider REINFORCE or REBAR/RELAX with advanced variance-reduction."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Computational cost"),": Some methods require additional neural networks to approximate baselines or local expectations."),"\n"),"\n",r.createElement(t.h2,{id:"future-directions-and-conclusion",style:{position:"relative"}},r.createElement(t.a,{href:"#future-directions-and-conclusion","aria-label":"future directions and conclusion permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"future directions and conclusion"),"\n",r.createElement(t.h3,{id:"summary-of-key-takeaways",style:{position:"relative"}},r.createElement(t.a,{href:"#summary-of-key-takeaways","aria-label":"summary of key takeaways permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"summary of key takeaways"),"\n",r.createElement(t.p,null,"Throughout this article, I've dived into how ",r.createElement(i.A,null,"Gumbel-based reparameterization")," can open the door to end-to-end differentiable training for discrete random variables. The fundamental pipeline is:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,"Express a discrete draw as ",r.createElement(o.A,{text:"\\( \\log \\pi_i + G_i \\)"})," with ",r.createElement(o.A,{text:"\\( G_i \\sim \\text{Gumbel}(0,1) \\)"}),"."),"\n",r.createElement(t.li,null,"Replace ",r.createElement(o.A,{text:"\\( \\arg\\max \\)"})," or ",r.createElement(o.A,{text:"\\( \\text{top-k} \\)"})," or ",r.createElement(o.A,{text:"\\( \\text{permutation} \\)"})," with a ",r.createElement(i.A,null,"continuous relaxation")," that can be backpropagated through."),"\n",r.createElement(t.li,null,"Use temperature ",r.createElement(o.A,{text:"\\( \\tau \\)"})," to balance the trade-off between discreteness and smoothness."),"\n"),"\n",r.createElement(t.p,null,"This approach generalizes to:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Categorical variables")," (Gumbel-softmax)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Subsets")," (top-k Gumbel)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Permutations")," (Gumbel-Sinkhorn)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Graphs")," (edge sampling with Gumbel-softmax)."),"\n"),"\n",r.createElement(t.h3,{id:"advanced-topics-reinforcement-learning-policy-gradients-and-discrete-controls",style:{position:"relative"}},r.createElement(t.a,{href:"#advanced-topics-reinforcement-learning-policy-gradients-and-discrete-controls","aria-label":"advanced topics reinforcement learning policy gradients and discrete controls permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"advanced topics: reinforcement learning, policy gradients, and discrete controls"),"\n",r.createElement(t.p,null,"Discrete sampling arises in reinforcement learning for selecting actions. Although Gumbel-based methods can be used to approximate policy gradients, the standard approach in RL is to use policy gradient estimators or Q-learning-based methods. However, there is ongoing work that merges Gumbel reparameterization with policy optimization to reduce variance or to incorporate discrete structures in more complex environments."),"\n",r.createElement(t.h3,{id:"potential-improvements-and-research-trends",style:{position:"relative"}},r.createElement(t.a,{href:"#potential-improvements-and-research-trends","aria-label":"potential improvements and research trends permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"potential improvements and research trends"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Better temperature schedules"),": Learning or adapting the temperature automatically is a hot research direction, so that the model can choose the right level of discreteness."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Hybrid methods"),": Combining Gumbel-softmax with a baseline network (as in RELAX) might yield lower variance."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Structured compositional tasks"),": New research addresses hierarchical subsets or permutations, e.g. building trees with Gumbel-based sampling."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Larger combinatorial spaces"),": Ongoing research explores parallelization and specialized approximations for extremely large discrete sets (like sequences)."),"\n"),"\n",r.createElement(t.h3,{id:"final-remarks-and-further-reading",style:{position:"relative"}},r.createElement(t.a,{href:"#final-remarks-and-further-reading","aria-label":"final remarks and further reading permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"final remarks and further reading"),"\n",r.createElement(t.p,null,"In sum, sampling in deep learning must often address the question of how to deal with discrete variables in a differentiable way. The Gumbel approach, along with related reparameterization techniques, has become a foundational tool for training discrete latent variable models. While no single solution works best in all scenarios, Gumbel-based methods are widely used due to their relative simplicity, efficiency, and flexibility."),"\n",r.createElement(t.p,null,"Those interested in further details may consult:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,'Jang and gang, "Categorical Reparameterization with Gumbel-Softmax" (ICLR 2017).'),"\n",r.createElement(t.li,null,'Maddison and gang, "The Concrete Distribution" (ICLR 2017).'),"\n",r.createElement(t.li,null,'Mena and gang, "Learning Latent Permutations with Gumbel-Sinkhorn" (ICLR 2018).'),"\n",r.createElement(t.li,null,'Tucker and gang, "REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models" (ICLR 2017).'),"\n",r.createElement(t.li,null,'Grathwohl and gang, "Backpropagation through the void: Optimizing control variates for black-box gradient estimation" (ICLR 2018).'),"\n"),"\n",r.createElement(a,{alt:"Generic schematic of Gumbel-based sampling",path:"",caption:"Illustration of Gumbel noise being added to logits, then passed through a continuous function (like a softmax or sinkhorn normalization).",zoom:"false"}),"\n",r.createElement(t.p,null,"The ability to incorporate discrete structures while still retaining end-to-end trainability unlocks many new directions in generative modeling, combinatorial optimization, and structured prediction. With these methods in hand, you can design neural architectures that tackle discrete decisions at scale without sacrificing the advantages of gradient-based learning."))}var c=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,n.RP)(),e.components);return t?r.createElement(t,e,r.createElement(s,e)):s(e)};var m=a(54506),h=a(88864),d=a(58481),u=a.n(d),p=a(5984),g=a(43672),f=a(27042),b=a(72031),v=a(81817),E=a(27105),x=a(17265),y=a(2043),w=a(95751),k=a(94328),S=a(80791),A=a(78137);const _=e=>{let{toc:t}=e;if(!t||!t.items)return null;return r.createElement("nav",{className:S.R},r.createElement("ul",null,t.items.map(((e,t)=>r.createElement("li",{key:t},r.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const a=t.replace("#",""),n=document.getElementById(a);n&&n.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&r.createElement(_,{toc:{items:e.items}}))))))};function z(e){let{data:{mdx:t,allMdx:i,allPostImages:l},children:o}=e;const{frontmatter:s,body:c,tableOfContents:h}=t,d=s.index,b=s.slug.split("/")[1],S=i.nodes.filter((e=>e.frontmatter.slug.includes(`/${b}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),z=S.findIndex((e=>e.frontmatter.index===d)),H=S[z+1],I=S[z-1],C=s.slug.replace(/\/$/,""),T=/[^/]*$/.exec(C)[0],M=`posts/${b}/content/${T}/`,{0:V,1:G}=(0,r.useState)(s.flagWideLayoutByDefault),{0:L,1:B}=(0,r.useState)(!1);var N;(0,r.useEffect)((()=>{B(!0);const e=setTimeout((()=>B(!1)),340);return()=>clearTimeout(e)}),[V]),"adventures"===b?N=x.cb:"research"===b?N=x.Qh:"thoughts"===b&&(N=x.T6);const R=u()(c).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,O=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),a=e%60;return a<=30?`~${t}${a>0?".5":""} h`:`~${t+1} h`}(Math.ceil(R/N)+(s.extraReadTimeMin||0)),P=[{flag:s.flagDraft,component:()=>Promise.all([a.e(5850),a.e(9833)]).then(a.bind(a,49833))},{flag:s.flagMindfuckery,component:()=>Promise.all([a.e(5850),a.e(7805)]).then(a.bind(a,27805))},{flag:s.flagRewrite,component:()=>Promise.all([a.e(5850),a.e(8916)]).then(a.bind(a,78916))},{flag:s.flagOffensive,component:()=>Promise.all([a.e(5850),a.e(6731)]).then(a.bind(a,49112))},{flag:s.flagProfane,component:()=>Promise.all([a.e(5850),a.e(3336)]).then(a.bind(a,83336))},{flag:s.flagMultilingual,component:()=>Promise.all([a.e(5850),a.e(2343)]).then(a.bind(a,62343))},{flag:s.flagUnreliably,component:()=>Promise.all([a.e(5850),a.e(6865)]).then(a.bind(a,11627))},{flag:s.flagPolitical,component:()=>Promise.all([a.e(5850),a.e(4417)]).then(a.bind(a,24417))},{flag:s.flagCognitohazard,component:()=>Promise.all([a.e(5850),a.e(8669)]).then(a.bind(a,18669))},{flag:s.flagHidden,component:()=>Promise.all([a.e(5850),a.e(8124)]).then(a.bind(a,48124))}],{0:j,1:K}=(0,r.useState)([]);return(0,r.useEffect)((()=>{P.forEach((e=>{let{flag:t,component:a}=e;t&&a().then((e=>{K((t=>[].concat((0,m.A)(t),[e.default])))}))}))}),[]),r.createElement(f.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},r.createElement(v.A,{postNumber:s.index,date:s.date,updated:s.updated,readTime:O,difficulty:s.difficultyLevel,title:s.title,desc:s.desc,banner:s.banner,section:b,postKey:T,isMindfuckery:s.flagMindfuckery,mainTag:s.mainTag}),r.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},s.otherTags.map(((e,t)=>r.createElement("span",{key:t,className:`noselect ${A.MW}`,style:{margin:"0 5px 5px 0"}},e)))),r.createElement("div",{className:"postBody"},r.createElement(_,{toc:h})),r.createElement("br",null),r.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},r.createElement(f.P.button,{className:`noselect ${k.pb}`,id:k.xG,onClick:()=>{G(!V)},whileTap:{scale:.93}},r.createElement(f.P.div,{className:w.DJ,key:V,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},V?"Switch to default layout":"Switch to wide layout"))),r.createElement("br",null),r.createElement("div",{className:"postBody",style:{margin:V?"0 -14%":"",maxWidth:V?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},r.createElement("div",{className:`${k.P_} ${L?k.Xn:k.qG}`},j.map(((e,t)=>r.createElement(e,{key:t}))),s.indexCourse?r.createElement(y.A,{index:s.indexCourse,category:s.courseCategoryName}):"",r.createElement(p.Z.Provider,{value:{images:l.nodes,basePath:M.replace(/\/$/,"")+"/"}},r.createElement(n.xA,{components:{Image:g.A}},o)))),r.createElement(E.A,{nextPost:H,lastPost:I,keyCurrent:T,section:b}))}function H(e){return r.createElement(z,e,r.createElement(c,e))}function I(e){var t,a,n,i,l;let{data:o}=e;const{frontmatter:s}=o.mdx,c=s.titleSEO||s.title,m=s.titleOG||c,d=s.titleTwitter||c,u=s.descSEO||s.desc,p=s.descOG||u,g=s.descTwitter||u,f=s.schemaType||"BlogPosting",v=s.keywordsSEO,E=s.date,x=s.updated||E,y=s.imageOG||(null===(t=s.banner)||void 0===t||null===(a=t.childImageSharp)||void 0===a||null===(n=a.gatsbyImageData)||void 0===n||null===(i=n.images)||void 0===i||null===(l=i.fallback)||void 0===l?void 0:l.src),w=s.imageAltOG||p,k=s.imageTwitter||y,S=s.imageAltTwitter||g,A=s.canonicalURL,_=s.flagHidden||!1,z=s.mainTag||"Posts",H=s.slug.split("/")[1]||"posts",{siteUrl:I}=(0,h.Q)(),C={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:I},{"@type":"ListItem",position:2,name:z,item:`${I}/${s.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${I}${s.slug}`}]};return r.createElement(b.A,{title:c+" - avrtt.blog",titleOG:m,titleTwitter:d,description:u,descriptionOG:p,descriptionTwitter:g,schemaType:f,keywords:v,datePublished:E,dateModified:x,imageOG:y,imageAltOG:w,imageTwitter:k,imageAltTwitter:S,canonicalUrl:A,flagHidden:_,mainTag:z,section:H,type:"article"},r.createElement("script",{type:"application/ld+json"},JSON.stringify(C)))}},90548:function(e,t,a){var n=a(96540),r=a(7978);t.A=e=>{let{text:t}=e;return n.createElement(r.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-tsx-content-file-path-src-pages-posts-research-sampling-in-depth-mdx-0fced9b3b2d161ecc836.js.map