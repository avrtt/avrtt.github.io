"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[1278],{3962:function(e,t){t.A="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iaXNvLTg4NTktMSI/Pgo8IS0tIEdlbmVyYXRvcjogQWRvYmUgSWxsdXN0cmF0b3IgMTYuMC4wLCBTVkcgRXhwb3J0IFBsdWctSW4gLiBTVkcgVmVyc2lvbjogNi4wMCBCdWlsZCAwKSAgLS0+CjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+CjxzdmcgdmVyc2lvbj0iMS4xIiBpZD0iQ2FwYV8xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB4PSIwcHgiIHk9IjBweCIKCSB3aWR0aD0iNDE2Ljk3OXB4IiBoZWlnaHQ9IjQxNi45NzlweCIgdmlld0JveD0iMCAwIDQxNi45NzkgNDE2Ljk3OSIgc3R5bGU9ImVuYWJsZS1iYWNrZ3JvdW5kOm5ldyAwIDAgNDE2Ljk3OSA0MTYuOTc5OyIKCSB4bWw6c3BhY2U9InByZXNlcnZlIj4KPGc+Cgk8cGF0aCBkPSJNMzU2LjAwNCw2MS4xNTZjLTgxLjM3LTgxLjQ3LTIxMy4zNzctODEuNTUxLTI5NC44NDgtMC4xODJjLTgxLjQ3LDgxLjM3MS04MS41NTIsMjEzLjM3OS0wLjE4MSwyOTQuODUKCQljODEuMzY5LDgxLjQ3LDIxMy4zNzgsODEuNTUxLDI5NC44NDksMC4xODFDNDM3LjI5MywyNzQuNjM2LDQzNy4zNzUsMTQyLjYyNiwzNTYuMDA0LDYxLjE1NnogTTIzNy42LDM0MC43ODYKCQljMCwzLjIxNy0yLjYwNyw1LjgyMi01LjgyMiw1LjgyMmgtNDYuNTc2Yy0zLjIxNSwwLTUuODIyLTIuNjA1LTUuODIyLTUuODIyVjE2Ny44ODVjMC0zLjIxNywyLjYwNy01LjgyMiw1LjgyMi01LjgyMmg0Ni41NzYKCQljMy4yMTUsMCw1LjgyMiwyLjYwNCw1LjgyMiw1LjgyMlYzNDAuNzg2eiBNMjA4LjQ5LDEzNy45MDFjLTE4LjYxOCwwLTMzLjc2Ni0xNS4xNDYtMzMuNzY2LTMzLjc2NQoJCWMwLTE4LjYxNywxNS4xNDctMzMuNzY2LDMzLjc2Ni0zMy43NjZjMTguNjE5LDAsMzMuNzY2LDE1LjE0OCwzMy43NjYsMzMuNzY2QzI0Mi4yNTYsMTIyLjc1NSwyMjcuMTA3LDEzNy45MDEsMjA4LjQ5LDEzNy45MDF6Ii8+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPC9zdmc+Cg=="},9360:function(e,t,a){a.d(t,{A:function(){return r}});var n=a(96540),i=a(3962),l="styles-module--tooltiptext--a263b";var r=e=>{let{text:t,isBadge:a=!1}=e;const{0:r,1:s}=(0,n.useState)(!1),c=(0,n.useRef)(null);return(0,n.useEffect)((()=>{function e(e){c.current&&e.target instanceof Node&&!c.current.contains(e.target)&&s(!1)}return document.addEventListener("click",e),()=>{document.removeEventListener("click",e)}}),[]),n.createElement("span",{className:"styles-module--tooltipWrapper--75ebf",ref:c},n.createElement("img",{id:a?"styles-module--infoBadge--e3d66":"styles-module--info--26c1f",src:i.A,alt:"info",onClick:e=>{e.stopPropagation(),s((e=>!e))}}),n.createElement("span",{className:r?`${l} styles-module--visible--c063c`:l},t))}},67570:function(e,t,a){a.r(t),a.d(t,{Head:function(){return I},PostTemplate:function(){return _},default:function(){return A}});var n=a(28453),i=a(96540),l=a(9360),r=a(61992),s=a(62087),c=a(90548);function o(e){const t=Object.assign({p:"p",h2:"h2",a:"a",span:"span",strong:"strong",h3:"h3",ul:"ul",li:"li",ol:"ol",h4:"h4",em:"em"},(0,n.RP)(),e.components),{Image:a}=t;return a||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),i.createElement(i.Fragment,null,"\n",i.createElement("br"),"\n","\n","\n","\n",i.createElement(t.p,null,'Classification metrics lie at the heart of any thorough evaluation of machine learning models that attempt to categorize data points (instances) into distinct classes. As soon as you venture beyond toy examples in binary classification, you realize that choosing the ""best"" metric is rarely a trivial task. The classification space offers many different ways to quantify performance, from the simple notion of overall accuracy to more nuanced measures that handle class imbalance, varying thresholds, or multi-class settings. A single model can exhibit starkly different performance profiles depending on which metrics you examine, and it is often necessary to combine multiple metrics or carefully select a relevant metric for your problem domain.'),"\n",i.createElement(t.p,null,"I find that classification metrics can be easily misunderstood or misapplied when one does not consider the details of a dataset or a model's underlying assumptions. Metrics such as ",i.createElement(r.A,null,"accuracy")," might be hugely misleading in domains with severe class imbalance (e.g., fraud detection, where only a tiny fraction of transactions is fraudulent). By contrast, metrics such as the ",i.createElement(r.A,null,"precision-recall curve")," or ",i.createElement(r.A,null,"Area under the Precision-Recall Curve (PR AUC)")," can provide a clearer picture in such domains. In other settings, especially with multiple classes, ",i.createElement(r.A,null,"macro"),", ",i.createElement(r.A,null,"micro"),", or ",i.createElement(r.A,null,"weighted")," averaging variants of F1 or other metrics can become crucial."),"\n",i.createElement(t.p,null,"In this article, I will walk you through core concepts and advanced considerations pertaining to classification metrics. Throughout, I will weave in references to cutting-edge research and highlight relevant use-cases, from typical academic tasks (like image classification from the ",i.createElement(l.A,{text:"ImageNet, CIFAR-10, etc."})," corpus) to specialized industry scenarios (like medical diagnosis, anomaly detection, or credit default prediction). At times, I will draw on some insights from papers that have appeared in conferences such as NeurIPS, ICML, and from journals such as the Journal of Machine Learning Research (JMLR)."),"\n",i.createElement(t.p,null,"Given the breadth and importance of classification metrics, I will start by clarifying essential terminology and continue toward sophisticated topics like threshold-based curves, advanced metrics for imbalanced data, and multi-class classification approaches. This article is aimed at readers with some experience in machine learning, but I will strive for clarity and thoroughness. The ultimate goal is to help you interpret, compare, and select the right classification metrics for your problem."),"\n",i.createElement(t.h2,{id:"basic-terminology",style:{position:"relative"}},i.createElement(t.a,{href:"#basic-terminology","aria-label":"basic terminology permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Basic terminology"),"\n",i.createElement(t.p,null,"Classification tasks involve predicting discrete labels for data instances. These tasks can be roughly divided into ",i.createElement(t.strong,null,"binary classification")," (two labels: e.g., positive or negative) and ",i.createElement(t.strong,null,"multiclass classification")," (two+ labels: e.g., {cat, dog, bird, horse, …}). Most classification metrics originate in the binary domain but can be extended or adapted for multiple classes in various ways."),"\n",i.createElement(t.h3,{id:"defining-binary-vs-multiclass-classification",style:{position:"relative"}},i.createElement(t.a,{href:"#defining-binary-vs-multiclass-classification","aria-label":"defining binary vs multiclass classification permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Defining binary vs. multiclass classification"),"\n",i.createElement(t.p,null,"In a ",i.createElement(t.strong,null,"binary classification"),' setting, we typically talk about a "positive" class and a "negative" class. For instance, in medical diagnostics, "positive" might indicate that a patient does have a particular condition and "negative" that the patient does not. In credit risk modeling, "positive" might refer to a borrower who will default. Even if the notion of "positive" is purely conventional, it helps unify the interpretation of various metrics: "positive" is the class of prime interest, where we suspect an elevated cost of errors or a higher business or scientific significance.'),"\n",i.createElement(t.p,null,"In a ",i.createElement(t.strong,null,"multiclass classification")," setting, there are more than two possible classes (e.g., {spam, promotional, important, social} in an email classification system). Certain terminologies (positive vs. negative) do not directly apply to the entire problem in the same way. Instead, we might break down the classification into multiple one-vs.-rest or one-vs.-one subtasks, or we might adopt specialized multiclass metrics."),"\n",i.createElement(t.h3,{id:"classes-labels-and-decision-boundaries",style:{position:"relative"}},i.createElement(t.a,{href:"#classes-labels-and-decision-boundaries","aria-label":"classes labels and decision boundaries permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Classes, labels, and decision boundaries"),"\n",i.createElement(t.p,null,"Consider the concept of a decision boundary: a model typically uses an internal rule or function (often learned from training data) to map an input ",i.createElement(c.A,{text:"\\( x \\)"})," to a probability ",i.createElement(c.A,{text:"\\( p \\)"})," that ",i.createElement(c.A,{text:"\\( x \\)"})," belongs to a certain class. In ",i.createElement(t.strong,null,"binary")," classification, for example, we might compare ",i.createElement(c.A,{text:"\\( p \\)"})," with a threshold ",i.createElement(c.A,{text:"\\(\\theta\\)"})," (commonly 0.5) and then produce a label of 1 if ",i.createElement(c.A,{text:"\\( p \\ge \\theta \\)"})," or 0 if ",i.createElement(c.A,{text:"\\( p < \\theta \\)"}),". The geometry or shape of the resulting decision boundary in feature space can vary drastically depending on the model type (linear, tree-based, neural network, etc.)."),"\n",i.createElement(t.h3,{id:"understanding-positive-and-negative-predictions",style:{position:"relative"}},i.createElement(t.a,{href:"#understanding-positive-and-negative-predictions","aria-label":"understanding positive and negative predictions permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Understanding positive and negative predictions"),"\n",i.createElement(t.p,null,'When we adopt the binary viewpoint, "positive prediction" means the model predicted label 1 (e.g., fraudulent, diseased, default). "Negative prediction" means the model predicted label 0 (e.g., legitimate, healthy, non-default). In the confusion matrix sections that follow, we will use these ideas to define ',i.createElement(t.strong,null,"true positive"),", ",i.createElement(t.strong,null,"false positive"),", ",i.createElement(t.strong,null,"true negative"),", and ",i.createElement(t.strong,null,"false negative")," predictions."),"\n",i.createElement(t.p,null,'For advanced tasks, especially in fields like computational biology or anomaly detection, this positive vs. negative nomenclature can become a bit arbitrary or domain-specific. Nonetheless, the core definitions for classification metrics remain consistent once you fix which label is "positive."'),"\n",i.createElement(t.h2,{id:"the-confusion-matrix",style:{position:"relative"}},i.createElement(t.a,{href:"#the-confusion-matrix","aria-label":"the confusion matrix permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"The confusion matrix"),"\n",i.createElement(t.p,null,"One of the most fundamental tools for evaluating classification results is the ",i.createElement(t.strong,null,"confusion matrix"),". In a ",i.createElement(t.strong,null,"binary classification")," setting, the confusion matrix is a 2×2 table that tabulates how many instances fall into each combination of actual and predicted classes."),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"True Positives (TP):"),' The model predicted "positive" and the actual label is indeed "positive."'),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"False Positives (FP):"),' The model predicted "positive" but the actual label is "negative."'),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"True Negatives (TN):"),' The model predicted "negative" and the actual label is indeed "negative."'),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"False Negatives (FN):"),' The model predicted "negative" but the actual label is actually "positive."'),"\n"),"\n",i.createElement(t.p,null,"You can visualize the confusion matrix as follows:"),"\n",i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">                Predicted Positive    Predicted Negative\nActual Positive        TP                   FN\nActual Negative        FP                   TN</code></pre></div>'}}),"\n",i.createElement(t.p,null,"Placing real values in this matrix is quite straightforward in practice:"),"\n",i.createElement(s.A,{text:"\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n\n# Example arrays (actual vs. predicted)\ny_true = np.array([1, 0, 1, 1, 0, 0, 1])\ny_pred = np.array([1, 0, 1, 0, 0, 1, 1])\n\ncm = confusion_matrix(y_true, y_pred, labels=[1,0])\n# By default, confusion_matrix returns rows in sorted order of labels,\n# which can sometimes be [0,1]. Here we force [1,0] for clarity if desired.\n\nprint(cm)\n"}),"\n",i.createElement(t.p,null,"For a typical binary classification confusion matrix, the shape will be (2, 2). Many times you might want to visually plot it:"),"\n",i.createElement(s.A,{text:'\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                              display_labels=["Positive","Negative"])\ndisp.plot()\nplt.show()\n'}),"\n",i.createElement(a,{alt:"A confusion matrix illustration",path:"",caption:"A typical confusion matrix illustrating TP, FP, TN, and FN.",zoom:"false"}),"\n",i.createElement(t.h3,{id:"common-pitfalls-in-reading-a-confusion-matrix",style:{position:"relative"}},i.createElement(t.a,{href:"#common-pitfalls-in-reading-a-confusion-matrix","aria-label":"common pitfalls in reading a confusion matrix permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Common pitfalls in reading a confusion matrix"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Forgetting about imbalance:"),' If one class (e.g., negative) is overwhelmingly more frequent than the positive class, the absolute values in the matrix can be misleading. You might see a large diagonal (e.g., huge TN) that dwarfs the other entries. This can give the false impression that the classifier is "highly accurate," while in reality it might be missing almost all of the minority class.'),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Mixing up rows vs. columns:")," Different textbooks or software libraries transpose the confusion matrix. Always confirm whether the rows or columns correspond to predicted vs. actual labels."),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Confusion about positive vs. negative:"),' You must decide which label is "positive." In medical or anomaly contexts, it is typically the less frequent or more "critical" condition.'),"\n"),"\n"),"\n",i.createElement(t.p,null,"With the confusion matrix in mind, we can now introduce the simplest and most common classification metrics derived from it."),"\n",i.createElement(t.h2,{id:"core-metrics-derived-from-the-confusion-matrix",style:{position:"relative"}},i.createElement(t.a,{href:"#core-metrics-derived-from-the-confusion-matrix","aria-label":"core metrics derived from the confusion matrix permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Core metrics derived from the confusion matrix"),"\n",i.createElement(t.h3,{id:"accuracy",style:{position:"relative"}},i.createElement(t.a,{href:"#accuracy","aria-label":"accuracy permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Accuracy"),"\n",i.createElement(t.p,null,i.createElement(c.A,{text:"\\( \\mathrm{Accuracy} \\)"})," is the simplest metric to understand: it is the proportion of correct predictions among all predictions. Formally,"),"\n",i.createElement(c.A,{text:"\\( \n\\mathrm{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}.\n\\)"}),"\n",i.createElement(t.p,null,"It tells us, in straightforward terms, the fraction of instances for which the predicted class aligns with the true class. An accuracy of 1.0 (or 100%) means the classifier is perfect on the test set."),"\n",i.createElement(t.h4,{id:"drawbacks-of-accuracy",style:{position:"relative"}},i.createElement(t.a,{href:"#drawbacks-of-accuracy","aria-label":"drawbacks of accuracy permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Drawbacks of accuracy"),"\n",i.createElement(t.p,null,"Accuracy can be ",i.createElement(t.strong,null,"highly misleading")," in the presence of ",i.createElement(t.strong,null,"imbalanced classes"),". Consider a dataset where only 1% of instances belong to the positive class (e.g., fraud detection). A naive classifier that predicts everything as negative obtains 99% accuracy, even though it fails to identify any fraud. This is why, in many practical contexts, we avoid relying solely on accuracy, or we complement it with more robust metrics."),"\n",i.createElement(s.A,{text:'\nfrom sklearn.metrics import accuracy_score\n\nacc = accuracy_score(y_true, y_pred)\nprint("Accuracy:", acc)\n'}),"\n",i.createElement(t.h3,{id:"precision",style:{position:"relative"}},i.createElement(t.a,{href:"#precision","aria-label":"precision permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Precision"),"\n",i.createElement(t.p,null,"Also called the ",i.createElement(t.strong,null,"positive predictive value (PPV)"),', precision answers the question: "Of all instances predicted positive, how many are actually positive?" Formally,'),"\n",i.createElement(c.A,{text:"\\( \n\\mathrm{Precision} = \\frac{TP}{TP + FP}.\n\\)"}),"\n",i.createElement(t.p,null,"A high precision means that, when a classifier flags an instance as positive, it is likely to be correct. In certain domains, such as spam detection or law enforcement, precision might be critical, because a false positive can be highly problematic (e.g., inconveniencing legitimate users or accusing innocents)."),"\n",i.createElement(t.h3,{id:"recall-sensitivity",style:{position:"relative"}},i.createElement(t.a,{href:"#recall-sensitivity","aria-label":"recall sensitivity permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Recall (sensitivity)"),"\n",i.createElement(t.p,null,"Also called ",i.createElement(t.strong,null,"true positive rate (TPR)")," or ",i.createElement(t.strong,null,"sensitivity"),', recall answers the question: "Out of all actual positive instances, how many does the classifier detect as positive?" Formally,'),"\n",i.createElement(c.A,{text:"\\( \n\\mathrm{Recall} = \\frac{TP}{TP + FN}.\n\\)"}),"\n",i.createElement(t.p,null,"A high recall means that the classifier succeeds in identifying most of the positives. In medical diagnostics, for instance, you might want to ensure that almost all genuinely sick patients receive further tests (i.e., you want to minimize false negatives). If the cost of missing a positive is extremely high, recall is an essential metric."),"\n",i.createElement(t.h3,{id:"f1-score",style:{position:"relative"}},i.createElement(t.a,{href:"#f1-score","aria-label":"f1 score permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"F1 score"),"\n",i.createElement(t.p,null,"Precision and recall often trade off against each other — improving one can degrade the other. The ",i.createElement(r.A,null,"F1 score")," (or ",i.createElement(t.strong,null,"F1")," measure) is the ",i.createElement(t.strong,null,"harmonic mean")," of precision and recall:"),"\n",i.createElement(c.A,{text:"\\[\n\\mathrm{F1} = 2 \\cdot \\frac{\\mathrm{Precision} \\cdot \\mathrm{Recall}}{\\mathrm{Precision} + \\mathrm{Recall}}.\n\\]"}),"\n",i.createElement(t.p,null,"Equivalently,"),"\n",i.createElement(c.A,{text:"\\( \n\\mathrm{F1} = \\frac{2\\,TP}{2\\,TP + FP + FN}.\n\\)"}),"\n",i.createElement(t.p,null,"F1 can be viewed as a single summary statistic that balances precision and recall in an equal proportion. It is thus particularly popular when both false positives and false negatives carry significant cost, or if you want a single measure that punishes an extremely low recall or precision."),"\n",i.createElement(t.p,null,"Because the F1 score is the harmonic mean, it only achieves a high value if ",i.createElement(t.strong,null,"both")," precision and recall are comparably high. If either is low, F1 remains relatively low. This is more balanced than using just arithmetic means, because it penalizes large disparities."),"\n",i.createElement(s.A,{text:'\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\nprecision_val = precision_score(y_true, y_pred)\nrecall_val = recall_score(y_true, y_pred)\nf1_val = f1_score(y_true, y_pred)\n\nprint("Precision:", precision_val)\nprint("Recall:", recall_val)\nprint("F1 Score:", f1_val)\n'}),"\n",i.createElement(t.h2,{id:"advanced-metrics-and-considerations",style:{position:"relative"}},i.createElement(t.a,{href:"#advanced-metrics-and-considerations","aria-label":"advanced metrics and considerations permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Advanced metrics and considerations"),"\n",i.createElement(t.h3,{id:"specificity-and-its-role-in-performance-evaluation",style:{position:"relative"}},i.createElement(t.a,{href:"#specificity-and-its-role-in-performance-evaluation","aria-label":"specificity and its role in performance evaluation permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Specificity and its role in performance evaluation"),"\n",i.createElement(t.p,null,"Often referred to as ",i.createElement(t.strong,null,"true negative rate (TNR)"),", specificity measures how well the classifier correctly identifies negatives:"),"\n",i.createElement(c.A,{text:"\\( \n\\mathrm{Specificity} = \\frac{TN}{TN + FP}.\n\\)"}),"\n",i.createElement(t.p,null,"In other words, out of all actual negatives, how many are predicted negative? Specificity is crucial when the cost of false positives is large. For example, in an oncology test, a false positive might create undue alarm and lead to expensive or invasive follow-up procedures. Specificity is complementary to recall (or sensitivity)."),"\n",i.createElement(t.p,null,"In practice, medical professionals, for instance, often examine both sensitivity (",i.createElement(c.A,{text:"\\( \\mathrm{Recall} \\)"}),') and specificity together — these are sometimes called "orthogonal metrics" because they gauge different kinds of errors. In certain contexts, you may also see the "Youden\'s J statistic," which is'),"\n",i.createElement(c.A,{text:"\\( \n\\mathrm{Youden's\\ J} = \\mathrm{Sensitivity} + \\mathrm{Specificity} - 1,\n\\)"}),"\n",i.createElement(t.p,null,"an older measure that tries to unify those two into a single scale."),"\n",i.createElement(t.h3,{id:"balanced-accuracy-for-imbalanced-datasets",style:{position:"relative"}},i.createElement(t.a,{href:"#balanced-accuracy-for-imbalanced-datasets","aria-label":"balanced accuracy for imbalanced datasets permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Balanced accuracy for imbalanced datasets"),"\n",i.createElement(t.p,null,i.createElement(r.A,null,"Balanced accuracy")," is designed to tackle the pitfalls of simple accuracy in ",i.createElement(t.strong,null,"imbalanced")," classification settings. Formally,"),"\n",i.createElement(c.A,{text:"\\[\n\\mathrm{Balanced\\ Accuracy} = \\frac{\\mathrm{Sensitivity} + \\mathrm{Specificity}}{2}.\n\\]"}),"\n",i.createElement(t.p,null,"It is the average of recall (sensitivity) for the positive class and recall for the negative class (equivalent to specificity if the negative class is denoted by 0). If the dataset is significantly skewed toward the negative class, the standard accuracy might be inflated by large TN counts, whereas balanced accuracy puts an equal emphasis on both classes."),"\n",i.createElement(t.p,null,"Some references generalize balanced accuracy to multi-class contexts by averaging the per-class recall values. If you are working with scikit-learn, you can use:"),"\n",i.createElement(s.A,{text:'\nfrom sklearn.metrics import balanced_accuracy_score\n\nbal_acc = balanced_accuracy_score(y_true, y_pred)\nprint("Balanced Accuracy:", bal_acc)\n'}),"\n",i.createElement(t.h3,{id:"matthews-correlation-coefficient-mcc",style:{position:"relative"}},i.createElement(t.a,{href:"#matthews-correlation-coefficient-mcc","aria-label":"matthews correlation coefficient mcc permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Matthews correlation coefficient (MCC)"),"\n",i.createElement(t.p,null,"The ",i.createElement(t.strong,null,"Matthews correlation coefficient (MCC)")," is another robust metric for binary classification, especially with class imbalance. It is defined as"),"\n",i.createElement(c.A,{text:"\\[\n\\mathrm{MCC} = \\frac{TP \\times TN - FP \\times FN}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}.\n\\]"}),"\n",i.createElement(t.p,null,"MCC can be interpreted as a correlation coefficient between the observed and predicted classifications. A coefficient of +1 indicates a perfect prediction, 0 indicates random prediction, and -1 indicates total disagreement between actual and predicted. One reason MCC is prized is that it is a ",i.createElement(t.strong,null,"single")," statistic that captures all four confusion matrix categories in a balanced way, and it is not inflated by high counts of a single category (like many TN in a highly skewed dataset)."),"\n",i.createElement(s.A,{text:'\nfrom sklearn.metrics import matthews_corrcoef\n\nmcc_val = matthews_corrcoef(y_true, y_pred)\nprint("MCC:", mcc_val)\n'}),"\n",i.createElement(t.h2,{id:"threshold-based-evaluation-and-curves",style:{position:"relative"}},i.createElement(t.a,{href:"#threshold-based-evaluation-and-curves","aria-label":"threshold based evaluation and curves permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Threshold-based evaluation and curves"),"\n",i.createElement(t.p,null,"Most modern classifiers (e.g., logistic regression, random forests, neural networks) do not merely generate a binary label. Instead, they output a ",i.createElement(t.strong,null,"continuous score")," or a probability that an instance belongs to a certain class. One can then apply different thresholds ",i.createElement(c.A,{text:"\\(\\theta\\)"})," on that score to produce a discrete 0/1 decision. Varying ",i.createElement(c.A,{text:"\\(\\theta\\)"})," from 0 to 1 changes the rates of TP, FP, TN, and FN, yielding different trade-offs in metrics such as precision vs. recall or TPR vs. FPR."),"\n",i.createElement(t.h3,{id:"precision-recall-curve",style:{position:"relative"}},i.createElement(t.a,{href:"#precision-recall-curve","aria-label":"precision recall curve permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Precision-recall curve"),"\n",i.createElement(t.p,null,"When dealing with heavily imbalanced data (like 1% positives and 99% negatives), the ",i.createElement(t.strong,null,"precision-recall (PR) curve")," often proves more illuminating than the ROC curve. You construct it by computing precision and recall for a range of threshold values. The result is a 2D plot with recall on the x-axis and precision on the y-axis:"),"\n",i.createElement(a,{alt:"Precision-recall curve depiction",path:"",caption:"Precision-recall curve illustrating the trade-off between precision and recall as a classification threshold is varied.",zoom:"false"}),"\n",i.createElement(t.p,null,"By reading off different points on this curve, you can see how your classifier's precision changes as you demand higher or lower recall."),"\n",i.createElement(t.h4,{id:"area-under-the-precision-recall-curve-pr-auc",style:{position:"relative"}},i.createElement(t.a,{href:"#area-under-the-precision-recall-curve-pr-auc","aria-label":"area under the precision recall curve pr auc permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Area Under the Precision-Recall Curve (PR AUC)"),"\n",i.createElement(t.p,null,"To summarize the overall shape of the PR curve, we often compute ",i.createElement(t.strong,null,"average precision (AP)")," or ",i.createElement(r.A,null,"PR AUC"),", a single number between 0 and 1. This metric is typically computed as the integral of precision as a function of recall from 0 to 1, though in practice scikit-learn uses a stepwise approximation technique. A random classifier can achieve a PR AUC that is roughly equal to the fraction of positive instances (which can be very low in highly imbalanced tasks)."),"\n",i.createElement(s.A,{text:"\nfrom sklearn.metrics import precision_recall_curve, average_precision_score\nimport matplotlib.pyplot as plt\n\ny_scores = np.array([0.9,0.2,0.85,0.1,0.3,0.05,0.99]) # example probabilities\nprecision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n\nap_val = average_precision_score(y_true, y_scores)\n\nplt.plot(recall, precision, marker='.')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title(f'Precision-Recall Curve (AP={ap_val:.3f})')\nplt.show()\n"}),"\n",i.createElement(t.h3,{id:"roc-curve",style:{position:"relative"}},i.createElement(t.a,{href:"#roc-curve","aria-label":"roc curve permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"ROC curve"),"\n",i.createElement(t.p,null,"One of the most classic threshold-based curves is the ",i.createElement(t.strong,null,"Receiver Operating Characteristic (ROC)")," curve, which plots the ",i.createElement(t.strong,null,"true positive rate (TPR)")," against the ",i.createElement(t.strong,null,"false positive rate (FPR)")," as the threshold changes."),"\n",i.createElement(c.A,{text:"\\( \n\\mathrm{TPR} = \\frac{TP}{TP + FN} = \\mathrm{Recall} \\)"}),"\n",i.createElement(c.A,{text:"\\( \n\\mathrm{FPR} = \\frac{FP}{FP + TN}. \n\\)"}),"\n",i.createElement(t.p,null,"As we move the threshold from 1.0 (predict everything negative) down to 0.0 (predict everything positive), we trace out a curve in TPR–FPR space. The resulting plot typically starts at (0,0) for high threshold and ends at (1,1) for threshold = 0. A random classifier yields points scattered near the diagonal from (0,0) to (1,1). A highly capable classifier will produce a curve that bows sharply toward the top-left corner, reflecting high TPR for relatively low FPR."),"\n",i.createElement(a,{alt:"ROC curve depiction",path:"",caption:"An example ROC curve with TPR on the y-axis and FPR on the x-axis.",zoom:"false"}),"\n",i.createElement(t.h3,{id:"area-under-the-curve-auc-for-roc-and-pr",style:{position:"relative"}},i.createElement(t.a,{href:"#area-under-the-curve-auc-for-roc-and-pr","aria-label":"area under the curve auc for roc and pr permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Area under the curve (AUC) for ROC and PR"),"\n",i.createElement(t.p,null,"The ",i.createElement(t.strong,null,"Area Under the ROC Curve (AUROC or simply ROC AUC)")," is the integral or area under the ROC curve. It is a widely cited summary statistic:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"ROC AUC = 1.0 means a perfect rank ordering of positives above negatives."),"\n",i.createElement(t.li,null,"ROC AUC = 0.5 typically denotes a random classifier."),"\n",i.createElement(t.li,null,'ROC AUC < 0.5 implies an "inversely predictive" classifier (one might flip its predictions to get > 0.5).'),"\n"),"\n",i.createElement(t.p,null,"However, be aware that ",i.createElement(t.strong,null,"ROC AUC can sometimes be overly optimistic")," in highly imbalanced scenarios, because the false positive rate uses ",i.createElement(c.A,{text:"\\( FP + TN \\)"})," in the denominator, and ",i.createElement(c.A,{text:"\\( TN \\)"})," might be extremely large relative to ",i.createElement(c.A,{text:"\\( FP \\)"}),". For heavily imbalanced data, the PR AUC might be more useful."),"\n",i.createElement(s.A,{text:"\nfrom sklearn.metrics import roc_curve, roc_auc_score\n\nfpr, tpr, thresholds = roc_curve(y_true, y_scores)\nauc_val = roc_auc_score(y_true, y_scores)\n\nplt.plot(fpr, tpr, label=f'ROC curve (AUC = {auc_val:.3f})')\nplt.plot([0,1],[0,1], 'r--', label='Random classifier')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend()\nplt.show()\n"}),"\n",i.createElement(t.h3,{id:"choosing-appropriate-thresholds",style:{position:"relative"}},i.createElement(t.a,{href:"#choosing-appropriate-thresholds","aria-label":"choosing appropriate thresholds permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Choosing appropriate thresholds"),"\n",i.createElement(t.p,null,"Selecting a good threshold ",i.createElement(c.A,{text:"\\(\\theta\\)"})," is often critical. If you set ",i.createElement(c.A,{text:"\\(\\theta\\)"}),' too low, you might achieve high recall at the cost of more false positives. If you set it too high, you might reduce false positives but also lose many true positives. The best threshold depends on your project\'s objectives and the relative costs of errors. In certain real-world pipelines, you might determine the threshold from business constraints, such as: "We want to keep the false discovery rate under 5%." That requirement can be re-expressed in terms of precision or specificity, guiding the threshold choice accordingly.'),"\n",i.createElement(t.p,null,"In advanced scenarios, you might even let the threshold vary across subgroups, or perform cost-sensitive classification, where the cost matrix changes for different types of errors."),"\n",i.createElement(t.h2,{id:"multiclass-classification-metrics",style:{position:"relative"}},i.createElement(t.a,{href:"#multiclass-classification-metrics","aria-label":"multiclass classification metrics permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Multiclass classification metrics"),"\n",i.createElement(t.p,null,"When you have more than two classes, many of the above definitions require extension. For instance, we can compute a confusion matrix for K classes, resulting in a K×K grid, or we can reduce the problem to multiple binary tasks with strategies such as ",i.createElement(t.strong,null,"One-vs.-All (OvA)")," or ",i.createElement(t.strong,null,"One-vs.-One (OvO)"),"."),"\n",i.createElement(a,{alt:"Multiclass confusion matrix example",path:"",caption:"A 3×3 confusion matrix for a three-class classification problem.",zoom:"false"}),"\n",i.createElement(t.h3,{id:"macro-micro-and-weighted-averaging-methods",style:{position:"relative"}},i.createElement(t.a,{href:"#macro-micro-and-weighted-averaging-methods","aria-label":"macro micro and weighted averaging methods permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Macro, micro, and weighted averaging methods"),"\n",i.createElement(t.p,null,"Most libraries define ways to compute precision, recall, and F1 in a multi-class context by different averaging strategies:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Micro averaging:")," Aggregates the contributions of all classes to compute the average metric. In micro averaging, each instance carries the same weight, effectively summing up global TP, FP, FN across all classes, then computing the metric. This approach works well in imbalanced situations if you want each instance to matter equally."),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Macro averaging:")," Computes the metric independently for each class and then takes the average (hence treating all classes equally). This might not reflect the overall performance if there is a class that has very few instances. By default, each class is given the same weight regardless of frequency."),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Weighted averaging:")," Similar to macro averaging, but it weights each class's metric by the proportion of instances in that class. This can be a middle ground that accounts for imbalance while also measuring performance across classes distinctly."),"\n"),"\n"),"\n",i.createElement(t.p,null,"Using scikit-learn:"),"\n",i.createElement(s.A,{text:"\nfrom sklearn.metrics import f1_score\n\n# 'micro' aggregates the contributions of all classes\nf1_micro = f1_score(y_true, y_pred, average='micro')\n# 'macro' computes the mean of metrics computed per class\nf1_macro = f1_score(y_true, y_pred, average='macro')\n# 'weighted' accounts for class frequency\nf1_weighted = f1_score(y_true, y_pred, average='weighted')\n"}),"\n",i.createElement(t.p,null,'Choosing which averaging is "best" depends on your domain. If you want to treat all classes equally, macro might be appropriate. If you want to treat all instances equally, micro might be better. Weighted is useful if you want to reflect class frequencies in the final single metric but still partially decouple it from purely micro-based calculations.'),"\n",i.createElement(t.h3,{id:"one-vs-all-ova-and-one-vs-one-ovo-strategies",style:{position:"relative"}},i.createElement(t.a,{href:"#one-vs-all-ova-and-one-vs-one-ovo-strategies","aria-label":"one vs all ova and one vs one ovo strategies permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"One-vs-all (OvA) and one-vs-one (OvO) strategies"),"\n",i.createElement(t.p,null,"When your base classifier is inherently binary (like SVM in classical usage), you can still handle multiple classes by constructing multiple binary subproblems:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"In ",i.createElement(t.strong,null,"OvA")," (also known as ",i.createElement(t.strong,null,"One-vs.-Rest"),"), for each class ",i.createElement(c.A,{text:"\\( k \\)"})," out of K, you build a separate classifier that separates class ",i.createElement(c.A,{text:"\\( k \\)"})," from the other ",i.createElement(c.A,{text:"\\( K-1 \\)"})," classes. If you have K classes, you end up training K distinct classifiers."),"\n",i.createElement(t.li,null,"In ",i.createElement(t.strong,null,"OvO"),", for each possible pair of classes, you build a classifier that distinguishes those two only. This yields ",i.createElement(c.A,{text:"\\(\\frac{K(K-1)}{2}\\)"})," classifiers. At prediction time, one might use a voting scheme or other logic to combine the binary sub-predictions."),"\n"),"\n",i.createElement(t.p,null,"Although these strategies originate in model training, they are relevant here because each OvA or OvO sub-classifier might produce its own metrics, and you might then combine them in various ways to arrive at an overall measure."),"\n",i.createElement(t.h3,{id:"extensions-of-f1-and-accuracy-to-multiclass-problems",style:{position:"relative"}},i.createElement(t.a,{href:"#extensions-of-f1-and-accuracy-to-multiclass-problems","aria-label":"extensions of f1 and accuracy to multiclass problems permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Extensions of F1 and accuracy to multiclass problems"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Multiclass accuracy"),' still means "the fraction of instances whose predicted label matches the true label." That is straightforward.'),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Multiclass F1"),' can be computed in micro, macro, or weighted ways. A single "global" F1 can be less intuitive to interpret, but libraries handle the details.'),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"MCC")," also has a multiclass extension, though it is more rarely used in day-to-day modeling. The formula generalizes to higher dimensions in a way that captures the correlation between predicted and actual label distributions (see ",i.createElement(t.em,null,'Wikipedia, "Phi coefficient — Multiclass case"'),")."),"\n"),"\n",i.createElement(t.h2,{id:"model-comparison-and-selection",style:{position:"relative"}},i.createElement(t.a,{href:"#model-comparison-and-selection","aria-label":"model comparison and selection permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Model comparison and selection"),"\n",i.createElement(t.h3,{id:"selecting-the-right-metric-for-a-given-problem-when-to-use-each-of-the-described-metrics",style:{position:"relative"}},i.createElement(t.a,{href:"#selecting-the-right-metric-for-a-given-problem-when-to-use-each-of-the-described-metrics","aria-label":"selecting the right metric for a given problem when to use each of the described metrics permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Selecting the right metric for a given problem. When to use each of the described metrics."),"\n",i.createElement(t.p,null,"Your choice of metric depends heavily on your domain and your goals:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Accuracy"),' is simplest when classes are balanced, and your primary goal is "just get as many correct as possible."'),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Precision")," is key when you want to reduce false positives, for instance in tasks where a false alarm is costly (e.g., incorrectly labeling a legitimate user as a fraudster)."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Recall (sensitivity)")," is key when you want to minimize false negatives, e.g., not missing any truly fraudulent transaction or not missing a patient with disease."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"F1")," is a balanced measure that punishes extreme divergence between precision and recall."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Specificity"),' is crucial when you want to reduce the fraction of negative instances misclassified as positive, sometimes used in medical tests to measure the "true negative rate."'),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Balanced accuracy")," helps when classes are imbalanced, as it ensures that each class's recall is weighted equally."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"MCC")," is also robust to imbalance and can be a good single-number measure for overall correlation."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"ROC AUC")," is popular for comparing ranking ability or overall discriminative power, but it may overestimate performance on highly skewed data."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"PR AUC")," or average precision is often more appropriate than ROC AUC in heavily imbalanced problems where the positive class is rare, as it focuses specifically on the performance in retrieving those positives."),"\n"),"\n",i.createElement(t.p,null,"In research contexts (for example, in ",i.createElement(t.em,null,'Saito & Rehmsmeier, "The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets," PLoS ONE 2015'),"), it is increasingly common to use precision-recall analysis for problems with large class imbalance."),"\n",i.createElement(t.h3,{id:"interpreting-multiple-metrics-together",style:{position:"relative"}},i.createElement(t.a,{href:"#interpreting-multiple-metrics-together","aria-label":"interpreting multiple metrics together permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Interpreting multiple metrics together"),"\n",i.createElement(t.p,null,"In many practical projects, I strongly recommend that you do not rely on a single number, but rather look at multiple metrics to gain a holistic picture. For instance, you might examine:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,"The confusion matrix."),"\n",i.createElement(t.li,null,"The F1 score."),"\n",i.createElement(t.li,null,"The precision-recall curve."),"\n",i.createElement(t.li,null,"The ROC curve (and the ROC AUC)."),"\n",i.createElement(t.li,null,"Possibly the MCC or Balanced Accuracy."),"\n"),"\n",i.createElement(t.p,null,"This multi-dimensional approach is beneficial because each metric highlights different aspects of performance (like how many negatives are misclassified, how many positives are missed, etc.). If your domain has a well-defined cost matrix (where a false positive has cost ",i.createElement(c.A,{text:"\\( C_{FP} \\)"})," and a false negative has cost ",i.createElement(c.A,{text:"\\( C_{FN} \\)"}),"), you can combine these metrics or incorporate cost-based metrics (such as the total cost or expected monetary value) to guide your final model selection."),"\n",i.createElement(t.h3,{id:"practical-strategies-for-comparing-model-performance",style:{position:"relative"}},i.createElement(t.a,{href:"#practical-strategies-for-comparing-model-performance","aria-label":"practical strategies for comparing model performance permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Practical strategies for comparing model performance"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Cross-validation:")," Evaluate each candidate model or set of hyperparameters under cross-validation. Compute your chosen metrics on each fold, then average."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Statistical tests:")," If differences between metrics are small, you can use statistical methods (like a t-test or Wilcoxon signed-rank test on cross-validation folds) to see if the differences are robust."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Ranking vs. threshold metrics:")," Sometimes you might first examine how well the model ranks positive vs. negative with ROC AUC or PR AUC, then pick a threshold that yields a desirable operating point for precision, recall, or cost."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Holdout vs. multiple splits:")," If you have enough data, keep a large holdout set to measure generalization. If data is limited, repeated cross-validation can help."),"\n"),"\n",i.createElement(t.h3,{id:"handling-imbalanced-datasets",style:{position:"relative"}},i.createElement(t.a,{href:"#handling-imbalanced-datasets","aria-label":"handling imbalanced datasets permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Handling imbalanced datasets"),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Class imbalance")," is extremely common in fields like fraud detection, rare disease diagnosis, or system anomaly detection. Some ways to handle it:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Metrics"),": Use precision/recall, F1, balanced accuracy, PR AUC, or MCC instead of plain accuracy or ROC AUC alone."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Resampling"),": Oversample the minority class or undersample the majority class, or employ synthetic approaches (e.g., SMOTE)."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Adjust class weights"),": Many algorithms (like logistic regression, SVM, or tree-based methods) allow weighting the classes inversely to their frequencies."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Focus on cost-sensitive learning"),": If the cost ratio ",i.createElement(c.A,{text:"\\( C_{FN} / C_{FP} \\)"})," is high, tune the model threshold or training objective to reduce false negatives."),"\n"),"\n",i.createElement(t.h3,{id:"overfitting-and-proper-validation-techniques",style:{position:"relative"}},i.createElement(t.a,{href:"#overfitting-and-proper-validation-techniques","aria-label":"overfitting and proper validation techniques permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Overfitting and proper validation techniques"),"\n",i.createElement(t.p,null,"When you evaluate classification metrics, be aware of ",i.createElement(t.strong,null,"overfitting"),': it might inflate your reported performance on the training set. Always measure metrics on a separate validation or test set or via cross-validation. If your dataset is small or if hyperparameters have been heavily tuned, ensure that the test set remains purely out-of-sample. In Kaggle competitions, for instance, an unseen "private leaderboard" portion of the test set is used to detect overfitting to the public leaderboard.'),"\n",i.createElement(t.h3,{id:"importance-of-domain-context-in-metric-selection",style:{position:"relative"}},i.createElement(t.a,{href:"#importance-of-domain-context-in-metric-selection","aria-label":"importance of domain context in metric selection permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Importance of domain context in metric selection"),"\n",i.createElement(t.p,null,"In practice, always consult with domain experts. For example, in diagnosing cancer, you might prefer near-perfect recall, because missing a cancer case is extremely costly. You might tolerate an elevated rate of false positives if they can be quickly double-checked with a cheaper follow-up. Meanwhile, in spam detection, a single false positive (legitimate email flagged as spam) can cause user frustration, so you might focus on precision or specificity. The domain context shapes the trade-offs among metrics."),"\n",i.createElement(t.h2,{id:"conclusion",style:{position:"relative"}},i.createElement(t.a,{href:"#conclusion","aria-label":"conclusion permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Conclusion"),"\n",i.createElement(t.p,null,"Classification metrics encompass a vast array of methods to evaluate and compare model performance, each revealing a different facet of how well a classifier is doing. By starting with the confusion matrix and deriving ",i.createElement(t.strong,null,"accuracy"),", ",i.createElement(t.strong,null,"precision"),", ",i.createElement(t.strong,null,"recall"),", and ",i.createElement(t.strong,null,"F1"),", you acquire a baseline understanding of your model's performance. From there, you can incorporate advanced considerations like ",i.createElement(t.strong,null,"specificity"),", ",i.createElement(t.strong,null,"balanced accuracy"),", ",i.createElement(t.strong,null,"MCC"),", ",i.createElement(t.strong,null,"precision-recall curves"),", ",i.createElement(t.strong,null,"ROC curves"),", and the corresponding AUC values."),"\n",i.createElement(t.p,null,"For ",i.createElement(t.strong,null,"binary")," classification, these metrics can be extended in multiple ways to handle ",i.createElement(t.strong,null,"imbalanced")," classes or to produce thorough comparisons using threshold-based approaches. In ",i.createElement(t.strong,null,"multiclass")," settings, you can rely on micro, macro, or weighted averaging, or adopt the OvA/OvO strategies in tandem with well-chosen metrics to get a comprehensive understanding."),"\n",i.createElement(t.p,null,"Model evaluation is rarely about a single metric in isolation. The wise approach is to combine domain knowledge (about costs of errors, imbalance levels, etc.) with a multifaceted metric approach (looking at confusion matrices, threshold-based curves, and single-number summaries). Through this lens, you can select the model and the threshold that truly optimizes for your real-world objectives, ensuring that you neither over- nor underestimate your classifier's capabilities."),"\n",i.createElement(t.p,null,"When used properly, classification metrics are the essential lens through which you can interpret the predictions of your model and ensure that it genuinely aligns with practical, scientific, or economic requirements. I recommend exploring each of the metrics introduced here in your own data experiments. By systematically analyzing them, you will gain profound insight into how your classifier is behaving and where you might direct your next steps in model improvement or data collection."))}var m=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,n.RP)(),e.components);return t?i.createElement(t,e,i.createElement(o,e)):o(e)};var h=a(54506),d=a(88864),u=a(58481),p=a.n(u),f=a(5984),g=a(43672),v=a(27042),y=a(72031),b=a(81817),E=a(27105),w=a(17265),x=a(2043),S=a(95751),C=a(94328),M=a(80791),T=a(78137);const N=e=>{let{toc:t}=e;if(!t||!t.items)return null;return i.createElement("nav",{className:M.R},i.createElement("ul",null,t.items.map(((e,t)=>i.createElement("li",{key:t},i.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const a=t.replace("#",""),n=document.getElementById(a);n&&n.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&i.createElement(N,{toc:{items:e.items}}))))))};function _(e){let{data:{mdx:t,allMdx:l,allPostImages:r},children:s}=e;const{frontmatter:c,body:o,tableOfContents:m}=t,d=c.index,u=c.slug.split("/")[1],y=l.nodes.filter((e=>e.frontmatter.slug.includes(`/${u}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),M=y.findIndex((e=>e.frontmatter.index===d)),_=y[M+1],A=y[M-1],I=c.slug.replace(/\/$/,""),z=/[^/]*$/.exec(I)[0],H=`posts/${u}/content/${z}/`,{0:P,1:k}=(0,i.useState)(c.flagWideLayoutByDefault),{0:L,1:j}=(0,i.useState)(!1);var O;(0,i.useEffect)((()=>{j(!0);const e=setTimeout((()=>j(!1)),340);return()=>clearTimeout(e)}),[P]),"adventures"===u?O=w.cb:"research"===u?O=w.Qh:"thoughts"===u&&(O=w.T6);const R=p()(o).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,F=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),a=e%60;return a<=30?`~${t}${a>0?".5":""} h`:`~${t+1} h`}(Math.ceil(R/O)+(c.extraReadTimeMin||0)),V=[{flag:c.flagDraft,component:()=>Promise.all([a.e(5850),a.e(9833)]).then(a.bind(a,49833))},{flag:c.flagMindfuckery,component:()=>Promise.all([a.e(5850),a.e(7805)]).then(a.bind(a,27805))},{flag:c.flagRewrite,component:()=>Promise.all([a.e(5850),a.e(8916)]).then(a.bind(a,78916))},{flag:c.flagOffensive,component:()=>Promise.all([a.e(5850),a.e(6731)]).then(a.bind(a,49112))},{flag:c.flagProfane,component:()=>Promise.all([a.e(5850),a.e(3336)]).then(a.bind(a,83336))},{flag:c.flagMultilingual,component:()=>Promise.all([a.e(5850),a.e(2343)]).then(a.bind(a,62343))},{flag:c.flagUnreliably,component:()=>Promise.all([a.e(5850),a.e(6865)]).then(a.bind(a,11627))},{flag:c.flagPolitical,component:()=>Promise.all([a.e(5850),a.e(4417)]).then(a.bind(a,24417))},{flag:c.flagCognitohazard,component:()=>Promise.all([a.e(5850),a.e(8669)]).then(a.bind(a,18669))},{flag:c.flagHidden,component:()=>Promise.all([a.e(5850),a.e(8124)]).then(a.bind(a,48124))}],{0:B,1:D}=(0,i.useState)([]);return(0,i.useEffect)((()=>{V.forEach((e=>{let{flag:t,component:a}=e;t&&a().then((e=>{D((t=>[].concat((0,h.A)(t),[e.default])))}))}))}),[]),i.createElement(v.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},i.createElement(b.A,{postNumber:c.index,date:c.date,updated:c.updated,readTime:F,difficulty:c.difficultyLevel,title:c.title,desc:c.desc,banner:c.banner,section:u,postKey:z,isMindfuckery:c.flagMindfuckery,mainTag:c.mainTag}),i.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},c.otherTags.map(((e,t)=>i.createElement("span",{key:t,className:`noselect ${T.MW}`,style:{margin:"0 5px 5px 0"}},e)))),i.createElement("div",{className:"postBody"},i.createElement(N,{toc:m})),i.createElement("br",null),i.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},i.createElement(v.P.button,{className:`noselect ${C.pb}`,id:C.xG,onClick:()=>{k(!P)},whileTap:{scale:.93}},i.createElement(v.P.div,{className:S.DJ,key:P,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},P?"Switch to default layout":"Switch to wide layout"))),i.createElement("br",null),i.createElement("div",{className:"postBody",style:{margin:P?"0 -14%":"",maxWidth:P?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},i.createElement("div",{className:`${C.P_} ${L?C.Xn:C.qG}`},B.map(((e,t)=>i.createElement(e,{key:t}))),c.indexCourse?i.createElement(x.A,{index:c.indexCourse,category:c.courseCategoryName}):"",i.createElement(f.Z.Provider,{value:{images:r.nodes,basePath:H.replace(/\/$/,"")+"/"}},i.createElement(n.xA,{components:{Image:g.A}},s)))),i.createElement(E.A,{nextPost:_,lastPost:A,keyCurrent:z,section:u}))}function A(e){return i.createElement(_,e,i.createElement(m,e))}function I(e){var t,a,n,l,r;let{data:s}=e;const{frontmatter:c}=s.mdx,o=c.titleSEO||c.title,m=c.titleOG||o,h=c.titleTwitter||o,u=c.descSEO||c.desc,p=c.descOG||u,f=c.descTwitter||u,g=c.schemaType||"BlogPosting",v=c.keywordsSEO,b=c.date,E=c.updated||b,w=c.imageOG||(null===(t=c.banner)||void 0===t||null===(a=t.childImageSharp)||void 0===a||null===(n=a.gatsbyImageData)||void 0===n||null===(l=n.images)||void 0===l||null===(r=l.fallback)||void 0===r?void 0:r.src),x=c.imageAltOG||p,S=c.imageTwitter||w,C=c.imageAltTwitter||f,M=c.canonicalURL,T=c.flagHidden||!1,N=c.mainTag||"Posts",_=c.slug.split("/")[1]||"posts",{siteUrl:A}=(0,d.Q)(),I={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:A},{"@type":"ListItem",position:2,name:N,item:`${A}/${c.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:o,item:`${A}${c.slug}`}]};return i.createElement(y.A,{title:o+" - avrtt.blog",titleOG:m,titleTwitter:h,description:u,descriptionOG:p,descriptionTwitter:f,schemaType:g,keywords:v,datePublished:b,dateModified:E,imageOG:w,imageAltOG:x,imageTwitter:S,imageAltTwitter:C,canonicalUrl:M,flagHidden:T,mainTag:N,section:_,type:"article"},i.createElement("script",{type:"application/ld+json"},JSON.stringify(I)))}},90548:function(e,t,a){var n=a(96540),i=a(7978);t.A=e=>{let{text:t}=e;return n.createElement(i.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-tsx-content-file-path-src-pages-posts-research-classification-metrics-mdx-8fdb7f7e9b5267e19344.js.map