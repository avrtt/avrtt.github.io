"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[5368],{29143:function(e,t,a){a.r(t),a.d(t,{Head:function(){return T},PostTemplate:function(){return C},default:function(){return A}});var n=a(54506),i=a(28453),l=a(96540),r=a(16886),o=a(46295),s=a(96098);function c(e){const t=Object.assign({p:"p",ol:"ol",li:"li",ul:"ul",h2:"h2",a:"a",span:"span",h3:"h3",strong:"strong"},(0,i.RP)(),e.components),{Image:a}=t;return a||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),l.createElement(l.Fragment,null,"\n",l.createElement("br"),"\n","\n","\n",l.createElement(t.p,null,"Segmentation in the context of computer vision and image analysis is the process of partitioning an image into meaningful segments or regions, typically so that each region corresponds to a distinct object — or in some cases, a part of an object — based on shared visual characteristics such as color, texture, or semantic label. Unlike image classification, which assigns a single label to an entire image, or object detection, which seeks to place bounding boxes around discrete objects, segmentation drills down to a pixel-level assignment of labels. This pixel-level methodology allows us to visualize and comprehend the precise form, boundaries, and location of objects in an image."),"\n",l.createElement(t.p,null,"Essentially, there are three major subcategories of segmentation tasks that researchers and practitioners often distinguish:"),"\n",l.createElement(t.ol,null,"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(r.A,null,"Semantic segmentation"),': Here, each pixel is assigned to one of several possible classes (e.g., background, car, pedestrian, road). Two separate objects of the same class, such as two different cars, will be labeled identically. You care about whether a pixel belongs to a "car" or a "tree", but do not necessarily distinguish between distinct cars or separate trees.'),"\n"),"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(r.A,null,"Instance segmentation"),': This approach not only marks each pixel with a class label, it also differentiates between distinct object instances of the same class. For example, if there are three cars, each car is segmented separately and is treated as a unique instance, even if they share the same label of "car".'),"\n"),"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(r.A,null,"Panoptic segmentation"),': Panoptic segmentation combines the objectives of semantic segmentation and instance segmentation. In other words, it assigns every pixel a class label (like semantic segmentation) while also making separate masks for individual object instances when applicable (like instance segmentation). It offers a unified approach for understanding both "things" (countable objects like people, cars, or animals) and "stuff" (uncountable concepts such as sky, grass, or road).'),"\n"),"\n"),"\n",l.createElement(t.p,null,"Image object segmentation is crucial in numerous real-world applications. In ",l.createElement(r.A,null,"medical diagnostics"),", segmentation is employed for delineating tumors, organs, and tissues within medical scans, which then supports clinicians in making precise measurements or planning interventions. In ",l.createElement(r.A,null,"autonomous driving"),", an accurate delineation of the road, pedestrians, street signs, and other traffic participants is essential for safe path planning. In ",l.createElement(r.A,null,"image editing")," and design, having pixel-level information about object boundaries allows photo editors or content creators to manipulate and transform specific regions in a fine-grained manner (e.g., background removal or selective editing). Overall, segmentation stands as one of the most high-impact tasks in modern computer vision."),"\n",l.createElement(t.p,null,"It is instructive to compare segmentation with both classification and detection to fully understand where segmentation lies on the continuum of computer vision complexity:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(r.A,null,"Classification"),': You feed an entire image into a model and get a label such as "cat" or "dog".'),"\n",l.createElement(t.li,null,l.createElement(r.A,null,"Detection"),': You find bounding boxes that localize each object instance in the image, often accompanied by the appropriate class label (e.g., a bounding box around a cat plus the label "cat").'),"\n",l.createElement(t.li,null,l.createElement(r.A,null,"Segmentation"),": You go one step further to identify the precise spatial extent (boundary) of the object at the pixel level."),"\n"),"\n",l.createElement(t.p,null,"Different subfields of segmentation — semantic, instance, and panoptic — reflect these distinct but overlapping perspectives on how to parse an image. Modern segmentation solutions often require extremely large, meticulously labeled datasets, advanced neural architectures, and specialized training techniques. Over the past decade, deep learning approaches have largely outperformed traditional, handcrafted segmentation methods by leveraging convolutional neural networks (CNNs), fully convolutional architectures, and more recently, attention mechanisms and transformer-based models. In the sections that follow, I will trace the historical development of segmentation methods from classical graph-based and region-based algorithms to modern deep segmentation networks, then dive into their architectures, evaluation metrics, real-world deployment considerations, and the latest research directions."),"\n",l.createElement(t.h2,{id:"historical-background-and-evolution",style:{position:"relative"}},l.createElement(t.a,{href:"#historical-background-and-evolution","aria-label":"historical background and evolution permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"historical background and evolution"),"\n",l.createElement(t.h3,{id:"traditional-segmentation-methods",style:{position:"relative"}},l.createElement(t.a,{href:"#traditional-segmentation-methods","aria-label":"traditional segmentation methods permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"traditional segmentation methods"),"\n",l.createElement(t.p,null,"Segmentation in its broad sense is an old problem in computer vision. Early segmentation approaches, such as thresholding, region growing, split-and-merge methods, or edge-based segmentation, date back to the foundational years of image processing. These techniques rely on heuristics around pixel intensities and spatial consistency to cluster pixels into coherent regions. Some iconic methods include:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Thresholding"),": A simple yet effective approach for certain controlled scenarios (e.g., binary segmentation of grayscale images). The famous Otsu's thresholding method computes an optimal threshold by maximizing the inter-class variance and minimizing the intra-class variance of pixel intensity distributions."),"\n"),"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Edge-based segmentation"),": Canny edge detection or Sobel filters to highlight pixels where large intensity gradients occur. By connecting edges, one forms boundaries that demarcate potential objects."),"\n"),"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Region-based segmentation"),": Region growing or region splitting and merging. The basic premise is grouping neighboring pixels that satisfy a certain homogeneity criterion (e.g., similar color, intensity, or texture)."),"\n"),"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Graph-based segmentation"),": Approaches that formalize the segmentation problem as a graph partition task. For instance, in ",l.createElement(r.A,null,"graph-based segmentation"),", pixels are treated as graph vertices, with edges representing similarity or dissimilarity among neighboring pixels. Felzenszwalb and Huttenlocher's algorithm (2004) is a classic in this category."),"\n"),"\n"),"\n",l.createElement(t.p,null,"Another advanced approach in traditional computer vision is ",l.createElement(r.A,null,"normalized cuts"),", where one attempts to partition an image into subgraphs such that similarity within each subgraph is high while similarity between subgraphs is minimal. These methods often involve building large, sometimes fully connected graphs of image pixels (or superpixels) and then solving an optimization problem that can be computationally demanding, though approximate solutions — often based on spectral methods — can scale to moderate-size images."),"\n",l.createElement(t.h3,{id:"transition-to-deep-learning-approaches",style:{position:"relative"}},l.createElement(t.a,{href:"#transition-to-deep-learning-approaches","aria-label":"transition to deep learning approaches permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"transition to deep learning approaches"),"\n",l.createElement(t.p,null,"The advent of deep learning revolutionized the landscape of segmentation. Handcrafted features, once the mainstay of classical segmentation approaches, began to be replaced by automatically learned representations. With the success of ",l.createElement(r.A,null,"convolutional neural networks (CNNs)")," — popularized by AlexNet (Krizhevsky and gang, NeurIPS 2012) for image classification — the computer vision community soon realized that CNNs could be adapted to solve segmentation tasks by generating dense, per-pixel predictions."),"\n",l.createElement(t.p,null,"One pivotal breakthrough was the ",l.createElement(r.A,null,"Fully Convolutional Network (FCN)")," architecture proposed by Long and gang (CVPR 2015). Instead of using fully connected layers that produce a single classification per forward pass, FCNs retained the convolutional layers but replaced the classification head with upsampling or deconvolution (transpose convolution) layers, enabling pixel-level output. This design provided the first end-to-end trainable deep network for semantic segmentation."),"\n",l.createElement(t.p,null,"Shortly thereafter, the ",l.createElement(r.A,null,"U-Net")," architecture was introduced for biomedical image segmentation by Ronneberger and gang (MICCAI 2015). U-Net employed an encoder-decoder framework, with the encoder capturing increasingly abstract features and the decoder reconstructing a spatially resolved segmentation map. Crucially, skip connections were introduced to bring high-resolution features from the encoder forward to the decoder, significantly improving the fine-grained accuracy of segmentation predictions."),"\n",l.createElement(t.h3,{id:"overview-of-improvements-over-classical-methods",style:{position:"relative"}},l.createElement(t.a,{href:"#overview-of-improvements-over-classical-methods","aria-label":"overview of improvements over classical methods permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"overview of improvements over classical methods"),"\n",l.createElement(t.p,null,"Deep learning-based segmentation techniques offer a leap in performance compared to classical methods for several reasons:"),"\n",l.createElement(t.ol,null,"\n",l.createElement(t.li,null,l.createElement(r.A,null,"Learned features rather than handcrafted"),": Instead of relying on fixed gradients or intensity thresholds, deep networks learn meaningful patterns directly from data."),"\n",l.createElement(t.li,null,l.createElement(r.A,null,"Robustness and generalization"),": CNN-based methods can handle complex scenes, lighting changes, and object appearances more robustly when trained on large, diverse datasets."),"\n",l.createElement(t.li,null,l.createElement(r.A,null,"Better synergy with large datasets"),": As annotated datasets grew (e.g., PASCAL VOC, MS COCO), CNNs scaled well, continuing to improve performance with more data."),"\n",l.createElement(t.li,null,l.createElement(r.A,null,"Hardware acceleration"),": Modern GPUs and specialized accelerators provide the computational horsepower necessary to train large, deep architectures."),"\n"),"\n",l.createElement(t.h3,{id:"influence-of-hardware-and-datasets",style:{position:"relative"}},l.createElement(t.a,{href:"#influence-of-hardware-and-datasets","aria-label":"influence of hardware and datasets permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"influence of hardware and datasets"),"\n",l.createElement(t.p,null,"Large-scale labeled datasets (e.g., MS COCO, Cityscapes for autonomous driving, LIDC/IDRI for lung CT segmentation) greatly accelerated progress. Similarly, developments in GPU hardware and distributed computing frameworks (PyTorch, TensorFlow, etc.) made training deeper and more capable segmentation networks feasible, spurring new lines of research in real-world segmentation tasks and advanced architectures."),"\n",l.createElement(t.h2,{id:"core-concepts-and-foundations",style:{position:"relative"}},l.createElement(t.a,{href:"#core-concepts-and-foundations","aria-label":"core concepts and foundations permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"core concepts and foundations"),"\n",l.createElement(t.h3,{id:"review-of-convolutional-networks-for-image-tasks",style:{position:"relative"}},l.createElement(t.a,{href:"#review-of-convolutional-networks-for-image-tasks","aria-label":"review of convolutional networks for image tasks permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"review of convolutional networks for image tasks"),"\n",l.createElement(t.p,null,"In standard computer vision tasks like classification, a ",l.createElement(r.A,null,"convolutional neural network")," typically consists of:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"Convolutional layers that learn filters capturing local spatial patterns"),"\n",l.createElement(t.li,null,"Pooling layers (such as max pooling) that reduce spatial resolution"),"\n",l.createElement(t.li,null,"Nonlinear activation functions (e.g., ReLU) that enable complex learned representations"),"\n",l.createElement(t.li,null,"Fully connected layers at the end to produce classification logits"),"\n"),"\n",l.createElement(t.p,null,"In segmentation, an important tweak is that the final layers usually output maps that match or closely approach the resolution of the input image. Rather than a single probability distribution over classes, we produce a per-pixel probability distribution. This shift from classification to dense prediction is the essence of going from traditional CNNs to fully convolutional structures."),"\n",l.createElement(t.h3,{id:"labeling-and-dataset-preparation-for-segmentation",style:{position:"relative"}},l.createElement(t.a,{href:"#labeling-and-dataset-preparation-for-segmentation","aria-label":"labeling and dataset preparation for segmentation permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"labeling and dataset preparation for segmentation"),"\n",l.createElement(t.p,null,"Preparing labeled data for segmentation can be far more labor-intensive than labeling data for classification or object detection. Instead of assigning a class label to an entire image or drawing bounding boxes, segmenters often need ",l.createElement(r.A,null,"masks")," that indicate, for every pixel, which object or class it belongs to. In the case of instance segmentation, each instance in the image requires a separate mask or ID."),"\n",l.createElement(t.p,null,"Common annotation workflows and tools:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Polygon annotation"),": Tools like Labelbox, Supervisely, or CVAT let an annotator draw polygons around objects and fill them to form masks."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Brush-based annotation"),": Some specialized software allows painting of object silhouettes in a manner reminiscent of digital painting."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Interactive segmentation"),": Some advanced methods combine classical algorithms (like grab-cut) with minimal human input, e.g., scribbles or bounding boxes, to speed up annotation."),"\n"),"\n",l.createElement(t.p,null,"Careful labeling is critical because segmentation tasks can be unforgiving of even minor boundary errors, especially in domains like medical imaging. In addition to ground truth masks, other label formats include instance ID images (each instance has a unique integer ID) and single channel masks for each class or instance."),"\n",l.createElement(t.h3,{id:"common-evaluation-metrics-iou-dice-coefficient-pixel-accuracy",style:{position:"relative"}},l.createElement(t.a,{href:"#common-evaluation-metrics-iou-dice-coefficient-pixel-accuracy","aria-label":"common evaluation metrics iou dice coefficient pixel accuracy permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"common evaluation metrics (iou, dice coefficient, pixel accuracy)"),"\n",l.createElement(t.p,null,"Measuring the performance of segmentation models requires specialized metrics that account for pixel-level matches or mismatches. The ",l.createElement(r.A,null,"Intersection-over-Union (IoU)")," or Jaccard Index is perhaps the most commonly used for semantic and instance segmentation. It is computed for a given class (or instance) as:"),"\n",l.createElement(s.A,{text:"\\( \n\\text{IoU} = \\frac{\\text{Intersection}(\\text{Prediction}, \\text{GroundTruth})}{\\text{Union}(\\text{Prediction}, \\text{GroundTruth})} \n\\)"}),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"Intersection: number of pixels labeled as class X in both the prediction and ground truth."),"\n",l.createElement(t.li,null,"Union: total number of pixels labeled as class X in either the prediction or the ground truth."),"\n"),"\n",l.createElement(t.p,null,"In medical applications, the ",l.createElement(r.A,null,"Dice coefficient")," (also known as F1 score in some contexts) is frequently used:"),"\n",l.createElement(s.A,{text:"\\[\n\\text{Dice} = \\frac{2 \\times |P \\cap G|}{|P| + |G|}\n\\]"}),"\n",l.createElement(t.p,null,"Where ",l.createElement(s.A,{text:"\\(P\\)"})," is the set of predicted pixels for a given class or instance, and ",l.createElement(s.A,{text:"\\(G\\)"})," is the set of ground truth pixels. Dice can be closely related to IoU but often is more prevalent in medical imaging literature."),"\n",l.createElement(t.p,null,"We also often see ",l.createElement(r.A,null,"pixel accuracy")," (the proportion of correctly labeled pixels out of total pixels), though it can be misleading if there is a large class imbalance (for example, images with a big background region overshadowing smaller but critical objects)."),"\n",l.createElement(t.h3,{id:"data-splitting-strategies-trainvalidationtest",style:{position:"relative"}},l.createElement(t.a,{href:"#data-splitting-strategies-trainvalidationtest","aria-label":"data splitting strategies trainvalidationtest permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"data splitting strategies (train/validation/test)"),"\n",l.createElement(t.p,null,"Just as with classification or detection tasks, it is vital to hold out a validation set and a final test set to mitigate overfitting. But an additional caution is that segmentation tasks are highly sensitive to distribution shifts. For instance, if you are building a medical segmentation model, ensuring that the images in the test set come from different patients, different scanners, or different clinical centers than those in the training set can help ensure that the resulting performance metrics are realistic. Cross-validation is also a popular approach in segmentation tasks, especially in fields with limited data availability (e.g., medical imaging)."),"\n",l.createElement(t.h2,{id:"deep-learning-based-segmentation-approaches",style:{position:"relative"}},l.createElement(t.a,{href:"#deep-learning-based-segmentation-approaches","aria-label":"deep learning based segmentation approaches permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"deep learning-based segmentation approaches"),"\n",l.createElement(t.h3,{id:"encoder-decoder-style-models-review-of-autoencoder-principles",style:{position:"relative"}},l.createElement(t.a,{href:"#encoder-decoder-style-models-review-of-autoencoder-principles","aria-label":"encoder decoder style models review of autoencoder principles permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"encoder-decoder style models (review of autoencoder principles)"),"\n",l.createElement(t.p,null,"A broad class of segmentation solutions use ",l.createElement(r.A,null,"encoder-decoder structures"),". In essence, an encoder compresses the spatial resolution — hopefully capturing meaningful abstract features — and the decoder uses these features to produce dense pixel-level predictions at high resolution."),"\n",l.createElement(t.p,null,l.createElement(r.A,null,"Autoencoders"),' in general attempt to compress an input to a latent representation (encoder) and then reconstruct that input from the latent vector (decoder). In segmentation networks, the principle is similar, except that the "reconstruction target" is not the raw input image but a segmentation mask. If you imagine an autoencoder for a segmentation mask, you see how the same pattern can be leveraged to create a mask from an input image.'),"\n",l.createElement(t.h3,{id:"basic-cnn-feature-extraction-recap",style:{position:"relative"}},l.createElement(t.a,{href:"#basic-cnn-feature-extraction-recap","aria-label":"basic cnn feature extraction recap permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"basic cnn feature extraction recap"),"\n",l.createElement(t.p,null,"Although readers of this course have likely encountered standard CNN modules through object detection or classification, it is useful to keep in mind:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Typical backbones"),": VGG, ResNet, DenseNet, MobileNet, etc. provide powerful feature extractors."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Downsampling"),": Repeated convolution and pooling reduce the spatial size of feature maps."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Decoding or upsampling"),": To get back to the original resolution, we need methods like bilinear upsampling, transpose convolution, or other variants (e.g., sub-pixel convolution)."),"\n"),"\n",l.createElement(t.p,null,"In object detection, the final layers predict bounding box coordinates and classification scores. In segmentation, the final layers produce segmentation logits at each pixel location."),"\n",l.createElement(t.h3,{id:"handling-overfitting-data-augmentation-and-regularization",style:{position:"relative"}},l.createElement(t.a,{href:"#handling-overfitting-data-augmentation-and-regularization","aria-label":"handling overfitting data augmentation and regularization permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"handling overfitting, data augmentation, and regularization"),"\n",l.createElement(t.p,null,"Segmentation models, with their large parameter counts and pixel-level predictions, are prone to overfitting — particularly if the dataset is small. Key strategies include:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Data augmentation"),": Random crops, flips, rotations, color jitter, perspective warping."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Regularization"),": Techniques like dropout, weight decay, and batch normalization can help."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Patch-based training"),": In some settings, especially with large medical images or satellite imagery, random patches can be extracted during training to reduce memory usage and increase sample diversity."),"\n"),"\n",l.createElement(t.h3,{id:"transfer-learning-for-segmentation",style:{position:"relative"}},l.createElement(t.a,{href:"#transfer-learning-for-segmentation","aria-label":"transfer learning for segmentation permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"transfer learning for segmentation"),"\n",l.createElement(t.p,null,"Many practitioners fine-tune a segmentation model from a classification checkpoint (e.g., a pre-trained ResNet) or from a detection checkpoint (e.g., a pre-trained model from MS COCO). This approach often yields faster convergence and higher accuracy, reflecting the fact that early convolutional layers learn generic features such as edges, corners, or textures that are beneficial across tasks."),"\n",l.createElement(t.h2,{id:"segmentation-architectures",style:{position:"relative"}},l.createElement(t.a,{href:"#segmentation-architectures","aria-label":"segmentation architectures permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"segmentation architectures"),"\n",l.createElement(t.h3,{id:"u-net",style:{position:"relative"}},l.createElement(t.a,{href:"#u-net","aria-label":"u net permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"u-net"),"\n",l.createElement(t.p,null,"Originally designed for biomedical image segmentation, ",l.createElement(r.A,null,"U-Net")," remains a highly popular architecture for a variety of domains:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Symmetric encoder-decoder"),": The left half of the U is the encoder, which typically uses standard convolutional blocks plus pooling to downsample. The right half is the decoder, which uses transpose convolutions or upsampling to restore spatial resolution."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Skip connections"),": Each encoder block at a given resolution is connected to the corresponding decoder block at the same resolution. This helps the model recover spatial details that might otherwise be lost during downsampling."),"\n"),"\n",l.createElement(t.p,null,"U-Net has proven exceptionally successful in contexts where training data is limited, thanks to these skip connections and the ability to capture both local and global context. Its success has led to numerous variants: U-Net++ (Zhou and gang, 2018), Attention U-Net, V-Net (for volumetric data), etc."),"\n",l.createElement(a,{alt:"High-level schematic of a U-Net architecture",path:"",caption:"Basic structure of a U-Net with skip connections from encoder to decoder stages",zoom:"false"}),"\n",l.createElement(t.h3,{id:"fpn-feature-pyramid-network",style:{position:"relative"}},l.createElement(t.a,{href:"#fpn-feature-pyramid-network","aria-label":"fpn feature pyramid network permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"fpn (feature pyramid network)"),"\n",l.createElement(t.p,null,l.createElement(r.A,null,"Feature Pyramid Networks")," (Lin and gang, CVPR 2017) propose a method to fuse multi-scale features from different levels of a backbone CNN. FPN is not exclusively for segmentation — it is also used extensively in object detection (e.g., in Mask R-CNN). The central idea is that features from shallow layers capture high resolution but maybe less semantic depth, while deeper layers capture more abstract, semantic information at lower resolution. By creating a top-down pathway with lateral connections, FPN effectively fuses these multi-level feature maps, yielding a feature pyramid that has both semantically strong and high-resolution representations. For segmentation tasks, FPN can help localize small objects better while also capturing large objects and global context."),"\n",l.createElement(t.h3,{id:"deeplabv3",style:{position:"relative"}},l.createElement(t.a,{href:"#deeplabv3","aria-label":"deeplabv3 permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"deeplabv3"),"\n",l.createElement(t.p,null,"DeepLab is a family of architectures (DeepLabv1, v2, v3, and v3+) that introduced and refined the concept of ",l.createElement(r.A,null,"atrous (dilated) convolutions")," to capture multi-scale context without losing resolution through pooling. Specifically, ",l.createElement(r.A,null,"DeepLabv3")," introduced the Atrous Spatial Pyramid Pooling (ASPP) module, which applies dilated convolutions at different rates (scales), then pools and concatenates the results to handle objects of varying sizes. The effective receptive field of the convolution is broadened without increasing the number of parameters significantly. This approach has proven to be among the best for semantic segmentation in many benchmarks."),"\n",l.createElement(a,{alt:"Atrous Spatial Pyramid Pooling concept",path:"",caption:"Atrous convolutions with different rates (dilations) aggregated together to form a spatial pyramid",zoom:"false"}),"\n",l.createElement(t.p,null,"In practice, these atrous convolutions can be arranged in parallel or in cascade, with the objective of allowing the network to see context from both near and far pixels. This multi-scale awareness is critical for images containing objects of significantly varying sizes — like a tiny person in the distance or a giant building occupying most of the image."),"\n",l.createElement(t.h3,{id:"other-noteworthy-architectures-eg-mask-r-cnn-pspnet",style:{position:"relative"}},l.createElement(t.a,{href:"#other-noteworthy-architectures-eg-mask-r-cnn-pspnet","aria-label":"other noteworthy architectures eg mask r cnn pspnet permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"other noteworthy architectures (e.g., mask r-cnn, pspnet)"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(r.A,null,"Mask R-CNN"),": This is an extension of Faster R-CNN for instance segmentation. After detecting bounding boxes, a small segmentation head is applied on the features inside each box to produce a mask for that instance. This approach naturally integrates detection and segmentation and has become a standard for instance-level tasks."),"\n"),"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(r.A,null,"PSPNet")," (Pyramid Scene Parsing Network): This network includes a pyramid pooling module to gather context from different subregions of the feature map, enabling global context modeling. The final feature representation is then aggregated and combined to yield strong semantic segmentation outputs."),"\n"),"\n"),"\n",l.createElement(t.h3,{id:"transformer-based-segmentation-models",style:{position:"relative"}},l.createElement(t.a,{href:"#transformer-based-segmentation-models","aria-label":"transformer based segmentation models permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"transformer-based segmentation models"),"\n",l.createElement(t.p,null,"Recently, ",l.createElement(r.A,null,"Vision Transformers (ViT)")," have gained popularity for classification tasks. Researchers have adapted them for segmentation in multiple ways:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"SETR"),": The authors replaced the CNN backbone with a pure transformer for semantic segmentation. A convolution-free encoder is followed by a decoder that aggregates tokens back into a dense mask."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Segmenter")," (Strudel and gang, ICCV 2021): Another approach harnessing transformers' self-attention for capturing long-range dependencies, potentially leading to better boundary delineation and more globally consistent segmentation."),"\n"),"\n",l.createElement(t.p,null,"Although these transformer-based models are still evolving and can be quite large, they are seen as a new frontier for potentially improved segmentation accuracy and a unified approach across different vision tasks."),"\n",l.createElement(t.h2,{id:"implementation-and-practical-considerations",style:{position:"relative"}},l.createElement(t.a,{href:"#implementation-and-practical-considerations","aria-label":"implementation and practical considerations permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"implementation and practical considerations"),"\n",l.createElement(t.h3,{id:"data-preprocessing-with-opencv",style:{position:"relative"}},l.createElement(t.a,{href:"#data-preprocessing-with-opencv","aria-label":"data preprocessing with opencv permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"data preprocessing with opencv"),"\n",l.createElement(t.p,null,"Preprocessing for segmentation typically includes:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(r.A,null,"Resizing")," images to a consistent shape (if large variability in input dimensions exists)."),"\n",l.createElement(t.li,null,l.createElement(r.A,null,"Normalization")," of pixel intensities or color channels. For instance, subtracting the mean and dividing by the standard deviation, or rescaling pixel values to [0,1]."),"\n",l.createElement(t.li,null,l.createElement(r.A,null,"Color space transformations")," (e.g., converting from BGR to RGB) as required by certain frameworks or networks."),"\n"),"\n",l.createElement(t.p,null,"Using OpenCV, a typical preprocessing snippet in Python might look like this:"),"\n",l.createElement(o.A,{text:"\nimport cv2\nimport numpy as np\n\ndef preprocess(image, new_size=(512, 512)):\n    # Resize\n    image_resized = cv2.resize(image, new_size, interpolation=cv2.INTER_LINEAR)\n    \n    # Convert to RGB if needed\n    image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)\n    \n    # Normalize by mean and std\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    \n    # Scale to [0, 1]\n    image_rgb = image_rgb / 255.0\n    \n    # Normalize\n    image_normalized = (image_rgb - mean) / std\n    \n    # Reorder dimensions to channel-first\n    image_transposed = np.transpose(image_normalized, (2, 0, 1))\n    \n    return image_transposed\n"}),"\n",l.createElement(t.h3,{id:"frameworks-and-libraries-tensorflow-pytorch-keras",style:{position:"relative"}},l.createElement(t.a,{href:"#frameworks-and-libraries-tensorflow-pytorch-keras","aria-label":"frameworks and libraries tensorflow pytorch keras permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"frameworks and libraries (tensorflow, pytorch, keras)"),"\n",l.createElement(t.p,null,"Both ",l.createElement(r.A,null,"TensorFlow")," (with Keras) and ",l.createElement(r.A,null,"PyTorch")," are popular for building segmentation pipelines:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"PyTorch"),' typically offers more "pythonic" dynamic graph building and a large ecosystem of open-source repositories (e.g., segmentation_models.pytorch).'),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"TensorFlow/Keras")," provides a higher-level API in many cases, with wide industrial adoption and integrated deployment solutions (TensorFlow Serving, TensorFlow Lite)."),"\n"),"\n",l.createElement(t.p,null,"They both offer specialized layers for transpose convolution or specialized upsampling methods. High-level libraries (like MONAI for medical imaging or Albumentations for image augmentation) provide convenience functions for segmentation tasks."),"\n",l.createElement(t.h3,{id:"training-pipelines-and-hyperparameter-tuning",style:{position:"relative"}},l.createElement(t.a,{href:"#training-pipelines-and-hyperparameter-tuning","aria-label":"training pipelines and hyperparameter tuning permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"training pipelines and hyperparameter tuning"),"\n",l.createElement(t.p,null,"Training a segmentation model typically follows the standard deep learning pipeline with specialized considerations:"),"\n",l.createElement(t.ol,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Batch size"),": Training in segmentation can be memory-intensive, because the input images or their feature maps can be large. Reducing batch size or cropping patches can help."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Learning rate schedules"),": Common to use an initial learning rate with step decay or cosine annealing."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Optimization"),": Standard optimizers like Adam or SGD with momentum are prevalent, but advanced optimizers (RAdam, LAMB) can also appear in large-scale training scenarios."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Loss functions"),": Typically cross-entropy or variants (e.g., focal loss for class imbalance, Dice loss in medical imaging)."),"\n"),"\n",l.createElement(t.p,null,"In practice, we might combine multiple losses — for instance, a weighted sum of cross-entropy and Dice loss — to better handle both large and small objects and mitigate class imbalance."),"\n",l.createElement(t.h3,{id:"managing-computational-resources-and-gpu-usage",style:{position:"relative"}},l.createElement(t.a,{href:"#managing-computational-resources-and-gpu-usage","aria-label":"managing computational resources and gpu usage permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"managing computational resources and gpu usage"),"\n",l.createElement(t.p,null,"Segmentation tasks are computationally heavy, especially for high-resolution images. Approaches include:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Mixed-precision training"),": Using half-precision floats (FP16) for speed and memory gains."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Gradient accumulation"),": Splitting a batch across multiple smaller micro-batches to effectively simulate a larger batch size on limited GPU memory."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Distributed training"),": If multiple GPUs or multi-node clusters are available, frameworks such as ",l.createElement(r.A,null,"PyTorch DistributedDataParallel")," or ",l.createElement(r.A,null,"Horovod")," can parallelize training and handle large datasets more efficiently."),"\n"),"\n",l.createElement(t.h3,{id:"model-deployment-and-optimization",style:{position:"relative"}},l.createElement(t.a,{href:"#model-deployment-and-optimization","aria-label":"model deployment and optimization permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"model deployment and optimization"),"\n",l.createElement(t.p,null,"Once a segmentation model is trained, real-world usage might require:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Export")," to a lightweight format: ONNX or TensorFlow Lite for edge devices."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Model quantization"),": 8-bit integer quantization can drastically reduce model size and improve inference speed (especially on specialized hardware) at a modest cost in accuracy."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Pruning and channel reduction"),": Techniques that remove redundant parameters can allow real-time or near-real-time inference on embedded devices."),"\n"),"\n",l.createElement(t.p,null,"Such deployment optimizations are especially important in scenarios like robotics or mobile/AR applications, where latency and memory overhead must be carefully managed."),"\n",l.createElement(t.h2,{id:"advanced-techniques-and-variations",style:{position:"relative"}},l.createElement(t.a,{href:"#advanced-techniques-and-variations","aria-label":"advanced techniques and variations permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"advanced techniques and variations"),"\n",l.createElement(t.h3,{id:"attention-mechanisms-in-segmentation",style:{position:"relative"}},l.createElement(t.a,{href:"#attention-mechanisms-in-segmentation","aria-label":"attention mechanisms in segmentation permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"attention mechanisms in segmentation"),"\n",l.createElement(t.p,null,"Attention modules can help a segmentation model focus on the most relevant parts of the feature maps:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Spatial attention"),": Learns an attention map that highlights relevant spatial locations."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Channel attention"),": Learns how to reweight feature channels based on their importance."),"\n"),"\n",l.createElement(t.p,null,"Examples include Squeeze-and-Excitation (Hu and gang) or more advanced self-attention blocks reminiscent of transformers, but integrated into CNN-based segmentation architectures (e.g., DANet)."),"\n",l.createElement(t.h3,{id:"semi-supervised-and-unsupervised-segmentation-approaches",style:{position:"relative"}},l.createElement(t.a,{href:"#semi-supervised-and-unsupervised-segmentation-approaches","aria-label":"semi supervised and unsupervised segmentation approaches permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"semi-supervised and unsupervised segmentation approaches"),"\n",l.createElement(t.p,null,"A major bottleneck in segmentation is the cost of obtaining per-pixel annotations. Recent research is exploring:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Semi-supervised segmentation"),": Combining a small fully labeled dataset with a larger unlabeled dataset, e.g., via consistency regularization."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Weakly supervised segmentation"),": Using bounding boxes, image-level tags, or scribbles to guide segmentation without full masks."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Unsupervised segmentation"),": Approaches like clustering in the feature space can segment images without labels, although the accuracy for complex tasks might be limited."),"\n"),"\n",l.createElement(t.h3,{id:"real-time-segmentation-methods-for-edge-devices",style:{position:"relative"}},l.createElement(t.a,{href:"#real-time-segmentation-methods-for-edge-devices","aria-label":"real time segmentation methods for edge devices permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"real-time segmentation methods for edge devices"),"\n",l.createElement(t.p,null,"Real-time segmentation has gained momentum thanks to applications in robotics, AR/VR, and mobile:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Lightweight backbones"),": E.g., MobileNet, ShuffleNet, or EfficientNet-based encoders."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Fast decoders"),": Minimizing skip connections, using simpler upsampling blocks."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Knowledge distillation"),": Transferring knowledge from a large teacher model to a smaller student for fast inference."),"\n"),"\n",l.createElement(t.p,null,"Popular real-time networks include ",l.createElement(r.A,null,"ENet"),", ",l.createElement(r.A,null,"ESPNet"),", and ",l.createElement(r.A,null,"Fast-SCNN"),"."),"\n",l.createElement(t.h3,{id:"domain-adaptation-for-specialized-tasks",style:{position:"relative"}},l.createElement(t.a,{href:"#domain-adaptation-for-specialized-tasks","aria-label":"domain adaptation for specialized tasks permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"domain adaptation for specialized tasks"),"\n",l.createElement(t.p,null,"When deploying segmentation solutions across different domains (e.g., from synthetic data to real-world data, or from one medical imaging modality to another), domain gaps arise due to different lighting conditions, noise characteristics, etc. Domain adaptation techniques aim to align feature distributions across source and target domains. Some approaches incorporate adversarial training to make the source and target distributions indistinguishable at some latent feature level."),"\n",l.createElement(t.h3,{id:"multi-task-learning-and-joint-training",style:{position:"relative"}},l.createElement(t.a,{href:"#multi-task-learning-and-joint-training","aria-label":"multi task learning and joint training permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"multi-task learning and joint training"),"\n",l.createElement(t.p,null,"Sometimes, segmentation is only part of the puzzle. One might also need depth estimation, surface normals, or optical flow. Training multiple tasks simultaneously with shared encoders can lead to performance improvements for all tasks due to shared feature representations. This synergy is especially helpful when the tasks are complementary (e.g., depth estimation can help the model learn shape cues that improve segmentation)."),"\n",l.createElement(t.h2,{id:"real-world-applications",style:{position:"relative"}},l.createElement(t.a,{href:"#real-world-applications","aria-label":"real world applications permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"real-world applications"),"\n",l.createElement(t.h3,{id:"medical-imaging-organtumor-segmentation",style:{position:"relative"}},l.createElement(t.a,{href:"#medical-imaging-organtumor-segmentation","aria-label":"medical imaging organtumor segmentation permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"medical imaging (organ/tumor segmentation)"),"\n",l.createElement(t.p,null,"In medical imaging, segmentation of anatomical structures (brain tumors, lung nodules, cardiac chambers) is a cornerstone for diagnosis, surgical planning, and treatment monitoring. For instance, a hospital might want an automated pipeline that takes MRI scans of the brain and precisely delineates tumor boundaries. Deep segmentation approaches like U-Net, V-Net (for 3D volumes), and specialized transformer-based medical segmentation models have been widely adopted. Researchers continue to address challenges such as scarce labeled data, class imbalance (tumors might occupy a tiny fraction of the volume), and domain shifts across different imaging devices."),"\n",l.createElement(t.h3,{id:"autonomous-vehicles-lane-and-pedestrian-segmentation",style:{position:"relative"}},l.createElement(t.a,{href:"#autonomous-vehicles-lane-and-pedestrian-segmentation","aria-label":"autonomous vehicles lane and pedestrian segmentation permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"autonomous vehicles (lane and pedestrian segmentation)"),"\n",l.createElement(t.p,null,"For self-driving cars, ",l.createElement(r.A,null,"pixel-level")," understanding of the scene is critical. Autonomous vehicles rely on segmentation networks to identify drivable regions, lane markings, pedestrians, cyclists, traffic signs, and more. This helps the downstream planning modules determine safe navigation paths. Datasets like Cityscapes or Mapillary Vistas specifically focus on street-level imagery with finely annotated masks, which has helped push the field forward."),"\n",l.createElement(a,{alt:"Street scene with segmentation masks",path:"",caption:"Segmentation for lanes, vehicles, pedestrians, and other scene components",zoom:"false"}),"\n",l.createElement(t.h3,{id:"satellite-imagery-and-agriculture",style:{position:"relative"}},l.createElement(t.a,{href:"#satellite-imagery-and-agriculture","aria-label":"satellite imagery and agriculture permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"satellite imagery and agriculture"),"\n",l.createElement(t.p,null,"Segmenting large-scale satellite or aerial images helps with:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"Land cover classification (e.g., farmland vs. forest vs. urban)"),"\n",l.createElement(t.li,null,"Crop monitoring (e.g., analyzing vegetation health)"),"\n",l.createElement(t.li,null,"Environmental conservation (e.g., tracking deforestation or water resources)"),"\n"),"\n",l.createElement(t.p,null,"Challenges in satellite segmentation include extremely high-resolution imagery and the need to handle huge numbers of pixels. Methods that leverage multi-spectral data and domain adaptation for different sensors (e.g., visible vs. infrared) are commonly used."),"\n",l.createElement(t.h3,{id:"interactive-tools-photo-editors-ar-applications",style:{position:"relative"}},l.createElement(t.a,{href:"#interactive-tools-photo-editors-ar-applications","aria-label":"interactive tools photo editors ar applications permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"interactive tools (photo editors, ar applications)"),"\n",l.createElement(t.p,null,"Many photo editing programs let users quickly select the foreground object or remove backgrounds. Behind the scenes, sophisticated segmentation algorithms are often at play. In augmented reality (AR), real-time segmentation can overlay or place virtual objects behind or in front of scene elements with correct occlusion relationships."),"\n",l.createElement(t.h3,{id:"industrial-inspection-and-robotics",style:{position:"relative"}},l.createElement(t.a,{href:"#industrial-inspection-and-robotics","aria-label":"industrial inspection and robotics permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"industrial inspection and robotics"),"\n",l.createElement(t.p,null,"Robot arms on manufacturing floors rely on segmentation to locate and manipulate parts accurately. In industrial inspection, segmentation might help localize defects on surfaces or detect anomalies in product assemblies. Given the potential variety of objects and the need for robust performance, fine-tuned segmentation models with domain knowledge are often deployed in such scenarios."),"\n",l.createElement(t.h2,{id:"challenges-and-future-directions",style:{position:"relative"}},l.createElement(t.a,{href:"#challenges-and-future-directions","aria-label":"challenges and future directions permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"challenges and future directions"),"\n",l.createElement(t.h3,{id:"class-imbalance-and-small-object-segmentation",style:{position:"relative"}},l.createElement(t.a,{href:"#class-imbalance-and-small-object-segmentation","aria-label":"class imbalance and small object segmentation permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"class imbalance and small object segmentation"),"\n",l.createElement(t.p,null,"Real-world data often has skewed distributions (e.g., large background areas, small foreground objects). To address this, researchers use specialized losses like ",l.createElement(r.A,null,"focal loss"),", or data-level methods like oversampling minority classes. Online Hard Example Mining (OHEM) can also help by focusing training on pixels that are misclassified more often. Small object segmentation remains challenging, since these objects can be easily overlooked by CNNs that downsample the image too aggressively."),"\n",l.createElement(t.h3,{id:"ethical-considerations-data-privacy-biased-datasets",style:{position:"relative"}},l.createElement(t.a,{href:"#ethical-considerations-data-privacy-biased-datasets","aria-label":"ethical considerations data privacy biased datasets permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"ethical considerations (data privacy, biased datasets)"),"\n",l.createElement(t.p,null,"Large segmentation datasets frequently include sensitive imagery, such as medical scans. Preserving patient privacy and complying with HIPAA or GDPR can become major concerns, requiring careful anonymization. Biased datasets can propagate or amplify unfair outcomes in real-world applications, such as mis-segmenting objects for certain demographics or incorrectly segmenting certain geographies in satellite data. Addressing these biases is a crucial step toward equitable AI systems."),"\n",l.createElement(t.h3,{id:"large-scale-datasets-and-computational-constraints",style:{position:"relative"}},l.createElement(t.a,{href:"#large-scale-datasets-and-computational-constraints","aria-label":"large scale datasets and computational constraints permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"large-scale datasets and computational constraints"),"\n",l.createElement(t.p,null,"With the rise of city-scale or planet-scale analysis (e.g., satellite imagery with billions of pixels), computational constraints become significant. Training a single epoch on such data might take days or weeks with naive approaches. Therefore, distributed training strategies and advanced data loading pipelines become essential, as does research into more efficient segmentation architectures."),"\n",l.createElement(t.h3,{id:"emerging-research-areas-transformers-for-segmentation-3d-segmentation",style:{position:"relative"}},l.createElement(t.a,{href:"#emerging-research-areas-transformers-for-segmentation-3d-segmentation","aria-label":"emerging research areas transformers for segmentation 3d segmentation permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"emerging research areas (transformers for segmentation, 3d segmentation)"),"\n",l.createElement(t.p,null,"Transformers are increasingly used to capture global pixel relationships. We see new architectures — like ",l.createElement(r.A,null,"Mask2Former"),", ",l.createElement(r.A,null,"SegFormer"),", and variations on ",l.createElement(r.A,null,"DeTR (Detection Transformer)"),"-style modules — that unify detection and segmentation. Meanwhile, 3D or volumetric segmentation is taking off in medical imaging (e.g., for MRI or CT scans), plus 3D point cloud segmentation for self-driving or robotics. These tasks demand specialized architectures that incorporate 3D convolutions or point-based operators."),"\n",l.createElement(t.h3,{id:"model-interpretability-and-explainability",style:{position:"relative"}},l.createElement(t.a,{href:"#model-interpretability-and-explainability","aria-label":"model interpretability and explainability permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"model interpretability and explainability"),"\n",l.createElement(t.p,null,"In high-stakes domains (e.g., healthcare), explaining why a segmentation model decided that certain pixels are part of a tumor boundary is vital. Techniques like ",l.createElement(r.A,null,"Grad-CAM")," or integrated gradients for segmentation can help visualize which regions of the input contributed most to the network's prediction, thereby supporting transparency and trust in the model's outputs."),"\n",l.createElement(t.h2,{id:"extra-chapter-classical-graph-based-methods-revisited",style:{position:"relative"}},l.createElement(t.a,{href:"#extra-chapter-classical-graph-based-methods-revisited","aria-label":"extra chapter classical graph based methods revisited permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"extra chapter: classical graph-based methods revisited"),"\n",l.createElement(t.p,null,"Although modern deep learning approaches dominate in accuracy and flexibility, classical ",l.createElement(r.A,null,"graph-based segmentation")," and ",l.createElement(r.A,null,"normalized cuts")," methods remain influential. They demonstrate fundamental principles of grouping pixels according to similarity and partitioning images into coherent regions."),"\n",l.createElement(t.h3,{id:"graph-based-segmentation",style:{position:"relative"}},l.createElement(t.a,{href:"#graph-based-segmentation","aria-label":"graph based segmentation permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"graph-based segmentation"),"\n",l.createElement(t.p,null,"We represent each pixel as a node in a graph. An edge connects nodes if they are neighbors in the image grid (often including diagonal adjacency), and the edge weight captures the dissimilarity (or conversely, similarity) of those pixels — e.g., color intensity difference. A widely cited method introduced by Felzenszwalb and Huttenlocher (2004) merges regions by sorting edges in order of increasing weight (Kruskal's MST-based approach). Let ",l.createElement(s.A,{text:"\\(w(e)\\)"})," be the difference measure for the edge ",l.createElement(s.A,{text:"\\(e = (v_1, v_2)\\)"}),". The internal difference of a region ",l.createElement(s.A,{text:"\\(R\\)"})," can be defined as:"),"\n",l.createElement(s.A,{text:"\\[\n\\mathrm{Int}(R) = \\max_{e \\in MST(R)} w(e)\n\\]"}),"\n",l.createElement(t.p,null,"The difference between two adjacent regions ",l.createElement(s.A,{text:"\\(R_1\\)"})," and ",l.createElement(s.A,{text:"\\(R_2\\)"})," can be defined as:"),"\n",l.createElement(s.A,{text:"\\[\n\\mathrm{Dif}(R_1, R_2) = \\min_{(v_1 \\in R_1, v_2 \\in R_2)} w(v_1, v_2)\n\\]"}),"\n",l.createElement(t.p,null,"The algorithm merges two regions if ",l.createElement(s.A,{text:"\\(\\mathrm{Dif}(R_1, R_2) < \\min(\\mathrm{Int}(R_1) + \\tau(R_1), \\mathrm{Int}(R_2) + \\tau(R_2))\\)"}),". A common heuristic sets ",l.createElement(s.A,{text:"\\(\\tau(R) = k / |R|\\)"}),". Merging these regions iteratively yields a hierarchical segmentation. Although overshadowed by deep learning solutions for complex tasks, this method is computationally efficient (",l.createElement(s.A,{text:"\\(O(N \\log N)\\)"}),") and can provide good initial over-segmentations (e.g., superpixels) for subsequent neural network processing."),"\n",l.createElement(t.h3,{id:"normalized-cuts",style:{position:"relative"}},l.createElement(t.a,{href:"#normalized-cuts","aria-label":"normalized cuts permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"normalized cuts"),"\n",l.createElement(t.p,null,l.createElement(r.A,null,"Normalized cuts")," focus on partitioning a graph into strongly connected subgraphs. The cut between subgraphs ",l.createElement(s.A,{text:"\\(A\\)"})," and ",l.createElement(s.A,{text:"\\(B\\)"})," is:"),"\n",l.createElement(s.A,{text:"\\( \n\\mathrm{cut}(A,B) = \\sum_{i \\in A, j \\in B} w_{ij} \n\\)"}),"\n",l.createElement(t.p,null,"But we prefer to minimize the ",l.createElement(r.A,null,"normalized cut"),":"),"\n",l.createElement(s.A,{text:"\\[\nNcut(A,B) = \\frac{\\mathrm{cut}(A,B)}{\\mathrm{assoc}(A,V)} + \\frac{\\mathrm{cut}(A,B)}{\\mathrm{assoc}(B,V)}\n\\]"}),"\n",l.createElement(t.p,null,"where ",l.createElement(s.A,{text:"\\(\\mathrm{assoc}(A,V)\\)"})," is the sum of connections between all nodes in ",l.createElement(s.A,{text:"\\(A\\)"})," and all nodes in ",l.createElement(s.A,{text:"\\(V\\)"}),". Minimizing this objective can be done via spectral graph theory, but at high computational cost if the graph is large. Multi-level or hierarchical approaches that coarsen the graph can reduce computational demands, leading to approximate solutions. While deep learning is more commonly used in production for semantic or instance segmentation tasks, classical normalized cuts remain an important building block in the theoretical foundations of image partitioning."),"\n",l.createElement(t.h2,{id:"final-remarks",style:{position:"relative"}},l.createElement(t.a,{href:"#final-remarks","aria-label":"final remarks permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"final remarks"),"\n",l.createElement(t.p,null,"Image object segmentation has come a long way from threshold-based heuristics and region-growing algorithms to modern end-to-end trainable architectures powered by convolutional layers, attention mechanisms, and even transformers. The journey has been enabled by big data, advanced GPUs, and the synergy of academic and industrial research. Today, segmentation is ubiquitous in fields as diverse as medical imaging, autonomous driving, satellite remote sensing, robotics, and interactive image editing. And with ongoing research in semi-supervised approaches, 3D segmentation, real-time methods for edge devices, and more, the field continues to expand."),"\n",l.createElement(t.p,null,"The shift toward ",l.createElement(r.A,null,"vision transformers"),", ",l.createElement(r.A,null,"multi-task learning"),", and ",l.createElement(r.A,null,"self-/unsupervised methods")," signals that segmentation's future will involve increasingly flexible and data-efficient solutions. Additionally, domain adaptation, privacy-preserving methods, and fairness considerations will shape how segmentation models are developed and deployed responsibly. For practitioners and researchers alike, staying abreast of these next-generation ideas while also mastering classical concepts remains key to pushing the boundaries of automated image understanding."),"\n",l.createElement(t.p,null,'Overall, the pixel-level accuracy of segmentation stands as a true litmus test for how thoroughly a model "understands" an image. Whether it is to delineate a malignant region in a CT scan or to localize every curb, lane marking, and pedestrian in a bustling city scene, segmentation methods have proven to be some of the most impactful, nuanced, and rapidly evolving areas in modern computer vision. Through a firm grasp of the core principles, architectures, and future directions outlined here, data scientists and machine learning engineers can effectively tackle challenging segmentation problems and continue advancing the frontier of visual perception.'),"\n",l.createElement(t.p,null,l.createElement(r.A,null,"References cited (selection)"),":"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,'Long and gang, "Fully Convolutional Networks for Semantic Segmentation", CVPR 2015'),"\n",l.createElement(t.li,null,'Ronneberger and gang, "U-Net: Convolutional Networks for Biomedical Image Segmentation", MICCAI 2015'),"\n",l.createElement(t.li,null,'Lin and gang, "Feature Pyramid Networks for Object Detection", CVPR 2017'),"\n",l.createElement(t.li,null,'Chen and gang, "DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs", TPAMI 2018'),"\n",l.createElement(t.li,null,'Strudel and gang, "Segmenter: Transformer for Semantic Segmentation", ICCV 2021'),"\n"),"\n",l.createElement(t.p,null,"You can find many more specific references for each sub-topic in specialized literature. Explore them if you want a deeper dive into the theoretical underpinnings or to learn from the open-source code repositories that have shaped state-of-the-art segmentation research."))}var m=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,i.RP)(),e.components);return t?l.createElement(t,e,l.createElement(c,e)):c(e)};var d=a(36710),h=a(58481),u=a.n(h),g=a(36310),p=a(87245),f=a(27042),v=a(59849),b=a(5591),y=a(61122),E=a(9219),w=a(33203),S=a(95751),k=a(94328),x=a(80791),H=a(78137);const z=e=>{let{toc:t}=e;if(!t||!t.items)return null;return l.createElement("nav",{className:x.R},l.createElement("ul",null,t.items.map(((e,t)=>l.createElement("li",{key:t},l.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const a=t.replace("#",""),n=document.getElementById(a);n&&n.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&l.createElement(z,{toc:{items:e.items}}))))))};function C(e){let{data:{mdx:t,allMdx:r,allPostImages:o},children:s}=e;const{frontmatter:c,body:m,tableOfContents:d}=t,h=c.index,v=c.slug.split("/")[1],x=r.nodes.filter((e=>e.frontmatter.slug.includes(`/${v}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),C=x.findIndex((e=>e.frontmatter.index===h)),A=x[C+1],T=x[C-1],M=c.slug.replace(/\/$/,""),N=/[^/]*$/.exec(M)[0],_=`posts/${v}/content/${N}/`,{0:I,1:V}=(0,l.useState)(c.flagWideLayoutByDefault),{0:L,1:B}=(0,l.useState)(!1);var P;(0,l.useEffect)((()=>{B(!0);const e=setTimeout((()=>B(!1)),340);return()=>clearTimeout(e)}),[I]),"adventures"===v?P=E.cb:"research"===v?P=E.Qh:"thoughts"===v&&(P=E.T6);const R=u()(m).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,j=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),a=e%60;return a<=30?`~${t}${a>0?".5":""} h`:`~${t+1} h`}(Math.ceil(R/P)+(c.extraReadTimeMin||0)),D=[{flag:c.flagDraft,component:()=>Promise.all([a.e(3231),a.e(8809)]).then(a.bind(a,28809))},{flag:c.flagMindfuckery,component:()=>Promise.all([a.e(3231),a.e(2471)]).then(a.bind(a,67709))},{flag:c.flagRewrite,component:()=>Promise.all([a.e(3231),a.e(6764)]).then(a.bind(a,62002))},{flag:c.flagOffensive,component:()=>Promise.all([a.e(3231),a.e(2443)]).then(a.bind(a,17681))},{flag:c.flagProfane,component:()=>Promise.all([a.e(3231),a.e(8048)]).then(a.bind(a,53286))},{flag:c.flagMultilingual,component:()=>Promise.all([a.e(3231),a.e(4069)]).then(a.bind(a,78831))},{flag:c.flagUnreliably,component:()=>Promise.all([a.e(3231),a.e(3417)]).then(a.bind(a,8179))},{flag:c.flagPolitical,component:()=>Promise.all([a.e(3231),a.e(5195)]).then(a.bind(a,30433))},{flag:c.flagCognitohazard,component:()=>Promise.all([a.e(3231),a.e(3175)]).then(a.bind(a,8413))},{flag:c.flagHidden,component:()=>Promise.all([a.e(3231),a.e(9556)]).then(a.bind(a,14794))}],{0:O,1:F}=(0,l.useState)([]);return(0,l.useEffect)((()=>{D.forEach((e=>{let{flag:t,component:a}=e;t&&a().then((e=>{F((t=>[].concat((0,n.A)(t),[e.default])))}))}))}),[]),l.createElement(f.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},l.createElement(b.A,{postNumber:c.index,date:c.date,updated:c.updated,readTime:j,difficulty:c.difficultyLevel,title:c.title,desc:c.desc,banner:c.banner,section:v,postKey:N,isMindfuckery:c.flagMindfuckery,mainTag:c.mainTag}),l.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},c.otherTags.map(((e,t)=>l.createElement("span",{key:t,className:`noselect ${H.MW}`,style:{margin:"0 5px 5px 0"}},e)))),l.createElement("div",{className:"postBody"},l.createElement(z,{toc:d})),l.createElement("br"),l.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},l.createElement(f.P.button,{className:`noselect ${k.pb}`,id:k.xG,onClick:()=>{V(!I)},whileTap:{scale:.93}},l.createElement(f.P.div,{className:S.DJ,key:I,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},I?"Switch to default layout":"Switch to wide layout"))),l.createElement("br"),l.createElement("div",{className:"postBody",style:{margin:I?"0 -14%":"",maxWidth:I?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},l.createElement("div",{className:`${k.P_} ${L?k.Xn:k.qG}`},O.map(((e,t)=>l.createElement(e,{key:t}))),c.indexCourse?l.createElement(w.A,{index:c.indexCourse,category:c.courseCategoryName}):"",l.createElement(g.Z.Provider,{value:{images:o.nodes,basePath:_.replace(/\/$/,"")+"/"}},l.createElement(i.xA,{components:{Image:p.A}},s)))),l.createElement(y.A,{nextPost:A,lastPost:T,keyCurrent:N,section:v}))}function A(e){return l.createElement(C,e,l.createElement(m,e))}function T(e){var t,a,n,i,r;let{data:o}=e;const{frontmatter:s}=o.mdx,c=s.titleSEO||s.title,m=s.titleOG||c,h=s.titleTwitter||c,u=s.descSEO||s.desc,g=s.descOG||u,p=s.descTwitter||u,f=s.schemaType||"BlogPosting",b=s.keywordsSEO,y=s.date,E=s.updated||y,w=s.imageOG||(null===(t=s.banner)||void 0===t||null===(a=t.childImageSharp)||void 0===a||null===(n=a.gatsbyImageData)||void 0===n||null===(i=n.images)||void 0===i||null===(r=i.fallback)||void 0===r?void 0:r.src),S=s.imageAltOG||g,k=s.imageTwitter||w,x=s.imageAltTwitter||p,H=s.canonicalURL,z=s.flagHidden||!1,C=s.mainTag||"Posts",A=s.slug.split("/")[1]||"posts",{siteUrl:T}=(0,d.Q)(),M={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:T},{"@type":"ListItem",position:2,name:C,item:`${T}/${s.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${T}${s.slug}`}]};return l.createElement(v.A,{title:c+" - avrtt.blog",titleOG:m,titleTwitter:h,description:u,descriptionOG:g,descriptionTwitter:p,schemaType:f,keywords:b,datePublished:y,dateModified:E,imageOG:w,imageAltOG:S,imageTwitter:k,imageAltTwitter:x,canonicalUrl:H,flagHidden:z,mainTag:C,section:A,type:"article"},l.createElement("script",{type:"application/ld+json"},JSON.stringify(M)))}},96098:function(e,t,a){var n=a(96540),i=a(7978);t.A=e=>{let{text:t}=e;return n.createElement(i.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-js-content-file-path-src-pages-posts-research-image-object-segmentation-mdx-6d2ed08c062c8c0dfd2b.js.map