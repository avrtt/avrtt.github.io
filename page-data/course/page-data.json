{"componentChunkName":"component---src-pages-main-course-js","path":"/course/","result":{"data":{"allMdx":{"nodes":[{"id":"a2ad462c-ca37-5329-a6e8-4000829dc661","frontmatter":{"indexCourse":null,"titleCourse":"","courseCategoryName":"","extraReadTimeMin":0,"difficultyLevel":1,"flagMindfuckery":false,"slug":"/research/on_research"},"body":"\nimport Notice from \"../../../components/Notice\"\nimport StickerPack from \"../../../components/StickerPack\"\n\n\n*\"Nothing has such power to broaden the mind as the ability to investigate systematically and truly all that comes under thy observation in life.\"*\n\n<br/>\n\nSo, I don't understand anything about website building, but I'll try to make it more complex in the future. Maybe that's how I'll learn. This is now my own personal platform, and I can write absolutely anything I want and how I want, no one will ever ban me like on those stupid social media sites.\n\nHere (in this section of the site), I'll be writing neither about website building nor downshifting, but artificial intelligence — my current career path, roughly speaking.\n\n<br/>\n\n## The world needs more nerds\n\nAI is a longstanding interest for me within philosophy and ethics, and now I've finally taken on applied questions, including making it possible to earn remotely at any time right from [my hobo travels](/adventures/venturing_forth). <StickerPack sticker=\"pepe_wink\"/>\n\nI think we need AGI. I want to believe that in the next 5 years it will be possible to automate most of the routine and near-creative work. For instance, coding will become modular prompting plus human validation, instead of long reading and rewriting thousands of lines manually by a programmer. Personally, this would be a gift for me: I don't like to write code for a long time, but I like to come up with solutions that work immediately. \n\nThe knowledge needed to develop AI must be accessible and understandable. That's why I want to dedicate Research posts to clear, simple, pleasantly readable explanations of AI concepts — both basic and complex. This world needs more people who understand AI, because AI is clearly our future, and it's something that people will have to work with whether they want to or not. Many will legitimately lose careers or businesses — just because they didn't foresee the need to incorporate AI into their lives. Creative professions, for example, will continue to be needed, but only with the ability to prompt models. Not many people understand this, unfortunately.\n\nSo, my little mission here is to study and systematize as much as possible to bring knowledge and experience, my mistakes, to other people, to give them this for free for the good of science, to contribute into popularization of important questions and tools.\n\nWe, the young, are the future of humanity, and we have to deal with the gravest ethical, technical, economic, political, and social implications of the rapid development of computer science. To do this, **we need more nerds, not reactionary**. Just so that [something doesn't quietly go wrong](https://www.youtube.com/watch?v=diboERFAjkE).\n\n<br/>\n\n*I'll come back here after a while to expand the post by outlining what I learned about research itself.*"},{"id":"93a80f10-1125-5407-8f63-22a8d3eb9876","frontmatter":{"indexCourse":89,"titleCourse":"Topic modeling","courseCategoryName":"Natural language processing","extraReadTimeMin":30,"difficultyLevel":1,"flagMindfuckery":false,"slug":"/research/topic_modeling"},"body":"\nimport Highlight from \"../../../components/Highlight\"\nimport Code from \"../../../components/Code\"\nimport Latex from \"../../../components/Latex\"\n\n\n{/* *(intro: a quote, catchphrase, joke, etc.)* */}\n\n<br/>\n\n\n{/*\n\nChapter 1. Motivation, applications and use cases  \nChapter 2. Foundational concepts  \n- Latent variables in text modeling  \n- Probability distributions and their role in topic modeling  \n- Dimensionality reduction and its connection to topic modeling  \n- Terminology of topic modeling  \nChapter 3. Popular topic modeling frameworks  \n- Latent Dirichlet allocation (LDA)  \n- Probabilistic latent semantic analysis (PLSA)  \n- Non-negative matrix factorization (NMF)  \n- Hierarchical Dirichlet processes (HDP)  \n- Dynamic topic models  \nChapter 4. Steps in building a topic model  \n- Data collection and corpus preparation  \n- Data preprocessing: text cleaning, tokenization, stopword removal and filtering, stemming vs. lemmatization  \n- Model training  \n- Selecting hyperparameters  \n- Ensuring model convergence  \n- Evaluating training progress  \n- Model evaluation  \n- Topic coherence metrics  \n- Perplexity and likelihood-based metrics  \n- Human evaluations and qualitative checks  \n- Model interpretation and visualization  \n- Topic-word distributions  \n- Topic labeling techniques  \n- Visualization ools and libraries  \nChapter 5. Advanced techniques and variations  \n- Neural topic modeling  \n- Combining topic modeling with word embeddings  \n- Deep learning approaches for topic modeling  \n- Cross-lingual topic modeling  \n- Ensemble methods for topic discovery  \nChapter 6. Let's code  \n- Building complex topic model from scratch  \n\n*/}\n\n\nTopic modeling remains one of the most prominent unsupervised learning techniques used to automatically infer the hidden thematic structure from large collections of documents. While originally devised for textual data, it has also been extended to a variety of domains such as bioinformatics, image processing (where \"topics\" can represent clusters of visual features), or event detection in social networks. The overall motivation stems from the desire to discover latent factors or hidden topics that shape the content of a corpus. These latent factors in text usually manifest themselves as sets of terms that frequently co-occur together — revealing emergent themes that an analyst might not have previously anticipated.\n\nTopic modeling enables researchers, data scientists, and domain experts to drastically reduce the complexity of large textual archives. Instead of manually reading thousands (or millions) of documents, one can employ topic modeling to group documents by high-level themes. This has found wide application in:\n\n- **Content recommendation systems**: A news website or a blog aggregator might use topic modeling to categorize articles, surface them to relevant audiences, and suggest thematically related stories.  \n- **Academic research**: Digital humanities scholars utilize topic modeling to explore large bodies of literature or historical archives, gleaning patterns across centuries of texts, and discovering how themes have evolved.  \n- **Market research and brand monitoring**: Social media content, customer feedback, or product reviews can be studied to find recurring topics or sentiments. Topic modeling helps to cluster these texts into marketing-relevant themes (for instance, design features, shipping problems, pricing concerns).  \n- **Search and indexing**: Topic models can be used to index documents more efficiently, so that searching by topic yields more relevant results than naive keyword-based retrieval.  \n- **Trend analysis in social networks**: Online communities often generate textual content at astonishing velocity. Topic modeling can be applied to identify trending topics, reveal emergent phenomena, or detect changes in discourse over time.  \n- **Fraud detection and security**: In certain contexts, textual logs (like emails, chat transcripts, or security logs with textual descriptions) might contain hidden patterns. Topic models may reveal suspicious themes or patterns that signal fraud or illicit behavior.  \n- **Medical and biomedical research**: Clinical notes, scientific publications in medicine, and patient feedback can be massive in scale. Topic modeling helps cluster key areas of concern and could even highlight previously overlooked subtopics.\n\nIn sum, topic modeling is a powerful lens through which textual data can be reinterpreted. The automatically extracted topics serve as an abstraction or compressed representation of the text, helping us to handle, navigate, and conceptualize immense corpora more effectively.\n\n## Chapter 2. Foundational concepts\n\n### Latent variables in text modeling\n\nIn most topic modeling approaches, the notion of \"latent variables\" is central. A latent variable is a hidden or unobserved variable that influences the observed data. In text modeling, each document is assumed to be generated by a mixture of underlying topics — topics themselves are distributions over words or terms. The intensities or probabilities with which topics appear in a document are these latent variables that we attempt to infer.\n\nFor instance, suppose you have a large collection of news articles about world events, sports, politics, technology, and the arts. Each article is formed by certain combinations of these topic distributions (latent variables). If you find a document with 50% focus on technology, 20% on business, 20% on politics, and 10% on sports, that is effectively a set of inferred latent variables describing the composition of that document. The presence of these hidden factors is not directly observable but can be inferred through statistical means.\n\n### Probability distributions and their role in topic modeling\n\nTopic modeling frameworks usually adopt a probabilistic generative perspective. In the classic generative story of Latent Dirichlet Allocation (LDA) — which we will discuss more thoroughly soon — each topic <Latex text=\"\\( z \\)\"/> is represented by a probability distribution over words from the vocabulary. Likewise, each document <Latex text=\"\\( d \\)\"/> is represented by a distribution over topics. Mathematically, you might see the topic distribution for document <Latex text=\"\\( d \\)\"/> denoted as <Latex text=\"\\( \\theta_d \\)\"/> (a vector of probabilities summing to 1), and the word distribution for topic <Latex text=\"\\( k \\)\"/> denoted as <Latex text=\"\\( \\phi_k \\)\"/>.\n\nThen, if we pick a particular word from document <Latex text=\"\\( d \\)\"/>, we do so by first choosing a topic <Latex text=\"\\( z \\)\"/> with probability <Latex text=\"\\( \\theta_d[z] \\)\"/> and then choosing a particular word <Latex text=\"\\( w \\)\"/> from the distribution over words for that topic <Latex text=\"\\( \\phi_z[w] \\)\"/>. Formally, you might see something along the lines of:\n\n<Latex text=\"\\[\nP(w \\mid d) = \\sum_{k=1}^K P(w \\mid z=k) P(z=k \\mid d)\n\\]\"/>\n\nWhere:\n- <Latex text=\"\\( K \\)\"/> is the total number of topics.\n- <Latex text=\"\\( P(z=k \\mid d) \\equiv \\theta_{d}[k] \\)\"/> is the probability that topic <Latex text=\"\\(k\\)\"/> is chosen when generating a word from document <Latex text=\"\\(d\\)\"/>.\n- <Latex text=\"\\( P(w \\mid z=k) \\equiv \\phi_k[w] \\)\"/> is the probability of word <Latex text=\"\\(w\\)\"/> under topic <Latex text=\"\\(k\\)\"/>.\n\nHence, each topic is defined as a distribution over words, and each document is defined as a distribution over topics. This perspective allows the data scientist to exploit the entire probabilistic machinery (e.g., Bayesian inference, maximum likelihood approaches, variational inference, or Gibbs sampling) to estimate these latent variables.\n\n### Dimensionality reduction and its connection to topic modeling\n\nTopic modeling can be viewed as a form of **dimensionality reduction**. A given corpus of documents can be extremely high-dimensional if we consider each unique word as a dimension. Suppose your vocabulary has <Latex text=\"\\( V \\)\"/> words, then each document is nominally represented as a point in a <Latex text=\"\\( V \\)\"/>-dimensional space (e.g., as a bag-of-words vector). However, in a topic model with <Latex text=\"\\( K \\)\"/> latent topics, each document is effectively captured by a <Latex text=\"\\( K \\)\"/>-dimensional representation: its topic mixture distribution. Thus, the raw dimensionality <Latex text=\"\\( V \\)\"/> is reduced to <Latex text=\"\\( K \\)\"/>, while hopefully retaining most of the semantic content.\n\nIn that sense, topic modeling is loosely analogous to other matrix factorization or decomposition techniques such as Principal Component Analysis (PCA) or Non-negative Matrix Factorization (NMF). However, the difference in most topic modeling approaches is the explicit probabilistic interpretation: each dimension in the reduced space (i.e., each topic) is a probability distribution over words rather than a purely algebraic factor.\n\n### Terminology of topic modeling\n\nSome commonly encountered terms in topic modeling:\n\n- **Topic**: A probabilistic distribution over the vocabulary that tends to reflect a coherent theme (e.g., sports, politics, or technology).  \n- **Topic mixture**: The distribution over the set of topics for a specific document. Usually denoted as <Latex text=\"\\( \\theta \\)\"/> in many topic modeling frameworks.  \n- **Word distribution**: For each topic, we have a distribution over words, typically denoted <Latex text=\"\\( \\phi \\)\"/> or sometimes <Latex text=\"\\( \\beta \\)\"/>.  \n- **Hyperparameters**: These are parameters controlling the shape of the distributions over topics and words (e.g., alpha and beta in LDA).  \n- **Variational inference / Gibbs sampling**: Algorithms used in some topic models to perform inference and find the posterior distribution of latent variables.  \n\nTopic modeling frameworks vary in how they formulate the relationships among these components, but the above terms tend to remain consistent across many methods.\n\n## Chapter 3. Popular topic modeling frameworks\n\nTopic modeling has evolved to encompass an array of frameworks and paradigms, each with unique strengths, assumptions, and computational complexities. Below are some of the most commonly used:\n\n### Latent Dirichlet Allocation (LDA)\n\n**Latent Dirichlet Allocation** (Blei, Ng, & Jordan, JMLR 2003) is probably the most well-known method of topic modeling. LDA introduced a fully generative Bayesian model for documents. The name \"Dirichlet\" arises from the prior placed over the per-document topic distribution and the per-topic word distribution.\n\n- **Generative process**:  \n  1. For each document <Latex text=\"\\( d \\)\"/>, draw a distribution over topics <Latex text=\"\\( \\theta_d \\)\"/> from a Dirichlet prior with parameter <Latex text=\"\\( \\alpha \\)\"/>.  \n  2. For each topic <Latex text=\"\\( k \\)\"/>, draw a distribution over words <Latex text=\"\\( \\phi_k \\)\"/> from a Dirichlet prior with parameter <Latex text=\"\\( \\beta \\)\"/>.  \n  3. For each word in a document, choose a topic assignment <Latex text=\"\\( z \\)\"/> according to <Latex text=\"\\( \\theta_d \\)\"/>, then choose a word <Latex text=\"\\( w \\)\"/> according to <Latex text=\"\\( \\phi_z \\)\"/>.\n\nA commonly used formula for the joint distribution of the latent variables and observed words in LDA is:\n\n<Latex text=\"\\[\nP(\\theta, z, w \\mid \\alpha, \\beta) = \\prod_{d=1}^{D} P(\\theta_d \\mid \\alpha)\n\\prod_{k=1}^{K} P(\\phi_k \\mid \\beta)\n\\prod_{d=1}^{D} \\prod_{n=1}^{N_d} P(z_{d,n} \\mid \\theta_d) P(w_{d,n} \\mid \\phi_{z_{d,n}})\n\\]\"/>\n\nWhere:\n- <Latex text=\"\\( D \\)\"/> is the number of documents.\n- <Latex text=\"\\( N_d \\)\"/> is the number of words in document <Latex text=\"\\( d \\)\"/>.\n- <Latex text=\"\\( K \\)\"/> is the number of topics.\n- <Latex text=\"\\( \\alpha \\)\"/> and <Latex text=\"\\( \\beta \\)\"/> are hyperparameters for the Dirichlet priors.\n- <Latex text=\"\\( \\theta_d \\)\"/> is the distribution of topics in document <Latex text=\"\\( d \\)\"/>.\n- <Latex text=\"\\( \\phi_k \\)\"/> is the distribution over words for topic <Latex text=\"\\( k \\)\"/>.\n- <Latex text=\"\\( z_{d,n} \\)\"/> is the topic assignment for the <Latex text=\"\\( n \\)\"/>-th word in document <Latex text=\"\\( d \\)\"/>.\n- <Latex text=\"\\( w_{d,n} \\)\"/> is the actual observed word.\n\n**Inference** for LDA can be done with Gibbs sampling, variational inference, or other approximate methods. Although LDA can be computationally expensive, it remains a standard reference due to its interpretability and strong theoretical grounding.\n\n### Probabilistic latent semantic analysis (PLSA)\n\n**Probabilistic Latent Semantic Analysis** (Hofmann, 1999) can be viewed as a precursor to LDA. It also models documents in terms of latent topics, but it lacks a prior over topic distributions and is thus considered a non-Bayesian approach. Instead, it relies on a maximum likelihood estimation with an Expectation-Maximization (EM) algorithm to find the parameters.\n\nA key difference from LDA is that PLSA can overfit, especially because it introduces a large number of parameters without having a proper prior. LDA, by contrast, introduces Dirichlet priors that control the distributions and reduce overfitting. Despite this, PLSA is still used in certain contexts and provides a simpler conceptual introduction to how text can be factorized into latent topics.\n\n### Non-negative matrix factorization (NMF)\n\n**Non-negative matrix factorization** is a purely algebraic approach to factor a term-document matrix into two lower-rank matrices, both of which have only non-negative entries. If <Latex text=\"\\( X \\)\"/> is a <Latex text=\"\\( V \\times D \\)\"/> matrix representing word counts (or TF-IDF scores) of <Latex text=\"\\( V \\)\"/> words across <Latex text=\"\\( D \\)\"/> documents, NMF attempts to find two matrices <Latex text=\"\\( W \\)\"/> (size <Latex text=\"\\( V \\times K \\)\"/>) and <Latex text=\"\\( H \\)\"/> (size <Latex text=\"\\( K \\times D \\)\"/>) such that:\n\n<Latex text=\"\\[\nX \\approx W \\times H\n\\]\"/>\n\nHere, <Latex text=\"\\( K \\)\"/> is the reduced rank — often analogous to the number of topics. If <Latex text=\"\\( W \\)\"/> is interpreted as containing basis vectors (topics) and <Latex text=\"\\( H \\)\"/> as document loadings, each column of <Latex text=\"\\( H \\)\"/> tells us how strongly a topic appears in a document. The non-negativity constraint makes the factorization more interpretable compared to, say, SVD-based methods like Latent Semantic Analysis (LSA).\n\nNMF is frequently used as a faster approach compared to LDA, though it does not have a fully probabilistic interpretation. However, in practice, it can yield coherent topics that are relatively easy to interpret, especially because it keeps the weighting of words and topics strictly positive.\n\n### Hierarchical Dirichlet processes (HDP)\n\n**Hierarchical Dirichlet Processes** (Teh and gang, 2006) generalize the LDA concept to a scenario where we do not fix the number of topics <Latex text=\"\\( K \\)\"/> in advance. Instead, the model can discover an appropriate number of topics automatically, guided by the data. This is done through a hierarchical Bayesian construction known as the Dirichlet process. Essentially, the model treats <Latex text=\"\\( K \\)\"/> as infinite in principle, but in practice the posterior distribution typically places most probability mass on a finite set of topics.\n\nHDP is very appealing for large corpora where you might be uncertain about the number of distinct themes. It lets the data guide how many topics are discovered. The trade-off is that inference becomes more sophisticated, often involving specialized MCMC approaches or variational methods. Also, HDP can sometimes discover more topics than are practically interpretable, so it's not always a silver bullet.\n\n### Dynamic topic models\n\n**Dynamic topic models** (Blei & Lafferty, ICML 2006) address the evolution of topics over time. Traditional LDA presumes that each document is exchangeable (i.e., no temporal ordering). But in many domains, documents come with timestamps, and the themes may shift over months or years. For example, the vocabulary around \"technology\" changes significantly over decades, as the popular jargon shifts from mainframe computing to cloud computing to machine learning.\n\nDynamic topic models incorporate temporal dependencies by indexing the latent distributions on discrete time slices. A typical approach is to model the topics at time slice <Latex text=\"\\( t \\)\"/> as evolved from the topics at time slice <Latex text=\"\\( t-1 \\)\"/>. This is often achieved via state space models or Brownian motion in the parameter space of the distributions. This allows the model to capture how word probabilities shift over time for each topic, providing a chronological storyline of how certain themes appear, evolve, and potentially vanish.\n\n## Chapter 4. Steps in building a topic model\n\n### Data collection and corpus preparation\n\nThe first step in building any topic model is collecting the relevant text data. The data could come from web pages, academic journals, social media posts, news articles, or specialized corporate documents. Frequently, large-scale corpora are stored as sets of text files or in a database. The typical tasks in this phase include:\n\n1. **Gather documents**: Make sure you have well-labeled or at least organized data sources.  \n2. **Ensure coverage**: If the corpus is supposed to capture a wide range of topics, you need a sufficiently broad variety of texts.  \n3. **Remove duplicates**: Check for repeated documents or near-duplicates that might bias the model.  \n\nAlso, keep in mind the final application: if you are building a specialized model for, say, social media data, you want to ensure your corpus is representative of relevant user posts, not just random data that might not reflect your end goals.\n\n### Data preprocessing: text cleaning, tokenization, stopword removal and filtering, stemming vs. lemmatization\n\nTextual data is often noisy or inconsistent, especially if it is scraped from the web or user-generated. Proper preprocessing typically involves:\n\n1. **Text cleaning**: Convert text to a consistent casing (often lowercase), remove non-alphanumeric characters (or decide how to handle them), handle punctuation, convert numbers or keep them if relevant.  \n2. **Tokenization**: Split the text into tokens, typically words or subwords. Tokenizers in languages like Python (NLTK, spaCy) provide robust ways to handle this.  \n3. **Stopword removal**: Many words (like \"the\", \"is\", \"and\", etc.) appear frequently but do not hold strong semantic content. Common practice is to remove them if your topic modeling approach focuses on content words.  \n4. **Stemming or lemmatization**: Reduce words to their root form. **Stemming** is a rule-based approach that chops off word endings (\"organization\" → \"organiz\") whereas **lemmatization** uses morphological analysis to map words to their actual lemma forms (\"feet\" → \"foot\", \"organized\" → \"organize\"). Lemmatization typically yields more coherent topics but is computationally heavier.  \n5. **Filtering by token length or frequency**: It may be beneficial to remove extremely rare words that appear in only a handful of documents, as well as overly frequent words that appear in almost every document.  \n\nThese steps can dramatically affect the quality of your topic model. Preprocessing transformations might remove noise and help the model focus on semantically meaningful content.\n\n### Model training\n\nOnce you have a clean corpus and you have decided on a specific approach (e.g., LDA, NMF, HDP, etc.), the next step is to train the model. For LDA, you might use:\n\n- **Gensim** in Python, which offers an efficient implementation of online LDA.  \n- **MALLET** (a Java-based library) for topic modeling, which uses an optimized Gibbs sampler.  \n- **Scikit-learn** for simpler, smaller-scale LDA or NMF.  \n\nIn a typical workflow, you do something like:\n\n<Code text={`\nfrom gensim import corpora, models\nfrom gensim.utils import simple_preprocess\n\n# Suppose you already have tokenized_texts: a list of lists of tokens\ndictionary = corpora.Dictionary(tokenized_texts)\ndictionary.filter_extremes(no_below=5, no_above=0.5)\nbow_corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\n\nlda_model = models.LdaModel(\n    bow_corpus, \n    num_topics=20, \n    id2word=dictionary, \n    passes=10, \n    alpha='auto', \n    random_state=42\n)\n\n# Inspect the topics\ntopics = lda_model.print_topics(num_words=5)\nfor topic_id, topic_words in topics:\n    print(f\"Topic {topic_id}: {topic_words}\")\n`}/>\n\nThis snippet demonstrates a typical approach to training an LDA topic model with Gensim. The <Highlight>LdaModel</Highlight> constructor requires a bag-of-words representation <Latex text=\"\\( bow\\_corpus \\)\"/>, a chosen number of topics, the dictionary mapping, and other hyperparameters. The specific arguments can vary based on dataset size, interpretability requirements, and computational resources.\n\n### Selecting hyperparameters\n\nIn LDA, there are several hyperparameters that can significantly influence the outcome:\n\n- **Number of topics (<Latex text=\"\\( K \\)\"/>)**: Possibly the most important parameter. In practice, domain knowledge or interpretability concerns guide the choice of <Latex text=\"\\( K \\)\"/>.  \n- **Alpha (<Latex text=\"\\( \\alpha \\)\"/>)**: The Dirichlet parameter that controls document–topic sparsity. A lower alpha typically yields more sparse distributions (each document is dominated by fewer topics).  \n- **Beta (<Latex text=\"\\( \\beta \\)\"/> or sometimes referred to as <Latex text=\"\\( \\eta \\)\"/>)**: The Dirichlet parameter that controls topic–word sparsity. A lower beta means topics focus on fewer words.  \n- **Number of passes / iterations**: Controls how many times the training algorithm iterates over the corpus. More passes can improve convergence but cost additional runtime.\n\nIn frameworks like HDP, there may be extra parameters controlling the base distribution of topics and the concentration parameters that govern how many new topics are introduced. NMF likewise has parameters for the rank <Latex text=\"\\( K \\)\"/> and different update rules (e.g., multiplicative updates, coordinate descent).\n\n### Ensuring model convergence\n\nDifferent inference algorithms have different stopping criteria and convergence diagnostics. For example:\n\n- **Gibbs sampling** might run for a specified number of iterations, after which the model's state is hopefully close to stationary. One can monitor log-likelihood or use heuristics to decide how many iterations are enough.  \n- **Variational inference** can track the Evidence Lower BOund (ELBO), stopping when improvement becomes negligible.  \n- **EM algorithm (for PLSA)** might track the change in log-likelihood from one iteration to the next.  \n\nIn practical scenarios, you also look at the stability of the resulting topics. If the top words in each topic keep shifting drastically between iterations, the model might not have converged yet.\n\n### Evaluating training progress\n\nDuring training, it's common to observe metrics like the held-out perplexity or the likelihood of a validation set, or simply to look at the topic-word distributions qualitatively. Tools like **pyLDAvis** are often used to visually inspect how topics are laid out in a two-dimensional representation (via multidimensional scaling). If training is going well, you tend to see well-separated topics in the visualization, each with a fairly coherent cluster of words.\n\n### Model evaluation\n\nEvaluating topic models is notoriously tricky because there is no ground-truth label for what a \"good\" topic is in many real-world data sets. Nonetheless, several strategies exist:\n\n#### Topic coherence metrics\n\nTopic coherence metrics (Mimno and gang, 2011) aim to quantify how semantically coherent the top words of a topic are. Common metrics include:\n\n- **C_v**: A popular approach that uses word co-occurrence counts and a sliding window to measure coherence.  \n- **C_uci, C_umass**: Based on pointwise mutual information (PMI).  \n- **NPMI**: Normalized pointwise mutual information-based metric.  \n\nCoherence scores are used heavily in practice to select the optimal number of topics <Latex text=\"\\( K \\)\"/>, or to compare different variants of a model.\n\n#### Perplexity and likelihood-based metrics\n\nIn the language modeling tradition, **perplexity** is sometimes used to measure how well a probabilistic model predicts a held-out set of documents. A lower perplexity indicates a better predictive model of text. However, perplexity can be somewhat misleading because an overly complex model can overfit, driving perplexity down but not necessarily providing more interpretable topics.\n\n#### Human evaluations and qualitative checks\n\nIn many practical scenarios, the final judge of a topic model is a domain expert. Does the topic's top words make sense as a coherent theme? Does the model separate distinct topics into separate clusters, or are multiple themes merged or split incorrectly?  \nHuman-based evaluation is time-consuming, but it provides the ultimate test of interpretability, which is often the main goal of topic modeling.\n\n### Model interpretation and visualization\n\nThe real power of topic modeling comes in interpreting the discovered topics and applying them to the original corpus. Key tasks include:\n\n- **Examining topic–word distributions**: Inspect the top words and their probabilities for each topic. Sometimes you might label the topic based on the top words.  \n- **Topic labeling techniques**: Automated techniques can be used to assign descriptive labels to each topic. For example, you can see which words appear frequently in that topic, or examine the most representative documents. Alternatively, some approaches (Mei and gang, 2007) propose using bigrams or salient phrases to label topics more precisely.  \n- **Visualizing topics**: Tools like **pyLDAvis** or custom embeddings-based plots (e.g., projecting topics into a 2D space) help you see how topics relate to each other and to the documents.  \n- **Document–topic distributions**: You can examine which topics are prevalent in each document, or how topics are distributed across different subsets of the corpus.  \n\n### Visualization tools and libraries\n\n- **pyLDAvis**: A popular Python library for interactive topic model visualization.  \n- **Gensim**'s built-in visualization: Some basic plot functions exist, though typically one uses pyLDAvis.  \n- **Word cloud**: Some practitioners generate word clouds from the top words for each topic.  \n- **Gephi or other graph-based tools**: Sometimes used if you represent topics as networks of words.\n\nInterpreting the output in these ways can provide valuable insights, especially for business intelligence or academic research where clarity is paramount.\n\n## Chapter 5. Advanced techniques and variations\n\n### Neural topic modeling\n\nRecently, **neural topic modeling** has gained momentum, aiming to leverage deep neural networks (often autoencoders or variational autoencoders, VAEs) to discover topics in text. Methods like **ProdLDA** (Srivastava & Sutton, 2017) re-parameterize the distribution over topics using neural networks, attempting to address some limitations of traditional LDA. The deep generative perspective can:\n\n- Better capture non-linear text representations.  \n- Integrate with word embeddings or other neural features.  \n- Potentially scale to massive datasets using GPU acceleration.  \n\nDespite these benefits, neural topic models may introduce new complexities in training (e.g., optimizing VAE objectives, balancing reconstruction vs. KL divergence) and might require more hyperparameter tuning.\n\n### Combining topic modeling with word embeddings\n\nClassic topic models treat words largely as discrete tokens, ignoring possible semantic similarities between them (for instance, synonyms). By combining topic modeling with **word embeddings** (e.g., Word2Vec, GloVe, or fastText), it becomes possible to produce topics that incorporate distributional semantics. Some approaches:\n\n- **Top2Vec** (Angelov, 2020): Leverages document embeddings and then clusters them, effectively discovering topic embeddings.  \n- **Embedding-based coherence**: You can use word embedding similarities to measure how coherent a topic's words are, providing a more nuanced evaluation.  \n\nThis synergy can produce topics that are more robust to synonyms or polysemy. On the downside, it adds another layer of complexity to the pipeline and can require a large, high-quality embedding model.\n\n### Deep learning approaches for topic modeling\n\nBeyond straightforward neural topic modeling, there is a whole ecosystem of deep learning solutions that incorporate attention mechanisms, Transformers, or graph-based neural networks to glean more contextual or structural information from text:\n\n- **Transformer-based encoders**: They can be used to generate rich contextual embeddings for tokens or entire sentences, potentially leading to refined topic representations.  \n- **Graph neural networks**: In specialized domains where the text might also be linked or references are crucial (e.g., scientific citation networks), GNN-based models can incorporate topological information.  \n\nAlthough these approaches can be powerful, they also come at a higher computational cost and often require carefully engineered architectures.\n\n### Cross-lingual topic modeling\n\n**Cross-lingual topic modeling** addresses the scenario where you have documents in multiple languages and you want to discover overarching topics that span those languages. This can be approached by:\n\n- **Sharing a common latent space** for topics, with separate word distributions for each language.  \n- **Using bilingual word embeddings** or multi-lingual representations so that synonyms across languages can align into a single topic dimension.  \n\nSuch models are highly relevant in globally oriented text analytics, especially for multinational companies, cross-lingual search engines, or multilingual social media analysis.\n\n### Ensemble methods for topic discovery\n\nEnsemble approaches can also be employed for topic modeling:\n\n- **Multiple random initializations**: Running LDA multiple times with different seeds and combining or merging similar topics to yield more stable solutions.  \n- **Combining multiple methods**: For example, using NMF to get an initial sense of topics, then refining those topics via an LDA-based approach. Or you can run LDA with different hyperparameters and ensemble the discovered topics if you find them complementary.  \n\nEnsemble methods may enhance robustness and produce well-rounded sets of topics that capture different aspects of the corpus.\n\n## Chapter 6. Let's code\n\n### Building complex topic model from scratch\n\nIn this final section, I will illustrate how you can build a more involved topic modeling pipeline in Python. Rather than just showing the typical Gensim snippet, we will aim to demonstrate the entire pipeline, complete with preprocessing, training, and evaluation of a topic model (such as LDA or NMF), along with a quick demonstration of how to visualize topics with pyLDAvis.\n\n#### Step 1: Data ingestion and cleanup\n\nLet's suppose we have a dataset of documents stored in a file or database. We'll do a simplified illustration:\n\n<Code text={`\nimport pandas as pd\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom gensim.utils import simple_preprocess\n\nstop_words = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()\n\ndef preprocess_text(text):\n    # Lowercase\n    text = text.lower()\n    # Remove some non-alphanumeric characters (for demonstration)\n    text = re.sub(r'[^a-z0-9\\s]', '', text)\n    # Tokenize\n    tokens = simple_preprocess(text, deacc=True)  # deacc=True removes punctuations\n    # Remove stopwords and lemmatize\n    filtered_tokens = []\n    for tok in tokens:\n        if tok not in stop_words:\n            lemma = lemmatizer.lemmatize(tok)\n            filtered_tokens.append(lemma)\n    return filtered_tokens\n\n# Suppose we load a CSV with a column 'text'\ndf = pd.read_csv('documents.csv')\ndf['processed'] = df['text'].apply(preprocess_text)\n`}/>\n\nHere, we do some minimal cleaning, tokenization, stopword removal, and lemmatization. In a real scenario, you might do more sophisticated text normalization.\n\n#### Step 2: Building a dictionary and corpus\n\nNext, we convert the processed tokens into a bag-of-words representation:\n\n<Code text={`\nfrom gensim import corpora\n\ndictionary = corpora.Dictionary(df['processed'])\ndictionary.filter_extremes(no_below=5, no_above=0.4)  # example thresholds\nbow_corpus = [dictionary.doc2bow(doc) for doc in df['processed']]\n`}/>\n\nWe have also used <Highlight>filter_extremes</Highlight> to remove very rare words (appear in fewer than 5 documents) and extremely common words (appear in more than 40% of documents). Adjust these thresholds depending on your data size and domain knowledge.\n\n#### Step 3: Training an LDA model\n\nNow, we can train an LDA model using Gensim's <Highlight>LdaModel</Highlight> or <Highlight>LdaMulticore</Highlight> (for parallelization):\n\n<Code text={`\nfrom gensim.models.ldamodel import LdaModel\n\nnum_topics = 10  # Choose your number of topics\nlda_model = LdaModel(\n    corpus=bow_corpus,\n    id2word=dictionary,\n    num_topics=num_topics,\n    random_state=42,\n    passes=10,\n    alpha='auto'\n)\n\n# Print out the topics\nfor idx, topic in lda_model.print_topics(num_words=5):\n    print(f\"Topic {idx}: {topic}\")\n`}/>\n\nHere, we have set the number of topics to 10, but you would typically try a range (like 5, 10, 15, 20, etc.) and pick the best number based on your chosen evaluation method (coherence, perplexity, or domain feedback).\n\n#### Step 4: Evaluating with topic coherence\n\nGensim has built-in coherence measures:\n\n<Code text={`\nfrom gensim.models import CoherenceModel\n\ncoherence_model_lda = CoherenceModel(\n    model=lda_model, \n    texts=df['processed'], \n    dictionary=dictionary, \n    coherence='c_v'\n)\ncoherence_lda = coherence_model_lda.get_coherence()\nprint(f'Coherence Score (c_v): {coherence_lda}')\n`}/>\n\nYou can also choose <Highlight>'u_mass'</Highlight> or <Highlight>'c_uci'</Highlight> if you have the necessary reference corpus or prefer those metrics. A higher coherence usually indicates more interpretable topics, although interpretation is still somewhat subjective.\n\n#### Step 5: Visualizing with pyLDAvis\n\nAn optional but highly recommended step is to visualize your topics:\n\n<Code text={`\nimport pyLDAvis\nimport pyLDAvis.gensim_models as gensimvis\n\npyLDAvis.enable_notebook()  # if you're in a notebook\nvis_data = gensimvis.prepare(lda_model, bow_corpus, dictionary)\npyLDAvis.save_html(vis_data, 'lda_visualization.html')\n`}/>\n\nThis produces an interactive visualization showing the relationships between topics and the top terms in each topic. You can open the resulting \"lda_visualization.html\" to hover over different circles (representing topics) and see the word distributions.\n\n#### Step 6: Trying NMF as an alternative\n\nSometimes, you might find that NMF is more straightforward or runs faster:\n\n<Code text={`\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import NMF\n\n# Convert documents to TF-IDF matrix\ntfidf_vectorizer = TfidfVectorizer(\n    min_df=5, \n    max_df=0.4, \n    stop_words='english'\n)\ntfidf_matrix = tfidf_vectorizer.fit_transform(\n    [' '.join(doc) for doc in df['processed']]\n)\n\n# Train NMF\nnum_topics = 10\nnmf_model = NMF(n_components=num_topics, random_state=42)\nW = nmf_model.fit_transform(tfidf_matrix)\nH = nmf_model.components_\n\n# Display top words for each topic\nterms = tfidf_vectorizer.get_feature_names_out()\nfor topic_idx, topic_vec in enumerate(H):\n    top_indices = topic_vec.argsort()[:-6:-1]  # top 5\n    top_terms = [terms[i] for i in top_indices]\n    print(f\"Topic {topic_idx}: {', '.join(top_terms)}\")\n`}/>\n\nWhile not probabilistic, NMF can be a very fast and intuitive approach, especially for exploring your data quickly.\n\n---\n\nThroughout these steps, you may incorporate advanced variations such as **hierarchical** approaches or **dynamic** topic modeling if your data is time-stamped. For more advanced neural methods, frameworks like PyTorch or TensorFlow are used to build the neural architecture for a topic model, often requiring additional custom code.\n\nRemember that real-world text analytics workflows often involve:\n\n- Larger corpora that require distributed solutions (like Spark or HPC clusters).  \n- More nuanced text cleaning, domain-specific tokenization or phrase detection.  \n- Ongoing iteration of topic number selection and hyperparameter tuning.  \n- Detailed interpretability checks with domain experts.  \n\nFinally, it's beneficial to combine computational metrics (like coherence or perplexity) with human feedback. Topic modeling is inherently interpretive: you want topics that not only fit the data statistically but also make sense to people who will use those insights for further decision-making.\n\n<Image alt=\"Topic model diagram\" path=\"\" caption=\"A conceptual diagram of a topic model illustrating a set of documents being mapped onto latent topics. Each topic is a distribution over terms, and each document is a distribution over topics.\" zoom=\"false\" />\n\n<Image alt=\"Dynamic topic model illustration\" path=\"\" caption=\"Visualization of how topics change over time in a dynamic topic model. Each color-coded topic evolves to reflect new words or shifting probabilities.\" zoom=\"false\" />\n\n---\n\nThis concludes the detailed exploration of topic modeling. By engaging with fundamental ideas (latent variables, probability distributions, dimension reduction) and advanced concepts (hierarchical Dirichlet processes, neural topic modeling, cross-lingual topic discovery), topic modeling can be adapted to a wide range of practical text analytics scenarios. The final code examples illustrate a typical pipeline — starting with data ingestion, preprocessing, model training, evaluation, and visualization — which can be extended or combined with new innovations in the field."}],"group":[{"fieldValue":""},{"fieldValue":"Natural language processing"}]}},"pageContext":{}},"staticQueryHashes":["2277363210","3093242562"],"slicesMap":{}}