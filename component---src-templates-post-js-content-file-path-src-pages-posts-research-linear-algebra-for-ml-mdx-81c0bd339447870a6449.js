"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[4708],{10312:function(e,t,n){n.r(t),n.d(t,{Head:function(){return z},PostTemplate:function(){return H},default:function(){return k}});var a=n(54506),r=n(28453),i=n(96540),l=n(96098),s=n(80420);function o(e){const t=Object.assign({p:"p",em:"em",strong:"strong",ul:"ul",li:"li",h2:"h2",a:"a",span:"span",h3:"h3",ol:"ol",h4:"h4"},(0,r.RP)(),e.components);return i.createElement(i.Fragment,null,"\n",i.createElement(t.p,null,i.createElement(t.em,null,'"You can\'t learn too much linear algebra."')," ",i.createElement(s.A,{sticker:"nerazdyplenish2"})),"\n",i.createElement("br"),"\n",i.createElement(t.p,null,"In machine learning, linear algebra is more than a mathematical discipline; it's the backbone that supports the development, interpretation, and optimization of algorithms. Understanding linear algebra is crucial for us - data scientists - since it allows to grasp the fundamental operations and structures that power various machine learning models, from simple linear regression to advanced neural networks."),"\n",i.createElement(t.p,null,"At its core, linear algebra provides a framework for working with data in high-dimensional spaces. This is a fundamental thing in machine learning, where data is typically represented as vectors and matrices. Linear algebra concepts help us understand ",i.createElement(t.strong,null,"data structures")," (like datasets in vector or matrix forms), ",i.createElement(t.strong,null,"transformations")," (such as rotations, scaling, and projections), and ",i.createElement(t.strong,null,"optimizations")," (minimizing or maximizing functions during model training)."),"\n",i.createElement(t.p,null,"Most machine learning algorithms rely heavily on linear algebraic operations, which include:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Representing data"),": data is often represented as vectors (for features in a sample) or matrices (for datasets with multiple samples and features). For instance, a dataset with ",i.createElement(l.A,{text:"\\( m \\)"})," samples and ",i.createElement(l.A,{text:"\\( n \\)"})," features is typically represented by an ",i.createElement(l.A,{text:"\\( m \\times n \\)"})," matrix:"),"\n",i.createElement(l.A,{text:"\\[\nX = \\begin{bmatrix} x_{1,1} & x_{1,2} & \\dots & x_{1,n} \\\\ x_{2,1} & x_{2,2} & \\dots & x_{2,n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{m,1} & x_{m,2} & \\dots & x_{m,n} \\end{bmatrix}\n\\]"}),"\n",i.createElement(t.p,null,"where each row represents a sample and each column represents a feature."),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Transformations and projections"),": linear transformations, represented as matrix multiplications, allow us to project data into different spaces. This is fundamental for tasks like ",i.createElement(t.strong,null,"dimensionality reduction")," (e.g., Principal Component Analysis) and ",i.createElement(t.strong,null,"feature extraction"),"."),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Optimization"),": many machine learning models, especially those based on gradient-based methods, rely on linear algebra for optimization. For example, finding the optimal parameters for a model often involves computing derivatives of functions and solving systems of linear equations."),"\n"),"\n"),"\n",i.createElement(t.p,null,"By grounding machine learning concepts in linear algebra, we gain a language and toolkit for systematically solving complex problems. As we dive deeper into various algorithms, you'll see that linear algebra simplifies processes that would otherwise be computationally infeasible, making it indispensable for both theory and practice."),"\n",i.createElement("br"),"\n",i.createElement(t.h2,{id:"core-concepts",style:{position:"relative"}},i.createElement(t.a,{href:"#core-concepts","aria-label":"core concepts permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Core concepts"),"\n",i.createElement(t.h3,{id:"vectors",style:{position:"relative"}},i.createElement(t.a,{href:"#vectors","aria-label":"vectors permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),i.createElement(t.strong,null,"Vectors")),"\n",i.createElement(t.p,null,"In linear algebra, a ",i.createElement(t.strong,null,"vector")," is an ordered list of numbers, typically represented as a column or row of values. Vectors can describe points, directions, and quantities in space, making them a fundamental building block."),"\n",i.createElement(t.p,null,"A vector in ",i.createElement(l.A,{text:"\\( n \\)"}),"-dimensional space is written as:"),"\n",i.createElement(l.A,{text:"\\[\n\\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix}\n\\]"}),"\n",i.createElement(t.p,null,"where each ",i.createElement(l.A,{text:"\\( v_i \\)"})," represents a component of the vector."),"\n",i.createElement(t.p,null,"You're probably know the common operations with vectors:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Addition"),": adding two vectors of the same dimension involves adding corresponding elements. For vectors ",i.createElement(l.A,{text:"\\( \\mathbf{u} \\)"})," and ",i.createElement(l.A,{text:"\\( \\mathbf{v} \\)"})," in ",i.createElement(l.A,{text:"\\( n \\)"}),"-dimensional space,"),"\n",i.createElement(l.A,{text:"\\[\n\\mathbf{u} + \\mathbf{v} = \\begin{bmatrix} u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_n \\end{bmatrix} + \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix} = \\begin{bmatrix} u_1 + v_1 \\\\ u_2 + v_2 \\\\ \\vdots \\\\ u_n + v_n \\end{bmatrix}\n\\]"}),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Scalar multiplication"),": scaling a vector by a scalar ",i.createElement(l.A,{text:"\\( \\alpha \\)"})," involves multiplying each component by ",i.createElement(l.A,{text:"\\( \\alpha \\)"}),":"),"\n",i.createElement(l.A,{text:"\\[\n\\alpha \\mathbf{v} = \\alpha \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix} = \\begin{bmatrix} \\alpha v_1 \\\\ \\alpha v_2 \\\\ \\vdots \\\\ \\alpha v_n \\end{bmatrix}\n\\]"}),"\n"),"\n"),"\n",i.createElement(t.p,null,"Vectors often represent individual ",i.createElement(t.strong,null,"feature sets"),". For instance, in image processing, each image pixel can be treated as a vector in 3D space (for RGB values), and in tabular data, each feature in a dataset can be represented as a dimension in a vector."),"\n",i.createElement(t.p,null,"Thus, vectors allow us to structure data and perform essential operations, like measuring similarity between data points (via ",i.createElement(t.strong,null,"dot products"),"), which is critical in algorithms such as ",i.createElement(t.strong,null,"k-nearest neighbors")," or ",i.createElement(t.strong,null,"support vector machines"),"."),"\n",i.createElement(t.h3,{id:"matrices",style:{position:"relative"}},i.createElement(t.a,{href:"#matrices","aria-label":"matrices permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),i.createElement(t.strong,null,"Matrices")),"\n",i.createElement(t.p,null,"A ",i.createElement(t.strong,null,"matrix")," is a 2D array of numbers, arranged in rows and columns. Matrices are used extensively in machine learning to store and manipulate large datasets efficiently."),"\n",i.createElement(t.p,null,"A matrix is typically denoted as:"),"\n",i.createElement(l.A,{text:"\\[\n\\mathbf{A} = \\begin{bmatrix} a_{1,1} & a_{1,2} & \\dots & a_{1,n} \\\\ a_{2,1} & a_{2,2} & \\dots & a_{2,n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m,1} & a_{m,2} & \\dots & a_{m,n} \\end{bmatrix}\n\\]"}),"\n",i.createElement(t.p,null,"where ",i.createElement(l.A,{text:"\\( a_{i,j} \\)"})," denotes the element in the ",i.createElement(l.A,{text:"\\( i \\)"}),"-th row and ",i.createElement(l.A,{text:"\\( j \\)"}),"-th column."),"\n",i.createElement(t.p,null,"Key types of matrices:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Square matrix"),": a matrix with the same number of rows and columns (",i.createElement(l.A,{text:"\\( m = n \\)"}),")."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Diagonal matrix"),": a square matrix with non-zero values only along its main diagonal."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Identity matrix"),": a square matrix with 1s on the diagonal and 0s elsewhere, denoted as ",i.createElement(l.A,{text:"\\( \\mathbf{I} \\)"}),". The identity matrix is the multiplicative identity in matrix multiplication."),"\n"),"\n",i.createElement(t.p,null,"The main operations to remember for now:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Addition"),": matrices of the same dimension can be added by adding corresponding elements:"),"\n",i.createElement(l.A,{text:"\\[\n\\mathbf{A} + \\mathbf{B} = \\begin{bmatrix} a_{1,1} + b_{1,1} & a_{1,2} + b_{1,2} & \\dots \\\\ a_{2,1} + b_{2,1} & a_{2,2} + b_{2,2} & \\dots \\\\ \\vdots & \\vdots & \\ddots \\end{bmatrix}\n\\]"}),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Multiplication"),": the ",i.createElement(t.strong,null,"dot product")," of matrices ",i.createElement(l.A,{text:"\\( \\mathbf{A} \\)"})," and ",i.createElement(l.A,{text:"\\( \\mathbf{B} \\)"})," involves multiplying each row of ",i.createElement(l.A,{text:"\\( \\mathbf{A} \\)"})," by each column of ",i.createElement(l.A,{text:"\\( \\mathbf{B} \\)"}),", resulting in a new matrix. This operation underpins many neural network computations."),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Element-wise operations"),": also known as the Hadamard product, this involves multiplying corresponding elements of matrices ",i.createElement(l.A,{text:"\\( \\mathbf{A} \\)"})," and ",i.createElement(l.A,{text:"\\( \\mathbf{B} \\)"}),", denoted as ",i.createElement(l.A,{text:"\\( \\mathbf{A} \\circ \\mathbf{B} \\)"}),"."),"\n"),"\n"),"\n",i.createElement(t.p,null,"Matrices are essential for storing and transforming data. For example, in image processing, images are often represented as matrices of pixel intensities, and in neural networks, large datasets are stored in matrices that undergo various transformations as they pass through the network's layers."),"\n",i.createElement(t.h3,{id:"tensors",style:{position:"relative"}},i.createElement(t.a,{href:"#tensors","aria-label":"tensors permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),i.createElement(t.strong,null,"Tensors")),"\n",i.createElement(t.p,null,"A ",i.createElement(t.strong,null,"tensor")," generalizes the concept of scalars (0D), vectors (1D), and matrices (2D) to ",i.createElement(t.strong,null,"higher dimensions"),". Tensors can be thought of as multi-dimensional arrays that are particularly useful in handling complex data structures, such as images with multiple color channels or batches of sequences in natural language processing."),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"1D tensors")," are vectors (e.g., a list of features)."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"2D tensors")," are matrices (e.g., a batch of data points)."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"3D tensors")," or higher-dimensional tensors represent more complex structures, such as a batch of RGB images (height, width, and color channels)."),"\n"),"\n",i.createElement(t.p,null,"For example, in a neural network training on batches of 28x28 grayscale images, the data might be stored in a 3D tensor of shape ",i.createElement(l.A,{text:"\\((\\text{batch size}, 28, 28)\\)"}),". In frameworks like ",i.createElement(t.strong,null,"TensorFlow")," and ",i.createElement(t.strong,null,"PyTorch"),", tensor operations are optimized to handle large data efficiently, making them indispensable for training complex models."),"\n",i.createElement("br"),"\n",i.createElement(t.h2,{id:"key-matrix-operations",style:{position:"relative"}},i.createElement(t.a,{href:"#key-matrix-operations","aria-label":"key matrix operations permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Key matrix operations"),"\n",i.createElement(t.h3,{id:"dot-product-and-inner-product",style:{position:"relative"}},i.createElement(t.a,{href:"#dot-product-and-inner-product","aria-label":"dot product and inner product permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),i.createElement(t.strong,null,"Dot product and inner product")),"\n",i.createElement(t.p,null,"The ",i.createElement(t.strong,null,"dot product")," (or ",i.createElement(t.strong,null,"inner product"),") is a fundamental operation between two vectors of the same length, resulting in a scalar. Given two vectors ",i.createElement(l.A,{text:"\\( \\mathbf{u} = [u_1, u_2, \\dots, u_n] \\)"})," and ",i.createElement(l.A,{text:"\\( \\mathbf{v} = [v_1, v_2, \\dots, v_n] \\)"}),", their dot product is computed as:"),"\n",i.createElement(l.A,{text:"\\[\n\\mathbf{u} \\cdot \\mathbf{v} = u_1 v_1 + u_2 v_2 + \\dots + u_n v_n = \\sum_{i=1}^{n} u_i v_i\n\\]"}),"\n",i.createElement(t.p,null,"The dot product is used, for instance, in ",i.createElement(t.strong,null,"cosine similarity"),", which measures the angle between two vectors and is commonly applied in ",i.createElement(t.strong,null,"text analysis")," or ",i.createElement(t.strong,null,"recommendation systems")," to assess how similar two data points are."),"\n",i.createElement(t.p,null,"In neural networks, the dot product between the input vector and the weight vector calculates the weighted sum, forming the basis of many neural computations."),"\n",i.createElement(t.h3,{id:"matrix-multiplication",style:{position:"relative"}},i.createElement(t.a,{href:"#matrix-multiplication","aria-label":"matrix multiplication permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),i.createElement(t.strong,null,"Matrix multiplication")),"\n",i.createElement(t.p,null,"Matrix multiplication is an operation where two matrices, ",i.createElement(l.A,{text:"\\( \\mathbf{A} \\)"})," (of size ",i.createElement(l.A,{text:"\\( m \\times n \\)"}),") and ",i.createElement(l.A,{text:"\\( \\mathbf{B} \\)"})," (of size ",i.createElement(l.A,{text:"\\( n \\times p \\)"}),"), produce a resulting matrix ",i.createElement(l.A,{text:"\\( \\mathbf{C} \\)"})," (of size ",i.createElement(l.A,{text:"\\( m \\times p \\)"}),"). This operation is only defined if the number of columns in ",i.createElement(l.A,{text:"\\( \\mathbf{A} \\)"})," matches the number of rows in ",i.createElement(l.A,{text:"\\( \\mathbf{B} \\)"}),"."),"\n",i.createElement(t.p,null,"For elements ",i.createElement(l.A,{text:"\\( c_{i,j} \\)"})," of the resulting matrix ",i.createElement(l.A,{text:"\\( \\mathbf{C} \\)"}),", we compute:"),"\n",i.createElement(l.A,{text:"\\[\nc_{i,j} = \\sum_{k=1}^{n} a_{i,k} \\cdot b_{k,j}\n\\]"}),"\n",i.createElement(t.p,null,"Matrix multiplication can be thought of as applying a ",i.createElement(t.strong,null,"linear transformation"),". It allows for transforming one data representation to another, such as projecting data onto a different feature space."),"\n",i.createElement(t.p,null,"Matrix multiplication is the core operation in transforming data through space, often used in algorithms that rely on dimensionality reduction or feature transformations."),"\n",i.createElement(t.p,null,"In each layer of a neural network, the matrix of weights is multiplied by the matrix of inputs to produce the activations, forming the core computation at each layer."),"\n",i.createElement(t.h3,{id:"transpose",style:{position:"relative"}},i.createElement(t.a,{href:"#transpose","aria-label":"transpose permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),i.createElement(t.strong,null,"Transpose")),"\n",i.createElement(t.p,null,"The ",i.createElement(t.strong,null,"transpose")," of a matrix ",i.createElement(l.A,{text:"\\( \\mathbf{A} \\)"}),", denoted ",i.createElement(l.A,{text:"\\( \\mathbf{A}^T \\)"}),", is formed by swapping its rows and columns. If:"),"\n",i.createElement(l.A,{text:"\\[\n\\mathbf{A} = \\begin{bmatrix} a_{1,1} & a_{1,2} \\\\ a_{2,1} & a_{2,2} \\end{bmatrix}\n\\]"}),"\n",i.createElement(t.p,null,"then"),"\n",i.createElement(l.A,{text:"\\[\n\\mathbf{A}^T = \\begin{bmatrix} a_{1,1} & a_{2,1} \\\\ a_{1,2} & a_{2,2} \\end{bmatrix}\n\\]"}),"\n",i.createElement(t.p,null,"The transpose operation is often used to reformat data, converting row vectors to column vectors (or vice versa) for easier computation, especially in matrix multiplications."),"\n",i.createElement(t.h3,{id:"determinant-and-trace",style:{position:"relative"}},i.createElement(t.a,{href:"#determinant-and-trace","aria-label":"determinant and trace permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),i.createElement(t.strong,null,"Determinant and trace")),"\n",i.createElement(t.p,null,"The ",i.createElement(t.strong,null,"determinant")," is a scalar that can be calculated for square matrices, representing the matrix's scaling factor and orientation. For a 2x2 matrix ",i.createElement(l.A,{text:"\\( \\mathbf{A} = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\)"}),", the determinant ",i.createElement(l.A,{text:"\\( \\det(\\mathbf{A}) \\)"})," is computed as:"),"\n",i.createElement(l.A,{text:"\\[\n\\det(\\mathbf{A}) = ad - bc\n\\]"}),"\n",i.createElement(t.p,null,"The determinant gives insights into whether a matrix is ",i.createElement(t.strong,null,"invertible")," (non-zero determinant) or ",i.createElement(t.strong,null,"singular")," (zero determinant)."),"\n",i.createElement(t.p,null,"The ",i.createElement(t.strong,null,"trace")," of a square matrix ",i.createElement(l.A,{text:"\\( \\mathbf{A} \\)"})," is the sum of its diagonal elements. For ",i.createElement(l.A,{text:"\\( \\mathbf{A} = \\begin{bmatrix} a_{1,1} & a_{1,2} \\\\ a_{2,1} & a_{2,2} \\end{bmatrix} \\)"}),", the trace ",i.createElement(l.A,{text:"\\( \\text{tr}(\\mathbf{A}) \\)"})," is:"),"\n",i.createElement(l.A,{text:"\\[\n\\text{tr}(\\mathbf{A}) = a_{1,1} + a_{2,2}\n\\]"}),"\n",i.createElement(t.p,null,"The determinant is used to assess matrix ",i.createElement(t.strong,null,"invertibility"),", which is critical for operations like solving systems of linear equations in linear regression, and the trace is used in certain ",i.createElement(t.strong,null,"loss functions")," and optimization criteria. For example, in matrix factorization, minimizing the trace can be part of regularization."),"\n",i.createElement(t.h3,{id:"inverse-and-pseudo-inverse",style:{position:"relative"}},i.createElement(t.a,{href:"#inverse-and-pseudo-inverse","aria-label":"inverse and pseudo inverse permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),i.createElement(t.strong,null,"Inverse and pseudo-inverse")),"\n",i.createElement(t.p,null,"The ",i.createElement(t.strong,null,"inverse")," of a matrix ",i.createElement(l.A,{text:"\\( \\mathbf{A} \\)"}),", denoted ",i.createElement(l.A,{text:"\\( \\mathbf{A}^{-1} \\)"}),", exists only if ",i.createElement(l.A,{text:"\\( \\mathbf{A} \\)"})," is square and invertible (i.e., ",i.createElement(l.A,{text:"\\( \\det(\\mathbf{A}) \\neq 0 \\)"}),"). The inverse has the property that ",i.createElement(l.A,{text:"\\( \\mathbf{A} \\cdot \\mathbf{A}^{-1} = \\mathbf{I} \\)"}),", where ",i.createElement(l.A,{text:"\\( \\mathbf{I} \\)"})," is the identity matrix."),"\n",i.createElement(t.p,null,"The ",i.createElement(t.strong,null,"Moore-Penrose pseudo-inverse")," extends the concept of inversion to non-square or singular matrices, where the traditional inverse does not exist. It is commonly used to solve ",i.createElement(t.strong,null,"underdetermined")," or ",i.createElement(t.strong,null,"overdetermined")," systems."),"\n",i.createElement(t.p,null,"In linear regression, the inverse of the covariance matrix is often used in solving for the best-fit parameters via the ",i.createElement(t.strong,null,"normal equation"),". This allows the direct computation of weights that minimize the error."),"\n",i.createElement(t.p,null,"In cases where a dataset has more features than samples (overdetermined), the pseudo-inverse allows us to find a solution that minimizes the error, even if an exact solution doesn't exist."),"\n",i.createElement("br"),"\n",i.createElement(t.h2,{id:"special-matrix-types",style:{position:"relative"}},i.createElement(t.a,{href:"#special-matrix-types","aria-label":"special matrix types permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Special matrix types"),"\n",i.createElement(t.p,null,"Special matrices have unique properties that simplify computations and play crucial roles in various machine learning algorithms. Understanding these types of matrices helps in optimizing calculations, ensuring stability, and preserving key properties in model transformations."),"\n",i.createElement(t.h3,{id:"identity-matrix",style:{position:"relative"}},i.createElement(t.a,{href:"#identity-matrix","aria-label":"identity matrix permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),i.createElement(t.strong,null,"Identity Matrix")),"\n",i.createElement(t.p,null,"The ",i.createElement(t.strong,null,"identity matrix"),", denoted ",i.createElement(l.A,{text:"\\( \\mathbf{I} \\)"}),", is a square matrix with 1s on the diagonal and 0s elsewhere:"),"\n",i.createElement(l.A,{text:"\\[\n\\mathbf{I} = \\begin{bmatrix} 1 & 0 & \\dots & 0 \\\\ 0 & 1 & \\dots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\dots & 1 \\end{bmatrix}\n\\]"}),"\n",i.createElement(t.p,null,"The identity matrix acts as a ",i.createElement(t.strong,null,"multiplicative neutral element")," in matrix operations. For any matrix ",i.createElement(l.A,{text:"\\( \\mathbf{A} \\)"})," of compatible dimensions:"),"\n",i.createElement(l.A,{text:"\\[\n\\mathbf{A} \\cdot \\mathbf{I} = \\mathbf{I} \\cdot \\mathbf{A} = \\mathbf{A}\n\\]"}),"\n",i.createElement(t.p,null,"In certain neural network architectures, initializing weights with identity matrices or scaled identity matrices helps in stabilizing the network. This approach is sometimes used to prevent vanishing or exploding gradients."),"\n",i.createElement(t.p,null,"In transformations, multiplying by an identity matrix leaves the original matrix unchanged. This is particularly useful in maintaining the integrity of data during transformations."),"\n",i.createElement(t.h3,{id:"diagonal-matrix",style:{position:"relative"}},i.createElement(t.a,{href:"#diagonal-matrix","aria-label":"diagonal matrix permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),i.createElement(t.strong,null,"Diagonal matrix")),"\n",i.createElement(t.p,null,"A ",i.createElement(t.strong,null,"diagonal matrix")," has non-zero elements only along the main diagonal, with all other elements being zero:"),"\n",i.createElement(l.A,{text:"\\[\n\\mathbf{D} = \\begin{bmatrix} d_1 & 0 & \\dots & 0 \\\\ 0 & d_2 & \\dots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\dots & d_n \\end{bmatrix}\n\\]"}),"\n",i.createElement(t.p,null,"Diagonal matrices make certain calculations more efficient. For instance, the product of two diagonal matrices ",i.createElement(l.A,{text:"\\( \\mathbf{D}_1 \\)"})," and ",i.createElement(l.A,{text:"\\( \\mathbf{D}_2 \\)"})," is also a diagonal matrix, where each diagonal element is the product of the corresponding diagonal elements from ",i.createElement(l.A,{text:"\\( \\mathbf{D}_1 \\)"})," and ",i.createElement(l.A,{text:"\\( \\mathbf{D}_2 \\)"}),":"),"\n",i.createElement(l.A,{text:"\\[\n\\mathbf{D}_1 \\cdot \\mathbf{D}_2 = \\begin{bmatrix} d_{1,1} \\cdot d_{2,1} & 0 & \\dots & 0 \\\\ 0 & d_{1,2} \\cdot d_{2,2} & \\dots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\dots & d_{n,n} \\cdot d_{n,n} \\end{bmatrix}\n\\]"}),"\n",i.createElement(t.p,null,"Diagonal matrices allow for faster calculations because only the diagonal elements need to be considered. This property is valuable in ",i.createElement(t.strong,null,"optimization algorithms"),", where computational efficiency is critical."),"\n",i.createElement(t.p,null,"In cases where features are independent, the covariance matrix becomes diagonal, simplifying the calculations in ",i.createElement(t.strong,null,"Principal Component Analysis (PCA)")," and other statistical analyses."),"\n",i.createElement(t.h3,{id:"orthogonal-matrix",style:{position:"relative"}},i.createElement(t.a,{href:"#orthogonal-matrix","aria-label":"orthogonal matrix permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),i.createElement(t.strong,null,"Orthogonal matrix")),"\n",i.createElement(t.p,null,"An ",i.createElement(t.strong,null,"orthogonal matrix")," ",i.createElement(l.A,{text:"\\( \\mathbf{Q} \\)"})," has the property that its transpose is equal to its inverse:"),"\n",i.createElement(l.A,{text:"\\[\n\\mathbf{Q}^T \\mathbf{Q} = \\mathbf{Q} \\mathbf{Q}^T = \\mathbf{I}\n\\]"}),"\n",i.createElement(t.p,null,"This property implies that the columns (or rows) of an orthogonal matrix are mutually orthogonal unit vectors."),"\n",i.createElement(t.p,null,"Orthogonal matrices are significant because they ",i.createElement(t.strong,null,"preserve distances")," and ",i.createElement(t.strong,null,"angles")," during transformations, which is crucial for maintaining the structural integrity of data."),"\n",i.createElement(t.p,null,"Orthogonal transformations help reduce numerical errors, which is especially valuable in machine learning applications that involve iterative computations, such as ",i.createElement(t.strong,null,"gradient descent")," and other optimization algorithms."),"\n",i.createElement(t.p,null,"Orthogonal matrices are used in algorithms like ",i.createElement(t.strong,null,"PCA"),", where the eigenvectors of the covariance matrix form an orthogonal basis, allowing data to be projected onto lower-dimensional subspaces with minimal information loss."),"\n",i.createElement(t.h3,{id:"symmetric-matrix",style:{position:"relative"}},i.createElement(t.a,{href:"#symmetric-matrix","aria-label":"symmetric matrix permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),i.createElement(t.strong,null,"Symmetric matrix")),"\n",i.createElement(t.p,null,"A ",i.createElement(t.strong,null,"symmetric matrix")," ",i.createElement(l.A,{text:"\\( \\mathbf{A} \\)"})," is one where ",i.createElement(l.A,{text:"\\( \\mathbf{A} = \\mathbf{A}^T \\)"}),", meaning it is equal to its transpose. For instance:"),"\n",i.createElement(l.A,{text:"\\[\n\\mathbf{A} = \\begin{bmatrix} a_{1,1} & a_{1,2} & \\dots & a_{1,n} \\\\ a_{1,2} & a_{2,2} & \\dots & a_{2,n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{1,n} & a_{2,n} & \\dots & a_{n,n} \\end{bmatrix}\n\\]"}),"\n",i.createElement(t.p,null,"Symmetric matrices appear frequently in machine learning, particularly in ",i.createElement(t.strong,null,"statistics")," and ",i.createElement(t.strong,null,"linear algebra"),"."),"\n",i.createElement(t.p,null,"The ",i.createElement(t.strong,null,"covariance matrix"),", used to measure relationships between features, is always symmetric. Each entry ",i.createElement(l.A,{text:"\\( \\sigma_{ij} \\)"})," represents the covariance between features ",i.createElement(l.A,{text:"\\( i \\)"})," and ",i.createElement(l.A,{text:"\\( j \\)"}),", so ",i.createElement(l.A,{text:"\\( \\sigma_{ij} = \\sigma_{ji} \\)"}),". This symmetry is crucial in algorithms like ",i.createElement(t.strong,null,"PCA"),", where eigenvectors and eigenvalues of the covariance matrix are computed to identify principal components."),"\n",i.createElement(t.p,null,"Symmetric matrices have real eigenvalues, and their eigenvectors are orthogonal. This property simplifies computations, making symmetric matrices useful for ",i.createElement(t.strong,null,"spectral clustering")," and ",i.createElement(t.strong,null,"dimensionality reduction"),"."),"\n",i.createElement("br"),"\n",i.createElement(t.h2,{id:"eigenvalues-and-eigenvectors",style:{position:"relative"}},i.createElement(t.a,{href:"#eigenvalues-and-eigenvectors","aria-label":"eigenvalues and eigenvectors permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Eigenvalues and eigenvectors"),"\n",i.createElement(t.p,null,"Eigenvalues and eigenvectors are fundamental in linear algebra, providing insight into matrix transformations and data structures. Given a square matrix ",i.createElement(l.A,{text:"\\( \\mathbf{A} \\)"}),", an eigenvector ",i.createElement(l.A,{text:"\\( \\mathbf{v} \\)"})," and its associated eigenvalue ",i.createElement(l.A,{text:"\\( \\lambda \\)"})," satisfy the following equation:"),"\n",i.createElement(l.A,{text:"\\[\n\\mathbf{A} \\mathbf{v} = \\lambda \\mathbf{v}\n\\]"}),"\n",i.createElement(t.p,null,"where:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(l.A,{text:"\\( \\mathbf{v} \\)"})," is the ",i.createElement(t.strong,null,"eigenvector")," (a non-zero vector)."),"\n",i.createElement(t.li,null,i.createElement(l.A,{text:"\\( \\lambda \\)"})," is the ",i.createElement(t.strong,null,"eigenvalue"),", a scalar that indicates how much the eigenvector is stretched or shrunk."),"\n"),"\n",i.createElement(t.p,null,"Eigenvalues and eigenvectors are valuable for understanding transformations applied to data, especially when analyzing how data can be decomposed and represented in different bases. In machine learning, they enable simplifications and efficient data representations."),"\n",i.createElement(t.h3,{id:"geometric-interpretation",style:{position:"relative"}},i.createElement(t.a,{href:"#geometric-interpretation","aria-label":"geometric interpretation permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),i.createElement(t.strong,null,"Geometric interpretation")),"\n",i.createElement(t.p,null,"Geometrically, eigenvectors and eigenvalues reveal the underlying structure and characteristics of transformations. When a matrix transformation ",i.createElement(l.A,{text:"\\( \\mathbf{A} \\)"})," is applied to an eigenvector ",i.createElement(l.A,{text:"\\( \\mathbf{v} \\)"}),", the eigenvector's direction remains unchanged, although it may be scaled by the eigenvalue ",i.createElement(l.A,{text:"\\( \\lambda \\)"}),"."),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Eigenvectors")," represent the ",i.createElement(t.strong,null,"principal directions")," along which the transformation acts. These directions capture the axes along which data variation is highest, making them crucial in data analysis."),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Eigenvalues")," describe the ",i.createElement(t.strong,null,"magnitude of scaling")," applied in each eigenvector's direction. A large eigenvalue signifies a significant stretch, while a small (positive) eigenvalue indicates compression along that eigenvector's direction."),"\n"),"\n"),"\n",i.createElement(t.p,null,"For example, in Principal Component Analysis (PCA), eigenvectors of the covariance matrix correspond to principal components, with eigenvalues indicating the importance (variance) of each principal component."),"\n",i.createElement(t.h3,{id:"applications",style:{position:"relative"}},i.createElement(t.a,{href:"#applications","aria-label":"applications permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),i.createElement(t.strong,null,"Applications")),"\n",i.createElement(t.p,null,"Eigenvalues and eigenvectors have diverse applications in machine learning, particularly in tasks involving dimensionality reduction, data transformations, and optimization."),"\n",i.createElement(t.h4,{id:"1-principal-component-analysis-pca",style:{position:"relative"}},i.createElement(t.a,{href:"#1-principal-component-analysis-pca","aria-label":"1 principal component analysis pca permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"1. ",i.createElement(t.strong,null,"Principal Component Analysis (PCA)")),"\n",i.createElement(t.p,null,"PCA is a popular dimensionality reduction technique used to project high-dimensional data onto a lower-dimensional space with minimal loss of information. In PCA:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"The ",i.createElement(t.strong,null,"covariance matrix")," of the dataset is computed to capture feature relationships."),"\n",i.createElement(t.li,null,"Eigenvectors of this matrix (principal components) represent directions of maximum variance."),"\n",i.createElement(t.li,null,"Eigenvalues associated with these eigenvectors reflect the variance magnitude along each component, allowing us to prioritize components with the highest variance."),"\n"),"\n",i.createElement(t.p,null,"By retaining only the top components, PCA reduces data dimensions while preserving most of the variance, improving efficiency in data processing and visualization."),"\n",i.createElement(t.h4,{id:"2-feature-reduction-and-noise-reduction",style:{position:"relative"}},i.createElement(t.a,{href:"#2-feature-reduction-and-noise-reduction","aria-label":"2 feature reduction and noise reduction permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2. ",i.createElement(t.strong,null,"Feature reduction and noise reduction")),"\n",i.createElement(t.p,null,"Beyond PCA, eigenvalues and eigenvectors assist in identifying significant features or noise in data. Small eigenvalues often correspond to directions with minimal variance (possibly noise), allowing models to ignore less informative features and focus on relevant patterns."),"\n",i.createElement(t.h4,{id:"3-spectral-clustering",style:{position:"relative"}},i.createElement(t.a,{href:"#3-spectral-clustering","aria-label":"3 spectral clustering permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3. ",i.createElement(t.strong,null,"Spectral clustering")),"\n",i.createElement(t.p,null,"In ",i.createElement(t.strong,null,"spectral clustering"),", eigenvalues and eigenvectors of a similarity matrix or ",i.createElement(t.strong,null,"Laplacian matrix")," (derived from the adjacency matrix of a graph) are used to group similar data points. Spectral clustering projects data points into a space spanned by the largest eigenvectors of the Laplacian, revealing clusters based on structural relationships in data."),"\n",i.createElement(t.h4,{id:"4-optimization-and-stability-in-neural-networks",style:{position:"relative"}},i.createElement(t.a,{href:"#4-optimization-and-stability-in-neural-networks","aria-label":"4 optimization and stability in neural networks permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4. ",i.createElement(t.strong,null,"Optimization and stability in neural networks")),"\n",i.createElement(t.p,null,"In neural network training, the ",i.createElement(t.strong,null,"Hessian matrix")," (second derivative of the loss function) can be analyzed via eigenvalues and eigenvectors. The eigenvalues of the Hessian reveal the ",i.createElement(t.strong,null,"curvature")," of the loss surface:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"Large positive eigenvalues indicate steep areas of the surface, and very small or negative eigenvalues may suggest saddle points or flat regions, affecting convergence rates."),"\n"),"\n",i.createElement(t.p,null,"Understanding these eigenvalues can improve optimization strategies, helping to fine-tune learning rates and improve model stability."),"\n",i.createElement(t.h4,{id:"5-markov-chains-and-probabilistic-models",style:{position:"relative"}},i.createElement(t.a,{href:"#5-markov-chains-and-probabilistic-models","aria-label":"5 markov chains and probabilistic models permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5. ",i.createElement(t.strong,null,"Markov chains and probabilistic models")),"\n",i.createElement(t.p,null,"In probabilistic models, particularly ",i.createElement(t.strong,null,"Markov chains"),", eigenvalues and eigenvectors describe the ",i.createElement(t.strong,null,"long-term behavior")," of transition matrices. The eigenvector associated with an eigenvalue of 1 represents the ",i.createElement(t.strong,null,"steady-state distribution")," of states, offering insights into equilibrium and stability in sequential models."),"\n",i.createElement(t.p,null,"In sum, eigenvalues and eigenvectors are vital for transforming, compressing, and optimizing data in machine learning. They reveal essential data patterns, simplify complex calculations, and provide insights that enable efficient model design and interpretation."),"\n",i.createElement("br"),"\n",i.createElement(t.h2,{id:"linear-transformations-and-projections",style:{position:"relative"}},i.createElement(t.a,{href:"#linear-transformations-and-projections","aria-label":"linear transformations and projections permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Linear transformations and projections"),"\n",i.createElement(t.p,null,"Linear transformations are operations that map data points from one space to another while preserving the structure and relationships between points. In a linear transformation, applying a matrix ",i.createElement(l.A,{text:"\\( \\mathbf{A} \\)"})," to a vector ",i.createElement(l.A,{text:"\\( \\mathbf{x} \\)"})," produces a new vector ",i.createElement(l.A,{text:"\\( \\mathbf{y} \\)"})," in the transformed space:"),"\n",i.createElement(l.A,{text:"\\[\n\\mathbf{y} = \\mathbf{A} \\mathbf{x}\n\\]"}),"\n",i.createElement(t.p,null,"Linear transformations can include scaling, rotation, reflection, and shearing. They are essential in machine learning and data science, as they help manipulate and restructure data to highlight important features or simplify complex relationships."),"\n",i.createElement(t.p,null,"In machine learning, linear transformations are typically represented by matrix multiplication. This is especially prominent in ",i.createElement(t.strong,null,"neural networks")," and ",i.createElement(t.strong,null,"computer vision"),"."),"\n",i.createElement(t.p,null,"Each layer of a neural network applies a linear transformation (matrix multiplication by weights) followed by a non-linear activation function. This linear transformation reshapes the input data at each layer, gradually extracting features and enabling the model to learn complex patterns."),"\n",i.createElement(t.p,null,"In image processing, linear transformations can scale, rotate, and transform images. For instance, convolutional layers in deep learning apply linear transformations to extract spatial features in images. Techniques like image normalization and feature scaling also use linear transformations to adjust data distributions for more consistent and accurate model performance."),"\n",i.createElement(t.p,null,"A ",i.createElement(t.strong,null,"projection"),' is a specific type of linear transformation that "projects" data points onto a lower-dimensional subspace, effectively reducing the dimensionality while preserving the structure of the original space as closely as possible. Mathematically, projecting a vector ',i.createElement(l.A,{text:"\\( \\mathbf{x} \\)"})," onto a subspace spanned by vector ",i.createElement(l.A,{text:"\\( \\mathbf{u} \\)"})," involves calculating:"),"\n",i.createElement(l.A,{text:"\\[\n\\text{proj}_{\\mathbf{u}}(\\mathbf{x}) = \\frac{\\mathbf{u} \\cdot \\mathbf{x}}{\\mathbf{u} \\cdot \\mathbf{u}} \\mathbf{u}\n\\]"}),"\n",i.createElement(t.p,null,"where ",i.createElement(l.A,{text:"\\( \\frac{\\mathbf{u} \\cdot \\mathbf{x}}{\\mathbf{u} \\cdot \\mathbf{u}} \\)"})," is a scalar that represents the amount of ",i.createElement(l.A,{text:"\\( \\mathbf{x} \\)"})," in the direction of ",i.createElement(l.A,{text:"\\( \\mathbf{u} \\)"}),"."),"\n",i.createElement(t.p,null,"In machine learning, projections are used to simplify data by reducing its dimensions, thus improving computational efficiency and minimizing noise."),"\n",i.createElement(t.p,null,"Projections are the foundation of dimensionality reduction techniques like ",i.createElement(t.strong,null,"PCA"),", where data is projected onto principal components (directions of maximum variance) to create a low-dimensional representation. This not only reduces computational cost but also highlights the most important patterns, making models more interpretable and less prone to overfitting."),"\n",i.createElement(t.p,null,"Projections allow models to focus on relevant features by projecting high-dimensional data onto a smaller, informative subspace. In natural language processing, for instance, word embeddings are projected onto vector spaces to capture semantic relationships."),"\n",i.createElement(t.p,null,"In clustering tasks, projecting data into lower dimensions often reveals group structures and relationships that are not apparent in higher dimensions. Techniques like ",i.createElement(t.strong,null,"t-SNE")," or ",i.createElement(t.strong,null,"UMAP")," are commonly used for visualizing clusters by projecting data into 2D or 3D spaces."),"\n",i.createElement(t.p,null,"Projections maintain the integrity of the original data structure by preserving distances and angles to a reasonable extent, which is vital for tasks that require understanding of data relationships. By focusing on essential patterns, projections help models train faster, improve generalization, and reduce redundancy in data."),"\n",i.createElement("br"),"\n",i.createElement(t.h2,{id:"vector-spaces-and-basis",style:{position:"relative"}},i.createElement(t.a,{href:"#vector-spaces-and-basis","aria-label":"vector spaces and basis permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Vector spaces and basis"),"\n",i.createElement(t.p,null,"In linear algebra, vector spaces and their bases are fundamental for understanding the structure of data and how it can be represented in machine learning. These concepts allow us to define feature spaces, understand data embeddings, and ensure efficient, non-redundant representations of features in models."),"\n",i.createElement(t.h3,{id:"vector-spaces",style:{position:"relative"}},i.createElement(t.a,{href:"#vector-spaces","aria-label":"vector spaces permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),i.createElement(t.strong,null,"Vector spaces")),"\n",i.createElement(t.p,null,"A ",i.createElement(t.strong,null,"vector space")," is a collection of vectors that can be added together and multiplied by scalars (real numbers, for example) and still remain within the space. Formally, a vector space ",i.createElement(l.A,{text:"\\( V \\)"})," over a field ",i.createElement(l.A,{text:"\\( \\mathbb{R} \\)"})," satisfies two main operations: vector addition and scalar multiplication."),"\n",i.createElement(t.p,null,"An example of a vector space is ",i.createElement(l.A,{text:"\\( \\mathbb{R}^n \\)"}),", the space of all ",i.createElement(l.A,{text:"\\( n \\)"}),"-dimensional real-valued vectors. For instance, ",i.createElement(l.A,{text:"\\( \\mathbb{R}^2 \\)"})," consists of all 2D vectors, while ",i.createElement(l.A,{text:"\\( \\mathbb{R}^3 \\)"})," represents all 3D vectors. In machine learning, we work in vector spaces where each dimension corresponds to a feature or attribute of the data."),"\n",i.createElement(t.p,null,"In machine learning, data points are often represented as vectors in an ",i.createElement(l.A,{text:"\\( n \\)"}),"-dimensional vector space, where ",i.createElement(l.A,{text:"\\( n \\)"})," is the number of features. For example, in a dataset with 10 features, each data point can be viewed as a vector in ",i.createElement(l.A,{text:"\\( \\mathbb{R}^{10} \\)"}),"."),"\n",i.createElement(t.p,null,"Techniques like word embeddings in natural language processing (NLP) or image embeddings in computer vision use vector spaces to represent complex data. Embeddings map data to a vector space where similar items are closer together, allowing models to leverage relationships between items based on distances and angles in that space."),"\n",i.createElement(t.h3,{id:"basis-and-span",style:{position:"relative"}},i.createElement(t.a,{href:"#basis-and-span","aria-label":"basis and span permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),i.createElement(t.strong,null,"Basis and span")),"\n",i.createElement(t.p,null,"A ",i.createElement(t.strong,null,"basis")," of a vector space is a set of vectors that are linearly independent and span the entire space. The ",i.createElement(t.strong,null,"span")," of a set of vectors is the collection of all possible linear combinations of those vectors. If a vector space has a basis of ",i.createElement(l.A,{text:"\\( n \\)"})," vectors, we can represent any vector in the space as a unique combination of those ",i.createElement(l.A,{text:"\\( n \\)"})," basis vectors."),"\n",i.createElement(t.p,null,"For example, in ",i.createElement(l.A,{text:"\\( \\mathbb{R}^2 \\)"}),", the standard basis consists of the vectors ",i.createElement(l.A,{text:"\\( \\mathbf{e}_1 = [1, 0]^T \\)"})," and ",i.createElement(l.A,{text:"\\( \\mathbf{e}_2 = [0, 1]^T \\)"}),", which span the entire 2D plane. Any vector in ",i.createElement(l.A,{text:"\\( \\mathbb{R}^2 \\)"})," can be represented as a linear combination of ",i.createElement(l.A,{text:"\\( \\mathbf{e}_1 \\)"})," and ",i.createElement(l.A,{text:"\\( \\mathbf{e}_2 \\)"}),"."),"\n",i.createElement(t.p,null,"Basis vectors allow us to represent features as combinations of simpler components. In machine learning models, we often select or transform features into an efficient basis that highlights important patterns or directions in the data. For instance, in PCA, the principal components form a new basis that captures the directions of greatest variance in the data."),"\n",i.createElement(t.p,null,"Changing the basis (e.g., through eigenvectors) allows us to reduce dimensions by selecting a subset of basis vectors that capture most of the information, which is crucial for reducing computational cost and noise in data."),"\n",i.createElement(t.h3,{id:"linear-independence",style:{position:"relative"}},i.createElement(t.a,{href:"#linear-independence","aria-label":"linear independence permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),i.createElement(t.strong,null,"Linear independence")),"\n",i.createElement(t.p,null,"Vectors are ",i.createElement(t.strong,null,"linearly independent")," if no vector in the set can be represented as a linear combination of the others. If vectors are linearly dependent, some features may be redundant, providing no new information to the model. Linear independence is crucial because it ensures that each feature or vector contributes unique information to the data representation."),"\n",i.createElement(t.p,null,"Selecting linearly independent features helps avoid redundant information, improving model interpretability and efficiency. Highly correlated (or dependent) features often add noise without enhancing model performance."),"\n",i.createElement(t.p,null,"Redundant or dependent features can lead to overfitting, where the model learns specific details of the training data that don't generalize well to new data. By focusing on a basis of independent features, models are less likely to overfit."))}var c=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,r.RP)(),e.components);return t?i.createElement(t,e,i.createElement(o,e)):o(e)},m=n(36710),d=n(58481),h=n.n(d),u=n(36310),p=n(87245),g=n(27042),f=n(59849),v=n(5591),b=n(61122),E=n(9219),x=n(33203),y=n(95751),w=n(94328),A=n(80791),_=n(78137);const S=e=>{let{toc:t}=e;if(!t||!t.items)return null;return i.createElement("nav",{className:A.R},i.createElement("ul",null,t.items.map(((e,t)=>i.createElement("li",{key:t},i.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const n=t.replace("#",""),a=document.getElementById(n);a&&a.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&i.createElement(S,{toc:{items:e.items}}))))))};function H(e){let{data:{mdx:t,allMdx:l,allPostImages:s},children:o}=e;const{frontmatter:c,body:m,tableOfContents:d}=t,f=c.index,A=c.slug.split("/")[1],H=l.nodes.filter((e=>e.frontmatter.slug.includes(`/${A}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),k=H.findIndex((e=>e.frontmatter.index===f)),z=H[k+1],T=H[k-1],C=c.slug.replace(/\/$/,""),I=/[^/]*$/.exec(C)[0],M=`posts/${A}/content/${I}/`,{0:V,1:B}=(0,i.useState)(c.flagWideLayoutByDefault),{0:L,1:P}=(0,i.useState)(!1);var j;(0,i.useEffect)((()=>{P(!0);const e=setTimeout((()=>P(!1)),340);return()=>clearTimeout(e)}),[V]),"adventures"===A?j=E.cb:"research"===A?j=E.Qh:"thoughts"===A&&(j=E.T6);const N=h()(m).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,D=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),n=e%60;return n<=30?`~${t}${n>0?".5":""} h`:`~${t+1} h`}(Math.ceil(N/j)+(c.extraReadTimeMin||0)),q=[{flag:c.flagDraft,component:()=>Promise.all([n.e(3231),n.e(8809)]).then(n.bind(n,28809))},{flag:c.flagMindfuckery,component:()=>Promise.all([n.e(3231),n.e(7709)]).then(n.bind(n,67709))},{flag:c.flagRewrite,component:()=>Promise.all([n.e(3231),n.e(2002)]).then(n.bind(n,62002))},{flag:c.flagOffensive,component:()=>Promise.all([n.e(3231),n.e(7681)]).then(n.bind(n,17681))},{flag:c.flagProfane,component:()=>Promise.all([n.e(3231),n.e(3286)]).then(n.bind(n,53286))},{flag:c.flagMultilingual,component:()=>Promise.all([n.e(3231),n.e(8831)]).then(n.bind(n,78831))},{flag:c.flagUnreliably,component:()=>Promise.all([n.e(3231),n.e(8179)]).then(n.bind(n,8179))},{flag:c.flagPolitical,component:()=>Promise.all([n.e(3231),n.e(433)]).then(n.bind(n,30433))},{flag:c.flagCognitohazard,component:()=>Promise.all([n.e(3231),n.e(8413)]).then(n.bind(n,8413))},{flag:c.flagHidden,component:()=>Promise.all([n.e(3231),n.e(4794)]).then(n.bind(n,14794))}],{0:F,1:O}=(0,i.useState)([]);return(0,i.useEffect)((()=>{q.forEach((e=>{let{flag:t,component:n}=e;t&&n().then((e=>{O((t=>[].concat((0,a.A)(t),[e.default])))}))}))}),[]),i.createElement(g.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},i.createElement(v.A,{postNumber:c.index,date:c.date,updated:c.updated,readTime:D,difficulty:c.difficultyLevel,title:c.title,desc:c.desc,banner:c.banner,section:A,postKey:I,isMindfuckery:c.flagMindfuckery,mainTag:c.mainTag}),i.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},c.otherTags.map(((e,t)=>i.createElement("span",{key:t,className:`noselect ${_.MW}`,style:{margin:"0 5px 5px 0"}},e)))),i.createElement("div",{className:"postBody"},i.createElement(S,{toc:d})),i.createElement("br"),i.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},i.createElement(g.P.button,{className:`noselect ${w.pb}`,id:w.xG,onClick:()=>{B(!V)},whileTap:{scale:.93}},i.createElement(g.P.div,{className:y.DJ,key:V,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},V?"Switch to default layout":"Switch to wide layout"))),i.createElement("br"),i.createElement("div",{className:"postBody",style:{margin:V?"0 -14%":"",maxWidth:V?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},i.createElement("div",{className:`${w.P_} ${L?w.Xn:w.qG}`},F.map(((e,t)=>i.createElement(e,{key:t}))),c.indexCourse?i.createElement(x.A,{index:c.indexCourse,category:c.courseCategoryName}):"",i.createElement(u.Z.Provider,{value:{images:s.nodes,basePath:M.replace(/\/$/,"")+"/"}},i.createElement(r.xA,{components:{Image:p.A}},o)))),i.createElement(b.A,{nextPost:z,lastPost:T,keyCurrent:I,section:A}))}function k(e){return i.createElement(H,e,i.createElement(c,e))}function z(e){var t,n,a,r,l;let{data:s}=e;const{frontmatter:o}=s.mdx,c=o.titleSEO||o.title,d=o.titleOG||c,h=o.titleTwitter||c,u=o.descSEO||o.desc,p=o.descOG||u,g=o.descTwitter||u,v=o.schemaType||"BlogPosting",b=o.keywordsSEO,E=o.date,x=o.updated||E,y=o.imageOG||(null===(t=o.banner)||void 0===t||null===(n=t.childImageSharp)||void 0===n||null===(a=n.gatsbyImageData)||void 0===a||null===(r=a.images)||void 0===r||null===(l=r.fallback)||void 0===l?void 0:l.src),w=o.imageAltOG||p,A=o.imageTwitter||y,_=o.imageAltTwitter||g,S=o.canonicalURL,H=o.flagHidden||!1,k=o.mainTag||"Posts",z=o.slug.split("/")[1]||"posts",{siteUrl:T}=(0,m.Q)(),C={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:T},{"@type":"ListItem",position:2,name:k,item:`${T}/${o.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${T}${o.slug}`}]};return i.createElement(f.A,{title:c+" - avrtt.blog",titleOG:d,titleTwitter:h,description:u,descriptionOG:p,descriptionTwitter:g,schemaType:v,keywords:b,datePublished:E,dateModified:x,imageOG:y,imageAltOG:w,imageTwitter:A,imageAltTwitter:_,canonicalUrl:S,flagHidden:H,mainTag:k,section:z,type:"article"},i.createElement("script",{type:"application/ld+json"},JSON.stringify(C)))}},80420:function(e,t,n){var a=n(96540);const r={hater:()=>n.e(1448).then(n.bind(n,31448)),babel_tower:()=>n.e(6588).then(n.bind(n,66588)),bibizan:()=>n.e(5830).then(n.bind(n,95830)),cursed_emoji2:()=>n.e(5943).then(n.bind(n,95943)),cursed_emoji1:()=>n.e(7468).then(n.bind(n,27468)),mda:()=>n.e(2285).then(n.bind(n,54666)),cat_shake:()=>n.e(9690).then(n.bind(n,59690)),nerazdyplenish3:()=>n.e(1992).then(n.bind(n,51992)),burnt:()=>n.e(7656).then(n.bind(n,17656)),chad:()=>n.e(9365).then(n.bind(n,99365)),hedgehog:()=>n.e(6096).then(n.bind(n,86096)),yoba_dovolen:()=>n.e(6937).then(n.bind(n,46937)),pug_dance:()=>n.e(333).then(n.bind(n,30333)),pepe_chair:()=>n.e(689).then(n.bind(n,20689)),pepe_serious:()=>n.e(1808).then(n.bind(n,21808)),pepe_run:()=>n.e(8777).then(n.bind(n,8777)),pepe_punch:()=>n.e(7350).then(n.bind(n,87350)),pepe_agree:()=>n.e(5670).then(n.bind(n,85670)),pepe_pledik:()=>n.e(5650).then(n.bind(n,75650)),cat_stand:()=>n.e(8399).then(n.bind(n,38399)),cat_sleep:()=>n.e(8966).then(n.bind(n,88966)),nerazdyplenish2:()=>n.e(4498).then(n.bind(n,52117)),nerazdyplenish1:()=>n.e(9033).then(n.bind(n,39033)),morshu_gnome:()=>n.e(561).then(n.bind(n,30561)),cat_bw:()=>n.e(5302).then(n.bind(n,45302)),pepe_mage:()=>n.e(7239).then(n.bind(n,47239)),pepe_linux:()=>n.e(6083).then(n.bind(n,16083)),yoba_pledik:()=>n.e(5655).then(n.bind(n,45655)),pepe_chill:()=>n.e(3507).then(n.bind(n,23507)),pepe_meditation:()=>n.e(8125).then(n.bind(n,68125)),trollface:()=>n.e(8272).then(n.bind(n,78272)),cat_smile:()=>n.e(3987).then(n.bind(n,63987)),beluga:()=>n.e(5314).then(n.bind(n,35314)),pepe_money:()=>n.e(6674).then(n.bind(n,66674)),pepe_cry:()=>n.e(8345).then(n.bind(n,10726)),pepe_dance:()=>n.e(3691).then(n.bind(n,93691)),dog_nerd:()=>n.e(1896).then(n.bind(n,61896)),cat_ya_piska:()=>n.e(7040).then(n.bind(n,17040)),gandonio:()=>n.e(49).then(n.bind(n,49)),pepe_wink:()=>n.e(435).then(n.bind(n,20435))};t.A=e=>{let{sticker:t,marginLeft:n="4px",marginRight:i="0px"}=e;const{0:l,1:s}=(0,a.useState)(null);if((0,a.useEffect)((()=>{r[t]&&r[t]().then((e=>s(e.default)))}),[t]),!l)return null;const o={width:"1.8em",height:"1.8em",verticalAlign:"middle",marginTop:"-10px",marginRight:i,marginLeft:n};return a.createElement("img",{alt:"sticker",style:o,src:l})}},96098:function(e,t,n){var a=n(96540),r=n(7978);t.A=e=>{let{text:t}=e;return a.createElement(r.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-js-content-file-path-src-pages-posts-research-linear-algebra-for-ml-mdx-81c0bd339447870a6449.js.map