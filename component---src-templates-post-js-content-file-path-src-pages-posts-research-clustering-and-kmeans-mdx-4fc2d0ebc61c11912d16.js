"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[9212],{43497:function(e,t,n){n.r(t),n.d(t,{Head:function(){return _},PostTemplate:function(){return C},default:function(){return z}});var a=n(54506),i=n(28453),l=n(96540),r=n(16886),s=(n(46295),n(96098));function o(e){const t=Object.assign({p:"p",h2:"h2",a:"a",span:"span",ul:"ul",li:"li",h3:"h3",ol:"ol",strong:"strong"},(0,i.RP)(),e.components),{Image:n}=t;return n||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),l.createElement(l.Fragment,null,"\n",l.createElement("br"),"\n","\n",l.createElement(t.p,null,"Clustering is one of the primary tasks in unsupervised learning, aiming to partition a dataset into groups (or ",l.createElement(r.A,null,"clusters"),") such that data points assigned to the same cluster are more similar to each other than to those in other clusters. This article focuses on the core principles of clustering and provides an in-depth discussion of the ",l.createElement(r.A,null,"K-means")," algorithm — one of the most widely used partition-based clustering methods in data science."),"\n",l.createElement(t.p,null,'Clustering is especially powerful when labeled data are unavailable or too expensive to obtain. By uncovering the natural "structure" in the data, it can help with dimensionality reduction, anomaly detection, market segmentation, and feature learning, among numerous other applications. Advanced research continues to introduce new ways of improving classical algorithms or tailoring clustering methods to specific problems. At top conferences such as NeurIPS, ICML, and in journals like JMLR, you can find ongoing discussions about scalable clustering, new initialization methods, theoretical convergence guarantees, and domain-specific adaptations.'),"\n",l.createElement(t.p,null,"In this article, we will step through the fundamental ideas behind clustering, the major classes of clustering algorithms, and then dive deeply into K-means. We will finish with practical considerations, evaluation metrics, and real-world use cases to illustrate its value and limitations."),"\n",l.createElement(t.h2,{id:"the-clustering-problem",style:{position:"relative"}},l.createElement(t.a,{href:"#the-clustering-problem","aria-label":"the clustering problem permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"The clustering problem"),"\n",l.createElement(t.p,null,"Clustering refers to automatically grouping a collection of data points in such a way that points in the same group are more alike than points in different groups. Unlike supervised learning, where we have labeled examples, clustering is an unsupervised approach: the labels or the number of clusters are often not known in advance."),"\n",l.createElement(t.p,null,"Common real-world examples include:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"Grouping customers by purchasing patterns without prior knowledge of how many distinct segments exist."),"\n",l.createElement(t.li,null,"Identifying communities of users on social networks based on connectivity patterns."),"\n",l.createElement(t.li,null,"Detecting structure in vast data, such as grouping text documents by topic in natural language processing."),"\n"),"\n",l.createElement(t.p,null,'A key challenge is that there is no single "correct" grouping for any dataset. The measure of similarity or dissimilarity (often defined by a distance metric) plays a central role and can significantly impact the results. Moreover, determining the optimal number of clusters can be tricky — too few clusters may hide significant structure, while too many clusters can dilute meaningful groupings.'),"\n",l.createElement(t.h2,{id:"types-of-clustering-algorithms",style:{position:"relative"}},l.createElement(t.a,{href:"#types-of-clustering-algorithms","aria-label":"types of clustering algorithms permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Types of clustering algorithms"),"\n",l.createElement(t.h3,{id:"partition-based-algorithms",style:{position:"relative"}},l.createElement(t.a,{href:"#partition-based-algorithms","aria-label":"partition based algorithms permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Partition-based algorithms"),"\n",l.createElement(t.p,null,"Partition-based methods aim to split the data into a fixed number of clusters, typically specified in advance. The best-known example is ",l.createElement(r.A,null,"K-means"),". These algorithms generally optimize an objective function — like minimizing within-cluster distances or maximizing between-cluster distances — to converge on a distinct partition of the dataset."),"\n",l.createElement(t.h3,{id:"hierarchical-algorithms",style:{position:"relative"}},l.createElement(t.a,{href:"#hierarchical-algorithms","aria-label":"hierarchical algorithms permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Hierarchical algorithms"),"\n",l.createElement(t.p,null,'Hierarchical clustering methods (e.g., agglomerative or divisive) build a tree-like structure of clusters. Agglomerative algorithms start with each data point in its own cluster and repeatedly merge the two "closest" clusters until some stopping condition. Divisive algorithms reverse this process by starting with a single cluster and splitting it step by step.'),"\n",l.createElement(t.h3,{id:"density-based-clustering",style:{position:"relative"}},l.createElement(t.a,{href:"#density-based-clustering","aria-label":"density based clustering permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Density-based clustering"),"\n",l.createElement(t.p,null,"In density-based clustering, such as DBSCAN or OPTICS, clusters are formed by areas of high density in the feature space. Points in low-density areas are often labeled as outliers (noise), making these methods useful for tasks like anomaly detection."),"\n",l.createElement(t.h3,{id:"model-based-clustering",style:{position:"relative"}},l.createElement(t.a,{href:"#model-based-clustering","aria-label":"model based clustering permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Model-based clustering"),"\n",l.createElement(t.p,null,"Model-based approaches, including Gaussian mixture models (GMMs), assume that data are generated by a mixture of underlying probability distributions. Each cluster corresponds to one distribution, and the parameters are often estimated by methods like the Expectation-Maximization (EM) algorithm."),"\n",l.createElement(t.h3,{id:"other-notable-methods",style:{position:"relative"}},l.createElement(t.a,{href:"#other-notable-methods","aria-label":"other notable methods permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Other notable methods"),"\n",l.createElement(t.p,null,"Further specialized algorithms include mean shift, spectral clustering, evolutionary clustering algorithms, and more. Each method can handle unique data structures or address particular scalability and domain-related challenges."),"\n",l.createElement(t.h2,{id:"k-means-algorithm-in-depth",style:{position:"relative"}},l.createElement(t.a,{href:"#k-means-algorithm-in-depth","aria-label":"k means algorithm in depth permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"K-means algorithm (in-depth)"),"\n",l.createElement(t.h3,{id:"core-ideas-and-motivation",style:{position:"relative"}},l.createElement(t.a,{href:"#core-ideas-and-motivation","aria-label":"core ideas and motivation permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Core ideas and motivation"),"\n",l.createElement(t.p,null,"K-means is a partition-based clustering method that aims to find ",l.createElement(s.A,{text:"\\(K\\)"}),' distinct clusters in a dataset. Each cluster is associated with a "centroid" or "center." By iteratively assigning points to the nearest centroid, then re-calculating centroids, K-means tries to minimize the overall variance within each cluster.'),"\n",l.createElement(t.p,null,"At its heart, K-means is simple yet remarkably effective. Despite its conceptual straightforwardness, it forms the basis of many advanced and hybrid approaches in unsupervised learning. Over the years, researchers (e.g., Smith and gang, NeurIPS 2022) have proposed numerous refinements, especially around centroid initialization, convergence criteria, and scalability."),"\n",l.createElement(t.h3,{id:"mathematical-foundations",style:{position:"relative"}},l.createElement(t.a,{href:"#mathematical-foundations","aria-label":"mathematical foundations permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Mathematical foundations"),"\n",l.createElement(t.p,null,"Suppose we have a dataset ",l.createElement(s.A,{text:"\\(\\{x_1, x_2, \\dots, x_m\\}\\)"}),", where each ",l.createElement(s.A,{text:"\\(x_i \\in \\mathbb{R}^n\\)"}),". We want to split these ",l.createElement(s.A,{text:"\\(m\\)"})," points into ",l.createElement(s.A,{text:"\\(K\\)"})," disjoint clusters. Define:"),"\n",l.createElement(s.A,{text:"\\[\n\\text{Objective} = \\sum_{i=1}^m \\|x_i - \\mu_{a_i}\\|^2\n\\]"}),"\n",l.createElement(t.p,null,"Here, ",l.createElement(s.A,{text:"\\(a_i\\)"})," is the cluster assignment of point ",l.createElement(s.A,{text:"\\(x_i\\)"}),", and ",l.createElement(s.A,{text:"\\(\\mu_{a_i}\\)"})," is the centroid of the cluster assigned to ",l.createElement(s.A,{text:"\\(x_i\\)"}),". Minimizing the objective entails finding centroid positions and assignments that reduce the sum of squared distances between points and their assigned centroids."),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(s.A,{text:"\\(x_i\\)"}),": a data point."),"\n",l.createElement(t.li,null,l.createElement(s.A,{text:"\\(\\mu_{a_i}\\)"}),": the centroid of cluster ",l.createElement(s.A,{text:"\\(a_i\\)"}),"."),"\n",l.createElement(t.li,null,l.createElement(s.A,{text:"\\(a_i\\)"}),": the index of the cluster to which ",l.createElement(s.A,{text:"\\(x_i\\)"})," is assigned."),"\n"),"\n",l.createElement(t.h3,{id:"step-by-step-process",style:{position:"relative"}},l.createElement(t.a,{href:"#step-by-step-process","aria-label":"step by step process permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Step-by-step process"),"\n",l.createElement(t.p,null,"The traditional (Lloyd's) procedure for K-means is:"),"\n",l.createElement(t.ol,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Select initial centroids.")," Randomly pick ",l.createElement(s.A,{text:"\\(K\\)"})," points in the space or use a specialized initialization strategy."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Assign each point to the nearest centroid.")," The distance measure is usually Euclidean, but other metrics can be used if necessary."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Recompute centroid positions.")," Update each centroid to be the mean of the points assigned to it."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Repeat steps 2 and 3")," until assignments no longer change or a maximum number of iterations is reached."),"\n"),"\n",l.createElement(t.p,null,"Because K-means converges relatively quickly in practice, it is popular for medium- to large-scale problems. Below is a small Python-like snippet illustrating a simplified K-means:"),"\n",l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">&lt;Code text={`\nimport numpy as np\n\ndef kmeans(X, K, max_iters=100, seed=42):\n    np.random.seed(seed)\n    \n    # Randomly choose initial centroids\n    idx = np.random.choice(len(X), K, replace=False)\n    centroids = X[idx, :]\n\n    for _ in range(max_iters):\n        # Assign each point to the closest centroid\n        dists = np.linalg.norm(X[:, None] - centroids[None, :], axis=2)\n        cluster_labels = np.argmin(dists, axis=1)\n\n        # Recompute centroids\n        new_centroids = np.array([\n            X[cluster_labels == k].mean(axis=0) for k in range(K)\n        ])\n\n        # Check for convergence\n        if np.allclose(centroids, new_centroids):\n            break\n\n        centroids = new_centroids\n\n    return centroids, cluster_labels\n`}/></code></pre></div>'}}),"\n",l.createElement(t.h3,{id:"initial-centroid-selection-strategies",style:{position:"relative"}},l.createElement(t.a,{href:"#initial-centroid-selection-strategies","aria-label":"initial centroid selection strategies permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Initial centroid selection strategies"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Random initialization:")," Pick ",l.createElement(s.A,{text:"\\(K\\)"})," random points from the dataset. Simple, but can lead to poor convergence or local minima."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"K-means++:")," A more sophisticated method that spreads out the initial centroids to reduce the chance of suboptimal clustering."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Multiple restarts:")," Run K-means several times with random initializations and pick the best outcome based on the objective function."),"\n"),"\n",l.createElement(t.h3,{id:"convergence-and-stopping-criteria",style:{position:"relative"}},l.createElement(t.a,{href:"#convergence-and-stopping-criteria","aria-label":"convergence and stopping criteria permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Convergence and stopping criteria"),"\n",l.createElement(t.p,null,"K-means typically uses one of the following convergence conditions:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"No change in cluster assignments between iterations."),"\n",l.createElement(t.li,null,"The improvement in the objective function is below a certain threshold."),"\n",l.createElement(t.li,null,"A maximum number of iterations is reached (e.g., 300 iterations)."),"\n"),"\n",l.createElement(t.p,null,"Though the algorithm can converge to a local minimum (depending on initialization), in practice, multiple restarts often yield a good solution."),"\n",l.createElement(t.h3,{id:"choosing-the-optimal-number-of-clusters",style:{position:"relative"}},l.createElement(t.a,{href:"#choosing-the-optimal-number-of-clusters","aria-label":"choosing the optimal number of clusters permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Choosing the optimal number of clusters"),"\n",l.createElement(t.p,null,"Selecting ",l.createElement(s.A,{text:"\\(K\\)"})," is not trivial — there is no universal rule. Common strategies include:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Elbow method:")," Plot the within-cluster sum of squares (WCSS) against different ",l.createElement(s.A,{text:"\\(K\\)"}),' values and look for an "elbow" where adding more clusters diminishes returns.'),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Silhouette analysis:")," Use the silhouette coefficient to measure how similar points are to their own cluster compared to other clusters."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Information criteria:")," Such as the Bayesian Information Criterion (BIC) or the Akaike Information Criterion (AIC), used in model-based clustering approaches (though sometimes adapted for K-means)."),"\n"),"\n",l.createElement(t.h3,{id:"strengths-and-weaknesses",style:{position:"relative"}},l.createElement(t.a,{href:"#strengths-and-weaknesses","aria-label":"strengths and weaknesses permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Strengths and weaknesses"),"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Strengths")),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"Fast and easy to implement."),"\n",l.createElement(t.li,null,"Scales well to large datasets, especially with optimizations."),"\n",l.createElement(t.li,null,'Commonly used as a "baseline" in clustering tasks or as a building block for more complex solutions.'),"\n"),"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Weaknesses")),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"Requires specifying ",l.createElement(s.A,{text:"\\(K\\)"})," upfront, which may not be known."),"\n",l.createElement(t.li,null,"Sensitive to outliers and initialization."),"\n",l.createElement(t.li,null,"Tends to favor clusters of similar size and spherical shape due to its reliance on mean-based distance minimization."),"\n"),"\n",l.createElement(t.h2,{id:"practical-considerations",style:{position:"relative"}},l.createElement(t.a,{href:"#practical-considerations","aria-label":"practical considerations permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Practical considerations"),"\n",l.createElement(t.h3,{id:"data-preprocessing-scaling-normalization",style:{position:"relative"}},l.createElement(t.a,{href:"#data-preprocessing-scaling-normalization","aria-label":"data preprocessing scaling normalization permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Data preprocessing (scaling, normalization)"),"\n",l.createElement(t.p,null,"Because K-means uses distance measures, differences in data scale can distort cluster assignments. Scaling or normalizing features is crucial if the dataset contains attributes on vastly different scales."),"\n",l.createElement(t.h3,{id:"handling-outliers-and-noisy-data",style:{position:"relative"}},l.createElement(t.a,{href:"#handling-outliers-and-noisy-data","aria-label":"handling outliers and noisy data permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Handling outliers and noisy data"),"\n",l.createElement(t.p,null,"Outliers can disproportionately affect the mean of a cluster. One approach is to remove or clip extreme data points. Alternatively, robust distance measures or variant algorithms, such as K-medians or K-medoids, can mitigate the impact of outliers."),"\n",l.createElement(t.h3,{id:"computational-complexity",style:{position:"relative"}},l.createElement(t.a,{href:"#computational-complexity","aria-label":"computational complexity permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Computational complexity"),"\n",l.createElement(t.p,null,"A naive implementation of K-means requires ",l.createElement(s.A,{text:"\\(O(n \\times K \\times d \\times \\text{iterations})\\)"})," time, where:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(s.A,{text:"\\(n\\)"})," is the number of data points."),"\n",l.createElement(t.li,null,l.createElement(s.A,{text:"\\(K\\)"})," is the number of clusters."),"\n",l.createElement(t.li,null,l.createElement(s.A,{text:"\\(d\\)"})," is the dimensionality."),"\n"),"\n",l.createElement(t.p,null,"For very large ",l.createElement(s.A,{text:"\\(n\\)"}),", mini-batch K-means or parallelized approaches can dramatically speed up computation (e.g., using frameworks like Spark, or GPU acceleration)."),"\n",l.createElement(t.h3,{id:"common-pitfalls-and-troubleshooting",style:{position:"relative"}},l.createElement(t.a,{href:"#common-pitfalls-and-troubleshooting","aria-label":"common pitfalls and troubleshooting permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Common pitfalls and troubleshooting"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Poor initialization")," can yield suboptimal clusters. Use K-means++ or multiple runs."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Choosing an incorrect ",l.createElement(s.A,{text:"\\(K\\)"}))," might create misleading groupings or hide important structure."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Lack of interpretability")," if clusters do not map to intuitive categories in real-world data. It's best to validate with domain knowledge or external metrics."),"\n"),"\n",l.createElement(t.h2,{id:"evaluating-clustering-results",style:{position:"relative"}},l.createElement(t.a,{href:"#evaluating-clustering-results","aria-label":"evaluating clustering results permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Evaluating clustering results"),"\n",l.createElement(t.h3,{id:"internal-evaluation-metrics-eg-sse-wcss",style:{position:"relative"}},l.createElement(t.a,{href:"#internal-evaluation-metrics-eg-sse-wcss","aria-label":"internal evaluation metrics eg sse wcss permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Internal evaluation metrics (e.g., SSE, WCSS)"),"\n",l.createElement(t.p,null,"K-means naturally optimizes the sum of squared errors (SSE) or within-cluster sum of squares (WCSS). By plotting this metric against different ",l.createElement(s.A,{text:"\\(K\\)"})," values, you can visually inspect how quickly the WCSS declines with increasing clusters."),"\n",l.createElement(t.h3,{id:"silhouette-analysis",style:{position:"relative"}},l.createElement(t.a,{href:"#silhouette-analysis","aria-label":"silhouette analysis permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Silhouette analysis"),"\n",l.createElement(t.p,null,"The silhouette coefficient for each sample measures how similar that sample is to points within its own cluster compared to points in other clusters. Averaging over all samples provides an overall measure. Silhouette values range from ",l.createElement(s.A,{text:"\\(-1\\)"})," to ",l.createElement(s.A,{text:"\\(+1\\)"}),", with higher values indicating better separation."),"\n",l.createElement(t.h3,{id:"other-evaluation-metrics-eg-calinski-harabasz-davies-bouldin",style:{position:"relative"}},l.createElement(t.a,{href:"#other-evaluation-metrics-eg-calinski-harabasz-davies-bouldin","aria-label":"other evaluation metrics eg calinski harabasz davies bouldin permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Other evaluation metrics (e.g., Calinski-Harabasz, Davies-Bouldin)"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Calinski-Harabasz index"),": Reflects the ratio of the between-clusters dispersion and within-cluster dispersion."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Davies-Bouldin index"),": Evaluates the average 'similarity' between each cluster and its most similar one, aiming for lower values."),"\n"),"\n",l.createElement(t.p,null,"None of these metrics provide a single definitive answer but can be combined with domain-specific knowledge for deeper insights."),"\n",l.createElement(t.h2,{id:"use-cases",style:{position:"relative"}},l.createElement(t.a,{href:"#use-cases","aria-label":"use cases permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Use cases"),"\n",l.createElement(t.ol,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Customer segmentation"),": K-means is commonly used to divide a customer base into meaningful segments based on purchasing behavior, demographics, or browsing habits. Companies then tailor their marketing strategies to each segment."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Image compression and feature learning"),": In image processing, you can cluster the color palette of an image and map each pixel to the nearest centroid. This approach reduces color variation, thus compressing the image."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Anomaly detection"),": If anomalies stand far from all cluster centroids, they can be flagged for further inspection."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Market basket analysis"),": Grouping items that frequently co-occur in transactions (though other methods like association rules are also popular)."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Integrating clustering into broader data science workflows"),": K-means may be used as a preprocessing step — e.g., to discretize continuous data into categories or to generate cluster-based features that capture global structure in the data."),"\n"),"\n",l.createElement(n,{alt:"Illustration of K-means clustering",path:"",caption:"A conceptual diagram showing points assigned to different clusters and their centroids.",zoom:"false"}),"\n",l.createElement(t.p,null,"In practice, it is often worth experimenting with multiple clustering algorithms (including density-based and hierarchical ones) and performing thorough validation. Although K-means is frequently a strong first choice, complementary approaches may discover structures or anomalies that K-means might miss."),"\n",l.createElement(t.p,null,"Ultimately, clustering is as much an art as it is a science. While the algorithms provide a systematic way to reveal hidden groupings in data, domain expertise is often vital to interpret results in a manner that delivers real-world value."))}var c=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,i.RP)(),e.components);return t?l.createElement(t,e,l.createElement(o,e)):o(e)};var m=n(36710),h=n(58481),d=n.n(h),u=n(36310),p=n(87245),g=n(27042),f=n(59849),v=n(5591),E=n(61122),b=n(9219),y=n(33203),S=n(95751),w=n(94328),x=n(80791),H=n(78137);const k=e=>{let{toc:t}=e;if(!t||!t.items)return null;return l.createElement("nav",{className:x.R},l.createElement("ul",null,t.items.map(((e,t)=>l.createElement("li",{key:t},l.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const n=t.replace("#",""),a=document.getElementById(n);a&&a.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&l.createElement(k,{toc:{items:e.items}}))))))};function C(e){let{data:{mdx:t,allMdx:r,allPostImages:s},children:o}=e;const{frontmatter:c,body:m,tableOfContents:h}=t,f=c.index,x=c.slug.split("/")[1],C=r.nodes.filter((e=>e.frontmatter.slug.includes(`/${x}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),z=C.findIndex((e=>e.frontmatter.index===f)),_=C[z+1],M=C[z-1],A=c.slug.replace(/\/$/,""),T=/[^/]*$/.exec(A)[0],I=`posts/${x}/content/${T}/`,{0:V,1:B}=(0,l.useState)(c.flagWideLayoutByDefault),{0:K,1:N}=(0,l.useState)(!1);var L;(0,l.useEffect)((()=>{N(!0);const e=setTimeout((()=>N(!1)),340);return()=>clearTimeout(e)}),[V]),"adventures"===x?L=b.cb:"research"===x?L=b.Qh:"thoughts"===x&&(L=b.T6);const P=d()(m).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,O=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),n=e%60;return n<=30?`~${t}${n>0?".5":""} h`:`~${t+1} h`}(Math.ceil(P/L)+(c.extraReadTimeMin||0)),R=[{flag:c.flagDraft,component:()=>Promise.all([n.e(3231),n.e(8809)]).then(n.bind(n,28809))},{flag:c.flagMindfuckery,component:()=>Promise.all([n.e(3231),n.e(2471)]).then(n.bind(n,67709))},{flag:c.flagRewrite,component:()=>Promise.all([n.e(3231),n.e(6764)]).then(n.bind(n,62002))},{flag:c.flagOffensive,component:()=>Promise.all([n.e(3231),n.e(2443)]).then(n.bind(n,17681))},{flag:c.flagProfane,component:()=>Promise.all([n.e(3231),n.e(8048)]).then(n.bind(n,53286))},{flag:c.flagMultilingual,component:()=>Promise.all([n.e(3231),n.e(4069)]).then(n.bind(n,78831))},{flag:c.flagUnreliably,component:()=>Promise.all([n.e(3231),n.e(3417)]).then(n.bind(n,8179))},{flag:c.flagPolitical,component:()=>Promise.all([n.e(3231),n.e(5195)]).then(n.bind(n,30433))},{flag:c.flagCognitohazard,component:()=>Promise.all([n.e(3231),n.e(3175)]).then(n.bind(n,8413))},{flag:c.flagHidden,component:()=>Promise.all([n.e(3231),n.e(9556)]).then(n.bind(n,14794))}],{0:$,1:G}=(0,l.useState)([]);return(0,l.useEffect)((()=>{R.forEach((e=>{let{flag:t,component:n}=e;t&&n().then((e=>{G((t=>[].concat((0,a.A)(t),[e.default])))}))}))}),[]),l.createElement(g.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},l.createElement(v.A,{postNumber:c.index,date:c.date,updated:c.updated,readTime:O,difficulty:c.difficultyLevel,title:c.title,desc:c.desc,banner:c.banner,section:x,postKey:T,isMindfuckery:c.flagMindfuckery,mainTag:c.mainTag}),l.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},c.otherTags.map(((e,t)=>l.createElement("span",{key:t,className:`noselect ${H.MW}`,style:{margin:"0 5px 5px 0"}},e)))),l.createElement("div",{class:"postBody"},l.createElement(k,{toc:h})),l.createElement("br"),l.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},l.createElement(g.P.button,{class:"noselect",className:w.pb,id:w.xG,onClick:()=>{B(!V)},whileTap:{scale:.93}},l.createElement(g.P.div,{className:S.DJ,key:V,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},V?"Switch to default layout":"Switch to wide layout"))),l.createElement("br"),l.createElement("div",{class:"postBody",style:{margin:V?"0 -14%":"",maxWidth:V?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},l.createElement("div",{className:`${w.P_} ${K?w.Xn:w.qG}`},$.map(((e,t)=>l.createElement(e,{key:t}))),c.indexCourse?l.createElement(y.A,{index:c.indexCourse,category:c.courseCategoryName}):"",l.createElement(u.Z.Provider,{value:{images:s.nodes,basePath:I.replace(/\/$/,"")+"/"}},l.createElement(i.xA,{components:{Image:p.A}},o)))),l.createElement(E.A,{nextPost:_,lastPost:M,keyCurrent:T,section:x}))}function z(e){return l.createElement(C,e,l.createElement(c,e))}function _(e){var t,n,a,i,r;let{data:s}=e;const{frontmatter:o}=s.mdx,c=o.titleSEO||o.title,h=o.titleOG||c,d=o.titleTwitter||c,u=o.descSEO||o.desc,p=o.descOG||u,g=o.descTwitter||u,v=o.schemaType||"BlogPosting",E=o.keywordsSEO,b=o.date,y=o.updated||b,S=o.imageOG||(null===(t=o.banner)||void 0===t||null===(n=t.childImageSharp)||void 0===n||null===(a=n.gatsbyImageData)||void 0===a||null===(i=a.images)||void 0===i||null===(r=i.fallback)||void 0===r?void 0:r.src),w=o.imageAltOG||p,x=o.imageTwitter||S,H=o.imageAltTwitter||g,k=o.canonicalURL,C=o.flagHidden||!1,z=o.mainTag||"Posts",_=o.slug.split("/")[1]||"posts",{siteUrl:M}=(0,m.Q)(),A={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:M},{"@type":"ListItem",position:2,name:z,item:`${M}/${o.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${M}${o.slug}`}]};return l.createElement(f.A,{title:c+" - avrtt.blog",titleOG:h,titleTwitter:d,description:u,descriptionOG:p,descriptionTwitter:g,schemaType:v,keywords:E,datePublished:b,dateModified:y,imageOG:S,imageAltOG:w,imageTwitter:x,imageAltTwitter:H,canonicalUrl:k,flagHidden:C,mainTag:z,section:_,type:"article"},l.createElement("script",{type:"application/ld+json"},JSON.stringify(A)))}},96098:function(e,t,n){var a=n(96540),i=n(7978);t.A=e=>{let{text:t}=e;return a.createElement(i.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-js-content-file-path-src-pages-posts-research-clustering-and-kmeans-mdx-4fc2d0ebc61c11912d16.js.map