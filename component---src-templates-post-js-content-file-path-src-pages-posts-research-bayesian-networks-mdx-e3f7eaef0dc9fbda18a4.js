"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[2324],{70400:function(e,t,a){a.r(t),a.d(t,{Head:function(){return B},PostTemplate:function(){return C},default:function(){return M}});var n=a(54506),i=a(28453),r=a(96540),l=(a(16886),a(46295)),o=a(96098);function s(e){const t=Object.assign({p:"p",h3:"h3",a:"a",span:"span",ul:"ul",li:"li",strong:"strong",h2:"h2",ol:"ol",blockquote:"blockquote",em:"em"},(0,i.RP)(),e.components);return r.createElement(r.Fragment,null,"\n",r.createElement("br"),"\n","\n","\n",r.createElement(t.p,null,'As machine learning systems become more sophisticated and find applications in high-stakes domains — such as healthcare, financial forecasting, and autonomous driving — the need to quantify and manage uncertainty becomes increasingly paramount. Deterministic neural networks, which learn fixed parameters (weights and biases), offer powerful predictive capabilities but rarely provide insight into how confident these predictions are. In many real-world scenarios, risk assessment and reliability are as important as accuracy. For instance, a medical diagnosis model that outputs only a single class label ("benign" vs. "malignant") without a calibrated measure of uncertainty might lead to suboptimal or even dangerous decisions.'),"\n",r.createElement(t.p,null,'This is where probabilistic modeling enters the picture. By incorporating probability distributions directly into our models, we gain the ability to estimate uncertainty: we can learn not just a single best guess, but rather a distribution over plausible hypotheses. This extra information can lead to improved decisions under uncertainty and more robust modeling of complex phenomena. Instead of answering "What is the single most probable outcome?" a probabilistic model tries to capture "Which outcomes are likely, and how certain am I?"'),"\n",r.createElement(t.h3,{id:"from-deterministic-weights-to-probability-distributions",style:{position:"relative"}},r.createElement(t.a,{href:"#from-deterministic-weights-to-probability-distributions","aria-label":"from deterministic weights to probability distributions permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"From deterministic weights to probability distributions"),"\n",r.createElement(t.p,null,"Traditional feed-forward neural networks represent their learnable parameters (weights and biases) as single point values found by optimization — for example, via gradient descent on a loss function. In a Bayesian neural network (BNN), however, each parameter is endowed with a prior distribution. When we observe data, we use Bayesian inference to compute a posterior distribution over the parameters, reflecting the updated state of our knowledge about them. Crucially, this posterior distribution captures our uncertainty about parameters, taking into account the complexity of the model, the amount of data, and the noise inherent in the observations."),"\n",r.createElement(t.p,null,"When we feed a new input into a BNN, we integrate over all plausible parameter values (weighted by their posterior probability) to produce the so-called predictive distribution. Rather than a single point prediction, we obtain a distribution that can give a measure of how likely each possible outcome is. This distribution also allows us to compute credible intervals, predictive intervals, or other measures of uncertainty."),"\n",r.createElement(t.h3,{id:"scope-and-structure",style:{position:"relative"}},r.createElement(t.a,{href:"#scope-and-structure","aria-label":"scope and structure permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Scope and structure"),"\n",r.createElement(t.p,null,"In this article, we will cover:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Bayesian foundations"),": A thorough revisit of Bayes' theorem, the interplay of prior and likelihood in forming a posterior, and essential concepts like posterior predictive inference."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Distinction between Bayesian networks and Bayesian neural networks"),": We will introduce Bayesian networks — directed acyclic graphs that encode conditional dependencies among variables — as well as neural networks that embed uncertainty over parameters."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Building Bayesian neural networks"),": Practical aspects of constructing BNNs in frameworks like PyTorch and Pyro, focusing on how to place probability distributions over parameters."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Posterior estimation"),": Methods to handle the typically intractable integrals that arise in Bayesian models — covering Markov chain Monte Carlo (MCMC) techniques, Hamiltonian Monte Carlo (HMC), No-U-Turn sampler (NUTS), as well as Variational Inference (VI)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Advanced topics and best practices"),": Large-scale Bayesian networks, alternative approximate inference, network structure considerations such as d-separation and explaining away, etc."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Practical uncertainty estimation"),": Comparing point-estimate NNs vs. BNNs, deep ensembles, Monte Carlo dropout, conformal prediction, plus calibration metrics and reliability diagrams."),"\n"),"\n",r.createElement(t.p,null,"By the end, you should have a solid foundation in how Bayesian networks in general — and Bayesian neural networks in particular — can improve model reliability by capturing uncertainty."),"\n",r.createElement(t.h3,{id:"historical-context-and-frequentist-vs-bayesian-views",style:{position:"relative"}},r.createElement(t.a,{href:"#historical-context-and-frequentist-vs-bayesian-views","aria-label":"historical context and frequentist vs bayesian views permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Historical context and frequentist vs. Bayesian views"),"\n",r.createElement(t.p,null,"While Bayesian methods date back centuries (reviving from the work of Thomas Bayes, Pierre-Simon Laplace, and others), their popularity in modern machine learning has ebbed and flowed. Early AI approaches frequently used Bayesian reasoning to handle uncertainty in expert systems. With the advent of more data, frequentist methods and purely data-driven approaches like deep neural networks gained significant traction. However, as the demand for uncertainty estimation has grown, Bayesian approaches have re-emerged. Key references include the classic works on Bayesian belief networks (Pearl, 1988) and the pioneering Bayesian neural networks research from the early 1990s (Neal, 1996). More recent advances in scalable inference (e.g., variational inference and specialized MCMC variants) have made Bayesian neural networks more tractable for large datasets, spurring renewed research interest."),"\n",r.createElement(t.p,null,"Frequentist and Bayesian approaches differ fundamentally in how they treat parameters and data: frequentists see parameters as fixed but unknown quantities, whereas Bayesians treat parameters as random variables with prior distributions. When new data is observed, Bayesians update those distributions according to Bayes' theorem. The rise of specialized Bayesian software, along with improved computational power, has lowered the barriers to building Bayesian models on large real-world problems."),"\n",r.createElement(t.h2,{id:"bayesian-foundations",style:{position:"relative"}},r.createElement(t.a,{href:"#bayesian-foundations","aria-label":"bayesian foundations permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Bayesian foundations"),"\n",r.createElement(t.h3,{id:"revisiting-bayes-theorem",style:{position:"relative"}},r.createElement(t.a,{href:"#revisiting-bayes-theorem","aria-label":"revisiting bayes theorem permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Revisiting Bayes' theorem"),"\n",r.createElement(t.p,null,"At the heart of Bayesian inference lies Bayes' theorem. If we denote ",r.createElement(o.A,{text:"\\( \\theta \\)"})," as the parameters of a model and ",r.createElement(o.A,{text:"\\( D \\)"})," as observed data, Bayes' theorem states:"),"\n",r.createElement(o.A,{text:"\\[\np(\\theta \\mid D) = \\frac{p(D \\mid \\theta)\\, p(\\theta)}{p(D)}.\n\\]"}),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(o.A,{text:"\\( p(\\theta) \\)"})," is the prior distribution, describing our beliefs (or assumptions) about ",r.createElement(o.A,{text:"\\( \\theta \\)"})," before observing ",r.createElement(o.A,{text:"\\( D \\)"}),"."),"\n",r.createElement(t.li,null,r.createElement(o.A,{text:"\\( p(D \\mid \\theta) \\)"})," is the likelihood, describing how probable it is to observe ",r.createElement(o.A,{text:"\\( D \\)"})," if the parameters are ",r.createElement(o.A,{text:"\\( \\theta \\)"}),"."),"\n",r.createElement(t.li,null,r.createElement(o.A,{text:"\\( p(\\theta \\mid D) \\)"})," is the posterior distribution, encoding our updated belief about the parameters after observing data ",r.createElement(o.A,{text:"\\( D \\)"}),"."),"\n",r.createElement(t.li,null,r.createElement(o.A,{text:"\\( p(D) \\)"})," is the evidence or marginal likelihood, which acts as a normalizing constant ensuring that ",r.createElement(o.A,{text:"\\( p(\\theta \\mid D) \\)"})," is a valid distribution."),"\n"),"\n",r.createElement(t.p,null,"In practice, ",r.createElement(o.A,{text:"\\( p(D) \\)"})," is often intractable to compute directly because it involves integrating over all possible parameter values. This difficulty motivates approximate inference methods such as MCMC and Variational Inference."),"\n",r.createElement(t.h3,{id:"prior-likelihood-posterior-and-posterior-predictive",style:{position:"relative"}},r.createElement(t.a,{href:"#prior-likelihood-posterior-and-posterior-predictive","aria-label":"prior likelihood posterior and posterior predictive permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Prior, likelihood, posterior, and posterior predictive"),"\n",r.createElement(t.p,null,"Each component in the Bayesian pipeline has a specific role:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"Prior"),": Conveys domain knowledge or assumptions about ",r.createElement(o.A,{text:"\\( \\theta \\)"})," before data is observed. For example, if we assume parameters are likely to be small, we might choose a zero-centered Gaussian prior."),"\n"),"\n",r.createElement(t.li,null,"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"Likelihood"),": Specifies how the observed data ",r.createElement(o.A,{text:"\\( D \\)"})," is generated conditional on ",r.createElement(o.A,{text:"\\( \\theta \\)"}),". In a regression setting, we might assume Gaussian observation noise, whereas in classification we might use a Bernoulli or softmax likelihood."),"\n"),"\n",r.createElement(t.li,null,"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"Posterior"),": Combines prior and likelihood to reflect new understanding after seeing data. Bayesian updating means shifting from prior beliefs to posterior beliefs in response to evidence."),"\n"),"\n",r.createElement(t.li,null,"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"Posterior predictive distribution"),": To predict a new data point ",r.createElement(o.A,{text:"\\( x_{\\text{new}} \\)"}),", or its label ",r.createElement(o.A,{text:"\\( y_{\\text{new}} \\)"}),", we integrate over the posterior:"),"\n",r.createElement(o.A,{text:"\\[\np(y_{\\text{new}} \\mid x_{\\text{new}}, D) \n= \\int p(y_{\\text{new}} \\mid x_{\\text{new}}, \\theta)\\, p(\\theta \\mid D)\\, d\\theta.\n\\]"}),"\n"),"\n"),"\n",r.createElement(t.p,null,'This captures all parameter uncertainty when making predictions, unlike point estimates that use a single "best" set of parameters.'),"\n",r.createElement(t.h3,{id:"bayesian-networks-vs-bayesian-neural-networks",style:{position:"relative"}},r.createElement(t.a,{href:"#bayesian-networks-vs-bayesian-neural-networks","aria-label":"bayesian networks vs bayesian neural networks permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Bayesian networks vs. Bayesian neural networks"),"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"Bayesian networks")," (sometimes referred to as Bayesian belief networks or graphical models) are directed acyclic graphs (DAGs) whose nodes represent random variables and whose edges represent conditional dependencies. Formally, a Bayesian network is a graph ",r.createElement(o.A,{text:"\\( G = \\langle V, E \\rangle \\)"})," with vertices ",r.createElement(o.A,{text:"\\( v \\in V \\)"})," and directed edges ",r.createElement(o.A,{text:"\\( (u, v) \\in E \\)"})," indicating that ",r.createElement(o.A,{text:"\\( X_v \\)"})," depends on ",r.createElement(o.A,{text:"\\( X_u \\)"}),". Each node has a conditional probability distribution that encodes how it depends on its parents in the graph. The joint distribution over all variables then factorizes according to the graph's structure (the chain rule factorization)."),"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"Bayesian neural networks"),' share the same conceptual foundation of Bayesian inference — yet they place distributions specifically over the weights (and possibly biases) of a neural network. While a Bayesian network can be seen as a structured representation of conditional dependencies among random variables, a Bayesian neural network looks more like a "usual" neural network architecture for function approximation, but with priors on each weight parameter.'),"\n",r.createElement(t.p,null,"In short:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Bayesian networks"),": explicit graph structure with nodes representing random variables and edges capturing direct dependencies. Commonly used in knowledge representation, causal reasoning, or hierarchical modeling tasks."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Bayesian neural networks"),': standard NN architectures where each parameter is considered a random variable with a prior. The "graph" structure in a BNN is essentially the computational graph of the neural network rather than a DAG among observed and latent variables in the classical sense of a Bayesian network.'),"\n"),"\n",r.createElement(t.p,null,"Despite these differences, both frameworks rely on the fundamental principle of Bayes' theorem to combine prior knowledge with observed data."),"\n",r.createElement(t.h3,{id:"chain-rule-factorization-and-conditional-independence",style:{position:"relative"}},r.createElement(t.a,{href:"#chain-rule-factorization-and-conditional-independence","aria-label":"chain rule factorization and conditional independence permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Chain rule factorization and conditional independence"),"\n",r.createElement(t.p,null,"A key property of Bayesian networks is that the joint distribution factorizes over the nodes:"),"\n",r.createElement(o.A,{text:"\\[\np(X_1, \\ldots, X_n) = \\prod_{i=1}^{n} p(X_i \\mid \\text{parents}(X_i)).\n\\]"}),"\n",r.createElement(t.p,null,"Here, ",r.createElement(o.A,{text:"\\( \\text{parents}(X_i) \\)"})," indicates the parent nodes of ",r.createElement(o.A,{text:"\\( X_i \\)"}),' in the DAG. This factorization is sometimes called the "chain rule for Bayesian networks." It significantly reduces the complexity of representing the joint distribution, especially under conditional independence assumptions encoded by the DAG.'),"\n",r.createElement(t.p,null,"Conditional independence is a powerful concept. If a variable ",r.createElement(o.A,{text:"\\( X \\)"})," is conditionally independent of ",r.createElement(o.A,{text:"\\( Y \\)"})," given ",r.createElement(o.A,{text:"\\( Z \\)"}),", we have:"),"\n",r.createElement(o.A,{text:"\\[\np(X, Y \\mid Z) = p(X \\mid Z) \\, p(Y \\mid Z).\n\\]"}),"\n",r.createElement(t.p,null,"Bayesian networks exploit these structured independencies to simplify inference. In a well-designed network, the presence or absence of edges strongly constrains the possible factorizations of the joint probability."),"\n",r.createElement(t.p,null,'In neural networks, there is no explicit notion of a DAG for random variables in the same sense, but the parameter vector can be thought of as a set of random variables that generate predictions. Conditioned on the parameters, the outputs become deterministic (or follow some parametric likelihood). Still, BNNs can exhibit phenomena reminiscent of "explaining away," one of the hallmark behaviors in Bayesian networks.'),"\n",r.createElement(t.h3,{id:"conjugate-priors-and-bayesian-updating",style:{position:"relative"}},r.createElement(t.a,{href:"#conjugate-priors-and-bayesian-updating","aria-label":"conjugate priors and bayesian updating permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Conjugate priors and Bayesian updating"),"\n",r.createElement(t.p,null,"A ",r.createElement(t.strong,null,"conjugate prior")," is a prior distribution that, when combined with a certain likelihood function, yields a posterior of the same family. This property greatly simplifies analytical updates. For example, a Beta prior combined with a Bernoulli likelihood yields a Beta posterior, or a Normal prior combined with a Normal likelihood on the mean yields a Normal posterior (with updated parameters). In real-world Bayesian neural networks, the interplay between weights and data is often too complex for neat conjugate forms, leading us to rely on approximate or numerical inference methods."),"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"Bayesian updating")," is the process of taking a prior ",r.createElement(o.A,{text:"\\( p(\\theta) \\)"})," and arriving at the posterior ",r.createElement(o.A,{text:"\\( p(\\theta \\mid D) \\)"})," by multiplying the prior by the likelihood of the data. Each new dataset can be folded in sequentially, refining beliefs as we go. In principle, this is straightforward, but in practice, integrals can be intractable, and posterior distributions can be high-dimensional and multimodal."),"\n",r.createElement(t.h3,{id:"common-bayesian-pitfalls",style:{position:"relative"}},r.createElement(t.a,{href:"#common-bayesian-pitfalls","aria-label":"common bayesian pitfalls permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Common Bayesian pitfalls"),"\n",r.createElement(t.p,null,"Despite the conceptual clarity, Bayesian modeling can suffer from practical pitfalls:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Improper priors"),": Overly vague or unbounded priors can yield posteriors that are not well-defined."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Model misspecification"),": If the chosen likelihood or prior fails to capture the true data-generating process, the posterior might be systematically biased."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Computational complexity"),": In high-dimensional parameter spaces — such as large neural networks — exact Bayesian inference is generally infeasible. Approximate methods may require significant computational resources."),"\n"),"\n",r.createElement(t.p,null,"These challenges underscore why Bayesian networks and Bayesian neural networks require careful design and robust approximations."),"\n",r.createElement(t.h2,{id:"key-concepts-of-bayesian-neural-networks",style:{position:"relative"}},r.createElement(t.a,{href:"#key-concepts-of-bayesian-neural-networks","aria-label":"key concepts of bayesian neural networks permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Key concepts of Bayesian neural networks"),"\n",r.createElement(t.h3,{id:"representing-weights-and-biases-as-probability-distributions",style:{position:"relative"}},r.createElement(t.a,{href:"#representing-weights-and-biases-as-probability-distributions","aria-label":"representing weights and biases as probability distributions permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Representing weights and biases as probability distributions"),"\n",r.createElement(t.p,null,"In a Bayesian neural network, each weight ",r.createElement(o.A,{text:"\\( w_i \\)"})," and bias ",r.createElement(o.A,{text:"\\( b_j \\)"})," is assigned a probability distribution, e.g. a Gaussian with some mean and variance. Rather than storing a single numeric value for each parameter, we store a distribution that evolves as data is processed. Concretely, if ",r.createElement(o.A,{text:"\\( \\theta \\)"})," represents the entire parameter set:"),"\n",r.createElement(o.A,{text:"\\[\n\\theta = \\{\\ldots, w_i, \\ldots, b_j, \\ldots \\},\n\\]"}),"\n",r.createElement(t.p,null,"we might place a prior ",r.createElement(o.A,{text:"\\( p(\\theta) \\)"})," factorized as:"),"\n",r.createElement(o.A,{text:"\\[\np(\\theta) = \\prod_{i} \\mathcal{N}(w_i \\mid 0, \\sigma^2_w)\\, \\times \\prod_{j} \\mathcal{N}(b_j \\mid 0, \\sigma^2_b),\n\\]"}),"\n",r.createElement(t.p,null,"or a more general distribution that encodes complex dependencies among parameters. The final result is that the BNN no longer has a single feed-forward pass; predictions are integrated over the posterior distribution of parameters. We can sample a set of parameters from the posterior to get a distribution of predictions."),"\n",r.createElement(t.h3,{id:"estimating-uncertainty-and-predictive-distributions",style:{position:"relative"}},r.createElement(t.a,{href:"#estimating-uncertainty-and-predictive-distributions","aria-label":"estimating uncertainty and predictive distributions permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Estimating uncertainty and predictive distributions"),"\n",r.createElement(t.p,null,"The ultimate reason to go Bayesian is to estimate uncertainty in predictions. Suppose we want the probability ",r.createElement(o.A,{text:"\\( p(y^* \\mid x^*, D) \\)"})," of a new output ",r.createElement(o.A,{text:"\\( y^* \\)"})," given a new input ",r.createElement(o.A,{text:"\\( x^* \\)"})," and training data ",r.createElement(o.A,{text:"\\( D \\)"}),". In a BNN:"),"\n",r.createElement(o.A,{text:"\\[\np(y^* \\mid x^*, D) = \\int p(y^* \\mid x^*, \\theta)\\, p(\\theta \\mid D)\\, d\\theta.\n\\]"}),"\n",r.createElement(t.p,null,"Because ",r.createElement(o.A,{text:"\\( p(\\theta \\mid D) \\)"})," is generally high-dimensional, we approximate the integral by Monte Carlo sampling or by deriving a tractable approximation such as variational inference. Each sample from ",r.createElement(o.A,{text:"\\( \\theta \\sim p(\\theta \\mid D) \\)"})," yields a different neural network instance, and averaging predictions over many draws provides an approximation to the predictive distribution. The variance of that distribution is a measure of epistemic uncertainty (i.e., model uncertainty), while any noise in the likelihood (e.g., Gaussian observation noise) reflects aleatoric uncertainty."),"\n",r.createElement(t.h3,{id:"likelihood-functions-for-regression-and-classification",style:{position:"relative"}},r.createElement(t.a,{href:"#likelihood-functions-for-regression-and-classification","aria-label":"likelihood functions for regression and classification permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Likelihood functions for regression and classification"),"\n",r.createElement(t.p,null,"In a regression task, it is common to assume that the observed outputs ",r.createElement(o.A,{text:"\\( y \\)"})," are drawn from a Normal distribution whose mean is given by the neural network's output, and whose variance is either fixed or also inferred:"),"\n",r.createElement(o.A,{text:"\\[\np(y \\mid x, \\theta) = \\mathcal{N}\\bigl(y \\mid f_\\theta(x), \\sigma^2\\bigr).\n\\]"}),"\n",r.createElement(t.p,null,"For classification, a Bernoulli or categorical/softmax likelihood is typical. For example, in binary classification:"),"\n",r.createElement(o.A,{text:"\\[\np(y \\mid x, \\theta) = \\text{Bernoulli}\\bigl(y \\mid \\text{sigmoid}[f_\\theta(x)]\\bigr),\n\\]"}),"\n",r.createElement(t.p,null,"while in multi-class classification:"),"\n",r.createElement(o.A,{text:"\\[\np(y \\mid x, \\theta) = \\text{Categorical}\\bigl(y \\mid \\text{softmax}[f_\\theta(x)]\\bigr).\n\\]"}),"\n",r.createElement(t.h3,{id:"explaining-away--connection-to-probabilistic-graphical-models",style:{position:"relative"}},r.createElement(t.a,{href:"#explaining-away--connection-to-probabilistic-graphical-models","aria-label":"explaining away  connection to probabilistic graphical models permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Explaining away — connection to probabilistic graphical models"),"\n",r.createElement(t.p,null,'"Explaining away" is a phenomenon where the presence of one plausible cause for an observed effect can diminish the posterior probability of other potential causes. In Bayesian networks with multiple parent nodes pointing to a common child node, observing the child can introduce dependencies among parents that were previously independent. For instance, if a patient\'s fever can be caused by either the flu or food poisoning, once we know the patient definitely has the flu, the probability of food poisoning as a second cause may drop, even if initially they were considered independent causes of fever.'),"\n",r.createElement(t.p,null,'Bayesian neural networks can exhibit related behaviors: if multiple parameters can explain the same patterns in data, inferring a certain configuration might reduce the posterior probability of other configurations. Although the "graph" in a BNN is the architecture of the neural network, correlation structures among weights often lead to inter-causal or explaining-away effects.'),"\n",r.createElement(t.h3,{id:"hyperparameter-tuning-and-prior-selection",style:{position:"relative"}},r.createElement(t.a,{href:"#hyperparameter-tuning-and-prior-selection","aria-label":"hyperparameter tuning and prior selection permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Hyperparameter tuning and prior selection"),"\n",r.createElement(t.p,null,'In Bayesian neural networks, hyperparameters such as the prior variance control how "spread out" the parameter distributions are initially. If the prior is too narrow, the BNN might become overly confident and fail to capture the full range of plausible hypotheses; if too wide, the posterior might underfit the data or become multi-modal in ways that hinder sampling and optimization. Selecting priors often involves domain expertise — knowing whether parameters are likely to be large or small, or if certain layers require different constraints.'),"\n",r.createElement(t.p,null,"Common prior choices include:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Gaussian")," (e.g., ",r.createElement(o.A,{text:"\\( \\mathcal{N}(0, \\sigma^2 I) \\)"}),"): The simplest, reflecting an assumption that parameters are near zero but can vary in either direction."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Laplace")," (akin to L1-type regularization): Encourages sparsity."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Hierarchical")," or ",r.createElement(t.strong,null,"structured")," priors: Introduce relationships among parameters, e.g. kernel-based or group-level priors."),"\n"),"\n",r.createElement(t.h2,{id:"building-a-bayesian-neural-network",style:{position:"relative"}},r.createElement(t.a,{href:"#building-a-bayesian-neural-network","aria-label":"building a bayesian neural network permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Building a Bayesian neural network"),"\n",r.createElement(t.h3,{id:"simulating-data-and-problem-setup",style:{position:"relative"}},r.createElement(t.a,{href:"#simulating-data-and-problem-setup","aria-label":"simulating data and problem setup permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Simulating data and problem setup"),"\n",r.createElement(t.p,null,"To illustrate how Bayesian neural networks work, one often starts with a synthetic regression or classification problem. For instance, generating a wiggly function with added noise in certain intervals, then trying to fit a BNN so that it generalizes well outside the observed region while faithfully expressing high uncertainty there."),"\n",r.createElement(t.p,null,"You might do something like:"),"\n",r.createElement(l.A,{text:"\nimport numpy as np\n\ndef simulate_data_regression(num_points=200):\n    x = np.linspace(-1, 1, num_points)\n    noise = 0.2 * np.random.randn(num_points)\n    y = np.sin(2 * np.pi * x) + noise\n    return x, y\n"}),"\n",r.createElement(t.p,null,"In classification tasks, you can sample from known distributions or create toy examples (e.g., circles, spirals) to test how well a BNN captures complex decision boundaries."),"\n",r.createElement(t.h3,{id:"model-architecture-shallow-vs-deep-bnns",style:{position:"relative"}},r.createElement(t.a,{href:"#model-architecture-shallow-vs-deep-bnns","aria-label":"model architecture shallow vs deep bnns permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Model architecture: shallow vs. deep BNNs"),"\n",r.createElement(t.p,null,"A shallow BNN may have a single hidden layer with a small number of units, making it easier to demonstrate the inference process (such as MCMC sampling). Deeper models with multiple layers and more hidden units can capture richer function approximations but require more advanced or more computationally expensive inference methods."),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Shallow BNN"),": Often used in introductory tutorials to show how weights become distributions."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Deep BNN"),": Potentially more expressive but also more challenging to train. Large-scale BNNs can require specialized approximations."),"\n"),"\n",r.createElement(t.h3,{id:"gaussian-priors-on-weights-and-biases",style:{position:"relative"}},r.createElement(t.a,{href:"#gaussian-priors-on-weights-and-biases","aria-label":"gaussian priors on weights and biases permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Gaussian priors on weights and biases"),"\n",r.createElement(t.p,null,"Arguably the most common prior assumption is the isotropic Gaussian prior:"),"\n",r.createElement(t.p,null,r.createElement(o.A,{text:"\\( w_i \\sim \\mathcal{N}(0, \\sigma^2) \\)"})," and ",r.createElement(o.A,{text:"\\( b_j \\sim \\mathcal{N}(0, \\sigma^2) \\)"}),","),"\n",r.createElement(t.p,null,"for some ",r.createElement(o.A,{text:"\\( \\sigma \\)"})," controlling how wide the distribution is. This implies we expect parameters to be near zero unless the data strongly suggests otherwise. Simpler still, one might use ",r.createElement(o.A,{text:"\\( \\sigma^2 = 1 \\)"})," as a default, though in practice you might tune or place a hyperprior on ",r.createElement(o.A,{text:"\\( \\sigma^2 \\)"}),"."),"\n",r.createElement(t.h3,{id:"implementation-details-in-pytorch",style:{position:"relative"}},r.createElement(t.a,{href:"#implementation-details-in-pytorch","aria-label":"implementation details in pytorch permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Implementation details in PyTorch"),"\n",r.createElement(t.p,null,"Implementing BNNs in vanilla PyTorch can be done by manually specifying priors and performing MCMC or variational inference. However, you would need to write a fair amount of boilerplate code — managing distribution objects for each parameter, sampling them, computing the log probabilities, etc. This is a major reason for using high-level probabilistic programming frameworks such as Pyro or TensorFlow Probability."),"\n",r.createElement(t.p,null,"If you do attempt it in pure PyTorch, you might:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,"Initialize parameter tensors ",r.createElement(o.A,{text:"\\( w \\)"})," and ",r.createElement(o.A,{text:"\\( b \\)"})," with ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">requires_grad=True</code>'}}),"."),"\n",r.createElement(t.li,null,"Define a ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">log_prior(w, b)</code>'}})," function that sums the log densities of the prior for each parameter."),"\n",r.createElement(t.li,null,"Define a ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">log_likelihood(x, y, w, b)</code>'}})," function that computes the log of ",r.createElement(o.A,{text:"\\( p(y \\mid x, w, b) \\)"}),"."),"\n",r.createElement(t.li,null,"Combine them into ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">log_posterior(w, b) = log_prior(w,b) + log_likelihood(...)</code>'}}),"."),"\n",r.createElement(t.li,null,"Then run MCMC or VI updates to approximate the posterior."),"\n"),"\n",r.createElement(t.h3,{id:"introduction-to-pyro-for-bayesian-inference",style:{position:"relative"}},r.createElement(t.a,{href:"#introduction-to-pyro-for-bayesian-inference","aria-label":"introduction to pyro for bayesian inference permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Introduction to Pyro for Bayesian inference"),"\n",r.createElement(t.p,null,"Pyro is a probabilistic programming language built on PyTorch that automates much of the above. You specify a ",r.createElement(t.strong,null,"model")," function describing how data is generated, typically with calls like:"),"\n",r.createElement(l.A,{text:'\nimport pyro\nimport pyro.distributions as dist\n\ndef model(x, y):\n    w = pyro.sample("w", dist.Normal(0., 1.))\n    ...\n    with pyro.plate("data", size_of_dataset):\n        pyro.sample("obs", dist.Normal(...), obs=y)\n'}),"\n",r.createElement(t.p,null,"You also specify a ",r.createElement(t.strong,null,"guide")," function if doing variational inference, or choose an MCMC kernel if using sampling approaches. Pyro then orchestrates the parameter updates or sampling procedures. Because it is integrated with PyTorch, it supports GPU-accelerated tensor operations, automatic differentiation, and sophisticated neural network modules."),"\n",r.createElement(t.h3,{id:"practical-tips-for-network-initialization",style:{position:"relative"}},r.createElement(t.a,{href:"#practical-tips-for-network-initialization","aria-label":"practical tips for network initialization permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Practical tips for network initialization"),"\n",r.createElement(t.p,null,"When placing distributions over weights, initialization can matter. If the prior scale ",r.createElement(o.A,{text:"\\( \\sigma \\)"})," is large and the network is deep, forward passes can blow up easily, or gradient-based updates can become unstable. Common heuristics include:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"Setting prior means to zero or small random values."),"\n",r.createElement(t.li,null,"Setting prior variances (e.g., ",r.createElement(o.A,{text:"\\( \\sigma^2 \\)"}),") proportionally to the fan-in of each layer."),"\n",r.createElement(t.li,null,"Using smaller network architectures initially to debug inference procedures."),"\n"),"\n",r.createElement(t.h2,{id:"posterior-estimation-methods",style:{position:"relative"}},r.createElement(t.a,{href:"#posterior-estimation-methods","aria-label":"posterior estimation methods permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Posterior estimation methods"),"\n",r.createElement(t.h3,{id:"markov-chain-monte-carlo-covered-before",style:{position:"relative"}},r.createElement(t.a,{href:"#markov-chain-monte-carlo-covered-before","aria-label":"markov chain monte carlo covered before permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Markov chain Monte Carlo (covered before)"),"\n",r.createElement(t.p,null,"MCMC is a family of algorithms for sampling from complex, high-dimensional distributions — such as the posterior ",r.createElement(o.A,{text:"\\( p(\\theta \\mid D) \\)"}),". The idea is to construct a Markov chain whose stationary distribution is the desired posterior. Common MCMC approaches used for BNNs include Metropolis-Hastings, Hamiltonian Monte Carlo, and the No-U-Turn Sampler."),"\n",r.createElement(t.p,null,"While MCMC can provide asymptotically exact samples (given enough time), it can be slow to converge and scale poorly to huge datasets or very deep networks. Techniques like mini-batching are more complicated with MCMC but are possible in some specialized forms of stochastic gradient MCMC."),"\n",r.createElement(t.h3,{id:"hamiltonian-monte-carlo",style:{position:"relative"}},r.createElement(t.a,{href:"#hamiltonian-monte-carlo","aria-label":"hamiltonian monte carlo permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Hamiltonian Monte Carlo"),"\n",r.createElement(t.p,null,"Hamiltonian Monte Carlo (HMC) uses gradient information of the log-posterior to guide proposals in parameter space. Think of ",r.createElement(o.A,{text:"\\( \\theta \\)"})," as a particle moving in a potential energy landscape defined by the negative log-posterior. By simulating the Hamiltonian dynamics, HMC can perform larger, more informed jumps through parameter space, often reducing the random-walk behavior that plagues vanilla Metropolis-Hastings."),"\n",r.createElement(t.p,null,"In Pyro or Stan, HMC is implemented through methods that automatically compute gradients with respect to the parameters. However, HMC can still be quite computationally expensive for large networks."),"\n",r.createElement(t.h3,{id:"no-u-turn-sampler-nuts",style:{position:"relative"}},r.createElement(t.a,{href:"#no-u-turn-sampler-nuts","aria-label":"no u turn sampler nuts permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"No-U-Turn sampler (NUTS)"),"\n",r.createElement(t.p,null,'The No-U-Turn sampler is an extension of HMC that eliminates the need to hand-tune the trajectory length (the number of leapfrog steps). NUTS adaptively chooses when to stop the trajectory so that it does not "turn back" on itself, automating a crucial hyperparameter. This makes HMC more efficient, especially in high dimensions.'),"\n",r.createElement(t.h3,{id:"diagnosing-convergence",style:{position:"relative"}},r.createElement(t.a,{href:"#diagnosing-convergence","aria-label":"diagnosing convergence permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Diagnosing convergence"),"\n",r.createElement(t.p,null,"When using MCMC, we must ensure the chain has converged to a stationary distribution. Common diagnostics include:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Trace plots"),": Visual inspection of parameter samples over iterations."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Gelman–Rubin statistic")," (",r.createElement(o.A,{text:"\\( \\hat{R} \\)"}),"): Compares variance between multiple chains to variance within each chain. If ",r.createElement(o.A,{text:"\\( \\hat{R} \\approx 1 \\)"}),", the chains are likely converged."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Effective sample size"),": Measures how many effectively independent samples are obtained, accounting for autocorrelation."),"\n"),"\n",r.createElement(t.p,null,"If the chain is not mixing well, we might see poor effective sample sizes and ",r.createElement(o.A,{text:"\\( \\hat{R} \\)"})," far from 1."),"\n",r.createElement(t.h3,{id:"variational-inference-vi",style:{position:"relative"}},r.createElement(t.a,{href:"#variational-inference-vi","aria-label":"variational inference vi permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Variational inference (VI)"),"\n",r.createElement(t.p,null,"Variational inference offers a deterministic alternative to MCMC by reframing the inference problem as an optimization task. We choose a family of tractable distributions ",r.createElement(o.A,{text:"\\( q_\\phi(\\theta) \\)"}),", typically factorized, then find the parameters ",r.createElement(o.A,{text:"\\( \\phi \\)"})," that minimize the KL divergence ",r.createElement(o.A,{text:"\\( \\mathrm{KL}(q_\\phi(\\theta) \\,\\|\\, p(\\theta \\mid D)) \\)"}),". Because we cannot compute ",r.createElement(o.A,{text:"\\( p(\\theta \\mid D) \\)"})," directly, we instead maximize the Evidence Lower BOund (ELBO):"),"\n",r.createElement(o.A,{text:"\\[\n\\text{ELBO}(\\phi) = \\mathbb{E}_{q_\\phi(\\theta)}[\\log p(D \\mid \\theta)] - \\mathrm{KL}[q_\\phi(\\theta) \\,\\|\\, p(\\theta)].\n\\]"}),"\n",r.createElement(t.p,null,"This approach can scale to large datasets using stochastic gradient-based optimizers, but the approximation depends on the flexibility of the chosen family ",r.createElement(o.A,{text:"\\( q_\\phi(\\theta) \\)"}),"."),"\n",r.createElement(t.h3,{id:"mean-field-variational-inference",style:{position:"relative"}},r.createElement(t.a,{href:"#mean-field-variational-inference","aria-label":"mean field variational inference permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Mean-field variational inference"),"\n",r.createElement(t.p,null,"Mean-field VI is the simplest variant, where ",r.createElement(o.A,{text:"\\( q_\\phi(\\theta) \\)"})," factorizes across parameters. For instance:"),"\n",r.createElement(o.A,{text:"\\[\nq_\\phi(\\theta) = \\prod_{i} q_{\\phi_i}(w_i).\n\\]"}),"\n",r.createElement(t.p,null,"Each ",r.createElement(o.A,{text:"\\( w_i \\)"})," might have a distinct Gaussian distribution parameterized by a mean and variance. While computationally convenient, mean-field approximations can underrepresent correlations among parameters, possibly leading to an overconfident posterior."),"\n",r.createElement(t.h3,{id:"stochastic-gradient-and-the-elbo",style:{position:"relative"}},r.createElement(t.a,{href:"#stochastic-gradient-and-the-elbo","aria-label":"stochastic gradient and the elbo permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Stochastic gradient and the ELBO"),"\n",r.createElement(t.p,null,"Variational inference typically relies on gradient-based optimization. We can write:"),"\n",r.createElement(o.A,{text:"\\[\n\\nabla_\\phi \\text{ELBO} = \\nabla_\\phi \\mathbb{E}_{q_\\phi(\\theta)}[\\log p(D \\mid \\theta)] \n\\;-\\; \\nabla_\\phi \\mathrm{KL}[q_\\phi(\\theta) \\,\\|\\, p(\\theta)].\n\\]"}),"\n",r.createElement(t.p,null,'Using the "reparameterization trick" or other gradient estimators, we approximate the expectation by drawing samples ',r.createElement(o.A,{text:"\\( \\theta \\sim q_\\phi(\\theta) \\)"}),". Because each iteration is typically ",r.createElement("em",null,"much")," faster than a full MCMC iteration, VI can handle bigger models more easily."),"\n",r.createElement(t.h3,{id:"autodiagonalnormal-and-other-pyro-guides",style:{position:"relative"}},r.createElement(t.a,{href:"#autodiagonalnormal-and-other-pyro-guides","aria-label":"autodiagonalnormal and other pyro guides permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"AutoDiagonalNormal and other Pyro guides"),"\n",r.createElement(t.p,null,'Pyro provides convenient "auto-guide" classes that automatically create a parametric family ',r.createElement(o.A,{text:"\\( q_\\phi(\\theta) \\)"}),". For instance, ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">AutoDiagonalNormal</code>'}})," places an independent Gaussian distribution on each parameter dimension. More advanced guides exist, e.g. ",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">AutoMultivariateNormal</code>'}}),", normalizing flows, or hierarchical structures, that can better capture correlations."),"\n",r.createElement(t.h3,{id:"comparing-mcmc-and-vi-in-practice",style:{position:"relative"}},r.createElement(t.a,{href:"#comparing-mcmc-and-vi-in-practice","aria-label":"comparing mcmc and vi in practice permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Comparing MCMC and VI in practice"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"MCMC"),": Potentially more accurate asymptotically; can approximate multi-modal posteriors. But can be slow, and difficult to scale."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"VI"),': Typically faster and more scalable, especially for large models and datasets. But can yield biased or too "simple" approximations if the variational family is not expressive enough.'),"\n"),"\n",r.createElement(t.p,null,"Many researchers use whichever method is more tractable or whichever best matches their computational constraints. Hybrid approaches, or sophisticated flow-based variational distributions, can narrow the gap."),"\n",r.createElement(t.h3,{id:"updating-the-posterior-with-new-observations",style:{position:"relative"}},r.createElement(t.a,{href:"#updating-the-posterior-with-new-observations","aria-label":"updating the posterior with new observations permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Updating the posterior with new observations"),"\n",r.createElement(t.p,null,"In principle, we can treat newly arrived data as a second inference step:"),"\n",r.createElement(o.A,{text:"\\[\np(\\theta \\mid D_{\\text{old}}, D_{\\text{new}}) \\;\\propto\\; p(D_{\\text{new}} \\mid \\theta)\\; p(\\theta \\mid D_{\\text{old}}).\n\\]"}),"\n",r.createElement(t.p,null,"This is straightforward conceptually, but not always easy in practice if the prior or posterior is complex. For MCMC, we could continue sampling with the updated likelihood. For VI, we can initialize a new variational distribution from the old posterior's parameters and continue optimizing with new data. This is sometimes referred to as ",r.createElement(t.strong,null,"Bayesian updating")," or ",r.createElement(t.strong,null,"online Bayesian learning"),"."),"\n",r.createElement(t.h2,{id:"practical-uncertainty-estimation",style:{position:"relative"}},r.createElement(t.a,{href:"#practical-uncertainty-estimation","aria-label":"practical uncertainty estimation permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Practical uncertainty estimation"),"\n",r.createElement(t.h3,{id:"comparing-point-estimate-nns-and-bnns",style:{position:"relative"}},r.createElement(t.a,{href:"#comparing-point-estimate-nns-and-bnns","aria-label":"comparing point estimate nns and bnns permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Comparing point estimate NNs and BNNs"),"\n",r.createElement(t.p,null,"A point estimate neural network uses a single set of weights found by (for example) maximum likelihood or maximum a posteriori. If you plot predictions, the model may look extremely certain even in regions where there is little or no data. A Bayesian neural network, by contrast, typically shows high predictive uncertainty in data-scarce regions, reflecting limited information about the correct parameter settings."),"\n",r.createElement(t.h3,{id:"deep-ensembles-approximate-multi-modal-posteriors",style:{position:"relative"}},r.createElement(t.a,{href:"#deep-ensembles-approximate-multi-modal-posteriors","aria-label":"deep ensembles approximate multi modal posteriors permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Deep ensembles: approximate multi-modal posteriors"),"\n",r.createElement(t.p,null,"Deep ensembles (Lakshminarayanan and gang, 2017) train multiple independent neural networks from random initializations or different data folds. The ensemble average can mimic a Bayesian posterior by capturing multiple modes in parameter space, though it is not strictly a Bayesian procedure. Nevertheless, deep ensembles often yield impressive uncertainty estimates in practice, can be simpler to implement than BNN-specific methods, and scale well with modern hardware."),"\n",r.createElement(t.h3,{id:"monte-carlo-dropout-as-a-bayesian-approximation",style:{position:"relative"}},r.createElement(t.a,{href:"#monte-carlo-dropout-as-a-bayesian-approximation","aria-label":"monte carlo dropout as a bayesian approximation permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Monte Carlo dropout as a Bayesian approximation"),"\n",r.createElement(t.p,null,'Monte Carlo (MC) dropout (Gal & Ghahramani, 2016) interprets dropout at test time as sampling from an approximate posterior over weights. By leaving dropout layers active, each forward pass yields a different "thinned" network. Repeating multiple forward passes and averaging yields a predictive distribution. This method is easy to implement (simply do not disable dropout at test time) and can produce well-calibrated uncertainties in some cases, though it might not be as powerful as a full Bayesian approach or as stable as a carefully tuned ensemble.'),"\n",r.createElement(t.h3,{id:"conformal-prediction-theory-and-usage",style:{position:"relative"}},r.createElement(t.a,{href:"#conformal-prediction-theory-and-usage","aria-label":"conformal prediction theory and usage permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Conformal prediction: theory and usage"),"\n",r.createElement(t.p,null,'Conformal prediction (Vovk and gang) is a frequentist-driven approach to constructing prediction intervals or sets, guaranteeing certain coverage properties under mild assumptions. Unlike BNNs or ensembles, conformal prediction does not require changing the training procedure itself. Instead, it uses a held-out calibration set to compute a "nonconformity score," thereby building a set or interval for new observations guaranteed to have coverage ',r.createElement(o.A,{text:"\\( 1 - \\alpha \\)"})," (marginal coverage). This approach can be combined with any predictive model — Bayesian or not — to produce intervals that are valid in finite samples (assuming exchangeability)."),"\n",r.createElement(t.h3,{id:"trade-offs-in-computational-cost-and-performance",style:{position:"relative"}},r.createElement(t.a,{href:"#trade-offs-in-computational-cost-and-performance","aria-label":"trade offs in computational cost and performance permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Trade-offs in computational cost and performance"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"BNNs")," can yield rich posteriors but can be expensive to train via MCMC or advanced VI."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Deep ensembles")," can be trivially parallelized by training multiple networks, but require additional memory."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"MC dropout")," is easy to incorporate but might degrade raw performance if dropout significantly alters the training dynamics."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Conformal")," approaches are model-agnostic but require separate calibration steps and might produce intervals that fail to capture some structural uncertainties."),"\n"),"\n",r.createElement(t.h3,{id:"calibration-and-reliability-diagrams",style:{position:"relative"}},r.createElement(t.a,{href:"#calibration-and-reliability-diagrams","aria-label":"calibration and reliability diagrams permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Calibration and reliability diagrams"),"\n",r.createElement(t.p,null,"A well-calibrated model has the property that its predicted probabilities match empirical frequencies. For instance, among all predictions assigned a 70% probability of being correct, roughly 70% should be correct. Reliability diagrams plot predicted probability against empirical accuracy. Many Bayesian methods do not guarantee perfect calibration out-of-the-box, but in practice, they tend to calibrate better than purely deterministic point-estimate networks. Techniques like temperature scaling can further refine calibration of predictive distributions."),"\n",r.createElement(t.h2,{id:"advanced-topics",style:{position:"relative"}},r.createElement(t.a,{href:"#advanced-topics","aria-label":"advanced topics permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Advanced topics"),"\n",r.createElement(t.h3,{id:"d-separation-and-active-trails",style:{position:"relative"}},r.createElement(t.a,{href:"#d-separation-and-active-trails","aria-label":"d separation and active trails permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"d-separation and active trails"),"\n",r.createElement(t.p,null,"In classical Bayesian networks (graphical models), ",r.createElement(t.strong,null,"d-separation"),' is a criterion that tells us whether a set of observed variables "blocks" every path between two unobserved variables, thereby implying conditional independence. If there is no active trail (path) between two variables given the evidence, then those variables are conditionally independent given that evidence. Specifically:'),"\n",r.createElement(t.blockquote,null,"\n",r.createElement(t.p,null,r.createElement(o.A,{text:"\\( X \\)"})," and ",r.createElement(o.A,{text:"\\( Y \\)"})," are said to be ",r.createElement(o.A,{text:"\\( d \\)"}),"-separated by ",r.createElement(o.A,{text:"\\( Z \\)"})," if, in the graph ",r.createElement(o.A,{text:"\\( G \\)"}),", every path from ",r.createElement(o.A,{text:"\\( X \\)"})," to ",r.createElement(o.A,{text:"\\( Y \\)"})," is blocked by ",r.createElement(o.A,{text:"\\( Z \\)"}),"."),"\n"),"\n",r.createElement(t.p,null,'For example, consider the so-called "V-structure," ',r.createElement(o.A,{text:"\\( X \\rightarrow W \\leftarrow Y \\)"}),": here, ",r.createElement(o.A,{text:"\\( X \\)"})," and ",r.createElement(o.A,{text:"\\( Y \\)"})," are marginally independent, but they become dependent once ",r.createElement(o.A,{text:"\\( W \\)"}),' is observed — this is the classic "explaining away" phenomenon.'),"\n",r.createElement(t.p,null,'While BNNs do not typically frame their dependencies through explicit DAGs of observed variables, the concept of partial correlation among parameters is conceptually related to whether certain sets of parameters "block" or "activate" dependencies within the network\'s representation of data.'),"\n",r.createElement(t.h3,{id:"large-scale-bayesian-neural-networks",style:{position:"relative"}},r.createElement(t.a,{href:"#large-scale-bayesian-neural-networks","aria-label":"large scale bayesian neural networks permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Large-scale Bayesian neural networks"),"\n",r.createElement(t.p,null,"Scaling BNNs to massive architectures — e.g., modern convolutional or transformer networks — remains an area of active research. Naive MCMC can become infeasible for extremely large networks. Variational inference, especially with structured or flow-based approximate posteriors, is more promising at large scales. Another approach is to adopt a hybrid: use a deterministic backbone for most layers and only treat certain layers or subsets of parameters as Bayesian."),"\n",r.createElement(t.h3,{id:"complex-prior-distributions-eg-hierarchical-priors",style:{position:"relative"}},r.createElement(t.a,{href:"#complex-prior-distributions-eg-hierarchical-priors","aria-label":"complex prior distributions eg hierarchical priors permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Complex prior distributions, e.g., hierarchical priors"),"\n",r.createElement(t.p,null,"We are not constrained to isotropic Gaussian priors. We can design structured priors that encourage correlations among parameters — for instance, a hierarchical prior for weight matrices that share patterns across different layers or channels. Such priors can lead to better uncertainty estimates and can incorporate domain knowledge (e.g., images have spatial correlation, wavelet coefficients might have sparse structure, etc.)."),"\n",r.createElement(t.h3,{id:"alternative-approximate-inference-normalizing-flows-etc",style:{position:"relative"}},r.createElement(t.a,{href:"#alternative-approximate-inference-normalizing-flows-etc","aria-label":"alternative approximate inference normalizing flows etc permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Alternative approximate inference: normalizing flows, etc."),"\n",r.createElement(t.p,null,"Variational distributions can be made more flexible by using normalizing flows or invertible transformations that map simple base distributions (like Gaussian) into more complicated shapes. This can capture multi-modality or heavy tails in the posterior. Flow-based VI can approximate posteriors more accurately than simple mean-field approaches, albeit at higher computational cost."),"\n",r.createElement(t.h3,{id:"explaining-away-and-inter-causal-reasoning",style:{position:"relative"}},r.createElement(t.a,{href:"#explaining-away-and-inter-causal-reasoning","aria-label":"explaining away and inter causal reasoning permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Explaining away and inter-causal reasoning"),"\n",r.createElement(t.p,null,'We have mentioned "explaining away" in the context of Bayesian networks, but it can also manifest in Bayesian neural networks. When multiple sets of parameters can explain the data, observing the data can cause the posterior mass to concentrate more heavily on one set of parameters, decreasing probability assigned to alternative sets. This can be viewed as inter-causal reasoning: the presence of one cause (set of parameters) makes the other less necessary to explain the effect (the observed data).'),"\n",r.createElement(t.h3,{id:"transfer-learning-and-bayesian-fine-tuning",style:{position:"relative"}},r.createElement(t.a,{href:"#transfer-learning-and-bayesian-fine-tuning","aria-label":"transfer learning and bayesian fine tuning permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Transfer learning and Bayesian fine-tuning"),"\n",r.createElement(t.p,null,"In many deep learning applications, it is common to start with a model pretrained on a large dataset and then fine-tune it on a smaller target dataset. Bayesian approaches can incorporate uncertainty from the pretrained model by using its weights as a prior or by adopting some hierarchical structure that captures how the new data updates the old parameters. Bayesian fine-tuning can lead to robust adaptation, especially when target data is limited."),"\n",r.createElement(t.h2,{id:"additional-implementation-frameworks-and-best-practices",style:{position:"relative"}},r.createElement(t.a,{href:"#additional-implementation-frameworks-and-best-practices","aria-label":"additional implementation frameworks and best practices permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Additional implementation frameworks and best practices"),"\n",r.createElement(t.h3,{id:"jax-tensorflow-probability-and-others",style:{position:"relative"}},r.createElement(t.a,{href:"#jax-tensorflow-probability-and-others","aria-label":"jax tensorflow probability and others permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"JAX, TensorFlow Probability, and others"),"\n",r.createElement(t.p,null,"Beyond PyTorch + Pyro, other frameworks offer probabilistic programming or Bayesian neural network capabilities:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"TensorFlow Probability (TFP)"),": Tools for building Bayesian models and performing VI or MCMC with TensorFlow."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"JAX-based libraries"),": Haiku, Flax, NumPyro, and other ecosystems that combine JAX's auto-differentiation with probabilistic tools."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Stan"),": A powerful probabilistic language mostly used for classical Bayesian models, though sometimes used for smaller Bayesian NNs."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Edward2"),": An experimental interface for TFP with higher-level constructs for Bayesian neural networks."),"\n"),"\n",r.createElement(t.h3,{id:"optimization-tricks-and-debugging-tips",style:{position:"relative"}},r.createElement(t.a,{href:"#optimization-tricks-and-debugging-tips","aria-label":"optimization tricks and debugging tips permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Optimization tricks and debugging tips"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Gradual unfreezing"),": Sometimes it helps to fix certain parameters, then unfreeze them as the inference progresses."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Learning rate schedules"),": Because we are optimizing an ELBO or running an MCMC chain, the step size can have a big impact. In HMC, a too-large step size leads to high rejection rates; in VI, it can cause divergence."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Checking variance"),": Keep an eye on the scale of parameter distributions. If they explode, you may need to reduce the prior variance or re-initialize."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Intercept correlated parameters"),": For advanced networks, consider more expressive approximate posteriors that capture correlation among weights."),"\n"),"\n",r.createElement(t.h3,{id:"model-selection-and-comparison",style:{position:"relative"}},r.createElement(t.a,{href:"#model-selection-and-comparison","aria-label":"model selection and comparison permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Model selection and comparison"),"\n",r.createElement(t.p,null,"Selecting among Bayesian models can be done by comparing marginal likelihoods or approximate model evidence, though this is often computationally difficult. Alternatives include:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Widely Applicable Information Criterion (WAIC)"),": A generalization of AIC and DIC for Bayesian models."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Bayes factors"),": The ratio of marginal likelihoods for two models."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Predictive performance"),": In practice, many just compare predictive metrics like RMSE, log-likelihood, or calibration error on a validation set."),"\n"),"\n",r.createElement(t.h3,{id:"reproducibility-and-experiment-tracking",style:{position:"relative"}},r.createElement(t.a,{href:"#reproducibility-and-experiment-tracking","aria-label":"reproducibility and experiment tracking permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Reproducibility and experiment tracking"),"\n",r.createElement(t.p,null,"Due to the inherent stochasticity of sampling-based approaches, it is crucial to:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"Use fixed random seeds (though note that some GPU computations might be nondeterministic)."),"\n",r.createElement(t.li,null,"Log MCMC traces and diagnostic statistics."),"\n",r.createElement(t.li,null,"Track hyperparameters of priors and inference algorithms meticulously."),"\n",r.createElement(t.li,null,"Store final posterior samples or fitted variational distributions for later inspection and reproducibility."),"\n"),"\n",r.createElement(t.h2,{id:"applications-and-case-studies",style:{position:"relative"}},r.createElement(t.a,{href:"#applications-and-case-studies","aria-label":"applications and case studies permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Applications and case studies"),"\n",r.createElement(t.h3,{id:"regression-tasks--time-series-noisy-function-approximation",style:{position:"relative"}},r.createElement(t.a,{href:"#regression-tasks--time-series-noisy-function-approximation","aria-label":"regression tasks  time series noisy function approximation permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Regression tasks — time series, noisy function approximation"),"\n",r.createElement(t.p,null,"Bayesian neural networks are especially useful in regression tasks with limited data or high uncertainty. For time series forecasting, a BNN can produce credible intervals that expand as we forecast further into the future — reflecting the accumulation of uncertainty over time. In noisy function approximation (e.g., modeling physical processes), the BNN can separate measurement noise (aleatoric) from model uncertainty (epistemic)."),"\n",r.createElement(t.h3,{id:"classification-tasks--mnist-distribution-shift-detection",style:{position:"relative"}},r.createElement(t.a,{href:"#classification-tasks--mnist-distribution-shift-detection","aria-label":"classification tasks  mnist distribution shift detection permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Classification tasks — MNIST, distribution shift detection"),"\n",r.createElement(t.p,null,"In classification, BNNs can flag out-of-distribution inputs by showing large predictive uncertainty. For instance, on MNIST digit classification, a BNN can produce high-entropy predictions for images that do not look like typical handwritten digits (e.g., random noise or letters). This is beneficial for real-world applications that must detect anomalies or reject uncertain predictions."),"\n",r.createElement(t.h3,{id:"medical-financial-and-other-real-world-applications",style:{position:"relative"}},r.createElement(t.a,{href:"#medical-financial-and-other-real-world-applications","aria-label":"medical financial and other real world applications permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Medical, financial, and other real-world applications"),"\n",r.createElement(t.p,null,"Clinical diagnosis must often account for the costs of false positives and false negatives. A Bayesian model can incorporate domain knowledge about disease prevalence (the prior) and provide well-calibrated posteriors that reflect how uncertain it is about a diagnosis — imperative for medical decision making. In finance, capturing uncertainty about future market behavior can help risk management. In robotics or self-driving cars, Bayesian methods can help to quantify and reduce collisions or planning errors by incorporating uncertainty in sensor readings."),"\n",r.createElement(t.h3,{id:"interpreting-and-visualizing-uncertainty",style:{position:"relative"}},r.createElement(t.a,{href:"#interpreting-and-visualizing-uncertainty","aria-label":"interpreting and visualizing uncertainty permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Interpreting and visualizing uncertainty"),"\n",r.createElement(t.p,null,"Visualizing the posterior predictive distribution often involves plotting a mean prediction plus credible intervals (e.g., ±2 standard deviations). One can also visualize the distribution of network parameters or the distribution of predictions on a test set. Tools such as reliability diagrams help check calibration, while dimension-reduced embeddings of posterior samples can hint at multi-modal distributions."),"\n",r.createElement(t.h3,{id:"performance-metrics-in-real-world-scenarios",style:{position:"relative"}},r.createElement(t.a,{href:"#performance-metrics-in-real-world-scenarios","aria-label":"performance metrics in real world scenarios permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Performance metrics in real-world scenarios"),"\n",r.createElement(t.p,null,"When uncertainty matters, standard metrics like accuracy or MSE are insufficient. We might consider:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Brier score"),": A proper score that measures the accuracy of probabilistic predictions."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Log-likelihood / Log probability"),": Summation or average of ",r.createElement(o.A,{text:"\\( \\log p(y_i \\mid x_i, \\theta) \\)"}),"."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Calibration error"),": e.g. Expected Calibration Error (ECE) or reliability diagrams."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Coverage"),": For intervals or sets, what fraction of true data is covered by the predicted intervals/sets?"),"\n"),"\n",r.createElement(t.h2,{id:"conclusion",style:{position:"relative"}},r.createElement(t.a,{href:"#conclusion","aria-label":"conclusion permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Conclusion"),"\n",r.createElement(t.h3,{id:"key-takeaways-and-lessons-learned",style:{position:"relative"}},r.createElement(t.a,{href:"#key-takeaways-and-lessons-learned","aria-label":"key takeaways and lessons learned permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Key takeaways and lessons learned"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Uncertainty matters"),": Bayesian frameworks provide a systematic way to incorporate and update uncertainties, crucial for risk-sensitive domains."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Bayesian networks"),": Encode the factorization of a joint distribution in a DAG, capturing conditional independencies and enabling structured reasoning about latent and observed variables."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Bayesian neural networks"),": Extend neural networks by placing distributions over parameters, yielding powerful function approximators that reflect uncertainty in their predictions."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Inference"),": Exact Bayesian inference is typically intractable for high-dimensional models, but approximate methods like MCMC and variational inference provide practical solutions — each with trade-offs in computational cost, accuracy, and complexity."),"\n"),"\n",r.createElement(t.h3,{id:"open-challenges-and-future-research-directions",style:{position:"relative"}},r.createElement(t.a,{href:"#open-challenges-and-future-research-directions","aria-label":"open challenges and future research directions permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Open challenges and future research directions"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Scalability"),": MCMC for large models remains challenging, though specialized methods continue to appear."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Expressive approximate posteriors"),": Flow-based or implicit distributions can capture richer posterior structures but are computationally intensive."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Automated prior specification"),': Deciding "good" priors can be nontrivial, especially for very deep networks with tens of millions of parameters.'),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Multi-modal distributions"),": Real posteriors in deep models may be multi-modal. Handling these systematically remains an open research area."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Integration with big data"),": Stochastic gradient MCMC and distributed inference are areas of active research for data at web scale."),"\n"),"\n",r.createElement(t.h3,{id:"references-and-recommended-reading",style:{position:"relative"}},r.createElement(t.a,{href:"#references-and-recommended-reading","aria-label":"references and recommended reading permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"References and recommended reading"),"\n",r.createElement(t.p,null,"Below are several sources for further exploration. In addition, many references are mentioned inline throughout the text."),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"D. J. C. MacKay. ",r.createElement(t.em,null,"Information Theory, Inference, and Learning Algorithms.")," Cambridge University Press, 2003."),"\n",r.createElement(t.li,null,"R. M. Neal. ",r.createElement(t.em,null,"Bayesian Learning for Neural Networks.")," Springer, 1996."),"\n",r.createElement(t.li,null,'Y. Gal, Z. Ghahramani. "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning." ',r.createElement(t.em,null,"ICML"),", 2016."),"\n",r.createElement(t.li,null,"C. Robert. ",r.createElement(t.em,null,"The Bayesian Choice.")," 2nd ed. Springer, 2001."),"\n",r.createElement(t.li,null,'A. Kendall, Y. Gal. "What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?" ',r.createElement(t.em,null,"NIPS"),", 2017."),"\n",r.createElement(t.li,null,'A. Lakshminarayanan, and gang "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles." ',r.createElement(t.em,null,"NeurIPS"),", 2017."),"\n",r.createElement(t.li,null,"Judea Pearl. ",r.createElement(t.em,null,"Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.")," Morgan Kaufmann, 1988."),"\n",r.createElement(t.li,null,'Andrew D. Gordon, Thomas A. Henzinger, Aditya V. Nori, Sriram K. Rajamani. "Probabilistic programming." ',r.createElement(t.em,null,"FOSE 2014.")),"\n"),"\n",r.createElement(t.h3,{id:"final-thoughts-on-the-bayesian-perspective",style:{position:"relative"}},r.createElement(t.a,{href:"#final-thoughts-on-the-bayesian-perspective","aria-label":"final thoughts on the bayesian perspective permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Final thoughts on the Bayesian perspective"),"\n",r.createElement(t.p,null,"Bayesian neural networks and Bayesian networks elegantly combine foundational probability theory with modern machine learning. They empower practitioners to encode prior knowledge, rigorously update beliefs in light of data, and reason about uncertainty for safer and more interpretable AI. While there are still computational and conceptual hurdles, the field is rapidly evolving, and the fundamental ideas — grounded in Bayes' theorem — remain as relevant as ever for robust, trustworthy machine learning."),"\n",r.createElement(t.p,null,"Having walked through the motivations, mathematical foundations, computational methods, and practical issues, you now possess an extensive overview of Bayesian networks and Bayesian neural networks, including how to build them, how to approximate posteriors, and how to interpret the resulting uncertainty. In the broader machine learning landscape, these ideas represent a crucial step forward in developing models that both fit data and acknowledge what they do not know."))}var c=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,i.RP)(),e.components);return t?r.createElement(t,e,r.createElement(s,e)):s(e)},h=a(36710),m=a(58481),d=a.n(m),p=a(36310),u=a(87245),g=a(27042),f=a(59849),v=a(5591),b=a(61122),y=a(9219),E=a(33203),w=a(95751),x=a(94328),S=a(80791),k=a(78137);const H=e=>{let{toc:t}=e;if(!t||!t.items)return null;return r.createElement("nav",{className:S.R},r.createElement("ul",null,t.items.map(((e,t)=>r.createElement("li",{key:t},r.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const a=t.replace("#",""),n=document.getElementById(a);n&&n.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&r.createElement(H,{toc:{items:e.items}}))))))};function C(e){let{data:{mdx:t,allMdx:l,allPostImages:o},children:s}=e;const{frontmatter:c,body:h,tableOfContents:m}=t,f=c.index,S=c.slug.split("/")[1],C=l.nodes.filter((e=>e.frontmatter.slug.includes(`/${S}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),M=C.findIndex((e=>e.frontmatter.index===f)),B=C[M+1],_=C[M-1],z=c.slug.replace(/\/$/,""),N=/[^/]*$/.exec(z)[0],I=`posts/${S}/content/${N}/`,{0:T,1:A}=(0,r.useState)(c.flagWideLayoutByDefault),{0:V,1:L}=(0,r.useState)(!1);var P;(0,r.useEffect)((()=>{L(!0);const e=setTimeout((()=>L(!1)),340);return()=>clearTimeout(e)}),[T]),"adventures"===S?P=y.cb:"research"===S?P=y.Qh:"thoughts"===S&&(P=y.T6);const D=d()(h).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,q=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),a=e%60;return a<=30?`~${t}${a>0?".5":""} h`:`~${t+1} h`}(Math.ceil(D/P)+(c.extraReadTimeMin||0)),G=[{flag:c.flagDraft,component:()=>Promise.all([a.e(3231),a.e(8809)]).then(a.bind(a,28809))},{flag:c.flagMindfuckery,component:()=>Promise.all([a.e(3231),a.e(2471)]).then(a.bind(a,67709))},{flag:c.flagRewrite,component:()=>Promise.all([a.e(3231),a.e(6764)]).then(a.bind(a,62002))},{flag:c.flagOffensive,component:()=>Promise.all([a.e(3231),a.e(2443)]).then(a.bind(a,17681))},{flag:c.flagProfane,component:()=>Promise.all([a.e(3231),a.e(8048)]).then(a.bind(a,53286))},{flag:c.flagMultilingual,component:()=>Promise.all([a.e(3231),a.e(4069)]).then(a.bind(a,78831))},{flag:c.flagUnreliably,component:()=>Promise.all([a.e(3231),a.e(3417)]).then(a.bind(a,8179))},{flag:c.flagPolitical,component:()=>Promise.all([a.e(3231),a.e(5195)]).then(a.bind(a,30433))},{flag:c.flagCognitohazard,component:()=>Promise.all([a.e(3231),a.e(3175)]).then(a.bind(a,8413))},{flag:c.flagHidden,component:()=>Promise.all([a.e(3231),a.e(9556)]).then(a.bind(a,14794))}],{0:W,1:j}=(0,r.useState)([]);return(0,r.useEffect)((()=>{G.forEach((e=>{let{flag:t,component:a}=e;t&&a().then((e=>{j((t=>[].concat((0,n.A)(t),[e.default])))}))}))}),[]),r.createElement(g.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},r.createElement(v.A,{postNumber:c.index,date:c.date,updated:c.updated,readTime:q,difficulty:c.difficultyLevel,title:c.title,desc:c.desc,banner:c.banner,section:S,postKey:N,isMindfuckery:c.flagMindfuckery,mainTag:c.mainTag}),r.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},c.otherTags.map(((e,t)=>r.createElement("span",{key:t,className:`noselect ${k.MW}`,style:{margin:"0 5px 5px 0"}},e)))),r.createElement("div",{class:"postBody"},r.createElement(H,{toc:m})),r.createElement("br"),r.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},r.createElement(g.P.button,{class:"noselect",className:x.pb,id:x.xG,onClick:()=>{A(!T)},whileTap:{scale:.93}},r.createElement(g.P.div,{className:w.DJ,key:T,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},T?"Switch to default layout":"Switch to wide layout"))),r.createElement("br"),r.createElement("div",{class:"postBody",style:{margin:T?"0 -14%":"",maxWidth:T?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},r.createElement("div",{className:`${x.P_} ${V?x.Xn:x.qG}`},W.map(((e,t)=>r.createElement(e,{key:t}))),c.indexCourse?r.createElement(E.A,{index:c.indexCourse,category:c.courseCategoryName}):"",r.createElement(p.Z.Provider,{value:{images:o.nodes,basePath:I.replace(/\/$/,"")+"/"}},r.createElement(i.xA,{components:{Image:u.A}},s)))),r.createElement(b.A,{nextPost:B,lastPost:_,keyCurrent:N,section:S}))}function M(e){return r.createElement(C,e,r.createElement(c,e))}function B(e){var t,a,n,i,l;let{data:o}=e;const{frontmatter:s}=o.mdx,c=s.titleSEO||s.title,m=s.titleOG||c,d=s.titleTwitter||c,p=s.descSEO||s.desc,u=s.descOG||p,g=s.descTwitter||p,v=s.schemaType||"BlogPosting",b=s.keywordsSEO,y=s.date,E=s.updated||y,w=s.imageOG||(null===(t=s.banner)||void 0===t||null===(a=t.childImageSharp)||void 0===a||null===(n=a.gatsbyImageData)||void 0===n||null===(i=n.images)||void 0===i||null===(l=i.fallback)||void 0===l?void 0:l.src),x=s.imageAltOG||u,S=s.imageTwitter||w,k=s.imageAltTwitter||g,H=s.canonicalURL,C=s.flagHidden||!1,M=s.mainTag||"Posts",B=s.slug.split("/")[1]||"posts",{siteUrl:_}=(0,h.Q)(),z={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:_},{"@type":"ListItem",position:2,name:M,item:`${_}/${s.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${_}${s.slug}`}]};return r.createElement(f.A,{title:c+" - avrtt.blog",titleOG:m,titleTwitter:d,description:p,descriptionOG:u,descriptionTwitter:g,schemaType:v,keywords:b,datePublished:y,dateModified:E,imageOG:w,imageAltOG:x,imageTwitter:S,imageAltTwitter:k,canonicalUrl:H,flagHidden:C,mainTag:M,section:B,type:"article"},r.createElement("script",{type:"application/ld+json"},JSON.stringify(z)))}},96098:function(e,t,a){var n=a(96540),i=a(7978);t.A=e=>{let{text:t}=e;return n.createElement(i.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-js-content-file-path-src-pages-posts-research-bayesian-networks-mdx-e3f7eaef0dc9fbda18a4.js.map