"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[5680],{3962:function(e,t){t.A="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iaXNvLTg4NTktMSI/Pgo8IS0tIEdlbmVyYXRvcjogQWRvYmUgSWxsdXN0cmF0b3IgMTYuMC4wLCBTVkcgRXhwb3J0IFBsdWctSW4gLiBTVkcgVmVyc2lvbjogNi4wMCBCdWlsZCAwKSAgLS0+CjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+CjxzdmcgdmVyc2lvbj0iMS4xIiBpZD0iQ2FwYV8xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB4PSIwcHgiIHk9IjBweCIKCSB3aWR0aD0iNDE2Ljk3OXB4IiBoZWlnaHQ9IjQxNi45NzlweCIgdmlld0JveD0iMCAwIDQxNi45NzkgNDE2Ljk3OSIgc3R5bGU9ImVuYWJsZS1iYWNrZ3JvdW5kOm5ldyAwIDAgNDE2Ljk3OSA0MTYuOTc5OyIKCSB4bWw6c3BhY2U9InByZXNlcnZlIj4KPGc+Cgk8cGF0aCBkPSJNMzU2LjAwNCw2MS4xNTZjLTgxLjM3LTgxLjQ3LTIxMy4zNzctODEuNTUxLTI5NC44NDgtMC4xODJjLTgxLjQ3LDgxLjM3MS04MS41NTIsMjEzLjM3OS0wLjE4MSwyOTQuODUKCQljODEuMzY5LDgxLjQ3LDIxMy4zNzgsODEuNTUxLDI5NC44NDksMC4xODFDNDM3LjI5MywyNzQuNjM2LDQzNy4zNzUsMTQyLjYyNiwzNTYuMDA0LDYxLjE1NnogTTIzNy42LDM0MC43ODYKCQljMCwzLjIxNy0yLjYwNyw1LjgyMi01LjgyMiw1LjgyMmgtNDYuNTc2Yy0zLjIxNSwwLTUuODIyLTIuNjA1LTUuODIyLTUuODIyVjE2Ny44ODVjMC0zLjIxNywyLjYwNy01LjgyMiw1LjgyMi01LjgyMmg0Ni41NzYKCQljMy4yMTUsMCw1LjgyMiwyLjYwNCw1LjgyMiw1LjgyMlYzNDAuNzg2eiBNMjA4LjQ5LDEzNy45MDFjLTE4LjYxOCwwLTMzLjc2Ni0xNS4xNDYtMzMuNzY2LTMzLjc2NQoJCWMwLTE4LjYxNywxNS4xNDctMzMuNzY2LDMzLjc2Ni0zMy43NjZjMTguNjE5LDAsMzMuNzY2LDE1LjE0OCwzMy43NjYsMzMuNzY2QzI0Mi4yNTYsMTIyLjc1NSwyMjcuMTA3LDEzNy45MDEsMjA4LjQ5LDEzNy45MDF6Ii8+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPC9zdmc+Cg=="},9360:function(e,t,n){n.d(t,{A:function(){return l}});var a=n(96540),r=n(3962),i="styles-module--tooltiptext--a263b";var l=e=>{let{text:t,isBadge:n=!1}=e;const{0:l,1:o}=(0,a.useState)(!1),s=(0,a.useRef)(null);return(0,a.useEffect)((()=>{function e(e){s.current&&e.target instanceof Node&&!s.current.contains(e.target)&&o(!1)}return document.addEventListener("click",e),()=>{document.removeEventListener("click",e)}}),[]),a.createElement("span",{className:"styles-module--tooltipWrapper--75ebf",ref:s},a.createElement("img",{id:n?"styles-module--infoBadge--e3d66":"styles-module--info--26c1f",src:r.A,alt:"info",onClick:e=>{e.stopPropagation(),o((e=>!e))}}),a.createElement("span",{className:l?`${i} styles-module--visible--c063c`:i},t))}},78228:function(e,t,n){n.r(t),n.d(t,{Head:function(){return L},PostTemplate:function(){return _},default:function(){return M}});var a=n(28453),r=n(96540),i=n(9360),l=n(61992),o=(n(62087),n(90548));function s(e){const t=Object.assign({p:"p",ul:"ul",li:"li",h3:"h3",a:"a",span:"span",ol:"ol",strong:"strong",hr:"hr",h2:"h2"},(0,a.RP)(),e.components),{Image:n}=t;return n||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),r.createElement(r.Fragment,null,"\n",r.createElement("br"),"\n","\n",r.createElement(t.p,null,"When I discuss Transformers, and in particular their attention-based modules, the three foundational components that frequently appear are the ",r.createElement(l.A,null,"query"),", ",r.createElement(l.A,null,"key"),", and ",r.createElement(l.A,null,"value")," vectors. These elements form the bedrock of what is often called ",r.createElement(i.A,{text:"Self-attention is sometimes also referred to as intra-attention, since it relates different positions of a single sequence in order to compute a representation of the same sequence."}),"self-attention, which is the indispensable mechanism allowing the model to weigh different parts of the input sequence according to their importance."),"\n",r.createElement(t.p,null,"In the simplest terms:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"A ",r.createElement(l.A,null,"query")," vector ",r.createElement(o.A,{text:"\\(Q\\)"}),' represents the current token (or position) in the sequence. Intuitively, it poses the question: "To what should I pay attention?"'),"\n",r.createElement(t.li,null,"A ",r.createElement(l.A,null,"key")," vector ",r.createElement(o.A,{text:"\\(K\\)"}),' signifies the potential "address" of information. In a sense, it answers the question "Do I have what the query is looking for?"'),"\n",r.createElement(t.li,null,"A ",r.createElement(l.A,null,"value")," vector ",r.createElement(o.A,{text:"\\(V\\)"})," is the actual information content. It is the data that will be passed along or attended to if the query-key match is strong."),"\n"),"\n",r.createElement(t.p,null,'Concretely, each position in the input sequence produces a query, key, and value vector (often by passing the same embedding or hidden state through different learned linear transformations). Then, the model compares each query with all keys to determine the "attention weights" and uses those weights to aggregate a weighted sum of the value vectors.'),"\n",r.createElement(t.p,null,"If I have a sequence ",r.createElement(o.A,{text:"\\(X = (x_1, x_2, \\ldots, x_n)\\)"}),", each token ",r.createElement(o.A,{text:"\\(x_i\\)"})," is typically transformed into:"),"\n",r.createElement(o.A,{text:"\\[\nQ_i = x_i W_Q, \\quad K_i = x_i W_K, \\quad V_i = x_i W_V\n\\]"}),"\n",r.createElement(t.p,null,"where ",r.createElement(o.A,{text:"\\(W_Q, W_K, W_V\\)"})," are learned projection matrices. This operation produces the sets of queries ",r.createElement(o.A,{text:"(\\{Q_1, Q_2, ..., Q_n\\})"}),", keys ",r.createElement(o.A,{text:"(\\{K_1, K_2, ..., K_n\\})"}),", and values ",r.createElement(o.A,{text:"(\\{V_1, V_2, ..., V_n\\})"}),". The subsequent steps involve computing dot products between queries and keys, scaling, applying a softmax, and multiplying by the values â€” details that I will now explore in the next sub-chapter on scaled dot-product attention."),"\n",r.createElement(t.h3,{id:"scaled-dot-product-attention",style:{position:"relative"}},r.createElement(t.a,{href:"#scaled-dot-product-attention","aria-label":"scaled dot product attention permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"scaled dot-product attention"),"\n",r.createElement(t.p,null,"The scaled dot-product attention is one of the most succinct yet potent formulations of attention. It is typically written as:"),"\n",r.createElement(o.A,{text:"\\[\n\\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n\\]"}),"\n",r.createElement(t.p,null,"Here:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(o.A,{text:"\\(Q\\)"}),": a matrix holding all the queries stacked row-wise (one query vector per row)."),"\n",r.createElement(t.li,null,r.createElement(o.A,{text:"\\(K\\)"}),": similarly, a matrix of all key vectors."),"\n",r.createElement(t.li,null,r.createElement(o.A,{text:"\\(V\\)"}),": matrix of all value vectors."),"\n",r.createElement(t.li,null,r.createElement(o.A,{text:"\\(d_k\\)"}),": the dimensionality of each key vector."),"\n",r.createElement(t.li,null,r.createElement(o.A,{text:"\\(\\mathrm{softmax}(\\cdot)\\)"}),": applies softmax across the row dimension (i.e., across all keys for a particular query)."),"\n"),"\n",r.createElement(t.p,null,"The reason for the division by ",r.createElement(o.A,{text:"\\( \\sqrt{d_k} \\)"}),' is primarily related to stabilizing gradients. When the query and key vectors are high-dimensional, the dot products can grow large in magnitude, pushing the softmax function into regimes where it is almost "saturated" (leading to extremely small gradient updates). By scaling down by ',r.createElement(o.A,{text:"\\( \\sqrt{d_k} \\)"}),", the values of these dot products remain within a more manageable range, ensuring more stable training dynamics. This idea was first introduced by Vaswani and gang (NeurIPS 2017) in the seminal paper on the Transformer architecture."),"\n",r.createElement(t.p,null,"To visualize this in a step-by-step form:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,"Compute ",r.createElement(o.A,{text:"\\(QK^\\top\\)"}),". Each row in ",r.createElement(o.A,{text:"\\(Q\\)"})," corresponds to a different query (often from a different position in the sequence), and each column in ",r.createElement(o.A,{text:"\\(K^\\top\\)"})," is essentially a single key vector. The result is a matrix of shape ",r.createElement(o.A,{text:"\\(n \\times n\\)"})," when attention is computed over the same set of tokens (self-attention). Each element in this matrix measures how well a particular query aligns with a particular key."),"\n",r.createElement(t.li,null,"Divide by ",r.createElement(o.A,{text:"\\( \\sqrt{d_k} \\)"}),". This is the ",r.createElement(l.A,null,"scaling")," factor controlling for dimension growth in the dot product."),"\n",r.createElement(t.li,null,'Apply softmax. This converts raw similarity scores into a distribution over possible positions in the sequence, or "attention weights".'),"\n",r.createElement(t.li,null,"Multiply by ",r.createElement(o.A,{text:"\\(V\\)"}),". This step aggregates the relevant value vectors, weighed by the attention distribution from the previous step."),"\n"),"\n",r.createElement(t.p,null,"Conceptually, each row in the final matrix ",r.createElement(o.A,{text:"\\( \\mathrm{Attention}(Q, K, V) \\)"}),' is the "attended representation" for a specific query. If we are dealing with self-attention, each query, key, and value set is generated from the same input, so each position in the sequence can attend to all others. This allows the model to capture long-range dependencies far more effectively than a recurrent architecture constrained by sequential time steps.'),"\n",r.createElement(t.h3,{id:"multi-head-attention-interpretation",style:{position:"relative"}},r.createElement(t.a,{href:"#multi-head-attention-interpretation","aria-label":"multi head attention interpretation permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"multi-head attention interpretation"),"\n",r.createElement(t.p,null,"One of the defining breakthroughs of the Transformer architecture is the use of multi-head attention. Instead of computing one set of attention distributions with a single set of linear projections for ",r.createElement(o.A,{text:"\\(Q, K, V\\)"}),', the model uses multiple sets of linear projections, each set known as a "head." For a given attention layer, we might define multiple "heads," each of which learns its own projection matrices ',r.createElement(o.A,{text:"\\(W_{Q_i}, W_{K_i}, W_{V_i}\\)"}),". These heads then perform the scaled dot-product attention in parallel, each focusing on potentially different aspects of the sequence."),"\n",r.createElement(t.p,null,"Formally, for ",r.createElement(o.A,{text:"\\(h\\)"})," attention heads, we have:"),"\n",r.createElement(o.A,{text:"\\[\n\\mathrm{head}_i = \\mathrm{Attention}(Q W_{Q_i}, \\, K W_{K_i}, \\, V W_{V_i})\n\\]"}),"\n",r.createElement(t.p,null,"Then, the results of all heads are concatenated and combined by an output projection ",r.createElement(o.A,{text:"\\(W_O\\)"}),":"),"\n",r.createElement(o.A,{text:"\\[\n\\mathrm{MultiHead}(Q, K, V) = [\\mathrm{head}_1; \\ldots; \\mathrm{head}_h] \\, W_O\n\\]"}),"\n",r.createElement(t.p,null,"The motivation is that different \"heads\" can learn to specialize in different relationships or patterns, enhancing the model's capacity to represent complex dependencies in the data. For instance, one head may learn to attend heavily to preceding tokens that determine the next token's tense, another head may focus on capturing subject-verb agreement, and another could be oriented toward recognizing certain semantic cues. In the context of images (Vision Transformers), one head might learn to focus on edges or shapes, while others might capture more global structure or color correlations."),"\n",r.createElement(t.p,null,'Intuitively, by projecting the input into multiple subspaces, multi-head attention invites the model to see the data from multiple "angles" simultaneously, thereby improving its representational power. This approach has proven highly effective in tasks ranging from language modeling to image recognition to multi-modal tasks where textual and visual data co-occur.'),"\n",r.createElement(t.h3,{id:"computational-complexity-considerations",style:{position:"relative"}},r.createElement(t.a,{href:"#computational-complexity-considerations","aria-label":"computational complexity considerations permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"computational complexity considerations"),"\n",r.createElement(t.p,null,"One of the major departures of Transformer-based attention from recurrent or convolutional approaches is its computational complexity. For a sequence of length ",r.createElement(o.A,{text:"\\(n\\)"}),", self-attention involves computing ",r.createElement(o.A,{text:"\\(QK^\\top\\)"})," which is ",r.createElement(o.A,{text:"\\(n \\times n\\)"})," in shape. The cost of that multiplication is typically on the order of ",r.createElement(o.A,{text:"\\(n^2 \\times d\\)"}),", where ",r.createElement(o.A,{text:"\\(d\\)"})," is the dimensionality of the model's hidden representation. This squared dependence on ",r.createElement(o.A,{text:"\\(n\\)"})," is the primary reason that Transformers can become computationally expensive or memory-intensive for very long sequences. In contrast, recurrent networks run in time ",r.createElement(o.A,{text:"\\(O(n \\times d^2)\\)"}),", and convolutional networks can scale with ",r.createElement(o.A,{text:"\\(n \\times k \\times d\\)"})," where ",r.createElement(o.A,{text:"\\(k\\)"})," is the size of the convolution kernel (though one must keep in mind that capturing wide context might require deeper or larger convolution kernels)."),"\n",r.createElement(t.p,null,"Despite this ",r.createElement(o.A,{text:"\\(O(n^2)\\)"})," complexity, Transformers can still be faster to train than RNNs in practice for moderate sequence lengths, because the self-attention mechanism is highly parallelizable. All the pairwise interactions can be computed in a single or a few matrix multiplications on modern GPUs or TPUs. Meanwhile, recurrent architectures require time-step-by-time-step processing, which is more sequential and less easily parallelized."),"\n",r.createElement(t.p,null,"Nevertheless, for extremely long sequences (e.g., thousands to tens of thousands of tokens), the ",r.createElement(o.A,{text:"\\(O(n^2)\\)"})," complexity can become a bottleneck. This has spurred the development of many alternative attention formulations that attempt to alleviate or reduce this overhead, as I detail in the next subsection."),"\n",r.createElement(t.h3,{id:"alternative-attention-formulations",style:{position:"relative"}},r.createElement(t.a,{href:"#alternative-attention-formulations","aria-label":"alternative attention formulations permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"alternative attention formulations"),"\n",r.createElement(t.p,null,"To address the ",r.createElement(o.A,{text:"\\(O(n^2)\\)"})," cost, researchers have proposed a variety of alternative formulations. Some of these include:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"Sparse attention")," (e.g., the Sparse Transformer from OpenAI and the Longformer from Allen Institute for AI) restricts the attention mechanism to a subset of positions, typically local neighborhoods plus some form of global attention tokens or special patterns that allow for some cross-sequence interaction. By doing so, the complexity might be reduced to ",r.createElement(o.A,{text:"\\(O(n \\log n)\\)"})," or ",r.createElement(o.A,{text:"\\(O(n)\\)"}),", depending on the sparsity pattern chosen."),"\n"),"\n",r.createElement(t.li,null,"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"Linear attention")," refers to methods like the Performer or the Linformer, where the softmax operation is approximated or re-formulated so that attention computations can be done in ",r.createElement(o.A,{text:"\\(O(n)\\)"})," or ",r.createElement(o.A,{text:"\\(O(n \\times d)\\)"})," time. Often, these rely on kernel approximations of the softmax function or factorization strategies."),"\n"),"\n",r.createElement(t.li,null,"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"Memory-efficient attention")," includes approaches that carefully reorder the computation to avoid storing large intermediate tensors, thus reducing memory usage significantly. For instance, PyTorch's ",r.createElement(l.A,null,"torch.nn.functional.scaled_dot_product_attention")," has a ",r.createElement(l.A,null,"attn_dropout")," feature that helps reduce memory overhead, and additional research has explored explicit re-chunking of the computations to trade extra computation for lower memory usage."),"\n"),"\n",r.createElement(t.li,null,"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"NystrÃ¶m-based methods")," or ",r.createElement(t.strong,null,"rFA")," (random feature attention) reduce computational needs by approximating the attention matrix. These methods rely on projecting the key and query spaces into a lower-dimensional space, thus accelerating the multiplication."),"\n"),"\n"),"\n",r.createElement(t.p,null,"While these techniques hold promise and have proven success in tasks requiring extremely long context (such as analyzing entire books, large images, or long speech segments), they also introduce additional complexities in implementation and sometimes require specialized hardware or additional hyperparameter tuning. Nonetheless, they represent critical directions for attention-based research, especially as models grow in capacity and data volumes continue to skyrocket."),"\n",r.createElement(t.p,null,r.createElement(l.A,null,"In practice"),", the classic Transformer architecture with full ",r.createElement(o.A,{text:"\\(O(n^2)\\)"})," attention is still the mainstay for a wide variety of tasks, especially if the sequences are of moderate length (e.g., up to a few thousand tokens). For extremely large sequences, advanced forms of sparse or linear attention can be a game-changer."),"\n",r.createElement(t.hr),"\n",r.createElement(t.h2,{id:"2-training-and-optimization",style:{position:"relative"}},r.createElement(t.a,{href:"#2-training-and-optimization","aria-label":"2 training and optimization permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2. training and optimization"),"\n",r.createElement(t.h3,{id:"common-loss-functions",style:{position:"relative"}},r.createElement(t.a,{href:"#common-loss-functions","aria-label":"common loss functions permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"common loss functions"),"\n",r.createElement(t.p,null,"Training a Transformer generally involves selecting an appropriate loss function, typically:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Cross-entropy loss"),": For many language modeling tasks (e.g., next-token prediction, machine translation), cross-entropy is the gold standard. The loss is often calculated for every predicted token relative to the ground-truth token."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Label smoothing"),": Rather than using a one-hot target distribution, a smoothed label distribution (e.g., 0.9 for the correct class, 0.1 distributed among the incorrect ones) can help prevent overconfidence and may improve generalization. This approach is widely used in training large-scale Transformers."),"\n"),"\n",r.createElement(t.p,null,"When training Transformers for classification tasks, cross-entropy typically remains the most common choice. For sequence-to-sequence tasks such as translation, the model is often trained by feeding in the ground truth tokens in a teacher-forced manner (though there are also advanced strategies like reinforcement learning or scheduled sampling for bridging the gap between training and inference). In open-ended generation tasks, the standard approach is to minimize the negative log-likelihood of the next token."),"\n",r.createElement(t.h3,{id:"initialization-strategies",style:{position:"relative"}},r.createElement(t.a,{href:"#initialization-strategies","aria-label":"initialization strategies permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"initialization strategies"),"\n",r.createElement(t.p,null,"Weight initialization plays a significant role in stabilizing deep neural network training. For Transformers, the dimension of hidden representations and the multi-head attention structure can be quite large, so robust initialization is crucial. Common techniques include:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Xavier (Glorot) initialization"),": Often used when combining linear layers with activation functions like ReLU or tanh. It sets the variance of each layer's outputs to be roughly constant, preventing exploding or vanishing gradients."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"He initialization"),": Tailored to ReLU-like activations, ensuring variance is preserved through deeper networks."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Specialized initializations"),": Some Transformer frameworks tweak gains or incorporate scaling factors that reflect the presence of multi-head attention. For example, a smaller standard deviation might be used for attention projection matrices to keep the dot products stable initially."),"\n"),"\n",r.createElement(t.p,null,"It is also common to see the final layer normalization or specific embeddings scaled by a factor like ",r.createElement(o.A,{text:"\\(\\sqrt{d_\\text{model}}\\)"})," in the early phases of training. Given the prevalence of layer normalization, these details can be crucial in preventing instabilities in deeper layers."),"\n",r.createElement(t.h3,{id:"optimizers",style:{position:"relative"}},r.createElement(t.a,{href:"#optimizers","aria-label":"optimizers permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"optimizers"),"\n",r.createElement(t.p,null,"While plain stochastic gradient descent (SGD) can train Transformers, modern practice generally favors adaptive algorithms. ",r.createElement(t.strong,null,"Adam")," and ",r.createElement(t.strong,null,"AdamW")," (Adam with weight decay decoupled) are extremely widespread because they adapt the learning rate per-parameter, which can accelerate convergence for large, sparse gradients commonly encountered in NLP tasks."),"\n",r.createElement(t.p,null,"A unique hallmark of Transformer training is the ",r.createElement(t.strong,null,"learning rate warmup"),' strategy, as introduced by Vaswani and gang (NeurIPS 2017). The idea is to start with a relatively small learning rate, gradually increase ("warm up") over the initial training steps, and then switch to a decay schedule, often an inverse square-root schedule. This approach stabilizes training in the early iterations (when weights are near random initialization) and has become a standard convention:'),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Warmup steps"),": For a certain number of updates, the learning rate increases linearly."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Decay"),": After warmup, the learning rate might follow ",r.createElement(o.A,{text:"\\( \\text{lr} \\propto (d_\\text{model})^{-0.5} \\times \\min(\\text{step}^{-0.5}, \\text{step} \\times \\text{warmup\\_steps}^{-1.5}) \\)"})," in the original Transformer schedule, or a cosine decay or other popular schedules."),"\n"),"\n",r.createElement(t.h3,{id:"regularization-techniques",style:{position:"relative"}},r.createElement(t.a,{href:"#regularization-techniques","aria-label":"regularization techniques permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"regularization techniques"),"\n",r.createElement(t.p,null,"Since Transformers often have a large number of parameters, regularization is essential to mitigate overfitting. Common techniques include:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Dropout"),": Applied in multiple places â€” within the attention weight computation (i.e., dropout on the softmax matrix), within the feed-forward layers, and even to residual connections. By randomly zeroing out elements in the hidden layers, dropout helps prevent co-adaptation of neurons."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Label smoothing"),": Already mentioned as a type of regularization that can encourage better calibration."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Weight decay"),": Regulates the magnitude of weight vectors, effectively penalizing large weight values. This is typically combined with Adam, forming the AdamW variant."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Stochastic depth")," or ",r.createElement(t.strong,null,"layer dropping"),": In certain large-scale Transformer variants, some fraction of layers are randomly bypassed during training, akin to a deeper version of dropout."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Data augmentation"),': In NLP, "augmentation" might involve back-translation, random token masking, or synonyms injection. In vision tasks, transformations of images can play a similar role.'),"\n"),"\n",r.createElement(t.h3,{id:"hyperparameter-tuning",style:{position:"relative"}},r.createElement(t.a,{href:"#hyperparameter-tuning","aria-label":"hyperparameter tuning permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"hyperparameter tuning"),"\n",r.createElement(t.p,null,"Getting hyperparameters right can make or break the performance of Transformers. Key hyperparameters include:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Number of layers")," (depth of the encoder and/or decoder). Common values range from 6 to 12 in many models, but cutting-edge large-scale Transformers like GPT-3 or PaLM can go into hundreds of layers."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Hidden dimension")," (",r.createElement(o.A,{text:"\\(d_\\text{model}\\)"}),") and ",r.createElement(t.strong,null,"feed-forward dimension")," (",r.createElement(o.A,{text:"\\(d_\\text{ff}\\)"}),"). These often scale with the model size. A typical ratio might be ",r.createElement(o.A,{text:"\\(d_\\text{ff} = 4 \\times d_\\text{model}\\)"}),", but some variations exist."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Number of attention heads"),". Typically a divisor of ",r.createElement(o.A,{text:"\\(d_\\text{model}\\)"}),". More heads can capture more patterns, but at a higher computational cost."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Dropout rates"),". Common ranges are between 0.1 and 0.3, although these can vary depending on dataset size."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Learning rate"),". Often in the range of ",r.createElement(o.A,{text:"\\(1 \\times 10^{-4}\\)"})," to ",r.createElement(o.A,{text:"\\(5 \\times 10^{-4}\\)"})," for large models, with appropriate warmup steps."),"\n"),"\n",r.createElement(t.p,null,"Empirical tuning usually involves holding some hyperparameters constant (e.g., number of layers) and performing a grid or random search over others (e.g., learning rate, batch size, dropout). In extremely large-scale settings, more sophisticated hyperparameter search methods can become essential to save on computational costs."),"\n",r.createElement(t.h3,{id:"batch-size-and-gradient-accumulation",style:{position:"relative"}},r.createElement(t.a,{href:"#batch-size-and-gradient-accumulation","aria-label":"batch size and gradient accumulation permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"batch size and gradient accumulation"),"\n",r.createElement(t.p,null,"Training Transformers on large datasets typically requires high GPU memory capacities due to the large model size and the ",r.createElement(o.A,{text:"\\(n^2\\)"})," memory usage of attention. ",r.createElement(t.strong,null,"Batch size")," is a critical factor because training with bigger batch sizes can stabilize the gradient estimate and accelerate training. However, hardware constraints often limit how large a mini-batch can be in a single forward/backward pass."),"\n",r.createElement(t.p,null,"A common solution is ",r.createElement(t.strong,null,"gradient accumulation"),": the model processes several mini-batches sequentially, accumulating gradients in memory without updating parameters, and only after a certain number of mini-batches do we perform an optimizer step. This effectively simulates a larger batch size while working around hardware memory limits."),"\n",r.createElement(t.hr),"\n",r.createElement(t.h2,{id:"3-popular-transformer-variants",style:{position:"relative"}},r.createElement(t.a,{href:"#3-popular-transformer-variants","aria-label":"3 popular transformer variants permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3. popular transformer variants"),"\n",r.createElement(t.h3,{id:"bert-bidirectional-encoder-representations-from-transformers",style:{position:"relative"}},r.createElement(t.a,{href:"#bert-bidirectional-encoder-representations-from-transformers","aria-label":"bert bidirectional encoder representations from transformers permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"bert (bidirectional encoder representations from transformers)"),"\n",r.createElement(t.p,null,"BERT (Smith and gang, NAACL 2019, following the original Devlin and gang 2018 preprint) is a landmark Transformer variant that uses only the encoder portion of the original encoder-decoder structure. Its principal innovation is ",r.createElement(t.strong,null,"masked language modeling (MLM)"),", wherein random tokens in the input are replaced with a special ",r.createElement(l.A,null,"[MASK]")," symbol, and the model is trained to predict the original tokens. This allows BERT to learn bidirectional context representations â€” each token is trained to attend to tokens on both the left and the right, thereby capturing deeper contextual relationships compared to unidirectional language models."),"\n",r.createElement(t.p,null,"BERT also introduced ",r.createElement(t.strong,null,"next sentence prediction")," (NSP), a task where the model is given two sentences and must predict whether the second sentence is likely to follow the first in a coherent text. This signals an additional notion of inter-sentence coherence. However, some subsequent work has found NSP may not be strictly necessary; alternative tasks can similarly help the model learn robust sentence-level representations."),"\n",r.createElement(t.p,null,"Since its introduction, BERT has led to transformations across the entire NLP landscape. Fine-tuning a pre-trained BERT can yield high performance on tasks like question answering, sentiment classification, named entity recognition, and more. Dozens of variants have sprung up, such as RoBERTa, DistilBERT, ALBERT, and so forth, each making modifications to training data, training steps, or architecture to push performance or efficiency."),"\n",r.createElement(t.h3,{id:"gpt-series-generative-pre-trained-transformers",style:{position:"relative"}},r.createElement(t.a,{href:"#gpt-series-generative-pre-trained-transformers","aria-label":"gpt series generative pre trained transformers permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"gpt series (generative pre-trained transformers)"),"\n",r.createElement(t.p,null,"While BERT is an encoder-only model, GPT is a ",r.createElement(t.strong,null,"decoder-only")," model. It generates tokens autoregressively, always looking at the previously generated tokens (or the input prompt) to predict the next token. This forward-only attention approach is simpler in structure (no encoder-decoder cross-attention) but extremely effective for tasks where generation is key, such as chatbots, story writing, code generation, and more."),"\n",r.createElement(t.p,null,"The GPT series soared in popularity thanks to GPT-2's impressive text generation abilities and GPT-3's massive scale (175 billion parameters). The success behind these models relies heavily on:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Autoregressive language modeling"),": Training to predict ",r.createElement(o.A,{text:"\\(p(x_t | x_1, x_2, ..., x_{t-1})\\)"})," fosters strong generative capabilities."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Scaling laws"),": Empirical evidence suggests that performance improves with more parameters, more training data, and more compute."),"\n"),"\n",r.createElement(t.p,null,"GPT-based models have also demonstrated strong ",r.createElement(t.strong,null,"zero-shot")," and ",r.createElement(t.strong,null,"few-shot")," learning capabilities â€” by simply prompting them with a small number of examples, they can generalize to tasks they were never explicitly trained on. This phenomenon has fueled the rise of prompt engineering, as we carefully craft input prompts to elicit specific behaviors."),"\n",r.createElement(t.h3,{id:"t5-text-to-text-transfer-transformer",style:{position:"relative"}},r.createElement(t.a,{href:"#t5-text-to-text-transfer-transformer","aria-label":"t5 text to text transfer transformer permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"t5 (text-to-text transfer transformer)"),"\n",r.createElement(t.p,null,'T5, introduced by Google Research (Raffel and gang), uses an encoder-decoder Transformer architecture but standardizes all tasks (classification, translation, summarization, etc.) into a text-to-text paradigm. Under T5, everything becomes "feed the text in, get the text out," making it extremely general for a wide range of NLP tasks.'),"\n",r.createElement(t.p,null,"Two hallmark strategies in T5 are:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,'Pre-training on a large "fill-in-the-blank" style objective'),", similar to MLM, but with flexible masking strategies that allow entire spans of text to be masked out."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,'Task-specific "prefixes"'),' that instruct the model to behave in certain ways (e.g., "translate English to German: ...").'),"\n"),"\n",r.createElement(t.p,null,"T5 also emphasizes how the choice of pre-training tasks and data (termed ",r.createElement(t.strong,null,'"Colossal Clean Crawled Corpus"'),") can significantly impact the final performance across benchmarks like GLUE and SuperGLUE."),"\n",r.createElement(t.h3,{id:"vision-transformer-vit",style:{position:"relative"}},r.createElement(t.a,{href:"#vision-transformer-vit","aria-label":"vision transformer vit permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"vision transformer (vit)"),"\n",r.createElement(t.p,null,'Moving from text to images, the Vision Transformer (ViT) (Dosovitskiy and gang, ICLR 2021) showed that purely attention-based architectures can compete (and sometimes surpass) convolutional neural networks (CNNs) on large-scale image classification tasks. ViT divides the input image into patches (for instance, 16x16 pixel patches), flattens them, and then treats each patch as a "token," analogous to words in a sentence.'),"\n",r.createElement(t.p,null,"After a learned embedding for each patch, plus a position embedding that encodes patch location, the standard Transformer encoder layers compute self-attention among all patches. This approach dispenses with local receptive fields and weight sharing inherent in CNNs, relying purely on attention to capture image structure. ViT typically requires large datasets (like JFT-300M) to reach its full potential, highlighting that the success of attention-based modeling in vision also benefits from abundant data and compute."),"\n",r.createElement(t.h3,{id:"recent-trends-and-future-expansions",style:{position:"relative"}},r.createElement(t.a,{href:"#recent-trends-and-future-expansions","aria-label":"recent trends and future expansions permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"recent trends and future expansions"),"\n",r.createElement(t.p,null,"Transformers have undergone unceasing innovation:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Sparse mixtures of experts"),': Models like GLaM (Google) or Switch Transformers route tokens to specialized "experts," allowing the model to scale parameter count drastically while only activating a subset of parameters for each token.'),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Multimodal Transformers"),": Combining textual, visual, and even auditory data within a single model, sometimes using cross-modal attention to facilitate interactions between streams of data."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Long-sequence Transformers"),": As discussed earlier, with hardware improvements and new architectures, more attention-based models can handle entire lengthy documents, videos, or large images."),"\n"),"\n",r.createElement(t.p,null,"Additionally, there has been a trend toward better efficiency (via quantization, pruning, or distillation) and better interpretability, where attention maps and specialized tokens can provide glimpses into the model's internal reasoning."),"\n",r.createElement(t.h3,{id:"distillation-and-compression-techniques",style:{position:"relative"}},r.createElement(t.a,{href:"#distillation-and-compression-techniques","aria-label":"distillation and compression techniques permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"distillation and compression techniques"),"\n",r.createElement(t.p,null,"Large pre-trained Transformers often have billions of parameters, making them challenging to deploy on resource-constrained devices or under tight latency requirements. Model ",r.createElement(t.strong,null,"distillation"),' is one popular approach: train a smaller "student" model to mimic the logits or hidden states of a larger "teacher" model. In practice, distillation can preserve a significant fraction of the teacher model\'s performance, but with greatly reduced memory footprints and inference times.'),"\n",r.createElement(t.p,null,"Other forms of compression include:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Pruning"),": Eliminating weights (unstructured pruning) or entire attention heads/layers (structured pruning)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Quantization"),": Using lower-precision numeric formats (e.g., int8 or float16) to store and compute weights, which can drastically reduce memory usage and speed up inference on specialized hardware."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Low-rank factorization"),": Decomposing large weight matrices into products of smaller matrices."),"\n"),"\n",r.createElement(t.p,null,"These techniques are increasingly relevant as Transformers permeate real-time applications like mobile assistants, embedded systems, or large-scale cloud APIs that must optimize cost and environmental footprint."),"\n",r.createElement(t.hr),"\n",r.createElement(t.h2,{id:"4-implementation-details",style:{position:"relative"}},r.createElement(t.a,{href:"#4-implementation-details","aria-label":"4 implementation details permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4. implementation details"),"\n",r.createElement(t.h3,{id:"frameworks-and-libraries",style:{position:"relative"}},r.createElement(t.a,{href:"#frameworks-and-libraries","aria-label":"frameworks and libraries permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"frameworks and libraries"),"\n",r.createElement(t.p,null,"Many deep learning frameworks provide built-in tools for implementing Transformers:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"PyTorch")," offers a ",r.createElement(l.A,null,"torch.nn.Transformer")," module, which comes with multi-head attention, positional encoding, and a configurable encoder-decoder stack."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"TensorFlow")," and ",r.createElement(t.strong,null,"Keras")," have the ",r.createElement(l.A,null,"tf.keras.layers.MultiHeadAttention")," layer and other utility classes to construct custom Transformers or adopt standard building blocks."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"JAX/Flax")," from Google also provides a powerful environment for writing high-performance Transformer models with efficient parallelization and ",r.createElement(l.A,null,"pjit")," or ",r.createElement(l.A,null,"pmap")," for large-scale training."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Hugging Face Transformers")," library has become a go-to resource in the NLP community, offering pre-trained models (BERT, GPT-2, T5, etc.) and an easy interface to fine-tune them."),"\n"),"\n",r.createElement(t.p,null,"Leveraging these well-tested libraries can save considerable development time, as they handle numerous details: from positional embeddings to training loops to model serialization."),"\n",r.createElement(t.h3,{id:"pseudocode-for-a-basic-transformer",style:{position:"relative"}},r.createElement(t.a,{href:"#pseudocode-for-a-basic-transformer","aria-label":"pseudocode for a basic transformer permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"pseudocode for a basic transformer"),"\n",r.createElement(t.p,null,"Below is a high-level pseudocode structure for a basic Transformer, focusing on the forward pass for an encoder-decoder (in extremely simplified terms):"),"\n",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">&lt;Code text={`\n# Pseudocode for a basic Transformer forward pass:\n\ndef transformer_forward(src_tokens, tgt_tokens, src_mask, tgt_mask, model_params):\n    # 1. Embed the source tokens + add positional encoding\n    src_embedded = embed(src_tokens, model_params.src_embedding)\n    src_embedded = add_positional_encoding(src_embedded, model_params.positional_enc)\n\n    # 2. Pass through each encoder layer\n    encoder_output = src_embedded\n    for layer in model_params.encoder_layers:\n        encoder_output = encoder_layer_forward(encoder_output, src_mask, layer)\n    \n    # 3. Embed the target tokens + add positional encoding\n    tgt_embedded = embed(tgt_tokens, model_params.tgt_embedding)\n    tgt_embedded = add_positional_encoding(tgt_embedded, model_params.positional_enc)\n\n    # 4. Pass through each decoder layer\n    decoder_output = tgt_embedded\n    for layer in model_params.decoder_layers:\n        decoder_output = decoder_layer_forward(decoder_output, encoder_output, \n                                               tgt_mask, src_mask, layer)\n\n    # 5. Final linear + softmax for next-token prediction\n    logits = linear_layer(decoder_output, model_params.final_linear)\n    return logits\n`}/></code></pre></div>'}}),"\n",r.createElement(t.p,null,"Within each encoder layer, one finds:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,"Multi-head self-attention: The query, key, and value come from the same source (the encoder hidden states)."),"\n",r.createElement(t.li,null,"Feed-forward layer: Typically two linear layers with an activation (like ReLU) in between."),"\n",r.createElement(t.li,null,"Add & Norm: Each sub-block is followed by a residual connection and layer normalization."),"\n"),"\n",r.createElement(t.p,null,"The decoder layer is similar but includes an additional cross-attention sub-block that queries the encoder output while keys and values come from the encoder output."),"\n",r.createElement(t.h3,{id:"common-pitfalls-and-debugging-tips",style:{position:"relative"}},r.createElement(t.a,{href:"#common-pitfalls-and-debugging-tips","aria-label":"common pitfalls and debugging tips permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"common pitfalls and debugging tips"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Layer normalization placement"),": Transformers typically place layer normalization either before or after the sub-block (pre-norm vs. post-norm). Mismatched usage or forgetting to scale certain layers can lead to subpar results or training instability."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Learning rate scheduling"),": Not using a warmup schedule or using an inappropriate schedule can stall training or cause divergence."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Masking"),': Particularly in the decoder, forgetting to apply causal masks that disallow attention to future tokens can lead to impossible "future leaks" during training. Also, ignoring padding masks for variable-length sequences can pollute attention calculations.'),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Dimension mismatch"),": Because multi-head attention splits the hidden dimension across heads, ensuring shapes line up exactly is crucial. A single transposition error can break the entire pipeline."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Gradient explosion"),": Transformers, like other deep networks, can sometimes experience large gradient spikes. Gradient clipping or careful initialization can mitigate this."),"\n"),"\n",r.createElement(t.h3,{id:"example-code-snippets",style:{position:"relative"}},r.createElement(t.a,{href:"#example-code-snippets","aria-label":"example code snippets permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"example code snippets"),"\n",r.createElement(t.p,null,"Below is a minimal code snippet in PyTorch demonstrating the usage of the built-in Multi-head Attention layer:"),"\n",r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">&lt;Code text={`\nimport torch\nimport torch.nn as nn\n\nclass SimpleSelfAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        self.mha = nn.MultiheadAttention(embed_dim, num_heads)\n        \n    def forward(self, x, mask=None):\n        # x shape: (sequence_length, batch_size, embed_dim)\n        # PyTorch MultiheadAttention expects (sequence_length, batch_size, embed_dim)\n        # We use x for Q, K, and V in self-attention\n        attn_output, attn_weights = self.mha(x, x, x, attn_mask=mask)\n        return attn_output, attn_weights\n`}/></code></pre></div>'}}),"\n",r.createElement(t.p,null,"Here, ",r.createElement(l.A,null,"attn_mask")," could be used to enforce causal masking in a decoder by setting positions that should not be attended to ",r.createElement(o.A,{text:"\\( -\\infty \\)"}),". Note that for typical training, we also have feed-forward layers and normalization steps, which I've omitted for clarity."),"\n",r.createElement(t.h3,{id:"efficient-training-techniques",style:{position:"relative"}},r.createElement(t.a,{href:"#efficient-training-techniques","aria-label":"efficient training techniques permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"efficient training techniques"),"\n",r.createElement(t.p,null,"Transformers can be resource-hungry, so several techniques can reduce costs or speed up convergence:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Mixed precision (FP16/BF16)"),": Reduces memory usage and can significantly improve throughput on modern GPUs or TPUs supporting half-precision. Most frameworks have an automatic mixed precision (AMP) feature."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Gradient checkpointing"),": Trades compute for memory by recalculating forward passes during backpropagation instead of storing all intermediate activations."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Distributed training"),": Multiple GPUs or multiple nodes can split the data or model parameters. In large-scale setups, a combination of ",r.createElement(t.strong,null,"data parallelism")," (splitting batches) and ",r.createElement(t.strong,null,"model parallelism")," (splitting layers or even attention heads across devices) is common."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Dynamic sequence batching"),": Group sequences of similar lengths together to reduce wasted compute on padding tokens."),"\n"),"\n",r.createElement(t.hr),"\n",r.createElement(t.h2,{id:"5-real-world-applications",style:{position:"relative"}},r.createElement(t.a,{href:"#5-real-world-applications","aria-label":"5 real world applications permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5. real-world applications"),"\n",r.createElement(t.h3,{id:"machine-translation",style:{position:"relative"}},r.createElement(t.a,{href:"#machine-translation","aria-label":"machine translation permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"machine translation"),"\n",r.createElement(t.p,null,'Machine translation was the seminal application for which Transformers were developed (the original "Attention Is All You Need" paper). The encoder-decoder structure is particularly well-suited for sequence-to-sequence tasks:'),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,"The encoder reads the source sentence (e.g., in French)."),"\n",r.createElement(t.li,null,"The decoder generates the target sentence (e.g., in English), one token at a time, attending both to previously generated tokens and to the encoder outputs."),"\n"),"\n",r.createElement(t.p,null,"Transformers have significantly pushed forward state-of-the-art results in translation quality (often measured by BLEU score), surpassing or matching recurrent-based models in performance, while also enabling more efficient parallelizable training. Many modern production systems (such as Google Translate) rely on Transformer-based architectures."),"\n",r.createElement(t.h3,{id:"text-summarization",style:{position:"relative"}},r.createElement(t.a,{href:"#text-summarization","aria-label":"text summarization permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"text summarization"),"\n",r.createElement(t.p,null,"Summarizing documents succinctly while preserving key information has been transformed by attention-based architectures. During summarization, the Transformer can attend to the relevant sections of the input text to produce a coherent summary. There are two main approaches:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Extractive"),": Identify the most important sentences or paragraphs from the input. Transformers can be fine-tuned to rank sentences by importance."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Abstractive"),": Generate a new sequence that captures the main points. This is more challenging but allows the model to paraphrase and reorganize information."),"\n"),"\n",r.createElement(t.p,null,"Rouge metrics (ROUGE-1, ROUGE-2, ROUGE-L) are commonly used to evaluate summarization quality. Large pre-trained language models (like T5) often include summarization as a canonical demonstration of their text-to-text approach."),"\n",r.createElement(t.h3,{id:"sentiment-analysis-and-chatbots",style:{position:"relative"}},r.createElement(t.a,{href:"#sentiment-analysis-and-chatbots","aria-label":"sentiment analysis and chatbots permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"sentiment analysis and chatbots"),"\n",r.createElement(t.p,null,"For classification tasks like sentiment analysis, one can fine-tune a pre-trained Transformer model (BERT, for example) on a labeled dataset of text with sentiment categories (positive, negative, neutral, etc.). By leveraging pre-trained representations, the model typically requires far fewer labeled examples to reach high accuracy compared to training from scratch."),"\n",r.createElement(t.p,null,'In chatbots, especially with GPT-based architectures, attention-based decoding can handle multi-turn dialogues, referencing context from earlier parts of the conversation to craft responses that remain on-topic. The attention mechanism ensures that the model can "remember" relevant details from the user\'s conversation history, improving user experience in an interactive setting.'),"\n",r.createElement(t.h3,{id:"other-nlp-tasks",style:{position:"relative"}},r.createElement(t.a,{href:"#other-nlp-tasks","aria-label":"other nlp tasks permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"other nlp tasks"),"\n",r.createElement(t.p,null,"Transformers show up in nearly every modern NLP task:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Question answering"),": BERT or GPT variants can ingest a passage and a question, attending to relevant parts of the text to produce an answer span or a short textual response."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Named entity recognition"),": The model labels tokens or spans with entities (e.g., persons, locations), harnessing the context from the entire sequence."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Information retrieval"),": Models like ColBERT, SPLADE, or dense passage retrievers use Transformers to map queries and documents into embedding spaces for fast similarity search."),"\n"),"\n",r.createElement(t.p,null,"These tasks often exploit pretrained Transformer weights and then adapt them with a small classification head or a specialized output layer for the task at hand."),"\n",r.createElement(t.h3,{id:"use-cases-in-computer-vision-and-beyond",style:{position:"relative"}},r.createElement(t.a,{href:"#use-cases-in-computer-vision-and-beyond","aria-label":"use cases in computer vision and beyond permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"use cases in computer vision and beyond"),"\n",r.createElement(t.p,null,"Beyond the Vision Transformer for classification, Transformers can also appear in:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Object detection"),": DETR (Facebook AI) replaces traditional CNN backbones or region proposal networks with a Transformer that directly attends to image features, generating bounding boxes and class labels in a single pass."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Speech processing"),": Transformers can handle speech recognition or speech synthesis by working on spectrogram patches, akin to how ViT processes image patches."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Multimodal tasks"),": Combining image patches with word embeddings in a single Transformer-based architecture for tasks like image captioning (e.g., the CLIP model from OpenAI or the Flamingo model from DeepMind)."),"\n"),"\n",r.createElement(t.p,null,"Given the model's strong ability to fuse information from multiple modalities, Transformers continue to push the boundaries on tasks like visual question answering or video understanding, where attention can integrate signals from text, images, and sometimes audio."),"\n",r.createElement(t.hr),"\n",r.createElement(t.h2,{id:"6-best-practices",style:{position:"relative"}},r.createElement(t.a,{href:"#6-best-practices","aria-label":"6 best practices permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6. best practices"),"\n",r.createElement(t.h3,{id:"data-preprocessing-and-tokenization",style:{position:"relative"}},r.createElement(t.a,{href:"#data-preprocessing-and-tokenization","aria-label":"data preprocessing and tokenization permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"data preprocessing and tokenization"),"\n",r.createElement(t.p,null,"Before feeding data into a Transformer, it's crucial to:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Tokenize")," the input text. In NLP, subword tokenization (Byte-Pair Encoding, WordPiece, or SentencePiece) is popular to handle out-of-vocabulary words systematically."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Build or reuse a consistent vocabulary"),". Mismatched vocabularies can severely degrade performance."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Handle special tokens"),": [PAD], [CLS], [SEP], [MASK], etc. BERT-like models rely heavily on these tokens to delineate sequences or perform classification."),"\n"),"\n",r.createElement(t.p,null,"For Vision Transformers, images are usually resized, normalized, then split into patches. Consistent pre-processing across the training and inference phases is essential to avoid input distribution shifts."),"\n",r.createElement(t.h3,{id:"hardware-considerations-and-scaling",style:{position:"relative"}},r.createElement(t.a,{href:"#hardware-considerations-and-scaling","aria-label":"hardware considerations and scaling permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"hardware considerations and scaling"),"\n",r.createElement(t.p,null,"Transformers can be memory-intensive, especially if the sequence length and batch size are large. A few hardware considerations:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"GPUs")," (NVIDIA, AMD) or ",r.createElement(t.strong,null,"TPUs")," (Google Cloud) are generally necessary for any serious Transformer training."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Multi-GPU setups")," let you split large batches across devices, or distribute parts of the model across GPUs with model parallelism."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"CPU inference")," can still be viable with smaller distilled or pruned models, especially for simpler tasks."),"\n"),"\n",r.createElement(t.p,null,"When scaling up, specialized software libraries (e.g., DeepSpeed, Megatron-LM) manage parallelization, memory optimization, and partitioning large models across multiple nodes."),"\n",r.createElement(t.h3,{id:"monitoring-training-metrics",style:{position:"relative"}},r.createElement(t.a,{href:"#monitoring-training-metrics","aria-label":"monitoring training metrics permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"monitoring training metrics"),"\n",r.createElement(t.p,null,"In NLP tasks, besides the training loss, relevant metrics might include:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Validation perplexity"),": Common in language modeling, measuring how well the model predicts unseen text."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Accuracy"),": In classification tasks."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"BLEU"),": For machine translation."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"ROUGE"),": For summarization tasks."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"F1, precision, recall"),": In tasks like named entity recognition or question answering."),"\n"),"\n",r.createElement(t.p,null,"Regularly logging these metrics ensures that you catch potential regressions or overfitting. Early stopping or checkpoint selection can be based on these validation metrics."),"\n",r.createElement(t.h3,{id:"model-deployment-strategies",style:{position:"relative"}},r.createElement(t.a,{href:"#model-deployment-strategies","aria-label":"model deployment strategies permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"model deployment strategies"),"\n",r.createElement(t.p,null,"Once a Transformer is trained, deploying it to production involves:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Serializing")," or exporting the model weights in a standard format (e.g., ONNX, TorchScript)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Serving")," using a high-performance server solution (e.g., TorchServe, TensorFlow Serving, or custom GPU-serving frameworks)."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"On-device inference")," might require additional compression, pruning, or quantization to fit memory and latency constraints of edge devices."),"\n"),"\n",r.createElement(t.p,null,"For large-scale web services, containerization (Docker), orchestration (Kubernetes), and load balancing are standard tools. Typically, real-world applications also require ",r.createElement(t.strong,null,"monitoring")," the model's performance post-deployment, verifying that it behaves consistently when faced with shifting data distributions or malicious inputs."),"\n",r.createElement(t.h3,{id:"security-and-bias-considerations",style:{position:"relative"}},r.createElement(t.a,{href:"#security-and-bias-considerations","aria-label":"security and bias considerations permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"security and bias considerations"),"\n",r.createElement(t.p,null,"Finally, as Transformers become widely deployed, it's essential to acknowledge potential pitfalls:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Data biases"),': Large pre-trained models can inherit biases present in their training data, leading to harmful or unfair outcomes. Careful curation, filtering, or post-training "debiasing" techniques are important.'),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Adversarial inputs"),": Malicious inputs (e.g., prompting the model in ways that lead to misinformation) can be problematic, particularly in open-ended generative models."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Privacy"),": Some tasks may require data governance to ensure that sensitive personal information does not leak."),"\n"),"\n",r.createElement(t.p,null,"Ongoing research dives into interpretability, fairness, and robust adversarial training for Transformers, seeking to mitigate ethical and safety concerns."),"\n",r.createElement(t.hr),"\n",r.createElement(n,{alt:"transformer_attention",path:"",caption:"A schematic view of multi-head attention within a Transformer layer.",zoom:"false"}),"\n",r.createElement(t.p,null,"This concludes our second part of the Transformer architecture discussion, focusing on deeper dives into attention mechanisms, training details, popular variants, implementation tricks, real-world applications, and best practices. The Transformer has unleashed a tidal wave of innovation in machine learning, and I anticipate further breakthroughs as researchers tackle new frontiers in efficiency, interpretability, and multimodality."))}var c=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,a.RP)(),e.components);return t?r.createElement(t,e,r.createElement(s,e)):s(e)};var m=n(54506),d=n(88864),h=n(58481),u=n.n(h),p=n(5984),g=n(43672),f=n(27042),v=n(72031),y=n(81817),E=n(27105),b=n(17265),w=n(2043),x=n(95751),k=n(94328),T=n(80791),S=n(78137);const z=e=>{let{toc:t}=e;if(!t||!t.items)return null;return r.createElement("nav",{className:T.R},r.createElement("ul",null,t.items.map(((e,t)=>r.createElement("li",{key:t},r.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const n=t.replace("#",""),a=document.getElementById(n);a&&a.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&r.createElement(z,{toc:{items:e.items}}))))))};function _(e){let{data:{mdx:t,allMdx:i,allPostImages:l},children:o}=e;const{frontmatter:s,body:c,tableOfContents:d}=t,h=s.index,v=s.slug.split("/")[1],T=i.nodes.filter((e=>e.frontmatter.slug.includes(`/${v}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),_=T.findIndex((e=>e.frontmatter.index===h)),M=T[_+1],L=T[_-1],H=s.slug.replace(/\/$/,""),C=/[^/]*$/.exec(H)[0],A=`posts/${v}/content/${C}/`,{0:N,1:I}=(0,r.useState)(s.flagWideLayoutByDefault),{0:V,1:q}=(0,r.useState)(!1);var P;(0,r.useEffect)((()=>{q(!0);const e=setTimeout((()=>q(!1)),340);return()=>clearTimeout(e)}),[N]),"adventures"===v?P=b.cb:"research"===v?P=b.Qh:"thoughts"===v&&(P=b.T6);const j=u()(c).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,B=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),n=e%60;return n<=30?`~${t}${n>0?".5":""} h`:`~${t+1} h`}(Math.ceil(j/P)+(s.extraReadTimeMin||0)),D=[{flag:s.flagDraft,component:()=>Promise.all([n.e(5850),n.e(9833)]).then(n.bind(n,49833))},{flag:s.flagMindfuckery,component:()=>Promise.all([n.e(5850),n.e(7805)]).then(n.bind(n,27805))},{flag:s.flagRewrite,component:()=>Promise.all([n.e(5850),n.e(8916)]).then(n.bind(n,78916))},{flag:s.flagOffensive,component:()=>Promise.all([n.e(5850),n.e(6731)]).then(n.bind(n,49112))},{flag:s.flagProfane,component:()=>Promise.all([n.e(5850),n.e(3336)]).then(n.bind(n,83336))},{flag:s.flagMultilingual,component:()=>Promise.all([n.e(5850),n.e(2343)]).then(n.bind(n,62343))},{flag:s.flagUnreliably,component:()=>Promise.all([n.e(5850),n.e(6865)]).then(n.bind(n,11627))},{flag:s.flagPolitical,component:()=>Promise.all([n.e(5850),n.e(4417)]).then(n.bind(n,24417))},{flag:s.flagCognitohazard,component:()=>Promise.all([n.e(5850),n.e(8669)]).then(n.bind(n,18669))},{flag:s.flagHidden,component:()=>Promise.all([n.e(5850),n.e(8124)]).then(n.bind(n,48124))}],{0:G,1:O}=(0,r.useState)([]);return(0,r.useEffect)((()=>{D.forEach((e=>{let{flag:t,component:n}=e;t&&n().then((e=>{O((t=>[].concat((0,m.A)(t),[e.default])))}))}))}),[]),r.createElement(f.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},r.createElement(y.A,{postNumber:s.index,date:s.date,updated:s.updated,readTime:B,difficulty:s.difficultyLevel,title:s.title,desc:s.desc,banner:s.banner,section:v,postKey:C,isMindfuckery:s.flagMindfuckery,mainTag:s.mainTag}),r.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},s.otherTags.map(((e,t)=>r.createElement("span",{key:t,className:`noselect ${S.MW}`,style:{margin:"0 5px 5px 0"}},e)))),r.createElement("div",{className:"postBody"},r.createElement(z,{toc:d})),r.createElement("br",null),r.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},r.createElement(f.P.button,{className:`noselect ${k.pb}`,id:k.xG,onClick:()=>{I(!N)},whileTap:{scale:.93}},r.createElement(f.P.div,{className:x.DJ,key:N,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},N?"Switch to default layout":"Switch to wide layout"))),r.createElement("br",null),r.createElement("div",{className:"postBody",style:{margin:N?"0 -14%":"",maxWidth:N?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},r.createElement("div",{className:`${k.P_} ${V?k.Xn:k.qG}`},G.map(((e,t)=>r.createElement(e,{key:t}))),s.indexCourse?r.createElement(w.A,{index:s.indexCourse,category:s.courseCategoryName}):"",r.createElement(p.Z.Provider,{value:{images:l.nodes,basePath:A.replace(/\/$/,"")+"/"}},r.createElement(a.xA,{components:{Image:g.A}},o)))),r.createElement(E.A,{nextPost:M,lastPost:L,keyCurrent:C,section:v}))}function M(e){return r.createElement(_,e,r.createElement(c,e))}function L(e){var t,n,a,i,l;let{data:o}=e;const{frontmatter:s}=o.mdx,c=s.titleSEO||s.title,m=s.titleOG||c,h=s.titleTwitter||c,u=s.descSEO||s.desc,p=s.descOG||u,g=s.descTwitter||u,f=s.schemaType||"BlogPosting",y=s.keywordsSEO,E=s.date,b=s.updated||E,w=s.imageOG||(null===(t=s.banner)||void 0===t||null===(n=t.childImageSharp)||void 0===n||null===(a=n.gatsbyImageData)||void 0===a||null===(i=a.images)||void 0===i||null===(l=i.fallback)||void 0===l?void 0:l.src),x=s.imageAltOG||p,k=s.imageTwitter||w,T=s.imageAltTwitter||g,S=s.canonicalURL,z=s.flagHidden||!1,_=s.mainTag||"Posts",M=s.slug.split("/")[1]||"posts",{siteUrl:L}=(0,d.Q)(),H={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:L},{"@type":"ListItem",position:2,name:_,item:`${L}/${s.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${L}${s.slug}`}]};return r.createElement(v.A,{title:c+" - avrtt.blog",titleOG:m,titleTwitter:h,description:u,descriptionOG:p,descriptionTwitter:g,schemaType:f,keywords:y,datePublished:E,dateModified:b,imageOG:w,imageAltOG:x,imageTwitter:k,imageAltTwitter:T,canonicalUrl:S,flagHidden:z,mainTag:_,section:M,type:"article"},r.createElement("script",{type:"application/ld+json"},JSON.stringify(H)))}},90548:function(e,t,n){var a=n(96540),r=n(7978);t.A=e=>{let{text:t}=e;return a.createElement(r.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-tsx-content-file-path-src-pages-posts-research-transformer-architecture-2-mdx-6dd5698d0b1ec0911cd2.js.map