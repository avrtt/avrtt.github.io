"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[9280],{3962:function(e,t){t.A="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iaXNvLTg4NTktMSI/Pgo8IS0tIEdlbmVyYXRvcjogQWRvYmUgSWxsdXN0cmF0b3IgMTYuMC4wLCBTVkcgRXhwb3J0IFBsdWctSW4gLiBTVkcgVmVyc2lvbjogNi4wMCBCdWlsZCAwKSAgLS0+CjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+CjxzdmcgdmVyc2lvbj0iMS4xIiBpZD0iQ2FwYV8xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB4PSIwcHgiIHk9IjBweCIKCSB3aWR0aD0iNDE2Ljk3OXB4IiBoZWlnaHQ9IjQxNi45NzlweCIgdmlld0JveD0iMCAwIDQxNi45NzkgNDE2Ljk3OSIgc3R5bGU9ImVuYWJsZS1iYWNrZ3JvdW5kOm5ldyAwIDAgNDE2Ljk3OSA0MTYuOTc5OyIKCSB4bWw6c3BhY2U9InByZXNlcnZlIj4KPGc+Cgk8cGF0aCBkPSJNMzU2LjAwNCw2MS4xNTZjLTgxLjM3LTgxLjQ3LTIxMy4zNzctODEuNTUxLTI5NC44NDgtMC4xODJjLTgxLjQ3LDgxLjM3MS04MS41NTIsMjEzLjM3OS0wLjE4MSwyOTQuODUKCQljODEuMzY5LDgxLjQ3LDIxMy4zNzgsODEuNTUxLDI5NC44NDksMC4xODFDNDM3LjI5MywyNzQuNjM2LDQzNy4zNzUsMTQyLjYyNiwzNTYuMDA0LDYxLjE1NnogTTIzNy42LDM0MC43ODYKCQljMCwzLjIxNy0yLjYwNyw1LjgyMi01LjgyMiw1LjgyMmgtNDYuNTc2Yy0zLjIxNSwwLTUuODIyLTIuNjA1LTUuODIyLTUuODIyVjE2Ny44ODVjMC0zLjIxNywyLjYwNy01LjgyMiw1LjgyMi01LjgyMmg0Ni41NzYKCQljMy4yMTUsMCw1LjgyMiwyLjYwNCw1LjgyMiw1LjgyMlYzNDAuNzg2eiBNMjA4LjQ5LDEzNy45MDFjLTE4LjYxOCwwLTMzLjc2Ni0xNS4xNDYtMzMuNzY2LTMzLjc2NQoJCWMwLTE4LjYxNywxNS4xNDctMzMuNzY2LDMzLjc2Ni0zMy43NjZjMTguNjE5LDAsMzMuNzY2LDE1LjE0OCwzMy43NjYsMzMuNzY2QzI0Mi4yNTYsMTIyLjc1NSwyMjcuMTA3LDEzNy45MDEsMjA4LjQ5LDEzNy45MDF6Ii8+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPC9zdmc+Cg=="},66501:function(e,t,n){n.d(t,{A:function(){return r}});var a=n(96540),l=n(3962),i="styles-module--tooltiptext--a263b";var r=e=>{let{text:t,isBadge:n=!1}=e;const{0:r,1:o}=(0,a.useState)(!1),s=(0,a.useRef)(null);return(0,a.useEffect)((()=>{function e(e){s.current&&!s.current.contains(e.target)&&o(!1)}return document.addEventListener("click",e),()=>{document.removeEventListener("click",e)}}),[]),a.createElement("span",{className:"styles-module--tooltipWrapper--75ebf",ref:s},a.createElement("img",{id:n?"styles-module--infoBadge--e3d66":"styles-module--info--26c1f",src:l.A,alt:"info",onClick:e=>{e.stopPropagation(),o((e=>!e))}}),a.createElement("span",{className:r?`${i} styles-module--visible--c063c`:i},t))}},69395:function(e,t,n){n.r(t),n.d(t,{Head:function(){return M},PostTemplate:function(){return k},default:function(){return C}});var a=n(54506),l=n(28453),i=n(96540),r=n(66501),o=(n(16886),n(46295)),s=n(96098);function c(e){const t=Object.assign({p:"p",ol:"ol",li:"li",strong:"strong",h2:"h2",a:"a",span:"span",h3:"h3",ul:"ul",h4:"h4",em:"em",hr:"hr"},(0,l.RP)(),e.components),{Image:n}=t;return n||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),i.createElement(i.Fragment,null,"\n",i.createElement("br"),"\n","\n","\n",i.createElement(t.p,null,"Normalizing flows have grown rapidly in popularity as a flexible class of generative models that allow one to directly model the probability density of high-dimensional data. They belong to the broader family of likelihood-based approaches, which provide an explicit representation of the probability distribution and enable techniques such as exact likelihood evaluation or sample generation by a single model. Because they combine tractable likelihood estimation with expressive transformations parameterized by deep networks, normalizing flows present a powerful alternative or complement to more traditional generative models such as variational autoencoders ",i.createElement(r.A,{text:"VAEs: Variational Autoencoders are models that learn latent codes for data under an approximate posterior in a lower-dimensional space."})," or generative adversarial networks ",i.createElement(r.A,{text:"GANs: Generative Adversarial Networks rely on a min-max game between a generator that tries to fool a discriminator, and a discriminator that tries to distinguish real samples from generated ones."}),"."),"\n",i.createElement(t.p,null,"Broadly, the motivation behind normalizing flows can be summarized as follows:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Exact likelihood estimation"),": Unlike VAEs, which provide only a variational lower bound on the log-likelihood, or GANs, which do not directly learn the likelihood, normalizing flows can produce an exact log-likelihood for each data point (under a continuous-space assumption). This property is immensely useful in many tasks where density estimates or likelihood-based scoring is essential."),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Efficient sampling and invertibility"),": Flow-based models typically offer a direct and efficient way to sample from the learned distribution. By starting from a simple prior distribution (often a standard Gaussian) and then inverting the chain of learned transformations, one obtains new samples in data space. This invertibility is also crucial for tasks like reconstruction, and it is a unique characteristic compared to many other generative models."),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Continuous transformations"),": Many flows can be stacked to form increasingly expressive transformations of a simple distribution into a complex data distribution. Coupled with specific architectural choices (like coupling layers, invertible ",i.createElement(s.A,{text:"\\(1 \\times 1\\)"})," convolutions, multi-scale strategies, etc.), normalizing flows can capture intricate dependencies in the data while retaining exact invertibility."),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Flexibility for various data types"),": While normalizing flows were initially popularized on image data, subsequent research has demonstrated how they can be adapted to audio, text, point-cloud data, molecules, time series, and many other domains. The fundamental idea of transforming a base probability distribution into the target distribution remains widely applicable."),"\n"),"\n"),"\n",i.createElement(t.p,null,"Because of these motivations, normalizing flows have become a key technique in modern deep generative modeling. In this article, we will dive into the core theory behind normalizing flows, dissect important subcomponents such as coupling layers and multi-scale architectures, and walk through the practical details needed to implement them — particularly focusing on generative image modeling."),"\n",i.createElement(t.h2,{id:"2-fundamentals-of-normalizing-flows",style:{position:"relative"}},i.createElement(t.a,{href:"#2-fundamentals-of-normalizing-flows","aria-label":"2 fundamentals of normalizing flows permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2. fundamentals of normalizing flows"),"\n",i.createElement(t.p,null,"In this section, we dig into the formal foundation of normalizing flows, revisiting relevant concepts from probability, invertible transformations, and how these transformations produce tractable likelihoods in high dimensions."),"\n",i.createElement(t.h3,{id:"21-revisit-required-concepts",style:{position:"relative"}},i.createElement(t.a,{href:"#21-revisit-required-concepts","aria-label":"21 revisit required concepts permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.1 revisit required concepts"),"\n",i.createElement(t.p,null,"Before we dive into normalizing flows themselves, let's highlight a few concepts from probability and calculus that are essential:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Probability density functions (PDFs)"),": For continuous data ",i.createElement(s.A,{text:"\\(x \\in \\mathbb{R}^d\\)"})," with density ",i.createElement(s.A,{text:"\\(p_x(x)\\)"}),", we want a flexible model that can approximate this density."),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Change of variables"),": If ",i.createElement(s.A,{text:"\\(z = f(x)\\)"})," is an invertible transformation, then the density for ",i.createElement(s.A,{text:"\\(x\\)"})," can be expressed in terms of the density of ",i.createElement(s.A,{text:"\\(z\\)"}),". Specifically:"),"\n"),"\n"),"\n",i.createElement(o.A,{text:"\nlog p_x(x) = log p_z(f(x)) + log |det J_f(x)|,\n"}),"\n",i.createElement(t.p,null,"where ",i.createElement(s.A,{text:"\\(J_f(x)\\)"})," is the Jacobian matrix of partial derivatives of ",i.createElement(s.A,{text:"\\(f\\)"})," w.r.t. ",i.createElement(s.A,{text:"\\(x\\)"}),"."),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Determinants and invertibility"),": Computing ",i.createElement(s.A,{text:"(|det J_f(x)|)"})," is central to normalizing flows. For a transformation to be used in a normalizing flow, it must be invertible and the determinant of its Jacobian must be efficiently computable."),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Parametric families"),": We aim to make ",i.createElement(s.A,{text:"\\(f\\)"})," or ",i.createElement(s.A,{text:"\\(f^{-1}\\)"})," flexible, typically using deep networks with carefully chosen constraints that preserve invertibility (e.g., certain types of coupling, carefully structured convolution, etc.)."),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Types of prior distributions"),": Typically, the prior ",i.createElement(s.A,{text:"\\(p_z(z)\\)"})," is chosen to be a simple distribution like an isotropic Gaussian. The transformations then warp this simple distribution into the complex distribution of the data ",i.createElement(s.A,{text:"\\(p_x\\)"}),"."),"\n"),"\n"),"\n",i.createElement(t.h3,{id:"22-basic-definitions-and-change-of-variables",style:{position:"relative"}},i.createElement(t.a,{href:"#22-basic-definitions-and-change-of-variables","aria-label":"22 basic definitions and change of variables permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.2 basic definitions and change of variables"),"\n",i.createElement(t.p,null,"Let ",i.createElement(s.A,{text:"\\(x \\in \\mathbb{R}^d\\)"})," be our data variable, and ",i.createElement(s.A,{text:"\\(z \\in \\mathbb{R}^d\\)"})," a latent variable drawn from some simple distribution ",i.createElement(s.A,{text:"\\(p_z(z)\\)"})," (commonly ",i.createElement(s.A,{text:"\\(\\mathcal{N}(0,I)\\)"}),"). Consider an invertible mapping ",i.createElement(s.A,{text:"\\(f\\colon \\mathbb{R}^d \\to \\mathbb{R}^d\\)"})," with inverse ",i.createElement(s.A,{text:"\\(f^{-1}\\)"}),". Define ",i.createElement(s.A,{text:"\\(z = f(x)\\)"})," and ",i.createElement(s.A,{text:"\\(x = f^{-1}(z)\\)"}),"."),"\n",i.createElement(t.p,null,"Using the change of variables formula:"),"\n",i.createElement(s.A,{text:"\\[\np_x(x) = p_z(f(x)) \\left\\lvert \\det \\frac{\\partial f(x)}{\\partial x} \\right\\rvert.\n\\]"}),"\n",i.createElement(t.p,null,"Often, we instead store transformations in the inverse direction, ",i.createElement(s.A,{text:"\\(x = g(z)\\)"}),", because sampling from ",i.createElement(s.A,{text:"\\(p_x\\)"})," is most easily done by:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,"Sampling ",i.createElement(s.A,{text:"\\(z \\sim p_z\\)"}),"."),"\n",i.createElement(t.li,null,"Letting ",i.createElement(s.A,{text:"\\(x = g(z)\\)"}),"."),"\n"),"\n",i.createElement(t.p,null,"Either direction is feasible, but the standard approach depends on how easy it is to compute and store the Jacobian determinant."),"\n",i.createElement(t.h4,{id:"stacking-multiple-transformations",style:{position:"relative"}},i.createElement(t.a,{href:"#stacking-multiple-transformations","aria-label":"stacking multiple transformations permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"stacking multiple transformations"),"\n",i.createElement(t.p,null,"A practical design is to compose multiple invertible transformations:"),"\n",i.createElement(s.A,{text:"\\(z_0 = f_1(x), \\quad z_1 = f_2(z_0),\\quad \\ldots, \\quad z_K = f_{K}(z_{K-1})\\)"}),"\n",i.createElement(t.p,null,"so the overall mapping is ",i.createElement(s.A,{text:"\\(z_K = f_K \\circ \\cdots \\circ f_1 (x)\\)"}),". Each ",i.createElement(s.A,{text:"\\(f_i\\)"})," is structured to allow easy Jacobian determinant calculation. By stacking multiple such transformations, we can achieve a highly flexible model."),"\n",i.createElement(t.h3,{id:"23-probability-densities-invertibility-and-log-determinants",style:{position:"relative"}},i.createElement(t.a,{href:"#23-probability-densities-invertibility-and-log-determinants","aria-label":"23 probability densities invertibility and log determinants permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.3 probability densities, invertibility, and log-determinants"),"\n",i.createElement(t.p,null,"A crucial aspect of normalizing flows is ensuring that each transformation ",i.createElement(s.A,{text:"\\(f_i\\)"})," is bijective. This means:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Each flow layer must be invertible"),": So we can evaluate ",i.createElement(s.A,{text:"\\(\\log p_x(x)\\)"})," without approximation, and also generate samples by going in the reverse direction."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"We must efficiently compute the log-determinant"),": ",i.createElement(s.A,{text:"\\(\\log | \\det J_{f_i} | \\)"})," must be tractable. If computing or storing the Jacobian for a naive fully connected transformation is ",i.createElement(s.A,{text:"\\(\\mathcal{O}(d^2)\\)"})," or worse, this becomes impractical. Flow-based layer designs revolve around structures (e.g. coupling transformations, invertible convolutions) that reduce the complexity of determinant computation."),"\n"),"\n",i.createElement(t.h3,{id:"24-comparing-to-other-generative-approaches-vaes-gans-ebms",style:{position:"relative"}},i.createElement(t.a,{href:"#24-comparing-to-other-generative-approaches-vaes-gans-ebms","aria-label":"24 comparing to other generative approaches vaes gans ebms permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.4 comparing to other generative approaches (vaes, gans, ebms)"),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"VAEs")," (Kingma & Welling, ICLR 2014)"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"Offer an amortized variational inference approach, learning a lower bound on the log-likelihood."),"\n",i.createElement(t.li,null,"Typically have an encoder (to map data to a latent variable) and a decoder (to reconstruct data from the latent representation)."),"\n",i.createElement(t.li,null,"Have a stochastic latent space of lower dimension, which can hamper exact reconstruction but usually provides strong global structure."),"\n"),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"GANs")," (Goodfellow and gang, NeurIPS 2014)"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"Do not directly optimize or compute ",i.createElement(s.A,{text:"\\(p_x(x)\\)"}),". Instead, they train a generator to fool a discriminator that tries to tell real from fake."),"\n",i.createElement(t.li,null,"Sampling is extremely efficient (just pass noise through the generator)."),"\n",i.createElement(t.li,null,"They can produce high-quality, photorealistic samples but often lack a tractable density estimate and can be unstable to train."),"\n"),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"EBMs (Energy-Based Models)")," (Du & Mordatch, ICLR 2020; Grathwohl and gang, ICLR 2020)"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"Assign an unnormalized energy function ",i.createElement(s.A,{text:"\\(E(x)\\)"})," to data, from which an implicit distribution is defined as ",i.createElement(s.A,{text:"\\(p_x(x) = e^{-E(x)}/Z\\)"}),", but computing the partition function ",i.createElement(s.A,{text:"\\(Z\\)"})," can be challenging."),"\n",i.createElement(t.li,null,"Sampling from EBMs typically involves MCMC. They can, in principle, represent complicated distributions."),"\n",i.createElement(t.li,null,"They provide a flexible family but suffer from difficulties in inference and complex training loops."),"\n"),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Normalizing flows")),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"Provide an exact, tractable log-likelihood (under continuity assumptions)."),"\n",i.createElement(t.li,null,"Are fully invertible, enabling both easy sampling and density evaluation."),"\n",i.createElement(t.li,null,"When used for images, flows have historically required large network capacities and have sometimes struggled to match the sample quality of advanced GANs, though more recent variants (e.g., Flow++ or improved multi-scale designs) have significantly boosted performance."),"\n"),"\n",i.createElement(t.p,null,"Hence, normalizing flows fill a unique niche: they unify ",i.createElement(t.em,null,"exact likelihood estimation"),", ",i.createElement(t.em,null,"direct sampling"),", and ",i.createElement(t.em,null,"continuous invertible transformations")," within the same framework."),"\n",i.createElement(t.h2,{id:"3-normalizing-flows-for-image-modeling",style:{position:"relative"}},i.createElement(t.a,{href:"#3-normalizing-flows-for-image-modeling","aria-label":"3 normalizing flows for image modeling permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3. normalizing flows for image modeling"),"\n",i.createElement(t.p,null,"Although normalizing flows can be used on various data types (like audio waveforms, 3D point clouds, or text embeddings), image modeling remains one of the most visible and vibrant application areas. Training a flow-based model on images typically requires:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"An ",i.createElement(t.em,null,"architecture")," that is friendly to high-dimensional image data, often using ",i.createElement(t.em,null,"convolutional layers")," rather than fully connected layers."),"\n",i.createElement(t.li,null,i.createElement(t.em,null,"Multi-scale")," or ",i.createElement(t.em,null,"hierarchical")," factoring of the input to manage computational overhead and memory usage. Examples include RealNVP (Dinh and gang [2017], ICLR) or Glow (Kingma & Dhariwal, NeurIPS 2018)."),"\n"),"\n",i.createElement(t.h3,{id:"31-why-images-advantages-and-challenges",style:{position:"relative"}},i.createElement(t.a,{href:"#31-why-images-advantages-and-challenges","aria-label":"31 why images advantages and challenges permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.1 why images? advantages and challenges"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Images are large and complex"),": Modeling them showcases how well or poorly a model can capture complicated distributions with potentially thousands or millions of dimensions."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Rich structure"),": Convolutional neural networks (CNNs) and associated architectural designs have proved extremely effective for capturing image patterns. Normalizing flows can naturally incorporate convolutional layers in the transformations."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Discrete pixel intensities"),": Real-world digital images are typically discrete integers (0–255 per channel in 8-bit images). Normalizing flows rely on continuous transformations, so bridging this discrete/continuous gap becomes a design consideration (dequantization)."),"\n"),"\n",i.createElement(t.h3,{id:"32-introduction-to-hands-on-example-mnist-dataset-and-pytorch-lightning-setup",style:{position:"relative"}},i.createElement(t.a,{href:"#32-introduction-to-hands-on-example-mnist-dataset-and-pytorch-lightning-setup","aria-label":"32 introduction to hands on example mnist dataset and pytorch lightning setup permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.2 introduction to hands-on example: mnist dataset and pytorch lightning setup"),"\n",i.createElement(t.p,null,"Below is a small code snippet that demonstrates how to set up an MNIST-based normalizing flow in PyTorch. We highlight some lines from a typical training loop. In a normalizing-flow scenario, each batch of data ",i.createElement(s.A,{text:"\\(x\\)"})," is transformed into a latent representation ",i.createElement(s.A,{text:"\\(z\\)"})," by chaining flow layers. Then we compute:"),"\n",i.createElement(o.A,{text:"\nz, ldj = forward_flow(x)\nlog_pz = prior.log_prob(z).sum(...)\nlog_px = ldj + log_pz\nloss = - log_px.mean()\n"}),"\n",i.createElement(t.p,null,"During training, we minimize the negative log-likelihood or bits-per-dimension metric. Sampling is done in reverse: we sample ",i.createElement(s.A,{text:"\\(z\\)"})," from the prior, invert the flow, and obtain new synthetic images ",i.createElement(s.A,{text:"\\(x\\)"}),"."),"\n",i.createElement(t.p,null,"Here is a representative snippet to illustrate data loading, sampling, and so forth (simplified for demonstration):"),"\n",i.createElement(o.A,{text:"\nimport torch\nimport torchvision\nfrom torch import nn\nimport pytorch_lightning as pl\n\nclass ImageFlow(pl.LightningModule):\n    def __init__(self, flow_layers, prior):\n        super().__init__()\n        self.flow_layers = nn.ModuleList(flow_layers)\n        self.prior = prior  # e.g., Normal(0,1)\n\n    def forward(self, x):\n        # For logging or hooking; real forward pass is in the _get_likelihood function\n        return self._get_likelihood(x)\n\n    def _get_likelihood(self, x):\n        ldj = torch.zeros(x.size(0), device=x.device)\n        z = x\n        for layer in self.flow_layers:\n            z, ldj = layer(z, ldj, reverse=False)\n        log_pz = self.prior.log_prob(z).sum(dim=[1,2,3])\n        log_px = ldj + log_pz\n        return log_px\n\n    def training_step(self, batch, batch_idx):\n        x, _ = batch\n        log_px = self._get_likelihood(x)\n        loss = -log_px.mean()\n        self.log('train_nll', loss)\n        return loss\n\n    def sample(self, batch_size):\n        # sample z from prior\n        z = self.prior.sample((batch_size, 1, 28, 28))\n        ldj = torch.zeros(batch_size, device=z.device)\n        # invert the flow\n        for layer in reversed(self.flow_layers):\n            z, ldj = layer(z, ldj, reverse=True)\n        return z\n"}),"\n",i.createElement(t.p,null,"This skeleton can be expanded with advanced features, including multi-scale flow layers, dequantization strategies, coupling, and more."),"\n",i.createElement(t.h2,{id:"4-dequantization",style:{position:"relative"}},i.createElement(t.a,{href:"#4-dequantization","aria-label":"4 dequantization permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4. dequantization"),"\n",i.createElement(t.p,null,"A critical challenge for image flows is that images typically store pixel intensities as discrete integers (e.g., 0–255). Normalizing flows as described rely on continuous variables, so modeling discrete data directly can create degenerate solutions (the model might collapse probability mass onto discrete points, leading to infinite density)."),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Dequantization")," is used to circumvent this. In short, we map discrete pixel values ",i.createElement(s.A,{text:"\\(x \\in \\{0,1,\\dots,255\\}^D\\)"})," to a continuous space ",i.createElement(s.A,{text:"\\([0,1]^D\\)"})," by adding a small noise term. The idea is to treat each integer pixel as representing an interval, then sample from that interval to produce a real number."),"\n",i.createElement(t.h3,{id:"41-discrete-vs-continuous-space",style:{position:"relative"}},i.createElement(t.a,{href:"#41-discrete-vs-continuous-space","aria-label":"41 discrete vs continuous space permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.1 discrete vs. continuous space"),"\n",i.createElement(t.p,null,"In a purely continuous model, the integral of the PDF over the entire space must be 1, while points (with measure zero) effectively can receive infinite density. For a discrete value, the flow might be tempted to place enormous density at that exact integer location. This phenomenon can hamper training, since it does not produce a well-defined distribution in the continuous domain. Dequantization transforms each discrete pixel ",i.createElement(s.A,{text:"\\(x\\)"})," into ",i.createElement(s.A,{text:"\\(x + u\\)"}),", where ",i.createElement(s.A,{text:"\\(u\\)"})," is random noise in ",i.createElement(s.A,{text:"\\([0,1)\\)"}),". Then we scale appropriately, e.g. dividing by 256 if the pixel intensities range 0–255. The distribution of dequantized values better suits the smooth continuous transformations that flows apply."),"\n",i.createElement(t.h3,{id:"42-uniform-dequantization-adding-noise-to-integer-pixel-values",style:{position:"relative"}},i.createElement(t.a,{href:"#42-uniform-dequantization-adding-noise-to-integer-pixel-values","aria-label":"42 uniform dequantization adding noise to integer pixel values permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.2 uniform dequantization: adding noise to integer pixel values"),"\n",i.createElement(t.p,null,"The simplest approach: sample ",i.createElement(s.A,{text:"\\(u \\sim \\mathrm{Uniform}(0,1)\\)"})," (independently for each pixel), to get ",i.createElement(s.A,{text:"\\(x_{\\text{deq}} = (x + u)/256\\)"}),". This ensures ",i.createElement(s.A,{text:"\\(x_{\\text{deq}}\\in [0,1)\\)"}),". The log-likelihood of the original discrete pixel then becomes the integral of the flow-based continuous model over that small interval. In practice, we approximate or re-interpret the training objective with a single sample of ",i.createElement(s.A,{text:"\\(u\\)"}),". This yields a lower bound on the discrete log-likelihood."),"\n",i.createElement(t.p,null,"Here is a minimal code snippet for uniform dequantization in a flow:"),"\n",i.createElement(o.A,{text:"\nclass Dequantization(nn.Module):\n    def __init__(self, quants=256):\n        super().__init__()\n        self.quants = quants\n\n    def forward(self, x, ldj, reverse=False):\n        if reverse:\n            # rounding back to discrete\n            x = (x * self.quants).clamp(min=0, max=self.quants - 1)\n            x = torch.floor(x).to(torch.int32)\n            return x, ldj\n        else:\n            # add uniform noise\n            noise = torch.rand_like(x, dtype=torch.float32)\n            x = x.to(torch.float32) + noise\n            x = x / self.quants\n            # update ldj\n            ldj -= torch.log(torch.tensor(self.quants, device=x.device)) * x[0].numel()\n            return x, ldj\n"}),"\n",i.createElement(t.h3,{id:"43-variational-dequantization-learning-a-smoother-noise-distribution",style:{position:"relative"}},i.createElement(t.a,{href:"#43-variational-dequantization-learning-a-smoother-noise-distribution","aria-label":"43 variational dequantization learning a smoother noise distribution permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.3 variational dequantization: learning a smoother noise distribution"),"\n",i.createElement(t.p,null,"Although uniform dequantization is straightforward, the boundaries at integer points can still be a bit sharp. Variational dequantization (Ho and gang, 2019) instead learns a distribution ",i.createElement(s.A,{text:"\\(q_\\theta(u \\mid x)\\)"})," inside each discrete cell to produce less abrupt transitions. This is typically implemented as an additional flow network that outputs parameters of the noise distribution conditioned on the original image. This approach significantly boosts performance on challenging datasets, since the flow no longer must represent extremely sharp edges in the continuous space."),"\n",i.createElement(t.p,null,"A typical approach:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,"A smaller flow ",i.createElement(s.A,{text:"\\(f_\\theta\\)"})," takes ",i.createElement(s.A,{text:"\\(x\\)"})," as a conditional input and transforms a base uniform distribution ",i.createElement(s.A,{text:"\\(u\\sim U(0,1)\\)"})," into ",i.createElement(s.A,{text:"\\(u' = f_\\theta(u\\mid x)\\)"}),"."),"\n",i.createElement(t.li,null,i.createElement(s.A,{text:"\\(x_{\\text{deq}} = (x + u') / 256\\)"}),"."),"\n",i.createElement(t.li,null,"The log-likelihood objective is augmented with ",i.createElement(s.A,{text:"\\(\\log p(x + u')\\)"})," plus a correction term from ",i.createElement(s.A,{text:"\\(\\log q_\\theta(u'\\mid x)\\)"}),"."),"\n"),"\n",i.createElement(t.p,null,"The net effect: a learned noise distribution that better matches real-world data than uniform. Implementation wise, we see something like:"),"\n",i.createElement(o.A,{text:"\nclass VariationalDequantization(nn.Module):\n    def __init__(self, var_flow, quants=256):\n        super().__init__()\n        self.var_flow = var_flow  # smaller normalizing flow\n        self.quants = quants\n\n    def forward(self, x, ldj, reverse=False):\n        if reverse:\n            # revert to discrete\n            x = (x * self.quants).clamp(min=0, max=self.quants - 1)\n            x = torch.floor(x).to(torch.int32)\n            return x, ldj\n        else:\n            # transform x to [-1,1] or so if needed, and pass to var_flow\n            noise = torch.rand_like(x, dtype=torch.float32)\n            # invert some possible sigmoid or etc. in var_flow\n            # produce a nice distribution for noise\n            # ...\n            # combine x + learned noise, rescale\n            # update ldj accordingly\n            pass\n"}),"\n",i.createElement(t.p,null,"The exact code can get more involved, but the concept remains the same: we treat ",i.createElement(s.A,{text:"\\(x\\)"})," as discrete, learn a conditional distribution for the continuous offsets, and thereby produce a more accurate continuous data distribution that our main flow tries to transform from a prior."),"\n",i.createElement(t.h2,{id:"5-coupling-layers",style:{position:"relative"}},i.createElement(t.a,{href:"#5-coupling-layers","aria-label":"5 coupling layers permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5. coupling layers"),"\n",i.createElement(t.p,null,"A ",i.createElement(t.em,null,"coupling layer")," is one of the most common building blocks in flow-based models. Introduced by Dinh, Krueger, and Bengio in NICE (2014) and RealNVP (2017), coupling layers are designed to be easily invertible with a simple Jacobian determinant, while still allowing complex transformations."),"\n",i.createElement(t.h3,{id:"51-affine-coupling-planarradial-flows-vs-piecewise-transformations",style:{position:"relative"}},i.createElement(t.a,{href:"#51-affine-coupling-planarradial-flows-vs-piecewise-transformations","aria-label":"51 affine coupling planarradial flows vs piecewise transformations permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.1 affine coupling: planar/radial flows vs. piecewise transformations"),"\n",i.createElement(t.p,null,"In an ",i.createElement(t.strong,null,"affine coupling")," layer, we typically partition the data ",i.createElement(s.A,{text:"\\(z\\)"})," into two disjoint sets of dimensions (commonly via a binary mask). Let ",i.createElement(s.A,{text:"\\(z = (z_1, z_2)\\)"}),". The transformation updates only one subset of components (say ",i.createElement(s.A,{text:"\\(z_2\\)"}),"), while leaving the other (",i.createElement(s.A,{text:"\\(z_1\\)"}),") unchanged. Concretely:"),"\n",i.createElement(s.A,{text:"\\[\nz_2' = z_2 \\odot \\exp\\big(s_\\theta(z_1)\\big) + t_\\theta(z_1),\n\\]"}),"\n",i.createElement(t.p,null,"where ",i.createElement(s.A,{text:"\\(s_\\theta\\)"})," and ",i.createElement(s.A,{text:"\\(t_\\theta\\)"})," are outputs of a neural network that takes ",i.createElement(s.A,{text:"\\(z_1\\)"})," as input. The log-determinant of this transformation is:"),"\n",i.createElement(s.A,{text:"\\[\n\\log \\left|\\det \\frac{\\partial z'}{\\partial z}\\right| \n= \\sum_i s_\\theta(z_1)_i.\n\\]"}),"\n",i.createElement(t.p,null,"Because ",i.createElement(s.A,{text:"\\(z_1\\)"})," is unchanged, the Jacobian is triangular in block form, and computing the determinant is straightforward."),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Planar and radial flows")," (Rezende & Mohamed, 2015) are alternatives that can be inserted as transformations in a continuous normalizing flow, but for image-based flows, affine coupling or related piecewise transformations (e.g., Neural Spline Flows, Durkan and gang [2019]) are typically used because they scale better in high-dimensional convolutional setups."),"\n",i.createElement(t.h3,{id:"52-checkerboard-and-channel-masking-strategies",style:{position:"relative"}},i.createElement(t.a,{href:"#52-checkerboard-and-channel-masking-strategies","aria-label":"52 checkerboard and channel masking strategies permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.2 checkerboard and channel masking strategies"),"\n",i.createElement(t.p,null,"When we specify ",i.createElement(s.A,{text:"\\(z = (z_1, z_2)\\)"})," for an image, we must decide how to partition the channels, height, and width for the coupling transformations. Two popular mask patterns are:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Checkerboard masks"),": We alternate pixel positions across the image like a chessboard, so half the pixels belong to ",i.createElement(s.A,{text:"\\(z_1\\)"})," and the other half to ",i.createElement(s.A,{text:"\\(z_2\\)"}),". The advantage is that neighboring pixels often belong to different partitions, so local dependencies can be captured by the coupling."),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Channel masks"),": We split along channels (e.g., if there are ",i.createElement(s.A,{text:"\\(c\\)"})," channels, we might let the first ",i.createElement(s.A,{text:"\\(c/2\\)"})," channels be ",i.createElement(s.A,{text:"\\(z_1\\)"})," and the next ",i.createElement(s.A,{text:"\\(c/2\\)"})," be ",i.createElement(s.A,{text:"\\(z_2\\)"}),'). This can help the next layers "see" different subsets of channels more effectively.'),"\n"),"\n"),"\n",i.createElement(t.p,null,"RealNVP typically alternates between checkerboard and channel masks in successive blocks to ensure that all pixels eventually get transformed."),"\n",i.createElement(t.h3,{id:"53-implementing-a-gated-convolutional-network-for-parameterizing-st",style:{position:"relative"}},i.createElement(t.a,{href:"#53-implementing-a-gated-convolutional-network-for-parameterizing-st","aria-label":"53 implementing a gated convolutional network for parameterizing st permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.3 implementing a gated convolutional network for parameterizing (s,t)"),"\n",i.createElement(t.p,null,"For images, ",i.createElement(s.A,{text:"\\(s_\\theta\\)"})," and ",i.createElement(s.A,{text:"\\(t_\\theta\\)"}),' are usually outputs of a CNN that operates only on the unmasked portion of the data. A small, well-designed CNN can learn complex transformations and yield scale and translation parameters for the masked portion. One approach is a so-called "gated convolution," which follows a pattern:'),"\n",i.createElement(o.A,{text:"\nclass GatedConvNet(nn.Module):\n    def __init__(self, c_in, c_hidden):\n        super().__init__()\n        # Some convolutional layers, plus gating\n        # Typically we might do:\n        # 1) ConcatELU or other advanced activation\n        # 2) GatedResNet blocks\n        # 3) Final 1x1 conv\n        pass\n\n    def forward(self, x):\n        # returns [s, t] in shape\n        pass\n"}),"\n",i.createElement(t.p,null,"Then the coupling transform is:"),"\n",i.createElement(o.A,{text:"\nclass AffineCoupling(nn.Module):\n    def __init__(self, mask, network):\n        super().__init__()\n        self.mask = mask  # binary mask\n        self.network = network\n\n    def forward(self, x, ldj, reverse=False):\n        x_masked = x * self.mask\n        # pass x_masked through CNN to get [s, t]\n        s, t = torch.chunk(self.network(x_masked), chunks=2, dim=1)\n        # apply transform\n        if not reverse:\n            x_out = x * torch.exp(s * (1 - self.mask)) + t * (1 - self.mask)\n            ldj += torch.sum(s * (1 - self.mask), dim=[1,2,3])\n        else:\n            x_out = (x - t * (1 - self.mask)) * torch.exp(-s * (1 - self.mask))\n            ldj -= torch.sum(s * (1 - self.mask), dim=[1,2,3])\n        return x_out, ldj\n"}),"\n",i.createElement(t.h3,{id:"54-potential-pitfalls-in-coupling-layer-design",style:{position:"relative"}},i.createElement(t.a,{href:"#54-potential-pitfalls-in-coupling-layer-design","aria-label":"54 potential pitfalls in coupling layer design permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.4 potential pitfalls in coupling-layer design"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Mask design"),": If we always use the same mask, half of the pixels might never get updated or might always get updated with the same conditioning. Typically we alternate or flip the mask patterns each layer."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Gradient flow"),": Because one half of the variables are passed directly through or used to predict ",i.createElement(s.A,{text:"\\(s\\)"})," and ",i.createElement(s.A,{text:"\\(t\\)"}),", we need deeper, residual, or carefully structured networks to ensure good gradient propagation."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Scaling"),": Some flows use ",i.createElement(s.A,{text:"\\(\\tanh(s)\\)"})," or other constraints to keep scale factors stable."),"\n"),"\n",i.createElement(t.h2,{id:"6-multi-scale-architecture",style:{position:"relative"}},i.createElement(t.a,{href:"#6-multi-scale-architecture","aria-label":"6 multi scale architecture permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6. multi-scale architecture"),"\n",i.createElement(t.p,null,"As the dimensionality and resolution of images grow, naive flows can become extremely large and computationally expensive. A ",i.createElement(t.em,null,"multi-scale")," approach (Dinh and gang, 2017; Kingma & Dhariwal, 2018) alleviates this by successively factoring out some dimensions and modeling them at different scales."),"\n",i.createElement(t.h3,{id:"61-motivation-for-multi-scale-flows",style:{position:"relative"}},i.createElement(t.a,{href:"#61-motivation-for-multi-scale-flows","aria-label":"61 motivation for multi scale flows permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.1 motivation for multi-scale flows"),"\n",i.createElement(t.p,null,"If an image is ",i.createElement(s.A,{text:"\\(H \\times W \\times C\\)"}),", a naive flow that transforms all ",i.createElement(s.A,{text:"\\(HWC\\)"})," dimensions at once can lead to large memory footprints. Instead, multi-scale flows do the following:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,"Apply a certain number of flow layers to the full resolution."),"\n",i.createElement(t.li,null,i.createElement(t.em,null,"Factor out")," half of the dimensions (or channels) and directly store them in the latent variable ",i.createElement(s.A,{text:"\\(z_i\\)"})," at that stage."),"\n",i.createElement(t.li,null,'Continue applying flow transformations to the remaining half, possibly after a "squeeze" operation that rearranges pixels to reduce spatial resolution and increase channels.'),"\n"),"\n",i.createElement(t.p,null,"This approach reduces the computational burden for later flow layers and is beneficial in capturing coarse structures at lower resolution, then fine details at higher resolution."),"\n",i.createElement(t.h3,{id:"62-squeeze-and-split-operations",style:{position:"relative"}},i.createElement(t.a,{href:"#62-squeeze-and-split-operations","aria-label":"62 squeeze and split operations permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.2 squeeze and split operations"),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Squeeze"),": A ",i.createElement(s.A,{text:"\\(2\\times2\\)"})," squeeze rearranges blocks of 4 neighboring spatial pixels into 4 channels. For instance, ",i.createElement(s.A,{text:"\\((H, W, C)\\)"})," becomes ",i.createElement(s.A,{text:"\\(\\left(\\tfrac{H}{2}, \\tfrac{W}{2}, 4C\\right)\\)"}),". This operation helps a subsequent ",i.createElement(s.A,{text:"\\(1 \\times 1\\)"})," convolution or coupling layer to mix local spatial structure more effectively."),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Split"),": Suppose the new shape is ",i.createElement(s.A,{text:"\\((H/2, W/2, 4C)\\)"})," after the squeeze. We then split out half of the channels (say ",i.createElement(s.A,{text:"\\(2C\\)"}),"), treat them as part of the latent ",i.createElement(s.A,{text:"\\(z_i\\)"}),", and only continue flows on the remaining half. This drastically reduces dimension as we move deeper into the flow."),"\n",i.createElement(t.h3,{id:"63-building-a-multi-scale-flow-eg-realnvp--or-flow-style",style:{position:"relative"}},i.createElement(t.a,{href:"#63-building-a-multi-scale-flow-eg-realnvp--or-flow-style","aria-label":"63 building a multi scale flow eg realnvp  or flow style permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.3 building a multi-scale flow (e.g., realnvp- or flow++-style)"),"\n",i.createElement(t.p,null,"A typical multi-scale architecture might look like:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Coupling block")," (maybe 4–8 coupling layers)."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Squeeze"),"."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Coupling block"),"."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Split")," (factor out half the channels as latent)."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Squeeze")," again."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Coupling block")," (further transformations)."),"\n",i.createElement(t.li,null,"Possibly another split."),"\n"),"\n",i.createElement(t.p,null,"At each split, the factored out variables are immediately modeled by the prior ",i.createElement(s.A,{text:"\\(p_z\\)"})," or stored as is, contributing to the log-likelihood. This pattern repeats until the entire set of channels is accounted for, typically in multiple scales."),"\n",i.createElement(t.h3,{id:"64-quantitative-and-qualitative-comparisons-speed-bits-per-dimension-and-parameter-counts",style:{position:"relative"}},i.createElement(t.a,{href:"#64-quantitative-and-qualitative-comparisons-speed-bits-per-dimension-and-parameter-counts","aria-label":"64 quantitative and qualitative comparisons speed bits per dimension and parameter counts permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.4 quantitative and qualitative comparisons: speed, bits per dimension, and parameter counts"),"\n",i.createElement(t.p,null,"Because we're progressively removing dimensions, the flow at later stages has smaller resolution to transform. This leads to:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Faster training / sampling"),": The network doesn't always transform the entire ",i.createElement(s.A,{text:"\\(H \\times W \\times C\\)"})," simultaneously."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Often better bits-per-dimension"),": By devoting more capacity to lower resolution scales, the network captures large structural patterns of the data effectively."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Parameter efficiency"),": Multi-scale architectures can use their parameters in an efficient stepwise manner, sometimes achieving better generative performance with fewer parameters than a naive, single-scale approach."),"\n"),"\n",i.createElement(t.h2,{id:"7-invertible-1x1-convolution-and-actnorm",style:{position:"relative"}},i.createElement(t.a,{href:"#7-invertible-1x1-convolution-and-actnorm","aria-label":"7 invertible 1x1 convolution and actnorm permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7. invertible 1x1 convolution and actnorm"),"\n",i.createElement(t.p,null,"So far, we have seen coupling layers, dequantization, and multi-scale structures. Another powerful invention, particularly seen in the ",i.createElement(t.strong,null,"Glow")," model (Kingma & Dhariwal, 2018), is the use of invertible ",i.createElement(s.A,{text:"\\(1 \\times 1\\)"})," convolutions instead of fixed permutations of channels. Alongside it, the so-called ",i.createElement(t.strong,null,"ActNorm")," layer is used to replace batch normalization in an invertible manner."),"\n",i.createElement(t.h3,{id:"71-glow-model-overview-and-how-it-differs-from-realnvp",style:{position:"relative"}},i.createElement(t.a,{href:"#71-glow-model-overview-and-how-it-differs-from-realnvp","aria-label":"71 glow model overview and how it differs from realnvp permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.1 glow model overview and how it differs from realnvp"),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Glow")," is a flow-based model that introduced:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Invertible ",i.createElement(s.A,{text:"\\(1 \\times 1\\)"})," convolution"),": Instead of just permuting channels to ensure that different sets of variables eventually get transformed, Glow learns a full invertible linear transformation of the channels at each layer. This is a generalization of permutations."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"ActNorm"),': A per-channel scale-and-shift transformation that is initialized with data statistics in the first forward pass. It replaces the earlier "batch norm" style approach but in a way that is strictly invertible.'),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Multi-scale architecture")," similar to RealNVP, but the coupling layers are simplified (no checkerboard masks), and the model often uses channelwise masks plus the learned invertible ",i.createElement(s.A,{text:"\\(1 \\times 1\\)"})," convolutions to mix the variables."),"\n"),"\n",i.createElement(t.h3,{id:"72-actnorm-per-channel-scaling-and-shifting",style:{position:"relative"}},i.createElement(t.a,{href:"#72-actnorm-per-channel-scaling-and-shifting","aria-label":"72 actnorm per channel scaling and shifting permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.2 actnorm: per-channel scaling and shifting"),"\n",i.createElement(t.p,null,"Instead of using standard batch normalization or layer normalization inside a flow, Glow employs ActNorm. The transformation is:"),"\n",i.createElement(s.A,{text:"\\[\ny = s \\odot x + b,\n\\]"}),"\n",i.createElement(t.p,null,"where ",i.createElement(s.A,{text:"\\(s\\)"})," and ",i.createElement(s.A,{text:"\\(b\\)"})," are learned parameters that are broadcast across spatial dimensions but are distinct for each channel. The log-determinant is straightforward:"),"\n",i.createElement(s.A,{text:"\\[\n\\log \\left|\\det \\frac{\\partial y}{\\partial x}\\right| \n= \\sum_{c=1}^C \\log|s_c| \\times H \\times W,\n\\]"}),"\n",i.createElement(t.p,null,"assuming the image is shape ",i.createElement(s.A,{text:"\\(C\\times H \\times W\\)"}),". Instead of computing data-dependent statistics each forward pass, ActNorm is typically initialized by one pass of data — computing the mean and standard deviation per channel — to set ",i.createElement(s.A,{text:"\\(s\\)"})," and ",i.createElement(s.A,{text:"\\(b\\)"}),". Once initialized, those parameters remain learned and do not average across the batch in subsequent iterations. This avoids some instabilities that can arise with batch norm for invertible architectures."),"\n",i.createElement(t.p,null,"A simplified snippet:"),"\n",i.createElement(o.A,{text:"\nclass ActNorm(nn.Module):\n    def __init__(self, num_channels):\n        super().__init__()\n        self.loc = nn.Parameter(torch.zeros(1, num_channels, 1, 1))\n        self.log_scale = nn.Parameter(torch.zeros(1, num_channels, 1, 1))\n        self.register_buffer('initialized', torch.tensor(0))\n\n    def forward(self, x, ldj, reverse=False):\n        B, C, H, W = x.shape\n        if self.initialized.item() == 0:\n            # initialize using data stats\n            with torch.no_grad():\n                flat_x = x.permute(1,0,2,3).reshape(C, -1)\n                mean = flat_x.mean(dim=1).view(1, C, 1, 1)\n                std = flat_x.std(dim=1).view(1, C, 1, 1)\n                self.loc.data = -mean\n                self.log_scale.data = -std.log()\n                self.initialized.fill_(1)\n\n        if not reverse:\n            x = (x + self.loc) * torch.exp(self.log_scale)\n            ldj += (self.log_scale.sum(dim=[1,2,3]) * H * W)\n        else:\n            x = x * torch.exp(-self.log_scale) - self.loc\n            ldj -= (self.log_scale.sum(dim=[1,2,3]) * H * W)\n\n        return x, ldj\n"}),"\n",i.createElement(t.h3,{id:"73-invertible-1x1-convolution-rethinking-permutations-for-channel-mixing",style:{position:"relative"}},i.createElement(t.a,{href:"#73-invertible-1x1-convolution-rethinking-permutations-for-channel-mixing","aria-label":"73 invertible 1x1 convolution rethinking permutations for channel mixing permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.3 invertible 1x1 convolution: rethinking permutations for channel mixing"),"\n",i.createElement(t.p,null,'Before Glow, RealNVP typically permuted channels or used "channel flips." Glow generalizes this idea by inserting a linear transformation:'),"\n",i.createElement(s.A,{text:"\\[\ny = W x\n\\]"}),"\n",i.createElement(t.p,null,"where ",i.createElement(s.A,{text:"\\(W\\)"})," is a ",i.createElement(s.A,{text:"\\(C \\times C\\)"})," matrix applied identically at every pixel location. The transformation is a ",i.createElement(s.A,{text:"\\(1\\times1\\)"})," convolution with no spatial kernel dimension. The log-determinant is:"),"\n",i.createElement(s.A,{text:"\\[\n\\log\\left| \\det\\left(\\frac{\\partial y}{\\partial x}\\right)\\right|\n= H \\times W \\times \\log | \\det W |.\n\\]"}),"\n",i.createElement(t.p,null,"Hence, if we ensure ",i.createElement(s.A,{text:"\\(W\\)"})," is invertible, we can easily compute the determinant once per forward pass, multiplied by the number of spatial elements. The inverse pass is ",i.createElement(s.A,{text:"\\(x = W^{-1} y\\)"}),"."),"\n",i.createElement(t.h4,{id:"forward-and-inverse-pass",style:{position:"relative"}},i.createElement(t.a,{href:"#forward-and-inverse-pass","aria-label":"forward and inverse pass permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"forward and inverse pass"),"\n",i.createElement(t.p,null,"Forward pass:"),"\n",i.createElement(o.A,{text:"\ny = F.conv2d(x, W.reshape(C, C, 1, 1))\nldj += (H * W) * logdet(W)\n"}),"\n",i.createElement(t.p,null,"Inverse pass:"),"\n",i.createElement(o.A,{text:"\nx = F.conv2d(y, W_inv.reshape(C, C, 1, 1))\nldj -= (H * W) * logdet(W)\n"}),"\n",i.createElement(t.p,null,"where ",i.createElement(s.A,{text:"\\(W^{-1}\\)"})," is the inverse of ",i.createElement(s.A,{text:"\\(W\\)"}),". Typically we reparameterize or store an LU decomposition so that we can invert it quickly and compute ",i.createElement(s.A,{text:"\\(\\log|\\det W|\\)"})," cheaply."),"\n",i.createElement(t.h3,{id:"74-determinant-calculation-with-lu-or-qr-decompositions",style:{position:"relative"}},i.createElement(t.a,{href:"#74-determinant-calculation-with-lu-or-qr-decompositions","aria-label":"74 determinant calculation with lu or qr decompositions permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.4 determinant calculation with lu or qr decompositions"),"\n",i.createElement(t.p,null,"A naive determinant of a ",i.createElement(s.A,{text:"\\(C\\times C\\)"})," matrix is ",i.createElement(s.A,{text:"\\(\\mathcal{O}(C^3)\\)"}),". While ",i.createElement(s.A,{text:"\\(C\\)"})," might not be huge (for typical images, ",i.createElement(s.A,{text:"\\(C\\leq64\\)"})," in early layers or ",i.createElement(s.A,{text:"\\(C\\leq512\\)"})," in deeper layers), repeated factorization can still be slow. To speed things up:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"LU decomposition"),": Factor ",i.createElement(s.A,{text:"\\(W = P\\,L\\,U\\)"}),". The determinant is just the product of diagonal entries of ",i.createElement(s.A,{text:"\\(L\\)"})," and ",i.createElement(s.A,{text:"\\(U\\)"}),", combined with the sign of the permutation matrix ",i.createElement(s.A,{text:"\\(P\\)"}),"."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"QR decomposition"),": Factor ",i.createElement(s.A,{text:"\\(W = Q\\,R\\)"}),", and ",i.createElement(s.A,{text:"\\(\\log|\\det W|\\)"}),"=",i.createElement(s.A,{text:"\\(\\log|\\det R|\\)"}),". This also works well, but LU is typically simpler to handle in practice for invertible flows."),"\n"),"\n",i.createElement(t.p,null,"In Glow, the authors used an LU-based parameterization of ",i.createElement(s.A,{text:"\\(W\\)"}),". The weight matrix is optimized in that factorized form, which ensures invertibility by construction, and the log-determinant is trivial to compute from the triangular factor."),"\n",i.createElement(t.h3,{id:"75-edge-cases-and-ensuring-invertibility",style:{position:"relative"}},i.createElement(t.a,{href:"#75-edge-cases-and-ensuring-invertibility","aria-label":"75 edge cases and ensuring invertibility permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.5 edge cases and ensuring invertibility"),"\n",i.createElement(t.p,null,"A risk is that ",i.createElement(s.A,{text:"\\(W\\)"})," might approach a singular matrix. Usually, the learned parameterization ensures ",i.createElement(s.A,{text:"\\(W\\)"})," is invertible. But numerical issues may occasionally arise with large channels or near-singular updates. In practice, one might clamp or re-initialize if a near-singular matrix is detected."),"\n",i.createElement(t.hr),"\n",i.createElement(t.p,null,"Bringing these components together — coupling layers, multi-scale factoring, invertible ",i.createElement(s.A,{text:"\\(1 \\times 1\\)"})," convolutions, and actnorm — yields a powerful flow-based architecture. By stacking a sequence of these blocks, one can achieve state-of-the-art or near-state-of-the-art log-likelihood performance on standard image benchmarks like CIFAR-10 or ImageNet (at moderate resolutions)."),"\n",i.createElement(t.p,null,"Below is a schematic (adapted from Kingma & Dhariwal, 2018) of repeated flow steps:"),"\n",i.createElement(n,{alt:"Glow-like architecture",path:"",caption:"Illustration of repeated steps of ActNorm, invertible 1x1 convolution, and affine coupling, as used in Glow.",zoom:"false"}),"\n",i.createElement(t.p,null,'Here is a condensed pseudo-code block for a single "Glow step," ignoring multi-scale splits:'),"\n",i.createElement(o.A,{text:"\nclass GlowStep(nn.Module):\n    def __init__(self, c_in):\n        super().__init__()\n        self.actnorm = ActNorm(c_in)\n        self.invconv = Invertible1x1Conv(c_in)\n        self.coupling = AffineCoupling(...)  # or an additive coupling\n\n    def forward(self, x, ldj, reverse=False):\n        if not reverse:\n            x, ldj = self.actnorm(x, ldj, reverse=False)\n            x, ldj = self.invconv(x, ldj, reverse=False)\n            x, ldj = self.coupling(x, ldj, reverse=False)\n        else:\n            x, ldj = self.coupling(x, ldj, reverse=True)\n            x, ldj = self.invconv(x, ldj, reverse=True)\n            x, ldj = self.actnorm(x, ldj, reverse=True)\n        return x, ldj\n"}),"\n",i.createElement(t.p,null,"Stack ",i.createElement(s.A,{text:"\\(K\\)"})," such steps, interleave with multi-scale splits or squeezes, and you have a full Glow."),"\n",i.createElement(t.hr),"\n",i.createElement(t.h2,{id:"8-additional-advanced-topics-beyond-the-standard-outline",style:{position:"relative"}},i.createElement(t.a,{href:"#8-additional-advanced-topics-beyond-the-standard-outline","aria-label":"8 additional advanced topics beyond the standard outline permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8. additional advanced topics (beyond the standard outline)"),"\n",i.createElement(t.p,null,"Given the richness of flow-based methods, it's helpful to briefly outline some further developments and directions in research:"),"\n",i.createElement(t.h3,{id:"81-continuous-normalizing-flows",style:{position:"relative"}},i.createElement(t.a,{href:"#81-continuous-normalizing-flows","aria-label":"81 continuous normalizing flows permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.1 continuous normalizing flows"),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Neural ODEs"),' (Chen and gang, NeurIPS 2018) propose modeling the transformation as a continuous-time process governed by an ordinary differential equation. This leads to "continuous normalizing flows" where the log-determinant can be integrated over time using the trace of the Jacobian. While powerful in theory and opening new doors for invertible modeling, computing the continuous-time log-determinant can be numerically intensive. Further improvements like FFJORD (Grathwohl and gang, ICLR 2019) mitigate some of these costs.'),"\n",i.createElement(t.h3,{id:"82-neural-spline-flows",style:{position:"relative"}},i.createElement(t.a,{href:"#82-neural-spline-flows","aria-label":"82 neural spline flows permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.2 neural spline flows"),"\n",i.createElement(t.p,null,"The RealNVP coupling transformation is affine. Neural Spline Flows (Durkan and gang, NeurIPS 2019) propose using learnable monotonic piecewise polynomials to transform data, capturing sharper nonlinearities. They maintain easy invertibility and a simple log-determinant formula."),"\n",i.createElement(t.h3,{id:"83-unconditional-vs-conditional-flows",style:{position:"relative"}},i.createElement(t.a,{href:"#83-unconditional-vs-conditional-flows","aria-label":"83 unconditional vs conditional flows permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.3 unconditional vs. conditional flows"),"\n",i.createElement(t.p,null,"Flow-based models can be conditioned on side information (class labels, partial input, or other data). The transformations typically incorporate the conditioning signal in the coupling layers (e.g., ",i.createElement(s.A,{text:"\\(s_\\theta(z_1, y)\\)"})," and ",i.createElement(s.A,{text:"\\(t_\\theta(z_1, y)\\)"}),"). This approach is used for ",i.createElement(t.em,null,"conditional generation")," or ",i.createElement(t.em,null,"data imputation")," tasks."),"\n",i.createElement(t.h3,{id:"84-multi-resolution-and-parallel-architectures",style:{position:"relative"}},i.createElement(t.a,{href:"#84-multi-resolution-and-parallel-architectures","aria-label":"84 multi resolution and parallel architectures permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.4 multi-resolution and parallel architectures"),"\n",i.createElement(t.p,null,"Techniques like i-ResNet, Residual Flows (Behrmann and gang, 2019, NeurIPS) expand possible invertible architectures by imposing constraints on residual blocks, ensuring invertibility. In practice, they can struggle with speed or capacity, but they broaden the design space significantly."),"\n",i.createElement(t.hr),"\n",i.createElement(t.h2,{id:"9-large-code-example-a-complete-normalizing-flow-on-mnist",style:{position:"relative"}},i.createElement(t.a,{href:"#9-large-code-example-a-complete-normalizing-flow-on-mnist","aria-label":"9 large code example a complete normalizing flow on mnist permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9. large code example: a complete normalizing flow on mnist"),"\n",i.createElement(t.p,null,"Below is a more extended snippet that sketches out a normalizing flow architecture combining everything we've discussed: dequantization, multi-scale splits, invertible ",i.createElement(s.A,{text:"\\(1 \\times 1\\)"})," convolutions, actnorm, and affine coupling. This example (though partial) aims to highlight how all the pieces fit together in code using PyTorch."),"\n",i.createElement(o.A,{text:"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\n# Suppose we already have classes: ActNorm, Invertible1x1Conv, AffineCoupling, Dequantization, etc.\n\nclass GlowBlock(nn.Module):\n    def __init__(self, c_in):\n        super().__init__()\n        self.actnorm = ActNorm(c_in)\n        self.invconv = Invertible1x1Conv(c_in)\n        # A typical coupling might require a mask or channel-split approach\n        self.coupling = AffineCoupling(...)\n\n    def forward(self, x, ldj, reverse=False):\n        if not reverse:\n            x, ldj = self.actnorm(x, ldj, reverse=False)\n            x, ldj = self.invconv(x, ldj, reverse=False)\n            x, ldj = self.coupling(x, ldj, reverse=False)\n        else:\n            x, ldj = self.coupling(x, ldj, reverse=True)\n            x, ldj = self.invconv(x, ldj, reverse=True)\n            x, ldj = self.actnorm(x, ldj, reverse=True)\n        return x, ldj\n\nclass MultiScaleFlow(nn.Module):\n    def __init__(self, c_in):\n        super().__init__()\n        # Example: 2 levels of multi-scale\n        self.dequant = Dequantization()  # or VariationalDequantization\n        self.flow1 = nn.ModuleList([\n            GlowBlock(c_in) for _ in range(4)\n        ])\n        # Squeeze\n        # Possibly do a SplitFlow here\n        self.flow2 = nn.ModuleList([\n            GlowBlock(4*c_in) for _ in range(4)\n        ])\n        # Another split if needed\n        self.prior = torch.distributions.Normal(0,1)\n\n    def forward(self, x, reverse=False):\n        ldj = torch.zeros(x.size(0), device=x.device)\n        if not reverse:\n            x, ldj = self.dequant(x, ldj, reverse=False)\n            # pass through flow1\n            for block in self.flow1:\n                x, ldj = block(x, ldj, reverse=False)\n            # maybe do squeeze, split\n            # pass through flow2\n            for block in self.flow2:\n                x, ldj = block(x, ldj, reverse=False)\n            # final latent distribution\n            # compute final ldj if factoring out dims\n            return x, ldj\n        else:\n            # sample or pass a latent back\n            pass\n"}),"\n",i.createElement(t.p,null,"This sort of structure can be further elaborated upon. For real workloads, one typically implements the multi-scale factoring carefully, keeps track of all split latents, and ensures correct log-determinant computation for each stage."),"\n",i.createElement(t.hr),"\n",i.createElement(t.h2,{id:"10-conclusion",style:{position:"relative"}},i.createElement(t.a,{href:"#10-conclusion","aria-label":"10 conclusion permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"10. conclusion"),"\n",i.createElement(t.p,null,"Normalizing flows provide a unique bridge between classical change-of-variables in probability theory and modern deep architectures. They deliver:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Exact log-likelihood")," for continuous data (with an appropriate dequantization step for discrete data)."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Direct sampling")," from a tractable prior."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Invertibility")," by design, enabling reconstructions and manipulations in latent space that precisely map back to data space."),"\n"),"\n",i.createElement(t.p,null,"Flows have been successfully applied in a myriad of scenarios: image generation (RealNVP, Glow, Flow++), audio waveform modeling (WaveGlow), molecular generation, text-based tasks with discrete dequantization, and beyond. While they can be parameter-heavy and sometimes less sample-efficient than specialized architectures (like advanced GANs), they remain a powerful and conceptually elegant generative modeling strategy."),"\n",i.createElement(t.p,null,'Moving forward, areas of active research include continuous normalizing flows, neural spline flows, augmented flows that incorporate discrete variables, invertible transformations for latent variable expansions, and the marriage of flows with other generative paradigms (like "Flow Matching," diffusion-meets-flow). Overall, normalizing flows will likely stay at the forefront of generative modeling research, offering a robust framework for many real-world applications.'),"\n",i.createElement(t.p,null,"This concludes our deep dive into normalizing flows for image modeling, including essential topics like dequantization, affine coupling, multi-scale architectures, invertible ",i.createElement(s.A,{text:"\\(1 \\times 1\\)"})," convolutions, and actnorm."),"\n",i.createElement(t.p,null,"For further reading:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,'Dinh, L., Sohl-Dickstein, J., & Bengio, S. (2017). "Density estimation using Real NVP". In: ',i.createElement(t.em,null,"5th International Conference on Learning Representations (ICLR)"),"."),"\n",i.createElement(t.li,null,'Kingma, D. P., & Dhariwal, P. (2018). "Glow: Generative Flow with Invertible 1x1 Convolutions". In: ',i.createElement(t.em,null,"Advances in Neural Information Processing Systems")," (NeurIPS)."),"\n",i.createElement(t.li,null,'Ho, J. and gang (2019). "Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design". ',i.createElement(t.em,null,"ICML"),"."),"\n",i.createElement(t.li,null,'Durkan, C. and gang (2019). "Neural Spline Flows". ',i.createElement(t.em,null,"NeurIPS"),"."),"\n",i.createElement(t.li,null,'Rezende, D., & Mohamed, S. (2015). "Variational Inference with Normalizing Flows". ',i.createElement(t.em,null,"ICML"),"."),"\n",i.createElement(t.li,null,'Chen, T. Q. and gang (2018). "Neural Ordinary Differential Equations". ',i.createElement(t.em,null,"NeurIPS"),"."),"\n"),"\n",i.createElement(t.p,null,"And many others exploring advanced or specialized normalizing flow techniques."))}var m=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,l.RP)(),e.components);return t?i.createElement(t,e,i.createElement(c,e)):c(e)};var d=n(36710),h=n(58481),u=n.n(h),p=n(36310),f=n(87245),g=n(27042),v=n(59849),E=n(5591),b=n(61122),w=n(9219),y=n(33203),x=n(95751),z=n(94328),_=n(80791),S=n(78137);const A=e=>{let{toc:t}=e;if(!t||!t.items)return null;return i.createElement("nav",{className:_.R},i.createElement("ul",null,t.items.map(((e,t)=>i.createElement("li",{key:t},i.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const n=t.replace("#",""),a=document.getElementById(n);a&&a.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&i.createElement(A,{toc:{items:e.items}}))))))};function k(e){let{data:{mdx:t,allMdx:r,allPostImages:o},children:s}=e;const{frontmatter:c,body:m,tableOfContents:d}=t,h=c.index,v=c.slug.split("/")[1],_=r.nodes.filter((e=>e.frontmatter.slug.includes(`/${v}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),k=_.findIndex((e=>e.frontmatter.index===h)),C=_[k+1],M=_[k-1],H=c.slug.replace(/\/$/,""),N=/[^/]*$/.exec(H)[0],T=`posts/${v}/content/${N}/`,{0:j,1:I}=(0,i.useState)(c.flagWideLayoutByDefault),{0:L,1:V}=(0,i.useState)(!1);var q;(0,i.useEffect)((()=>{V(!0);const e=setTimeout((()=>V(!1)),340);return()=>clearTimeout(e)}),[j]),"adventures"===v?q=w.cb:"research"===v?q=w.Qh:"thoughts"===v&&(q=w.T6);const B=u()(m).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,D=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),n=e%60;return n<=30?`~${t}${n>0?".5":""} h`:`~${t+1} h`}(Math.ceil(B/q)+(c.extraReadTimeMin||0)),P=[{flag:c.flagDraft,component:()=>Promise.all([n.e(3231),n.e(8809)]).then(n.bind(n,28809))},{flag:c.flagMindfuckery,component:()=>Promise.all([n.e(3231),n.e(2471)]).then(n.bind(n,67709))},{flag:c.flagRewrite,component:()=>Promise.all([n.e(3231),n.e(6764)]).then(n.bind(n,62002))},{flag:c.flagOffensive,component:()=>Promise.all([n.e(3231),n.e(2443)]).then(n.bind(n,17681))},{flag:c.flagProfane,component:()=>Promise.all([n.e(3231),n.e(8048)]).then(n.bind(n,53286))},{flag:c.flagMultilingual,component:()=>Promise.all([n.e(3231),n.e(4069)]).then(n.bind(n,78831))},{flag:c.flagUnreliably,component:()=>Promise.all([n.e(3231),n.e(3417)]).then(n.bind(n,8179))},{flag:c.flagPolitical,component:()=>Promise.all([n.e(3231),n.e(5195)]).then(n.bind(n,30433))},{flag:c.flagCognitohazard,component:()=>Promise.all([n.e(3231),n.e(3175)]).then(n.bind(n,8413))},{flag:c.flagHidden,component:()=>Promise.all([n.e(3231),n.e(9556)]).then(n.bind(n,14794))}],{0:F,1:W}=(0,i.useState)([]);return(0,i.useEffect)((()=>{P.forEach((e=>{let{flag:t,component:n}=e;t&&n().then((e=>{W((t=>[].concat((0,a.A)(t),[e.default])))}))}))}),[]),i.createElement(g.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},i.createElement(E.A,{postNumber:c.index,date:c.date,updated:c.updated,readTime:D,difficulty:c.difficultyLevel,title:c.title,desc:c.desc,banner:c.banner,section:v,postKey:N,isMindfuckery:c.flagMindfuckery,mainTag:c.mainTag}),i.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},c.otherTags.map(((e,t)=>i.createElement("span",{key:t,className:`noselect ${S.MW}`,style:{margin:"0 5px 5px 0"}},e)))),i.createElement("div",{class:"postBody"},i.createElement(A,{toc:d})),i.createElement("br"),i.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},i.createElement(g.P.button,{class:"noselect",className:z.pb,id:z.xG,onClick:()=>{I(!j)},whileTap:{scale:.93}},i.createElement(g.P.div,{className:x.DJ,key:j,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},j?"Switch to default layout":"Switch to wide layout"))),i.createElement("br"),i.createElement("div",{class:"postBody",style:{margin:j?"0 -14%":"",maxWidth:j?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},i.createElement("div",{className:`${z.P_} ${L?z.Xn:z.qG}`},F.map(((e,t)=>i.createElement(e,{key:t}))),c.indexCourse?i.createElement(y.A,{index:c.indexCourse,category:c.courseCategoryName}):"",i.createElement(p.Z.Provider,{value:{images:o.nodes,basePath:T.replace(/\/$/,"")+"/"}},i.createElement(l.xA,{components:{Image:f.A}},s)))),i.createElement(b.A,{nextPost:C,lastPost:M,keyCurrent:N,section:v}))}function C(e){return i.createElement(k,e,i.createElement(m,e))}function M(e){var t,n,a,l,r;let{data:o}=e;const{frontmatter:s}=o.mdx,c=s.titleSEO||s.title,m=s.titleOG||c,h=s.titleTwitter||c,u=s.descSEO||s.desc,p=s.descOG||u,f=s.descTwitter||u,g=s.schemaType||"BlogPosting",E=s.keywordsSEO,b=s.date,w=s.updated||b,y=s.imageOG||(null===(t=s.banner)||void 0===t||null===(n=t.childImageSharp)||void 0===n||null===(a=n.gatsbyImageData)||void 0===a||null===(l=a.images)||void 0===l||null===(r=l.fallback)||void 0===r?void 0:r.src),x=s.imageAltOG||p,z=s.imageTwitter||y,_=s.imageAltTwitter||f,S=s.canonicalURL,A=s.flagHidden||!1,k=s.mainTag||"Posts",C=s.slug.split("/")[1]||"posts",{siteUrl:M}=(0,d.Q)(),H={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:M},{"@type":"ListItem",position:2,name:k,item:`${M}/${s.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${M}${s.slug}`}]};return i.createElement(v.A,{title:c+" - avrtt.blog",titleOG:m,titleTwitter:h,description:u,descriptionOG:p,descriptionTwitter:f,schemaType:g,keywords:E,datePublished:b,dateModified:w,imageOG:y,imageAltOG:x,imageTwitter:z,imageAltTwitter:_,canonicalUrl:S,flagHidden:A,mainTag:k,section:C,type:"article"},i.createElement("script",{type:"application/ld+json"},JSON.stringify(H)))}},96098:function(e,t,n){var a=n(96540),l=n(7978);t.A=e=>{let{text:t}=e;return a.createElement(l.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-js-content-file-path-src-pages-posts-research-normalizing-flows-mdx-aef08480ad6e03dc9120.js.map