{"version":3,"file":"component---src-templates-post-js-content-file-path-src-pages-posts-research-information-theory-for-ml-mdx-beff34d488bb9c017252.js","mappings":"4GAAA,ooD,kNCmEA,SAASA,EAAkBC,GACzB,MAAMC,EAAcC,OAAOC,OAAO,CAChCC,EAAG,IACHC,GAAI,KACJC,EAAG,IACHC,KAAM,OACNC,GAAI,KACJC,GAAI,KACJC,GAAI,KACJC,GAAI,KACJC,OAAQ,SACRC,GAAI,KACJC,GAAI,OACHC,EAAAA,EAAAA,MAAsBf,EAAMgB,YAC/B,OAAOC,EAAAA,cAAoBA,EAAAA,SAAgB,KAAM,KAAMA,EAAAA,cAAoB,MAAO,KAAM,KAAM,KAAMA,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,0vBAA+vB,KAAMa,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,omBAAqmB,KAAMa,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,kiBAAmiB,KAAMa,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,giBAAiiB,KAAMa,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,6gBAA8gB,KAAMa,EAAAA,cAAoBhB,EAAYI,GAAI,CACvyGa,GAAI,iCACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,kCACN,aAAc,2CACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,kCAAmC,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,iaAAua,KAAMa,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAM,KAAMQ,EAAAA,cAAoBhB,EAAYG,EAAG,KAAMa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,kBAAmB,kLAAmLR,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,WAAY,mDAAoD,MAAO,KAAMR,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAM,KAAMQ,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,kBAAmBa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,oBAAqB,mSAAoS,MAAO,KAAMR,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAM,KAAMQ,EAAAA,cAAoBhB,EAAYG,EAAG,KAAMa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,sBAAuB,mNAAoN,MAAO,KAAMR,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAM,KAAMQ,EAAAA,cAAoBhB,EAAYG,EAAG,KAAMa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,4BAA6B,yPAA0P,MAAO,MAAO,KAAMR,EAAAA,cAAoBhB,EAAYI,GAAI,CAClwEa,GAAI,gCACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,iCACN,aAAc,0CACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,iCAAkC,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,0EAA2E,KAAMa,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAM,KAAMQ,EAAAA,cAAoBhB,EAAYG,EAAG,KAAMa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,iCAAkC,uOAAwO,MAAO,KAAMR,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAM,KAAMQ,EAAAA,cAAoBhB,EAAYG,EAAG,KAAMa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,qCAAsC,mQAAsQ,MAAO,KAAMR,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAM,KAAMQ,EAAAA,cAAoBhB,EAAYG,EAAG,KAAMa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,qBAAsB,qPAAsP,MAAO,KAAMR,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAM,KAAMQ,EAAAA,cAAoBhB,EAAYG,EAAG,KAAMa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,2BAA4B,SAAUR,EAAAA,cAAoBS,EAAAA,EAAS,CACznDC,KAAM,iIACJ,2GAA4G,MAAO,KAAMV,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAM,KAAMQ,EAAAA,cAAoBhB,EAAYG,EAAG,KAAMa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,0BAA2B,kQAAmQ,MAAO,MAAO,KAAMR,EAAAA,cAAoBhB,EAAYI,GAAI,CAC/kBa,GAAI,2BACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,4BACN,aAAc,qCACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,4BAA6B,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,8WAA+W,KAAMa,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAM,2JAA4J,KAAMQ,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAM,sBAAuBQ,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,0BAA2B,2GAA4G,KAAMR,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAM,0JAA2J,KAAMQ,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAM,kJAAmJ,MAAO,KAAMQ,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,qUAAsU,KAAMa,EAAAA,cAAoBhB,EAAYS,IAAK,KAAMO,EAAAA,cAAoBhB,EAAYU,GAAI,CAC5wDO,GAAI,sCACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,uCACN,aAAc,gDACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,uCAAwC,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,wUAA6U,KAAMa,EAAAA,cAAoBhB,EAAYI,GAAI,CACjda,GAAI,kCACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,mCACN,aAAc,4CACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,mCAAoC,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAMa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,WAAY,uBAAwBR,EAAAA,cAAoBW,EAAAA,EAAO,CAC1LD,KAAM,eACJ,gFAAiFV,EAAAA,cAAoBW,EAAAA,EAAO,CAC9GD,KAAM,YACJ,kBAAmBV,EAAAA,cAAoBW,EAAAA,EAAO,CAChDD,KAAM,YACJ,iBAAkBV,EAAAA,cAAoBW,EAAAA,EAAO,CAC/CD,KAAM,YACJ,gBAAiBV,EAAAA,cAAoBW,EAAAA,EAAO,CAC9CD,KAAM,uBACJ,uBAAwBV,EAAAA,cAAoBW,EAAAA,EAAO,CACrDD,KAAM,eACJ,gCAAiC,KAAMV,EAAAA,cAAoBW,EAAAA,EAAO,CACpED,KAAM,sEACJ,KAAMV,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,SAAU,KAAMa,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBW,EAAAA,EAAO,CACtLD,KAAM,eACJ,kCAAmCV,EAAAA,cAAoBW,EAAAA,EAAO,CAChED,KAAM,YACJ,KAAM,KAAMV,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAM,iDAAkD,KAAMQ,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBW,EAAAA,EAAO,CACrLD,KAAM,eACJ,kFAAmF,MAAO,KAAMV,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,wFAAyFa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,wBAAyB,6RAA8R,KAAMR,EAAAA,cAAoBhB,EAAYG,EAAG,KAAMa,EAAAA,cAAoBhB,EAAYW,OAAQ,KAAM,8BAA+B,QAASK,EAAAA,cAAoBW,EAAAA,EAAO,CAChuBD,KAAM,eACJ,sDAAuDV,EAAAA,cAAoBW,EAAAA,EAAO,CACpFD,KAAM,YACJ,6LAA8LV,EAAAA,cAAoBW,EAAAA,EAAO,CAC3ND,KAAM,eACJ,kIAAmI,KAAMV,EAAAA,cAAoBhB,EAAYY,GAAI,CAC/KK,GAAI,sCACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,uCACN,aAAc,gDACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,uCAAwC,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,2CAA4Ca,EAAAA,cAAoBW,EAAAA,EAAO,CACjKD,KAAM,YACJ,4DAA6DV,EAAAA,cAAoBW,EAAAA,EAAO,CAC1FD,KAAM,YACJ,OAAQV,EAAAA,cAAoBW,EAAAA,EAAO,CACrCD,KAAM,iFACJ,iKAAkKV,EAAAA,cAAoBW,EAAAA,EAAO,CAC/LD,KAAM,YACJ,KAAM,KAAMV,EAAAA,cAAoBhB,EAAYY,GAAI,CAClDK,GAAI,kCACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,mCACN,aAAc,4CACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,mCAAoC,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,4EAA+Ea,EAAAA,cAAoBW,EAAAA,EAAO,CAChMD,KAAM,YACJ,OAAQV,EAAAA,cAAoBW,EAAAA,EAAO,CACrCD,KAAM,wBACJ,0SAA6S,KAAMV,EAAAA,cAAoBhB,EAAYI,GAAI,CACzVa,GAAI,0CACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,2CACN,aAAc,oDACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,2CAA4C,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAMa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,2BAA4B,aAAcR,EAAAA,cAAoBW,EAAAA,EAAO,CACxMD,KAAM,kBACJ,4CAA6CV,EAAAA,cAAoBW,EAAAA,EAAO,CAC1ED,KAAM,YACJ,gDAAiDV,EAAAA,cAAoBW,EAAAA,EAAO,CAC9ED,KAAM,YACJ,uBAAwB,KAAMV,EAAAA,cAAoBW,EAAAA,EAAO,CAC3DD,KAAM,+CACJ,KAAMV,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,oBAAqB,KAAMa,EAAAA,cAAoBW,EAAAA,EAAO,CACvGD,KAAM,oHACJ,KAAMV,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBW,EAAAA,EAAO,CAC7HD,KAAM,kBACJ,4BAA6BV,EAAAA,cAAoBW,EAAAA,EAAO,CAC1DD,KAAM,YACJ,QAASV,EAAAA,cAAoBW,EAAAA,EAAO,CACtCD,KAAM,YACJ,KAAM,KAAMV,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBW,EAAAA,EAAO,CACnFD,KAAM,kBACJ,qCAAsCV,EAAAA,cAAoBW,EAAAA,EAAO,CACnED,KAAM,mBACJ,iCAAkC,MAAO,KAAMV,EAAAA,cAAoBhB,EAAYG,EAAG,KAAMa,EAAAA,cAAoBhB,EAAYW,OAAQ,KAAM,kBAAmB,QAASK,EAAAA,cAAoBW,EAAAA,EAAO,CAC/LD,KAAM,kBACJ,UAAWV,EAAAA,cAAoBW,EAAAA,EAAO,CACxCD,KAAM,YACJ,QAASV,EAAAA,cAAoBW,EAAAA,EAAO,CACtCD,KAAM,YACJ,qHAAsHV,EAAAA,cAAoBW,EAAAA,EAAO,CACnJD,KAAM,gCACJ,KAAM,KAAMV,EAAAA,cAAoBhB,EAAYY,GAAI,CAClDK,GAAI,mBACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,oBACN,aAAc,6BACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,oBAAqB,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,4CAA6Ca,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,oBAAqB,IAAKR,EAAAA,cAAoBW,EAAAA,EAAO,CAC9MD,KAAM,YACJ,iJAAkJV,EAAAA,cAAoBW,EAAAA,EAAO,CAC/KD,KAAM,YACJ,eAAgBV,EAAAA,cAAoBW,EAAAA,EAAO,CAC7CD,KAAM,YACJ,KAAM,KAAMV,EAAAA,cAAoBW,EAAAA,EAAO,CACzCD,KAAM,wCACJ,KAAMV,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,gIAAiIa,EAAAA,cAAoBW,EAAAA,EAAO,CAC7MD,KAAM,YACJ,gDAAiDV,EAAAA,cAAoBW,EAAAA,EAAO,CAC9ED,KAAM,YACJ,6EAA8E,KAAMV,EAAAA,cAAoBhB,EAAYI,GAAI,CAC1Ha,GAAI,8BACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,+BACN,aAAc,wCACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,+BAAgC,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAMa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,+BAAgC,+FAAgGR,EAAAA,cAAoBW,EAAAA,EAAO,CAClRD,KAAM,YACJ,2CAA4CV,EAAAA,cAAoBW,EAAAA,EAAO,CACzED,KAAM,YACJ,iCAAkC,KAAMV,EAAAA,cAAoBW,EAAAA,EAAO,CACrED,KAAM,oGACJ,KAAMV,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAM,oBAAqBQ,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,OAAQ,2BAA4BR,EAAAA,cAAoBW,EAAAA,EAAO,CAC3ND,KAAM,8DACJ,KAAM,KAAMV,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAM,8CAA+CQ,EAAAA,cAAoBW,EAAAA,EAAO,CAClID,KAAM,gBACJ,gBAAiB,MAAO,KAAMV,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,mDAAoD,KAAMa,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,yBAA0B,iBAAkBR,EAAAA,cAAoBW,EAAAA,EAAO,CACzUD,KAAM,qDACJ,oEAAqEV,EAAAA,cAAoBW,EAAAA,EAAO,CAClGD,KAAM,uBACJ,gCAAiCV,EAAAA,cAAoBW,EAAAA,EAAO,CAC9DD,KAAM,qBACJ,KAAM,KAAMV,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,oBAAqB,gLAAiL,KAAMR,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,qCAAsC,2JAA4J,MAAO,KAAMR,EAAAA,cAAoBhB,EAAYI,GAAI,CAC3mBa,GAAI,2CACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,4CACN,aAAc,qDACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,4CAA6C,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAMa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,sBAAuB,oHAAqHR,EAAAA,cAAoBW,EAAAA,EAAO,CAC3SD,KAAM,uBACJ,mBAAoBV,EAAAA,cAAoBW,EAAAA,EAAO,CACjDD,KAAM,kBACJ,KAAM,KAAMV,EAAAA,cAAoBW,EAAAA,EAAO,CACzCD,KAAM,uJACJ,KAAMV,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAM,0HAA2H,KAAMQ,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAM,MAAOQ,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,6BAA8B,uKAAwK,MAAO,KAAMR,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,oQAAqQa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,4BAA6B,wBAAyB,KAAMR,EAAAA,cAAoBhB,EAAYS,IAAK,KAAMO,EAAAA,cAAoBhB,EAAYU,GAAI,CAC1+BO,GAAI,0CACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,2CACN,aAAc,oDACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,6CAA8C,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,qVAAsV,KAAMa,EAAAA,cAAoBhB,EAAYI,GAAI,CAChea,GAAI,oCACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,qCACN,aAAc,8CACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,qCAAsC,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,MAAOa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,0BAA2B,+KAAgLR,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,0BAA2B,0HAA2HR,EAAAA,cAAoBW,EAAAA,EAAO,CACriBD,KAAM,oBACJ,0DAA2D,KAAMV,EAAAA,cAAoBW,EAAAA,EAAO,CAC9FD,KAAM,qHACJ,KAAMV,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,UAAW,KAAMa,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBW,EAAAA,EAAO,CACvLD,KAAM,iBACJ,+CAAgDV,EAAAA,cAAoBW,EAAAA,EAAO,CAC7ED,KAAM,eACJ,KAAM,KAAMV,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBW,EAAAA,EAAO,CACnFD,KAAM,iBACJ,mBAAoB,KAAMV,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBW,EAAAA,EAAO,CACjGD,KAAM,0BACJ,sEAAuE,KAAMV,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBW,EAAAA,EAAO,CACpJD,KAAM,iBACJ,4EAA6E,MAAO,KAAMV,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,wYAA2Y,KAAMa,EAAAA,cAAoBhB,EAAYI,GAAI,CAC1jBa,GAAI,6BACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,8BACN,aAAc,uCACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,8BAA+B,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,4TAA6T,KAAMa,EAAAA,cAAoBhB,EAAYG,EAAG,KAAMa,EAAAA,cAAoBhB,EAAYW,OAAQ,KAAM,4BAA6B,4JAA6JK,EAAAA,cAAoBW,EAAAA,EAAO,CAChsBD,KAAM,iBACJ,8LAA+L,KAAMV,EAAAA,cAAoBhB,EAAYI,GAAI,CAC3Oa,GAAI,qBACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,sBACN,aAAc,+BACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,sBAAuB,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAMa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,iBAAkB,6EAA8ER,EAAAA,cAAoBW,EAAAA,EAAO,CACzOD,KAAM,YACJ,4EAA6EV,EAAAA,cAAoBW,EAAAA,EAAO,CAC1GD,KAAM,YACJ,2BAA4B,KAAMV,EAAAA,cAAoBW,EAAAA,EAAO,CAC/DD,KAAM,uDACJ,KAAMV,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,sCAAuCa,EAAAA,cAAoBW,EAAAA,EAAO,CACnHD,KAAM,kBACJ,gIAAiIV,EAAAA,cAAoBW,EAAAA,EAAO,CAC9JD,KAAM,eACJ,mCAAoCV,EAAAA,cAAoBW,EAAAA,EAAO,CACjED,KAAM,iCACJ,KAAM,KAAMV,EAAAA,cAAoBW,EAAAA,EAAO,CACzCD,KAAM,uDACJ,KAAMV,EAAAA,cAAoBhB,EAAYG,EAAG,KAAMa,EAAAA,cAAoBhB,EAAYW,OAAQ,KAAM,eAAgB,2WAA4W,KAAMK,EAAAA,cAAoBhB,EAAYI,GAAI,CACrgBa,GAAI,kBACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,mBACN,aAAc,4BACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,mBAAoB,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAMa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,mBAAoB,uEAAwER,EAAAA,cAAoBW,EAAAA,EAAO,CAClOD,KAAM,YACJ,+CAAgD,KAAMV,EAAAA,cAAoBW,EAAAA,EAAO,CACnFD,KAAM,0EACJ,KAAMV,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,OAAQa,EAAAA,cAAoBW,EAAAA,EAAO,CACpFD,KAAM,cACJ,oDAAqDV,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,0BAA2B,mIAAoIR,EAAAA,cAAoBW,EAAAA,EAAO,CACtRD,KAAM,YACJ,0JAA2J,KAAMV,EAAAA,cAAoBhB,EAAYS,IAAK,KAAMO,EAAAA,cAAoBhB,EAAYU,GAAI,CAClPO,GAAI,8DACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,+DACN,aAAc,wEACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,+DAAgE,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,gEAAiEa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,sBAAuB,oIAAqI,KAAMR,EAAAA,cAAoBhB,EAAYI,GAAI,CAC9Za,GAAI,6BACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,8BACN,aAAc,uCACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,8BAA+B,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,6BAA8Ba,EAAAA,cAAoBW,EAAAA,EAAO,CAC1ID,KAAM,kBACJ,0CAA2CV,EAAAA,cAAoBW,EAAAA,EAAO,CACxED,KAAM,YACJ,oDAAqDV,EAAAA,cAAoBW,EAAAA,EAAO,CAClFD,KAAM,YACJ,6DAA8DV,EAAAA,cAAoBW,EAAAA,EAAO,CAC3FD,KAAM,YACJ,iCAAkCV,EAAAA,cAAoBW,EAAAA,EAAO,CAC/DD,KAAM,YACJ,mBAAoB,KAAMV,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,qBAAsB,wCAAyCR,EAAAA,cAAoBW,EAAAA,EAAO,CACrPD,KAAM,2BACJ,yCAA0CV,EAAAA,cAAoBW,EAAAA,EAAO,CACvED,KAAM,kCACJ,qDAAsD,KAAMV,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,2BAA4B,qFAAsFR,EAAAA,cAAoBW,EAAAA,EAAO,CAC1RD,KAAM,YACJ,gLAAiL,MAAO,KAAMV,EAAAA,cAAoBY,EAAAA,EAAM,CAC1NF,KAAM,uRAQJ,KAAMV,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,mIAAoIa,EAAAA,cAAoBhB,EAAYM,KAAM,CAC3NgB,wBAAyB,CACvBC,OAAQ,4DAER,sHAAuH,KAAMP,EAAAA,cAAoBhB,EAAYI,GAAI,CACnKa,GAAI,yCACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,0CACN,aAAc,mDACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,0CAA2C,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAMa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,4BAA6B,2DAA4DR,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,WAAY,mPAAoPR,EAAAA,cAAoBW,EAAAA,EAAO,CAC3hBD,KAAM,YACJ,gBAAiBV,EAAAA,cAAoBW,EAAAA,EAAO,CAC9CD,KAAM,YACJ,0CAA2CV,EAAAA,cAAoBW,EAAAA,EAAO,CACxED,KAAM,kBACJ,gFAAiF,KAAMV,EAAAA,cAAoBhB,EAAYI,GAAI,CAC7Ha,GAAI,kCACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,mCACN,aAAc,4CACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,mCAAoC,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,0CAA2Ca,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,2BAA4B,QAASR,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,sBAAuB,iGAAkGR,EAAAA,cAAoBW,EAAAA,EAAO,CACpYD,KAAM,kBACJ,kIAAmI,KAAMV,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,0ZAA2Z,KAAMa,EAAAA,cAAoBhB,EAAYS,IAAK,KAAMO,EAAAA,cAAoBhB,EAAYU,GAAI,CACpqBO,GAAI,uCACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,wCACN,aAAc,iDACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,wCAAyC,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,OAAQa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,oCAAqC,mKAAoK,KAAMR,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAM,6BAA8BQ,EAAAA,cAAoBW,EAAAA,EAAO,CAC1eD,KAAM,YACJ,0BAA2BV,EAAAA,cAAoBW,EAAAA,EAAO,CACxDD,KAAM,YACJ,KAAM,KAAMV,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAM,oDAAqDQ,EAAAA,cAAoBW,EAAAA,EAAO,CACxID,KAAM,YACJ,mDAAoDV,EAAAA,cAAoBW,EAAAA,EAAO,CACjFD,KAAM,YACJ,eAAgBV,EAAAA,cAAoBW,EAAAA,EAAO,CAC7CD,KAAM,YACJ,mCAAoC,MAAO,KAAMV,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,oCAAqC,KAAMa,EAAAA,cAAoBW,EAAAA,EAAO,CAClKD,KAAM,gEACJ,KAAMV,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,UAAW,KAAMa,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBW,EAAAA,EAAO,CACvLD,KAAM,kBACJ,4HAA6H,KAAMV,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBW,EAAAA,EAAO,CAC1MD,KAAM,kBACJ,6HAA8H,KAAMV,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBW,EAAAA,EAAO,CAC3MD,KAAM,iBACJ,mDAAoD,MAAO,KAAMV,EAAAA,cAAoBhB,EAAYI,GAAI,CACvGa,GAAI,mCACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,oCACN,aAAc,6CACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,oCAAqC,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,4JAA6Ja,EAAAA,cAAoBW,EAAAA,EAAO,CAC/QD,KAAM,YACJ,8DAA+DV,EAAAA,cAAoBW,EAAAA,EAAO,CAC5FD,KAAM,YACJ,2EAA4EV,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,yBAA4B,6GAA8G,KAAMR,EAAAA,cAAoBhB,EAAYI,GAAI,CACvSa,GAAI,iCACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,kCACN,aAAc,2CACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,kCAAmC,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAMa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,2BAA4B,8BAA+BR,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,kBAAmB,8WAA+W,KAAMR,EAAAA,cAAoBhB,EAAYI,GAAI,CACtoBa,GAAI,gDACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,iDACN,aAAc,0DACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,iDAAkD,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,gZAAiZ,KAAMa,EAAAA,cAAoBhB,EAAYS,IAAK,KAAMO,EAAAA,cAAoBhB,EAAYU,GAAI,CAC1kBO,GAAI,qCACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,sCACN,aAAc,+CACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,sCAAuC,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,gGAAiGa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,+BAAgC,yIAA0I,KAAMR,EAAAA,cAAoBhB,EAAYI,GAAI,CACnba,GAAI,wBACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,yBACN,aAAc,kCACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,yBAA0B,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,0DAA2Da,EAAAA,cAAoBW,EAAAA,EAAO,CAClKD,KAAM,yBACJ,mCAAoCV,EAAAA,cAAoBW,EAAAA,EAAO,CACjED,KAAM,kBACJ,eAAgBV,EAAAA,cAAoBW,EAAAA,EAAO,CAC7CD,KAAM,YACJ,iDAAkDV,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,yBAA0B,4DAA6DR,EAAAA,cAAoBW,EAAAA,EAAO,CAC3MD,KAAM,qBACJ,kBAAmB,KAAMV,EAAAA,cAAoBW,EAAAA,EAAO,CACtDD,KAAM,oEACJ,KAAMV,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,iCAAkCa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,mCAAoC,+HAAgI,KAAMR,EAAAA,cAAoBhB,EAAYI,GAAI,CACtUa,GAAI,oBACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,qBACN,aAAc,8BACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,qBAAsB,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,MAAOa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,qBAAsB,6BAA8BR,EAAAA,cAAoBW,EAAAA,EAAO,CACnMD,KAAM,iBACJ,wBAAyBV,EAAAA,cAAoBW,EAAAA,EAAO,CACtDD,KAAM,iBACJ,4FAA6FV,EAAAA,cAAoBW,EAAAA,EAAO,CAC1HD,KAAM,qCACJ,OAAQV,EAAAA,cAAoBW,EAAAA,EAAO,CACrCD,KAAM,qCACJ,uEAAwEV,EAAAA,cAAoBW,EAAAA,EAAO,CACrGD,KAAM,2BACJ,oHAAqH,KAAMV,EAAAA,cAAoBhB,EAAYI,GAAI,CACjKa,GAAI,0CACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,2CACN,aAAc,oDACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,2CAA4C,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,sQAAuQ,KAAMa,EAAAA,cAAoBW,EAAAA,EAAO,CACtYD,KAAM,8FACJ,KAAMV,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,sJAAuJ,KAAMa,EAAAA,cAAoBhB,EAAYS,IAAK,KAAMO,EAAAA,cAAoBhB,EAAYU,GAAI,CAC7RO,GAAI,gDACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,iDACN,aAAc,0DACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,iDAAkD,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAMa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,0BAA2B,iVAAkV,KAAMR,EAAAA,cAAoBhB,EAAYI,GAAI,CAChiBa,GAAI,kCACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,mCACN,aAAc,4CACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,mCAAoC,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,2CAA4C,KAAMa,EAAAA,cAAoBW,EAAAA,EAAO,CACnKD,KAAM,mIACJ,KAAMV,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,UAAW,KAAMa,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBW,EAAAA,EAAO,CACvLD,KAAM,YACJ,4BAA6B,KAAMV,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBW,EAAAA,EAAO,CAC1GD,KAAM,mBACJ,sCAAuC,KAAMV,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBW,EAAAA,EAAO,CACpHD,KAAM,4BACJ,wDAAyD,KAAMV,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBW,EAAAA,EAAO,CACtID,KAAM,YACJ,4BAA6B,MAAO,KAAMV,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,iCAAkCa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,iBAAkB,QAASR,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,gBAAiB,uGAAwG,KAAMR,EAAAA,cAAoBhB,EAAYI,GAAI,CAC/Xa,GAAI,8BACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,+BACN,aAAc,wCACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,+BAAgC,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,kCAAmCa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,wBAAyB,sVAAuV,KAAMR,EAAAA,cAAoBhB,EAAYI,GAAI,CACpjBa,GAAI,gCACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,iCACN,aAAc,0CACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,iCAAkC,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,4CAA6Ca,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,sBAAuB,mEAAoER,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,sBAAuB,+OAAgP,KAAMR,EAAAA,cAAoBhB,EAAYI,GAAI,CACvlBa,GAAI,mBACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,oBACN,aAAc,6BACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,oBAAqB,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAMa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,oBAAqB,uVAAwV,KAAMR,EAAAA,cAAoBhB,EAAYS,IAAK,KAAMO,EAAAA,cAAoBhB,EAAYU,GAAI,CAC9iBO,GAAI,+CACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,gDACN,aAAc,yDACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,gDAAiD,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,uOAAwO,KAAMa,EAAAA,cAAoBhB,EAAYI,GAAI,CACrXa,GAAI,uCACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,wCACN,aAAc,iDACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,wCAAyC,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,qHAAwHa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,eAAgB,mGAAoG,KAAMR,EAAAA,cAAoBW,EAAAA,EAAO,CAC7YD,KAAM,iGACJ,KAAMV,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,wLAAyL,KAAMa,EAAAA,cAAoBhB,EAAYI,GAAI,CACpRa,GAAI,+BACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,gCACN,aAAc,yCACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,gCAAiC,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAMa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,2BAA4B,0GAA2GR,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,oBAAqB,gIAAiIR,EAAAA,cAAoBW,EAAAA,EAAO,CACrdD,KAAM,+DACJ,kKAAmK,KAAMV,EAAAA,cAAoBhB,EAAYI,GAAI,CAC/Ma,GAAI,gCACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,iCACN,aAAc,0CACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,iCAAkC,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAMa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,iCAAkC,wXAAyX,KAAMR,EAAAA,cAAoBhB,EAAYS,IAAK,KAAMO,EAAAA,cAAoBhB,EAAYU,GAAI,CACzmBO,GAAI,kDACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,mDACN,aAAc,4DACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,mDAAoD,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,mQAAoQ,KAAMa,EAAAA,cAAoBhB,EAAYI,GAAI,CACpZa,GAAI,mCACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,oCACN,aAAc,6CACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,oCAAqC,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,MAAOa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,0BAA2B,6QAA8QR,EAAAA,cAAoBW,EAAAA,EAAO,CACvcD,KAAM,wDACJ,OAAQV,EAAAA,cAAoBW,EAAAA,EAAO,CACrCD,KAAM,oDACJ,mIAAoI,KAAMV,EAAAA,cAAoBhB,EAAYI,GAAI,CAChLa,GAAI,uBACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,wBACN,aAAc,iCACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,wBAAyB,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAMa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,aAAc,gZAAiZ,KAAMR,EAAAA,cAAoBhB,EAAYI,GAAI,CACzjBa,GAAI,uCACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,wCACN,aAAc,iDACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,wCAAyC,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,MAAOa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,kBAAmB,oPAAqPR,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,yBAA0B,yLAA4LR,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,wBAAyB,OAAQR,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,WAAY,4GAA6G,KAAMR,EAAAA,cAAoBhB,EAAYS,IAAK,KAAMO,EAAAA,cAAoBhB,EAAYU,GAAI,CACn8BO,GAAI,kBACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,mBACN,aAAc,4BACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,mBAAoB,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,sLAAuL,KAAMa,EAAAA,cAAoBhB,EAAYI,GAAI,CACvSa,GAAI,0DACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,2DACN,aAAc,oEACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,4DAA6D,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,sBAAuBa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,0BAA2B,meAAoe,KAAMR,EAAAA,cAAoBhB,EAAYI,GAAI,CACptBa,GAAI,4CACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,6CACN,aAAc,sDACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,6CAA8C,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,0DAA2Da,EAAAA,cAAoBW,EAAAA,EAAO,CACtLD,KAAM,eACJ,KAAMV,EAAAA,cAAoBW,EAAAA,EAAO,CACnCD,KAAM,iBACJ,QAASV,EAAAA,cAAoBW,EAAAA,EAAO,CACtCD,KAAM,iCACJ,sDAAuDV,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,qBAAsB,wBAAyBR,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,+CAAgD,2IAA4IR,EAAAA,cAAoBW,EAAAA,EAAO,CACzYD,KAAM,YACJ,mHAAoHV,EAAAA,cAAoBW,EAAAA,EAAO,CACjJD,KAAM,iBACJ,KAAM,KAAMV,EAAAA,cAAoBhB,EAAYI,GAAI,CAClDa,GAAI,oCACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,qCACN,aAAc,8CACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,uCAAwC,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAMa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,uCAAwC,ieAAke,KAAMR,EAAAA,cAAoBhB,EAAYS,IAAK,KAAMO,EAAAA,cAAoBhB,EAAYU,GAAI,CAC9tBO,GAAI,QACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,SACN,aAAc,kBACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,SAAU,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,2LAA4L,KAAMa,EAAAA,cAAoBhB,EAAYI,GAAI,CAClSa,GAAI,wCACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,yCACN,aAAc,kDACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,yCAA0C,KAAMP,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,eAAgB,sKAAuK,KAAMR,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,WAAY,8CAA+CR,EAAAA,cAAoBS,EAAAA,EAAS,CACthBC,KAAM,iIACJ,KAAM,KAAMV,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,gBAAiB,4BAA6BR,EAAAA,cAAoBhB,EAAYM,KAAM,CACjLgB,wBAAyB,CACvBC,OAAQ,0DAER,QAASP,EAAAA,cAAoBhB,EAAYM,KAAM,CACjDgB,wBAAyB,CACvBC,OAAQ,4DAER,yDAA0D,MAAO,KAAMP,EAAAA,cAAoBhB,EAAYI,GAAI,CAC7Ga,GAAI,4BACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,6BACN,aAAc,sCACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,6BAA8B,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,wHAAyH,KAAMa,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,4BAA6B,oFAAqF,KAAMR,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,kBAAmB,iEAAkE,KAAMR,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,mCAAoC,mFAAoFR,EAAAA,cAAoBW,EAAAA,EAAO,CACl1BD,KAAM,kBACJ,KAAM,MAAO,KAAMV,EAAAA,cAAoBY,EAAAA,EAAM,CAC/CF,KAAM,wVAUJ,KAAMV,EAAAA,cAAoBhB,EAAYI,GAAI,CAC5Ca,GAAI,oEACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,qEACN,aAAc,8EACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,sEAAuE,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,8SAA+S,KAAMa,EAAAA,cAAoBhB,EAAYa,GAAI,KAAM,KAAMG,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYW,OAAQ,KAAM,mBAAoB,sEAAuE,KAAMK,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYW,OAAQ,KAAM,iBAAkB,kHAAmH,KAAMK,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYW,OAAQ,KAAM,iBAAkB,sBAAuBK,EAAAA,cAAoBW,EAAAA,EAAO,CACthCD,KAAM,cACJ,cAAeV,EAAAA,cAAoBW,EAAAA,EAAO,CAC5CD,KAAM,oBACJ,qDAAsD,KAAMV,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYW,OAAQ,KAAM,uBAAwB,uFAAwFK,EAAAA,cAAoBW,EAAAA,EAAO,CACjSD,KAAM,YACJ,KAAM,KAAMV,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYW,OAAQ,KAAM,kBAAmB,gHAAiH,MAAO,KAAMK,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,0TAA2T,KAAMa,EAAAA,cAAoBhB,EAAYS,IAAK,KAAMO,EAAAA,cAAoBhB,EAAYG,EAAG,KAAMa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,gBAAiB,oeAAqe,KAAMR,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,yIAA0I,KAAMa,EAAAA,cAAoBhB,EAAYS,IAAK,KAAMO,EAAAA,cAAoBhB,EAAYU,GAAI,CAC99CO,GAAI,sDACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,uDACN,aAAc,gEACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,uDAAwD,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,qdAAsd,KAAMa,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,6MAA8M,KAAMa,EAAAA,cAAoBhB,EAAYI,GAAI,CACv2Ba,GAAI,8CACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,+CACN,aAAc,wDACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,gDAAiD,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,uBAAwBa,EAAAA,cAAoBW,EAAAA,EAAO,CACtJD,KAAM,iCACJ,qDAAsDV,EAAAA,cAAoBW,EAAAA,EAAO,CACnFD,KAAM,YACJ,yBAA0BV,EAAAA,cAAoBW,EAAAA,EAAO,CACvDD,KAAM,YACJ,sCAAuCV,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,iBAAkB,KAAM,KAAMR,EAAAA,cAAoBW,EAAAA,EAAO,CACvID,KAAM,sFACJ,KAAMV,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,4BAA6Ba,EAAAA,cAAoBW,EAAAA,EAAO,CACzGD,KAAM,YACJ,kDAAmDV,EAAAA,cAAoBW,EAAAA,EAAO,CAChFD,KAAM,2BACJ,qBAAsBV,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,kBAAmB,gBAAiBR,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,SAAU,qBAAsBR,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,4BAA6B,gIAAiI,KAAMR,EAAAA,cAAoBhB,EAAYG,EAAG,KAAMa,EAAAA,cAAoBhB,EAAYW,OAAQ,KAAM,6BAA8B,8DAA+DK,EAAAA,cAAoBW,EAAAA,EAAO,CAC9jBD,KAAM,mBACJ,aAAcV,EAAAA,cAAoBW,EAAAA,EAAO,CAC3CD,KAAM,YACJ,sKAAuKV,EAAAA,cAAoBW,EAAAA,EAAO,CACpMD,KAAM,gBACJ,kCAAmC,KAAMV,EAAAA,cAAoBhB,EAAYI,GAAI,CAC/Ea,GAAI,kCACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,wCACN,aAAc,4CACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,oCAAqC,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAMa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,iBAAkB,yDAA0D,KAAMR,EAAAA,cAAoBW,EAAAA,EAAO,CACzOD,KAAM,8EACJ,KAAMV,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,SAAUa,EAAAA,cAAoBW,EAAAA,EAAO,CACtFD,KAAM,kBACJ,8BAA+BV,EAAAA,cAAoBW,EAAAA,EAAO,CAC5DD,KAAM,yBACJ,0FAA2FV,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,oBAAqB,gBAAiBR,EAAAA,cAAoBW,EAAAA,EAAO,CACnMD,KAAM,oBACJ,oNAAqN,KAAMV,EAAAA,cAAoBhB,EAAYI,GAAI,CACjQa,GAAI,oCACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,qCACN,aAAc,8CACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,sCAAuC,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,OAAQa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,4BAA6B,gPAAiP,KAAMR,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,+BAAgC,+EAAgF,KAAMR,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,sBAAuB,qJAAsJ,MAAO,KAAMR,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,iGAAkGa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,mBAAoB,OAAQR,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,qBAAsB,uNAAwN,KAAMR,EAAAA,cAAoBhB,EAAYI,GAAI,CACj6Ca,GAAI,uDACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,wDACN,aAAc,iEACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,yDAA0D,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,2CAA4Ca,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,yCAA0C,wHAAyHR,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,eAAgB,wTAAyT,KAAMR,EAAAA,cAAoBhB,EAAYG,EAAG,KAAMa,EAAAA,cAAoBhB,EAAYW,OAAQ,KAAM,eAAgB,qZAAsZ,KAAMK,EAAAA,cAAoBhB,EAAYI,GAAI,CAC3vCa,GAAI,+BACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,gCACN,aAAc,yCACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,iCAAkC,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAMa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,8BAA+B,eAAgBR,EAAAA,cAAoBW,EAAAA,EAAO,CACnMD,KAAM,kBACJ,WAAYV,EAAAA,cAAoBW,EAAAA,EAAO,CACzCD,KAAM,kBACJ,wEAAyEV,EAAAA,cAAoBW,EAAAA,EAAO,CACtGD,KAAM,YACJ,4BAA6BV,EAAAA,cAAoBW,EAAAA,EAAO,CAC1DD,KAAM,kBACJ,0CAA2CV,EAAAA,cAAoBW,EAAAA,EAAO,CACxED,KAAM,kBACJ,wNAA2N,KAAMV,EAAAA,cAAoBhB,EAAYI,GAAI,CACvQa,GAAI,wCACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,yCACN,aAAc,kDACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,0CAA2C,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,yBAA0Ba,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,sBAAuB,mBAAoBR,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,uBAAwB,6eAA8e,KAAMR,EAAAA,cAAoBhB,EAAYI,GAAI,CAC5xBa,GAAI,gDACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,iDACN,aAAc,0DACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,kDAAmD,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,wCAAyCa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,wBAAyB,8OAA+OR,EAAAA,cAAoBW,EAAAA,EAAO,CACtdD,KAAM,+BACJ,2EAA4EV,EAAAA,cAAoBW,EAAAA,EAAO,CACzGD,KAAM,YACJ,oJAAqJ,KAAMV,EAAAA,cAAoBhB,EAAYI,GAAI,CACjMa,GAAI,mDACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,oDACN,aAAc,6DACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,qDAAsD,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,8DAA+Da,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,gBAAiB,KAAMR,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,yBAA0B,QAASR,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,uBAAwB,4CAA6CR,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,+CAAgD,KAAM,KAAMR,EAAAA,cAAoBW,EAAAA,EAAO,CACjhBD,KAAM,mFACJ,KAAMV,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,SAAUa,EAAAA,cAAoBW,EAAAA,EAAO,CACtFD,KAAM,wBACJ,sFAAuFV,EAAAA,cAAoBW,EAAAA,EAAO,CACpHD,KAAM,YACJ,sMAAuMV,EAAAA,cAAoBW,EAAAA,EAAO,CACpOD,KAAM,wBACJ,gEAAiE,KAAMV,EAAAA,cAAoBhB,EAAYG,EAAG,KAAMa,EAAAA,cAAoBhB,EAAYW,OAAQ,KAAM,qBAAsB,mGAAoGK,EAAAA,cAAoBW,EAAAA,EAAO,CACrTD,KAAM,wBACJ,oMAAqM,KAAMV,EAAAA,cAAoBhB,EAAYI,GAAI,CACjPa,GAAI,wCACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,yCACN,aAAc,kDACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,0CAA2C,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,OAAQa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,oCAAqC,oNAAqNR,EAAAA,cAAoBW,EAAAA,EAAO,CAC/ZD,KAAM,uBACJ,6FAA8FV,EAAAA,cAAoBW,EAAAA,EAAO,CAC3HD,KAAM,sBACJ,4MAA6M,KAAMV,EAAAA,cAAoBhB,EAAYS,IAAK,KAAMO,EAAAA,cAAoBhB,EAAYU,GAAI,CACpSO,GAAI,sDACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,uDACN,aAAc,gEACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,uDAAwD,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,wOAAyO,KAAMa,EAAAA,cAAoBhB,EAAYI,GAAI,CAC7Xa,GAAI,uDACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,wDACN,aAAc,iEACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,2DAA4D,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,yJAA4Ja,EAAAA,cAAoBW,EAAAA,EAAO,CACrSD,KAAM,sBACJ,sQAAuQ,KAAMV,EAAAA,cAAoBhB,EAAYI,GAAI,CACnTa,GAAI,yCACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,0CACN,aAAc,mDACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,2CAA4C,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,mMAAoMa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,aAAc,OAAQR,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,aAAc,gNAAiN,KAAMR,EAAAA,cAAoBhB,EAAYI,GAAI,CAC3oBa,GAAI,yBACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,0BACN,aAAc,mCACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,2BAA4B,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,qCAAsCa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,wBAAyB,wNAAyNR,EAAAA,cAAoBW,EAAAA,EAAO,CACtaD,KAAM,yDACJ,yOAA0O,KAAMV,EAAAA,cAAoBhB,EAAYS,IAAK,KAAMO,EAAAA,cAAoBhB,EAAYU,GAAI,CACjUO,GAAI,mDACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,oDACN,aAAc,6DACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,oDAAqD,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,uJAAwJ,KAAMa,EAAAA,cAAoBhB,EAAYI,GAAI,CACzSa,GAAI,uCACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,wCACN,aAAc,iDACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,wCAAyC,KAAMP,EAAAA,cAAoBY,EAAAA,EAAM,CAC5EF,KAAM,ofAoBJ,KAAMV,EAAAA,cAAoBhB,EAAYI,GAAI,CAC5Ca,GAAI,oEACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,qEACN,aAAc,8EACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,qEAAsE,KAAMP,EAAAA,cAAoBY,EAAAA,EAAM,CACzGF,KAAM,0tDAsDJ,KAAMV,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,oDAAqDa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,2BAA4B,0LAA2L,KAAMR,EAAAA,cAAoBhB,EAAYS,IAAK,KAAMO,EAAAA,cAAoBhB,EAAYU,GAAI,CACvbO,GAAI,gDACJC,MAAO,CACLC,SAAU,aAEXH,EAAAA,cAAoBhB,EAAYK,EAAG,CACpCe,KAAM,iDACN,aAAc,0DACdC,UAAW,iBACVL,EAAAA,cAAoBhB,EAAYM,KAAM,CACvCgB,wBAAyB,CACvBC,OAAQ,meAEP,iDAAkD,KAAMP,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,6JAA8J,KAAMa,EAAAA,cAAoBhB,EAAYO,GAAI,KAAM,KAAMS,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYW,OAAQ,KAAM,uBAAwB,kOAAmO,KAAMK,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYW,OAAQ,KAAM,sBAAuB,kCAAmCK,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,0BAA2B,OAAQR,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,2BAA4B,iJAAkJ,KAAMR,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYW,OAAQ,KAAM,wBAAyB,iQAAkQ,KAAMK,EAAAA,cAAoBhB,EAAYQ,GAAI,KAAMQ,EAAAA,cAAoBhB,EAAYW,OAAQ,KAAM,6BAA8B,sCAAuCK,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,+CAAgD,qMAAsM,MAAO,KAAMR,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,iLAAkLa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,sBAAuB,mVAAoV,KAAMR,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,iSAAkSa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,sBAAuB,gHAAiH,KAAMR,EAAAA,cAAoBhB,EAAYG,EAAG,KAAM,8FAA+Fa,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,qBAAsB,KAAMR,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,sBAAuB,QAASR,EAAAA,cAAoBQ,EAAAA,EAAW,KAAM,2CAA4C,8gBACp1G,CAKA,MAJA,SAAoBzB,QAAK,IAALA,IAAAA,EAAQ,CAAC,GAC3B,MAAO8B,QAASC,GAAa7B,OAAOC,OAAO,CAAC,GAAGY,EAAAA,EAAAA,MAAsBf,EAAMgB,YAC3E,OAAOe,EAAYd,EAAAA,cAAoBc,EAAW/B,EAAOiB,EAAAA,cAAoBlB,EAAmBC,IAAUD,EAAkBC,EAC9H,E,iKCzzCA,MAAMgC,EAAkBC,IAAW,IAAV,IAACC,GAAID,EAC5B,IAAKC,IAAQA,EAAIC,MAAO,OAAO,KAY/B,OAAOlB,EAAAA,cAAoB,MAAO,CAChCK,UAAWc,EAAAA,GACVnB,EAAAA,cAAoB,KAAM,KAAMiB,EAAIC,MAAME,KAAI,CAACC,EAAMC,IAAUtB,EAAAA,cAAoB,KAAM,CAC1FuB,IAAKD,GACJtB,EAAAA,cAAoB,IAAK,CAC1BI,KAAMiB,EAAKG,IACXC,QAASC,GAjBSC,EAACD,EAAGF,KACtBE,EAAEE,iBACF,MAAMC,EAAWL,EAAIM,QAAQ,IAAK,IAC5BC,EAAgBC,SAASC,eAAeJ,GAC1CE,GACFA,EAAcG,eAAe,CAC3BC,SAAU,SACVC,MAAO,SAEX,EAQcT,CAAYD,EAAGL,EAAKG,MACjCH,EAAKgB,OAAQhB,EAAKH,OAASlB,EAAAA,cAAoBe,EAAiB,CACjEE,IAAK,CACHC,MAAOG,EAAKH,aAEV,EAED,SAASoB,EAAYC,GAAiD,IAA/CC,MAAM,IAACC,EAAG,OAAEC,EAAM,cAAEC,GAAc,SAAEC,GAASL,EACzE,MAAM,YAACM,EAAW,KAAEC,EAAI,gBAAEC,GAAmBN,EACvCnB,EAAQuB,EAAYvB,MAEpB0B,EADOH,EAAYI,KACJC,MAAM,KAAK,GAE1BC,EADQT,EAAOU,MAAMC,QAAOC,GAAQA,EAAKT,YAAYI,KAAKM,SAAS,IAAIP,QACnDQ,MAAK,CAACnE,EAAGoE,IAAMpE,EAAEwD,YAAYvB,MAAQmC,EAAEZ,YAAYvB,QACvEoC,EAAeP,EAAYQ,WAAUL,GAAQA,EAAKT,YAAYvB,QAAUA,IACxEsC,EAAWT,EAAYO,EAAe,GACtCG,EAAWV,EAAYO,EAAe,GACtCI,EAAcjB,EAAYI,KAAKnB,QAAQ,MAAO,IAC9CiC,EAAc,SAAUC,KAAKF,GAAa,GAC1CG,EAAW,SAASjB,aAAmBe,MACvC,EAACG,EAAY,EAAEC,IAAmBC,EAAAA,EAAAA,UAASvB,EAAYwB,0BACvD,EAACC,EAAW,EAAEC,IAAkBH,EAAAA,EAAAA,WAAS,GAS/C,IAAII,GALJC,EAAAA,EAAAA,YAAU,KACRF,GAAe,GACf,MAAMG,EAAQC,YAAW,IAAMJ,GAAe,IAAQ,KACtD,MAAO,IAAMK,aAAaF,EAAM,GAC/B,CAACR,IAEY,eAAZlB,EACFwB,EAAiBK,EAAAA,GACI,aAAZ7B,EACTwB,EAAiBM,EAAAA,GACI,aAAZ9B,IACTwB,EAAiBO,EAAAA,IAEnB,MACMC,EADgBC,IAAenC,GAAMhB,QAAQ,wBAAyB,IAAIA,QAAQ,SAAU,IAAIA,QAAQ,wBAAyB,IAAIoD,OAC3GhC,MAAM,OAAOiC,OAIvCC,EA5ER,SAAwBC,GACtB,GAAIA,GAAW,GAAI,MAAO,UAC1B,GAAIA,GAAW,GAAI,MAAO,UAC1B,GAAIA,GAAW,GAAI,MAAO,UAC1B,GAAIA,GAAW,GAAI,MAAO,UAC1B,GAAIA,GAAW,GAAI,MAAO,UAC1B,GAAIA,GAAW,GAAI,MAAO,OAC1B,MAAMC,EAAQC,KAAKC,MAAMH,EAAU,IAC7BI,EAAYJ,EAAU,GAC5B,OAAII,GAAa,GACR,IAAIH,IAAQG,EAAY,EAAI,KAAO,OAErC,IAAIH,EAAQ,KACrB,CA+DmBI,CAHWH,KAAKI,KAAKX,EAAYR,IAChC3B,EAAY+C,kBAAoB,IAG5CC,EAAU,CAAC,CACfC,KAAMjD,EAAYkD,UAClBC,UAAWA,IAAM,0DAChB,CACDF,KAAMjD,EAAYoD,gBAClBD,UAAWA,IAAM,0DAChB,CACDF,KAAMjD,EAAYqD,YAClBF,UAAWA,IAAM,0DAChB,CACDF,KAAMjD,EAAYsD,cAClBH,UAAWA,IAAM,0DAChB,CACDF,KAAMjD,EAAYuD,YAClBJ,UAAWA,IAAM,0DAChB,CACDF,KAAMjD,EAAYwD,iBAClBL,UAAWA,IAAM,0DAChB,CACDF,KAAMjD,EAAYyD,eAClBN,UAAWA,IAAM,yDAChB,CACDF,KAAMjD,EAAY0D,cAClBP,UAAWA,IAAM,0DAChB,CACDF,KAAMjD,EAAY2D,kBAClBR,UAAWA,IAAM,yDAChB,CACDF,KAAMjD,EAAY4D,WAClBT,UAAWA,IAAM,4DAEb,EAACU,EAAa,EAAEC,IAAoBvC,EAAAA,EAAAA,UAAS,IAUnD,OATAK,EAAAA,EAAAA,YAAU,KACRoB,EAAQe,SAAQC,IAAuB,IAAtB,KAACf,EAAI,UAAEE,GAAUa,EAC5Bf,GACFE,IAAYc,MAAKC,IACfJ,GAAiBK,GAAQ,GAAJC,QAAAC,EAAAA,EAAAA,GAAQF,GAAI,CAAED,EAAOI,WAAS,GAEvD,GACA,GACD,IACInH,EAAAA,cAAoBoH,EAAAA,EAAOC,IAAK,CACrCC,QAAS,CACPC,QAAS,GAEXC,QAAS,CACPD,QAAS,GAEXE,KAAM,CACJF,QAAS,GAEXG,WAAY,CACVC,SAAU,MAEX3H,EAAAA,cAAoB4H,EAAAA,EAAY,CACjCC,WAAYhF,EAAYvB,MACxBwG,KAAMjF,EAAYiF,KAClBC,QAASlF,EAAYkF,QACrB3C,SAAUA,EACV4C,WAAYnF,EAAYoF,gBACxB5F,MAAOQ,EAAYR,MACnB6F,KAAMrF,EAAYqF,KAClBC,OAAQtF,EAAYsF,OACpBnF,QAASA,EACToF,QAASrE,EACTsE,cAAexF,EAAYoD,gBAC3BqC,QAASzF,EAAYyF,UACnBtI,EAAAA,cAAoB,MAAO,CAC7BE,MAAO,CACLqI,QAAS,OACTC,eAAgB,WAChBC,SAAU,OACVC,SAAU,MACVC,WAAY,OACZC,aAAc,MACdC,UAAW,OACXC,aAAc,QAEfjG,EAAYkG,UAAU3H,KAAI,CAAC4H,EAAK1H,IAAUtB,EAAAA,cAAoB,OAAQ,CACvEuB,IAAKD,EACLjB,UAAW,YAAY4I,EAAAA,KACvB/I,MAAO,CACLgJ,OAAQ,gBAETF,MAAQhJ,EAAAA,cAAoB,MAAO,CACpCmJ,MAAO,YACNnJ,EAAAA,cAAoBe,EAAiB,CACtCE,IAAK8B,KACF/C,EAAAA,cAAoB,MAAOA,EAAAA,cAAoB,MAAO,CACzDE,MAAO,CACLgJ,OAAQ,iBACRE,UAAW,UAEZpJ,EAAAA,cAAoBoH,EAAAA,EAAOiC,OAAQ,CACpCF,MAAO,WACP9I,UAAWiJ,EAAAA,GACXrJ,GAAIqJ,EAAAA,GACJ7H,QAvHmB8H,KACnBpF,GAAiBD,EAAa,EAuH9BsF,SAAU,CACRC,MAAO,MAERzJ,EAAAA,cAAoBoH,EAAAA,EAAOC,IAAK,CACjChH,UAAWqJ,EAAAA,GACXnI,IAAK2C,EACLoD,QAAS,CACPC,QAAS,GAEXC,QAAS,CACPD,QAAS,GAEXE,KAAM,CACJF,QAAS,GAEXG,WAAY,CACVC,SAAU,GACVgC,KAAM,cAEPzF,EAAe,2BAA6B,2BAA4BlE,EAAAA,cAAoB,MAAOA,EAAAA,cAAoB,MAAO,CAC/HmJ,MAAO,WACPjJ,MAAO,CACLgJ,OAAQhF,EAAe,SAAW,GAClCwE,SAAUxE,EAAe,OAAS,GAClCwD,WAAY,uDAEb1H,EAAAA,cAAoB,MAAO,CAC5BK,UAAW,GAAGiJ,EAAAA,MAAuChF,EAAcgF,EAAAA,GAAkCA,EAAAA,MACpG5C,EAActF,KAAI,CAACwI,EAAiBtI,IAAUtB,EAAAA,cAAoB4J,EAAiB,CACpFrI,IAAKD,MACFuB,EAAYgH,YAAc7J,EAAAA,cAAoB8J,EAAAA,EAAoB,CACrExI,MAAOuB,EAAYgH,YACnBE,SAAUlH,EAAYmH,qBACnB,GAAIhK,EAAAA,cAAoBiK,EAAAA,EAAaC,SAAU,CAClDC,MAAO,CACLC,OAAQzH,EAAcS,MACtBa,SAAUA,EAASnC,QAAQ,MAAO,IAAM,MAEzC9B,EAAAA,cAAoBqK,EAAAA,GAAa,CAClCtK,WAAY,CACVuK,MAAKA,EAAAA,IAEN1H,MAAc5C,EAAAA,cAAoBuK,EAAAA,EAAY,CAC/C3G,SAAUA,EACVC,SAAUA,EACVE,WAAYA,EACZf,QAASA,IAEb,CAEe,SAASwH,EAAiBzL,GACvC,OAAOiB,EAAAA,cAAoBsC,EAAcvD,EAAOiB,EAAAA,cAAoByK,EAAqB1L,GAC3F,CACO,SAAS2L,EAAIC,GAAS,IAAAC,EAAAC,EAAAC,EAAAC,EAAAC,EAAA,IAAR,KAACxI,GAAKmI,EACzB,MAAM,YAAC9H,GAAeL,EAAKC,IACrBJ,EAAQQ,EAAYoI,UAAYpI,EAAYR,MAC5C6I,EAAUrI,EAAYqI,SAAW7I,EACjC8I,EAAetI,EAAYsI,cAAgB9I,EAC3C+I,EAAcvI,EAAYwI,SAAWxI,EAAYqF,KACjDoD,EAAgBzI,EAAY0I,QAAUH,EACtCI,EAAqB3I,EAAY4I,aAAeL,EAChDM,EAAa7I,EAAY6I,YAAc,cACvCC,EAAW9I,EAAY+I,YACvBC,EAAgBhJ,EAAYiF,KAC5BgE,EAAejJ,EAAYkF,SAAW8D,EACtCE,EAAUlJ,EAAYkJ,UAA6B,QAAtBnB,EAAI/H,EAAYsF,cAAM,IAAAyC,GAAiB,QAAjBC,EAAlBD,EAAoBoB,uBAAe,IAAAnB,GAAiB,QAAjBC,EAAnCD,EAAqCoB,uBAAe,IAAAnB,GAAQ,QAARC,EAApDD,EAAsDV,cAAM,IAAAW,GAAU,QAAVC,EAA5DD,EAA8DmB,gBAAQ,IAAAlB,OAApD,EAAlBA,EAAwEmB,KACzGC,EAAavJ,EAAYuJ,YAAcd,EACvCe,EAAexJ,EAAYwJ,cAAgBN,EAC3CO,EAAkBzJ,EAAYyJ,iBAAmBd,EACjDe,EAAe1J,EAAY2J,aAC3B/F,EAAa5D,EAAY4D,aAAc,EACvC6B,EAAUzF,EAAYyF,SAAW,QACjCtF,EAAUH,EAAYI,KAAKC,MAAM,KAAK,IAAM,SAE5C,QAACuJ,IAAWC,EAAAA,EAAAA,KACZC,EAAiB,CACrB,WAAY,qBACZ,QAAS,iBACT,gBAAmB,CAAC,CAClB,QAAS,WACT,SAAY,EACZ,KAAQ,OACR,KAAQF,GACP,CACD,QAAS,WACT,SAAY,EACZ,KAAQnE,EACR,KAAQ,GAAGmE,KAAW5J,EAAYI,KAAKC,MAAM,KAAK,MACjD,CACD,QAAS,WACT,SAAY,EACZ,KAAQb,EACR,KAAQ,GAAGoK,IAAU5J,EAAYI,UAGrC,OAAOjD,EAAAA,cAAoB4M,EAAAA,EAAK,CAC9BvK,MAAOA,EAAQ,gBACf6I,QAASA,EACTC,aAAcA,EACdC,YAAaA,EACbE,cAAeA,EACfE,mBAAoBA,EACpBE,WAAYA,EACZC,SAAUA,EACVE,cAAeA,EACfC,aAAcA,EACdC,QAASA,EACTK,WAAYA,EACZC,aAAcA,EACdC,gBAAiBA,EACjBC,aAAcA,EACd9F,WAAYA,EACZ6B,QAASA,EACTtF,QAASA,EACT6J,KAzCW,WA0CV7M,EAAAA,cAAoB,SAAU,CAC/B6M,KAAM,uBACLC,KAAKC,UAAUJ,IACpB,C,iFCnTWK,EAAc,oCC+BzB,MA/BgBhM,IAA8B,IAA7B,KAAEN,EAAI,QAAEuM,GAAQ,GAAOjM,EACpC,MAAM,EAACkM,EAAO,EAACC,IAAa/I,EAAAA,EAAAA,WAAS,GAC/BgJ,GAAaC,EAAAA,EAAAA,QAAO,MAmB1B,OAZA5I,EAAAA,EAAAA,YAAU,KACN,SAAS6I,EAAmBC,GACpBH,EAAWI,UAAYJ,EAAWI,QAAQC,SAASF,EAAMG,SACzDP,GAAU,EAElB,CAEA,OADAnL,SAAS2L,iBAAiB,QAASL,GAC5B,KACHtL,SAAS4L,oBAAoB,QAASN,EAAmB,CAC5D,GACF,IAGCtN,EAAAA,cAAA,QAAMK,UDvBc,uCCuBoBwN,IAAKT,GACzCpN,EAAAA,cAAA,OAAKC,GAAIgN,EDzBE,kCADL,6BC0B6Cd,IAAK2B,EAAAA,EAAMC,IAAI,OAAOtM,QAnBxDC,IACrBA,EAAEsM,kBACFb,GAAWnG,IAAUA,GAAK,IAkBtBhH,EAAAA,cAAA,QAAMK,UAAW6M,EAAS,GAAGe,kCAAyCA,GACjEvN,GAEF,C,iDCtBf,IALUM,IAAe,IAAd,KAAEN,GAAMM,EACjB,OACEhB,EAAAA,cAACW,EAAAA,EAAK,KAAED,EAAa,C","sources":["webpack://avrtt.blog/./src/images/goals/info.svg","webpack://avrtt.blog/./src/pages/posts/research/information_theory_for_ml.mdx","webpack://avrtt.blog/./src/templates/post.js","webpack://avrtt.blog/./src/components/Tooltip/styles.module.scss","webpack://avrtt.blog/./src/components/Tooltip/index.js","webpack://avrtt.blog/./src/components/Latex/index.js"],"sourcesContent":["export default \"data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iaXNvLTg4NTktMSI/Pgo8IS0tIEdlbmVyYXRvcjogQWRvYmUgSWxsdXN0cmF0b3IgMTYuMC4wLCBTVkcgRXhwb3J0IFBsdWctSW4gLiBTVkcgVmVyc2lvbjogNi4wMCBCdWlsZCAwKSAgLS0+CjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+CjxzdmcgdmVyc2lvbj0iMS4xIiBpZD0iQ2FwYV8xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB4PSIwcHgiIHk9IjBweCIKCSB3aWR0aD0iNDE2Ljk3OXB4IiBoZWlnaHQ9IjQxNi45NzlweCIgdmlld0JveD0iMCAwIDQxNi45NzkgNDE2Ljk3OSIgc3R5bGU9ImVuYWJsZS1iYWNrZ3JvdW5kOm5ldyAwIDAgNDE2Ljk3OSA0MTYuOTc5OyIKCSB4bWw6c3BhY2U9InByZXNlcnZlIj4KPGc+Cgk8cGF0aCBkPSJNMzU2LjAwNCw2MS4xNTZjLTgxLjM3LTgxLjQ3LTIxMy4zNzctODEuNTUxLTI5NC44NDgtMC4xODJjLTgxLjQ3LDgxLjM3MS04MS41NTIsMjEzLjM3OS0wLjE4MSwyOTQuODUKCQljODEuMzY5LDgxLjQ3LDIxMy4zNzgsODEuNTUxLDI5NC44NDksMC4xODFDNDM3LjI5MywyNzQuNjM2LDQzNy4zNzUsMTQyLjYyNiwzNTYuMDA0LDYxLjE1NnogTTIzNy42LDM0MC43ODYKCQljMCwzLjIxNy0yLjYwNyw1LjgyMi01LjgyMiw1LjgyMmgtNDYuNTc2Yy0zLjIxNSwwLTUuODIyLTIuNjA1LTUuODIyLTUuODIyVjE2Ny44ODVjMC0zLjIxNywyLjYwNy01LjgyMiw1LjgyMi01LjgyMmg0Ni41NzYKCQljMy4yMTUsMCw1LjgyMiwyLjYwNCw1LjgyMiw1LjgyMlYzNDAuNzg2eiBNMjA4LjQ5LDEzNy45MDFjLTE4LjYxOCwwLTMzLjc2Ni0xNS4xNDYtMzMuNzY2LTMzLjc2NQoJCWMwLTE4LjYxNywxNS4xNDctMzMuNzY2LDMzLjc2Ni0zMy43NjZjMTguNjE5LDAsMzMuNzY2LDE1LjE0OCwzMy43NjYsMzMuNzY2QzI0Mi4yNTYsMTIyLjc1NSwyMjcuMTA3LDEzNy45MDEsMjA4LjQ5LDEzNy45MDF6Ii8+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPC9zdmc+Cg==\"","/*@jsxRuntime classic @jsx React.createElement @jsxFrag React.Fragment*/\n/**(intro: a quote, catchphrase, joke, etc.)**/\n/*\n\nhttps://www.geeksforgeeks.org/information-theory-in-machine-learning/\n\n*/\n/*\n\n1. Introduction\n- Overview of Information Theory: Introduce Claude Shannon's foundational work, entropy, and mutual information as measures of uncertainty and information transfer.\n- Relevance to Machine Learning: Explain how information theory underpins learning algorithms, compression, and decision-making in intelligent agents.\n- Objective of the Article: Bridge theoretical concepts (e.g., entropy, KL divergence) to practical applications in autonomous systems and AI.\n2. Core Concepts of Information Theory\n- Entropy and Shannon Information: Define entropy as a measure of uncertainty, with examples in data encoding and probabilistic modeling.\n- Mutual Information and Channel Capacity: Describe how mutual information quantifies dependencies between variables, critical for feature selection.\n- Kullback-Leibler Divergence: Explain KL divergence as a measure of distribution mismatch, used in model training and reinforcement learning.\n- Fisher Information for parametric models and its role in optimization.\n- etc.\n3. Entropy, Uncertainty, and Decision-Making\n- Entropy in Reinforcement Learning: Explore entropy regularization for policy optimization, balancing exploration and exploitation.\n- Uncertainty Quantification: Discuss entropy's role in Bayesian neural networks and uncertainty-aware decision-making for safe AI.\n- Cross-Entropy Loss: Link cross-entropy minimization to classification tasks and its limitations in overconfident predictions.\n- Tsallis Entropy for generalized uncertainty measures in reinforcement learning.\n- etc.\n4. Mutual Information for Feature Selection and Representation\n- Feature Relevance Analysis: Use mutual information to identify non-linear dependencies between input features and target outputs.\n- Information Maximization in Clustering: Apply mutual information for unsupervised representation learning (e.g., InfoGAN).\n- Multi-Agent Information Sharing: Optimize communication bandwidth by transmitting only mutually informative signals between agents.\n5. The Information Bottleneck Principle\n- Theory of Optimal Representation: Formulate the trade-off between compression and relevance in deep neural networks.\n- Applications in Explainability: Analyze how bottleneck layers discard irrelevant information, aiding interpretability.\n- Dynamic Bottlenecks in Reinforcement Learning: Use adaptive compression for efficient state representation in changing environments.\n6. KL Divergence and Model Adaptation\n- Variational Inference: Derive variational autoencoders (VAEs) using KL divergence to approximate posterior distributions.\n- Domain Adaptation: Minimize divergence between source and target domains for robust transfer learning.\n- Policy Alignment in Multi-Agent Systems: Align agent behaviors by minimizing KL divergence between policy distributions.\n7. Rate-Distortion Theory and Efficient Learning\n- Trade-offs in Lossy Compression: Frame model compression (e.g., pruning, quantization) as a rate-distortion optimization problem.\n- Resource-Constrained Agents: Apply rate-distortion to prioritize critical information in edge AI and IoT devices.\n- Perceptual Distortion Metrics: Integrate human perceptual models with information-theoretic distortion measures.\n- Bits-back coding and its use in lossless compression with VAEs.\n8. Information-Theoretic Reinforcement Learning\n- Empowerment and Intrinsic Motivation: Define empowerment as the maximum mutual information between actions and future states.\n- Curiosity-Driven Exploration: Use prediction error (information gain) as a reward signal for exploring novel states.\n- Information-Directed Sampling: Optimize policies by reducing uncertainty about environment dynamics.\n9. Multi-Agent Systems and Distributed Information\n- Emergent Communication Protocols: Use information theory to evolve efficient languages in cooperative agent populations.\n- Consensus Algorithms: Minimize distributed KL divergence for agreement in decentralized decision-making.\n- Information Asymmetry in Game Theory: Model strategic interactions where agents have unequal access to critical information.\n- Information cascades and herding effects in decentralized learning.\n10. Advanced topics\n- Information Bottleneck in Deep Learning: Recent research on how neural networks implicitly optimize the information bottleneck principle, compressing input data while preserving predictive power.\n- Neural Estimation of Information Measures: Techniques like Mutual Information Neural Estimation (MINE) for high-dimensional data.\n- Integrated Information Theory (IIT): Connections to consciousness in AI and its implications for modeling complex systems.\n11. Tools\n- Software for Information-Theoretic ML: Overview of libraries (e.g., `ITE toolbox`, `PyITLib`, `sklearn`'s mutual information functions).\n- Estimating Entropy and MI: Challenges in high-dimensional spaces and practical workarounds (e.g., binning, k-nearest neighbors).\n- Case Study: Applying mutual information for feature selection in a real-world dataset (e.g., healthcare or finance).\n\n*/\nimport {useMDXComponents as _provideComponents} from \"@mdx-js/react\";\nimport React from \"react\";\nimport Tooltip from \"../../../components/Tooltip\";\nimport Highlight from \"../../../components/Highlight\";\nimport Code from \"../../../components/Code\";\nimport Latex from \"../../../components/Latex\";\nfunction _createMdxContent(props) {\n  const _components = Object.assign({\n    p: \"p\",\n    h3: \"h3\",\n    a: \"a\",\n    span: \"span\",\n    ul: \"ul\",\n    li: \"li\",\n    hr: \"hr\",\n    h2: \"h2\",\n    strong: \"strong\",\n    h4: \"h4\",\n    ol: \"ol\"\n  }, _provideComponents(), props.components);\n  return React.createElement(React.Fragment, null, \"\\n\", React.createElement(\"br\"), \"\\n\", \"\\n\", \"\\n\", React.createElement(_components.p, null, \"Information theory, conceived by Claude Shannon in his seminal 1948 paper \\\"\\\"A Mathematical Theory of Communication\\\"\\\" (Shannon, Bell System Technical Journal, 1948), provides the mathematical foundations for quantifying uncertainty, information content, and the efficiency of data transmission. The birth of this field revolutionized the way engineers and scientists think about data representation, communication channels, and the very nature of information itself. Since then, information-theoretic concepts  such as entropy, mutual information, Kullback-Leibler divergence, and channel capacity  have permeated virtually every corner of technology, from digital communications to cryptography, and, as we will emphasize in this article, machine learning.\"), \"\\n\", React.createElement(_components.p, null, \"Machine learning is all about extracting useful patterns from data, making decisions under uncertainty, and building models that generalize well to novel situations. Although machine learning can often appear as a purely algorithmic discipline, information theory plays an essential role beneath many learning algorithms. Whether in supervised learning, unsupervised representation learning, or complex multi-agent reinforcement learning tasks, core information-theoretic quantities arise time and again to quantify uncertainty, measure predictive performance, and drive the optimization of model parameters.\"), \"\\n\", React.createElement(_components.p, null, \"Information theory offers a language and a toolkit to address fundamental questions in machine learning: How much information about the output labels do our features carry? How can we reduce uncertainty in model predictions through data acquisition? Why do certain regularizers, based on divergences like the Kullback-Leibler divergence, appear in objective functions? How does entropy encourage exploration in reinforcement learning? What does the concept of channel capacity tell us about feature selection in resource-constrained settings?\"), \"\\n\", React.createElement(_components.p, null, \"In this article, I dive into these and many related questions by examining the theoretical backbone of information theory. I explore the fundamental concepts of entropy, mutual information, and divergences, and illustrate their deep ties to practical machine learning applications. I also spotlight advanced topics such as the information bottleneck principle, the role of information measures in multi-agent and reinforcement learning settings, and various ways to estimate or approximate these measures in high-dimensional data scenarios.\"), \"\\n\", React.createElement(_components.p, null, \"Ultimately, my goal is to help you see information theory not as an esoteric subfield limited to theoretical research, but rather as a vibrant, unifying set of principles that can elevate your understanding and practice of machine learning. For readers seeking to bridge the gap between the classical theory  as laid out by Shannon  and cutting-edge AI research, I provide insights into how these quantitative measures of information can be harnessed in everything from deep learning to distributed multi-agent systems.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"overview-of-information-theory\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#overview-of-information-theory\",\n    \"aria-label\": \"overview of information theory permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Overview of information theory\"), \"\\n\", React.createElement(_components.p, null, \"Information theory measures the content of a message, the uncertainty present in random variables, and the capacity of channels to transmit information without loss. In short, it gives us a rigorous framework to answer questions like: \\\"How many bits are needed to describe a random variable's outcome?\\\" or \\\"How correlated are two random variables?\\\" or even \\\"How close is one probability distribution to another?\\\"\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, \"\\n\", React.createElement(_components.p, null, React.createElement(Highlight, null, \"Claude Shannon\"), \"'s contribution: Shannon proposed that the amount of information conveyed by a random variable is linked to its unpredictability, or uncertainty. He introduced the concept of \", React.createElement(Highlight, null, \"entropy\"), \" to quantify this uncertainty in terms of bits.\"), \"\\n\"), \"\\n\", React.createElement(_components.li, null, \"\\n\", React.createElement(_components.p, null, \"The concept of \", React.createElement(Highlight, null, \"channel capacity\"), \" tells us the maximum rate of information that can be reliably transmitted over a noisy channel. This concept, while originally developed for telecommunications, is surprisingly relevant to learning algorithms that can be viewed as data channels between input features and model outputs.\"), \"\\n\"), \"\\n\", React.createElement(_components.li, null, \"\\n\", React.createElement(_components.p, null, React.createElement(Highlight, null, \"Mutual information\"), \" is a measure of how much knowing one variable reduces our uncertainty about another. This measure is central to feature selection, data compression, and representation learning in machine learning contexts.\"), \"\\n\"), \"\\n\", React.createElement(_components.li, null, \"\\n\", React.createElement(_components.p, null, React.createElement(Highlight, null, \"Information theory in ML\"), \": Because training a model can be viewed as compressing or encoding data about the environment (inputs) into weights or model parameters, it is natural that information theory has emerged as one of the key theoretical lenses on machine learning.\"), \"\\n\"), \"\\n\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"relevance-to-machine-learning\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#relevance-to-machine-learning\",\n    \"aria-label\": \"relevance to machine learning permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Relevance to machine learning\"), \"\\n\", React.createElement(_components.p, null, \"Information-theoretic quantities show up in many fundamental ML tasks:\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, \"\\n\", React.createElement(_components.p, null, React.createElement(Highlight, null, \"Data encoding and compression\"), \": In unsupervised learning, autoencoders compress data into a latent representation. Variational autoencoders (VAEs) explicitly rely on Kullback-Leibler divergences to measure how well the latent distribution matches the prior.\"), \"\\n\"), \"\\n\", React.createElement(_components.li, null, \"\\n\", React.createElement(_components.p, null, React.createElement(Highlight, null, \"Decision-making under uncertainty\"), \": Reinforcement learning systems often strive to balance exploration and exploitation. Some approaches introduce an \\\"entropy bonus\\\" to keep the agent from collapsing to deterministic policies too early, as higher entropy encourages exploration of actions.\"), \"\\n\"), \"\\n\", React.createElement(_components.li, null, \"\\n\", React.createElement(_components.p, null, React.createElement(Highlight, null, \"Feature selection\"), \": Mutual information has often been used as a criterion for picking the most informative features to explain a target variable. In high-dimensional datasets, measuring or approximating mutual information helps prune out irrelevant variables.\"), \"\\n\"), \"\\n\", React.createElement(_components.li, null, \"\\n\", React.createElement(_components.p, null, React.createElement(Highlight, null, \"Representation learning\"), \": The \", React.createElement(Tooltip, {\n    text: \"Information Bottleneck Principle proposes that networks learn to compress input data to preserve only task-relevant signals.\"\n  }), \" (Tishby and gang, 2000). This principle has been used to interpret the hidden layers of deep networks.\"), \"\\n\"), \"\\n\", React.createElement(_components.li, null, \"\\n\", React.createElement(_components.p, null, React.createElement(Highlight, null, \"Probabilistic modeling\"), \": Bayesian neural networks, domain adaptation, and multi-agent policy alignment all rely on various divergences (e.g., KL divergence, Jensen-Shannon divergence) to keep distributions aligned and consistent with prior knowledge or other agents' behaviors.\"), \"\\n\"), \"\\n\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"objective-of-the-article\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#objective-of-the-article\",\n    \"aria-label\": \"objective of the article permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Objective of the article\"), \"\\n\", React.createElement(_components.p, null, \"In this article, I pursue a comprehensive exploration of major information-theoretic concepts and link them to a broad range of machine learning applications. My purpose is to offer intuitive explanations supplemented with relevant mathematical definitions, theoretical underpinnings, and references to cutting-edge developments. By the end, you will see how to:\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, \"Interpret key quantities such as entropy, mutual information, and divergences in the context of classification, clustering, and reinforcement learning.\"), \"\\n\", React.createElement(_components.li, null, \"Understand how the \", React.createElement(Highlight, null, \"information bottleneck\"), \" principle explains phenomena in deep neural networks, such as hidden-layer representation compression.\"), \"\\n\", React.createElement(_components.li, null, \"Harness information theory to guide feature selection, multi-agent communication, model compression (pruning and quantization), and domain adaptation.\"), \"\\n\", React.createElement(_components.li, null, \"Implement practical computations of information-theoretic measures in Python, including the intricacies of dealing with high-dimensional data.\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, \"I aim for a deeply technical yet approachable style so that professionals with advanced ML experience can discover new insights or solidify existing understanding. Throughout, I connect fundamental theory with real-world scenarios, bridging that often elusive gap between theoretical constructs and hands-on applications.\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.h2, {\n    id: \"core-concepts-of-information-theory\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#core-concepts-of-information-theory\",\n    \"aria-label\": \"core concepts of information theory permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Core concepts of information theory\"), \"\\n\", React.createElement(_components.p, null, \"Machine learning, at its root, involves mapping from input data to an output space, guided by an objective function. Information theory starts by quantifying what it means for data to \\\"contain\\\" or \\\"transmit\\\" knowledge. Below, I explore the core constructs of this discipline, explaining why they matter for machine learning.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"entropy-and-shannon-information\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#entropy-and-shannon-information\",\n    \"aria-label\": \"entropy and shannon information permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Entropy and shannon information\"), \"\\n\", React.createElement(_components.p, null, React.createElement(Highlight, null, \"Entropy\"), \", typically denoted \", React.createElement(Latex, {\n    text: \"\\\\(H(X)\\\\)\"\n  }), \", measures the uncertainty or average surprise of a discrete random variable \", React.createElement(Latex, {\n    text: \"\\\\(X\\\\)\"\n  }), \". Formally, if \", React.createElement(Latex, {\n    text: \"\\\\(X\\\\)\"\n  }), \" takes values \", React.createElement(Latex, {\n    text: \"\\\\(x\\\\)\"\n  }), \" in some set \", React.createElement(Latex, {\n    text: \"\\\\(\\\\mathcal{X}\\\\)\"\n  }), \" with probabilities \", React.createElement(Latex, {\n    text: \"\\\\(p(x)\\\\)\"\n  }), \", the entropy is defined as:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\nH(X) = - \\\\sum_{x \\\\in \\\\mathcal{X}} p(x) \\\\log_2 p(x).\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"Here:\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(Latex, {\n    text: \"\\\\(p(x)\\\\)\"\n  }), \" is the probability of outcome \", React.createElement(Latex, {\n    text: \"\\\\(x\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(_components.li, null, \"The logarithm is base 2, so the unit is bits.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(Latex, {\n    text: \"\\\\(H(X)\\\\)\"\n  }), \" is largest when each outcome is equally likely, implying maximal uncertainty.\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, \"In many ML contexts, we deal with continuous variables. The continuous analog is the \", React.createElement(Highlight, null, \"differential entropy\"), \", but it can have nuances (such as potentially being negative). Regardless, the discrete version of entropy is typically more intuitive and is widely used in classification tasks and decision trees (e.g., measuring the impurity of a node with the Shannon entropy or Gini impurity).\"), \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Intuition and significance\"), \": If \", React.createElement(Latex, {\n    text: \"\\\\(H(X)\\\\)\"\n  }), \" is high, we need more bits on average to describe \", React.createElement(Latex, {\n    text: \"\\\\(X\\\\)\"\n  }), \". In supervised learning, if the labels in a dataset are very uncertain, it suggests that the classification problem is challenging. In many algorithms, minimizing a function related to \", React.createElement(Latex, {\n    text: \"\\\\(H(X)\\\\)\"\n  }), \" or maximizing a function that reduces entropy (like maximizing the negative of it) can lead to better predictive performance.\"), \"\\n\", React.createElement(_components.h4, {\n    id: \"practical-example-in-classification\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#practical-example-in-classification\",\n    \"aria-label\": \"practical example in classification permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Practical example in classification\"), \"\\n\", React.createElement(_components.p, null, \"Suppose you have a dataset with a label \", React.createElement(Latex, {\n    text: \"\\\\(Y\\\\)\"\n  }), \" that takes three equally likely classes. The entropy of \", React.createElement(Latex, {\n    text: \"\\\\(Y\\\\)\"\n  }), \" is \", React.createElement(Latex, {\n    text: \"\\\\(H(Y) = -3 \\\\times (\\\\frac{1}{3}) \\\\log_2 (\\\\frac{1}{3}) \\\\approx 1.585\\\\)\"\n  }), \" bits. If your model can reduce the uncertainly about these classes significantly, you can say that your model is effectively capturing the information about \", React.createElement(Latex, {\n    text: \"\\\\(Y\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(_components.h4, {\n    id: \"shannon-information-of-an-event\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#shannon-information-of-an-event\",\n    \"aria-label\": \"shannon information of an event permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Shannon information of an event\"), \"\\n\", React.createElement(_components.p, null, \"The information content (also called the \\\"self-information\\\") of an event \", React.createElement(Latex, {\n    text: \"\\\\(x\\\\)\"\n  }), \" is \", React.createElement(Latex, {\n    text: \"\\\\(-\\\\log_2 p(x)\\\\)\"\n  }), \", meaning rare events carry more \\\"surprise\\\" or more information. This concept underscores the fundamental principle behind coding: it is more efficient to use short codes for frequent events and longer codes for rare events  an approach used in Huffman coding and other compression algorithms.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"mutual-information-and-channel-capacity\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#mutual-information-and-channel-capacity\",\n    \"aria-label\": \"mutual information and channel capacity permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Mutual information and channel capacity\"), \"\\n\", React.createElement(_components.p, null, React.createElement(Highlight, null, \"Mutual information (MI)\"), \", denoted \", React.createElement(Latex, {\n    text: \"\\\\(I(X; Y)\\\\)\"\n  }), \", measures how much knowing one variable \", React.createElement(Latex, {\n    text: \"\\\\(X\\\\)\"\n  }), \" reduces the uncertainty of another variable \", React.createElement(Latex, {\n    text: \"\\\\(Y\\\\)\"\n  }), \". It is defined as:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\nI(X; Y) = H(X) + H(Y) - H(X, Y),\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"or equivalently,\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\nI(X; Y) = \\\\sum_{x \\\\in \\\\mathcal{X}, y \\\\in \\\\mathcal{Y}} p(x, y) \\\\log_2 \\\\frac{p(x, y)}{p(x)p(y)}.\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(Latex, {\n    text: \"\\\\(H(X, Y)\\\\)\"\n  }), \" is the joint entropy of \", React.createElement(Latex, {\n    text: \"\\\\(X\\\\)\"\n  }), \" and \", React.createElement(Latex, {\n    text: \"\\\\(Y\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(_components.li, null, React.createElement(Latex, {\n    text: \"\\\\(p(x, y)\\\\)\"\n  }), \" is the joint distribution, while \", React.createElement(Latex, {\n    text: \"\\\\(p(x)p(y)\\\\)\"\n  }), \" is the product of marginals.\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Interpretation\"), \": If \", React.createElement(Latex, {\n    text: \"\\\\(I(X; Y)\\\\)\"\n  }), \" is 0, \", React.createElement(Latex, {\n    text: \"\\\\(X\\\\)\"\n  }), \" and \", React.createElement(Latex, {\n    text: \"\\\\(Y\\\\)\"\n  }), \" are independent, meaning knowledge of one does not tell you anything about the other. The maximum possible MI is \", React.createElement(Latex, {\n    text: \"\\\\(\\\\min\\\\{H(X), H(Y)\\\\}\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(_components.h4, {\n    id: \"channel-capacity\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#channel-capacity\",\n    \"aria-label\": \"channel capacity permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Channel capacity\"), \"\\n\", React.createElement(_components.p, null, \"In Shannon's channel coding theorem, the \", React.createElement(Highlight, null, \"channel capacity\"), \" \", React.createElement(Latex, {\n    text: \"\\\\(C\\\\)\"\n  }), \" of a noisy channel is the maximum rate at which information can be transmitted with arbitrarily small error. For a simple channel with input \", React.createElement(Latex, {\n    text: \"\\\\(X\\\\)\"\n  }), \" and output \", React.createElement(Latex, {\n    text: \"\\\\(Y\\\\)\"\n  }), \":\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\nC = \\\\max_{p(x)} I(X; Y).\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"We can interpret feature selection or representation learning in ML as searching for the distribution (or representation) of \", React.createElement(Latex, {\n    text: \"\\\\(X\\\\)\"\n  }), \" that maximizes the information about target \", React.createElement(Latex, {\n    text: \"\\\\(Y\\\\)\"\n  }), \" subject to constraints (like network capacity or computational budgets).\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"kullback-leibler-divergence\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#kullback-leibler-divergence\",\n    \"aria-label\": \"kullback leibler divergence permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Kullback-Leibler divergence\"), \"\\n\", React.createElement(_components.p, null, React.createElement(Highlight, null, \"Kullback-Leibler divergence\"), \" (KL divergence), also known as relative entropy, measures how one probability distribution \", React.createElement(Latex, {\n    text: \"\\\\(p\\\\)\"\n  }), \" diverges from a reference distribution \", React.createElement(Latex, {\n    text: \"\\\\(q\\\\)\"\n  }), \". For discrete distributions:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\nD_{KL}(p \\\\parallel q) = \\\\sum_{x \\\\in \\\\mathcal{X}} p(x) \\\\log_2 \\\\frac{p(x)}{q(x)}.\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, \"KL divergence is \", React.createElement(Highlight, null, \"not\"), \" symmetrical; typically \", React.createElement(Latex, {\n    text: \"\\\\(D_{KL}(p \\\\parallel q) \\\\neq D_{KL}(q \\\\parallel p)\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(_components.li, null, \"It is always non-negative and is 0 only if \", React.createElement(Latex, {\n    text: \"\\\\(p = q\\\\)\"\n  }), \" everywhere.\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, \"In machine learning, KL divergence shows up in:\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(Highlight, null, \"Variational inference\"), \": We minimize \", React.createElement(Latex, {\n    text: \"\\\\(D_{KL}(q(\\\\theta) \\\\parallel p(\\\\theta|D))\\\\)\"\n  }), \" or an equivalent term to approximate the posterior distribution \", React.createElement(Latex, {\n    text: \"\\\\(p(\\\\theta|D)\\\\)\"\n  }), \" with a simpler distribution \", React.createElement(Latex, {\n    text: \"\\\\(q(\\\\theta)\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(_components.li, null, React.createElement(Highlight, null, \"Policy alignment\"), \": In multi-agent RL, we can constrain each agent's policy to remain close to a prior or to another agent's policy distribution by minimizing the KL divergence between them.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(Highlight, null, \"Reinforcement learning objectives\"), \": Surrogate objectives in algorithms like Proximal Policy Optimization (PPO) incorporate KL divergence constraints to limit the size of policy updates.\"), \"\\n\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"fisher-information-for-parametric-models\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#fisher-information-for-parametric-models\",\n    \"aria-label\": \"fisher information for parametric models permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Fisher information for parametric models\"), \"\\n\", React.createElement(_components.p, null, React.createElement(Highlight, null, \"Fisher information\"), \" quantifies how sensitive a likelihood function is to changes in its parameters. Given a parametric distribution \", React.createElement(Latex, {\n    text: \"\\\\(p(x|\\\\theta)\\\\)\"\n  }), \" with parameter \", React.createElement(Latex, {\n    text: \"\\\\(\\\\theta\\\\)\"\n  }), \":\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\nF(\\\\theta) = \\\\mathbb{E}_{x \\\\sim p(x|\\\\theta)}\\\\left[\\\\nabla_\\\\theta \\\\log p(x|\\\\theta)\\\\,\\\\nabla_\\\\theta \\\\log p(x|\\\\theta)^T\\\\right].\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, \"Fisher information is a matrix that describes how much information a single observation provides about the parameters.\"), \"\\n\", React.createElement(_components.li, null, \"In \", React.createElement(Highlight, null, \"second-order optimization\"), \" techniques (e.g., Newton's method), the Hessian can be linked to the Fisher information matrix. Natural gradient methods specifically exploit the Fisher geometry.\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, \"In Bayesian statistics, the Fisher information also relates to the curvature of the posterior. In large-scale neural networks, approximations of the Fisher information matrix are sometimes used to create more robust and informed gradient updates (e.g., the \", React.createElement(Highlight, null, \"Natural Gradient Descent\"), \" approach by Amari).\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.h2, {\n    id: \"entropy-uncertainty-and-decision-making\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#entropy-uncertainty-and-decision-making\",\n    \"aria-label\": \"entropy uncertainty and decision making permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Entropy, uncertainty, and decision-making\"), \"\\n\", React.createElement(_components.p, null, \"Information theory provides an attractive way to reason about decisions under uncertainty, which is the lifeblood of many machine learning applications. Whether it's a reinforcement learning agent exploring a vast environment or a classification model dealing with uncertain labels, entropy-based measures can guide the learning process.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"entropy-in-reinforcement-learning\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#entropy-in-reinforcement-learning\",\n    \"aria-label\": \"entropy in reinforcement learning permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Entropy in reinforcement learning\"), \"\\n\", React.createElement(_components.p, null, \"In \", React.createElement(Highlight, null, \"reinforcement learning\"), \" (RL), an agent must balance exploring unknown actions with exploiting actions that have yielded high reward in the past. One popular technique to encourage exploration is \", React.createElement(Highlight, null, \"entropy regularization\"), \". The idea is to add an entropy term to the objective function, such that the agent is rewarded for keeping its policy \", React.createElement(Latex, {\n    text: \"\\\\(\\\\pi(a|s)\\\\)\"\n  }), \" more stochastic. A typical objective might look like:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\nJ(\\\\theta) = \\\\mathbb{E}_{s \\\\sim d^\\\\pi, a \\\\sim \\\\pi_\\\\theta} [r(s,a)] + \\\\beta \\\\, H(\\\\pi_\\\\theta),\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"where:\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(Latex, {\n    text: \"\\\\(d^\\\\pi\\\\)\"\n  }), \" is the state distribution under the policy \", React.createElement(Latex, {\n    text: \"\\\\(\\\\pi\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(_components.li, null, React.createElement(Latex, {\n    text: \"\\\\(r(s,a)\\\\)\"\n  }), \" is the reward.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(Latex, {\n    text: \"\\\\(H(\\\\pi_\\\\theta)\\\\)\"\n  }), \" is the (Shannon) entropy of the policy distribution over actions.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(Latex, {\n    text: \"\\\\(\\\\beta\\\\)\"\n  }), \" is a hyperparameter controlling the strength of entropy regularization.\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, \"This method ensures that if the agent becomes overconfident about its \\\"best\\\" action, it pays a penalty in terms of reduced entropy. Hence, it keeps searching for alternative actions that might yield higher long-term returns. Methods like A3C (Asynchronous Advantage Actor-Critic) and PPO frequently rely on an entropy bonus to keep the policy sufficiently exploratory in the early stages.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"uncertainty-quantification\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#uncertainty-quantification\",\n    \"aria-label\": \"uncertainty quantification permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Uncertainty quantification\"), \"\\n\", React.createElement(_components.p, null, \"In real-world ML systems, especially in safety-critical domains like autonomous driving or healthcare, it is essential to measure the uncertainty of predictions. High entropy in a model's output distribution can indicate that the model is unsure, perhaps due to insufficient data or an out-of-distribution input.\"), \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Bayesian neural networks\"), \" approximate posterior distributions of weights, allowing them to produce predictive distributions that reflect their confidence. The predictive entropy \", React.createElement(Latex, {\n    text: \"\\\\(H(Y|X)\\\\)\"\n  }), \" can be used to decide whether the model's predictions are reliable. If the entropy is too high, the system can trigger a fallback plan, request more data, or ask a human for assistance.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"cross-entropy-loss\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#cross-entropy-loss\",\n    \"aria-label\": \"cross entropy loss permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Cross-entropy loss\"), \"\\n\", React.createElement(_components.p, null, React.createElement(Highlight, null, \"Cross-entropy\"), \" is ubiquitous in classification tasks. Given a ground truth distribution \", React.createElement(Latex, {\n    text: \"\\\\(p\\\\)\"\n  }), \" (often a one-hot vector in classification) and a predicted distribution \", React.createElement(Latex, {\n    text: \"\\\\(q\\\\)\"\n  }), \", the cross-entropy is:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\nH(p, q) = - \\\\sum_{x} p(x) \\\\log_2 q(x).\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"In practice, we typically drop the \", React.createElement(Latex, {\n    text: \"\\\\(\\\\log_2\\\\)\"\n  }), \" in favor of the natural logarithm, but the interpretation is the same. The difference between cross-entropy and the entropy \", React.createElement(Latex, {\n    text: \"\\\\(H(p)\\\\)\"\n  }), \" is precisely the KL divergence \", React.createElement(Latex, {\n    text: \"\\\\(D_{KL}(p \\\\parallel q)\\\\)\"\n  }), \":\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\nH(p, q) = H(p) + D_{KL}(p \\\\parallel q).\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Implication\"), \": Minimizing cross-entropy is equivalent to minimizing KL divergence between the true labels and the model's predicted distribution. Cross-entropy can often lead to overconfident predictions, especially when used with powerful models. This phenomenon has spurred research into alternative losses (e.g., focal loss or label smoothing) to calibrate predictions.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"tsallis-entropy\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#tsallis-entropy\",\n    \"aria-label\": \"tsallis entropy permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Tsallis entropy\"), \"\\n\", React.createElement(_components.p, null, React.createElement(Highlight, null, \"Tsallis entropy\"), \" is a generalization of Shannon entropy that introduces a parameter \", React.createElement(Latex, {\n    text: \"\\\\(q\\\\)\"\n  }), \" controlling the degree of non-extensivity:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\nH_q(X) = \\\\frac{1}{q-1} \\\\left(1 - \\\\sum_x p(x)^q \\\\right).\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"For \", React.createElement(Latex, {\n    text: \"\\\\(q=1\\\\)\"\n  }), \", Tsallis entropy reduces to Shannon entropy. In \", React.createElement(Highlight, null, \"reinforcement learning\"), \", Tsallis entropy (or related generalized entropies) has been used to control how strongly exploration is encouraged. By tuning \", React.createElement(Latex, {\n    text: \"\\\\(q\\\\)\"\n  }), \", one can shift the policy's tendency toward deterministic or more stochastic behavior in a more flexible manner than standard Shannon entropy allows.\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.h2, {\n    id: \"mutual-information-for-feature-selection-and-representation\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#mutual-information-for-feature-selection-and-representation\",\n    \"aria-label\": \"mutual information for feature selection and representation permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Mutual information for feature selection and representation\"), \"\\n\", React.createElement(_components.p, null, \"Among the numerous metrics introduced by information theory, \", React.createElement(Highlight, null, \"mutual information\"), \" is particularly critical for ML tasks like feature selection, representation learning, clustering, and multi-agent cooperation.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"feature-relevance-analysis\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#feature-relevance-analysis\",\n    \"aria-label\": \"feature relevance analysis permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Feature relevance analysis\"), \"\\n\", React.createElement(_components.p, null, \"One of the oldest uses of \", React.createElement(Latex, {\n    text: \"\\\\(I(X; Y)\\\\)\"\n  }), \" in ML is to assess how well a feature \", React.createElement(Latex, {\n    text: \"\\\\(X\\\\)\"\n  }), \" (or set of features) explains a target variable \", React.createElement(Latex, {\n    text: \"\\\\(Y\\\\)\"\n  }), \". The higher the mutual information, the more effectively \", React.createElement(Latex, {\n    text: \"\\\\(X\\\\)\"\n  }), \" can reduce uncertainty about \", React.createElement(Latex, {\n    text: \"\\\\(Y\\\\)\"\n  }), \". For instance:\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(Highlight, null, \"Feature selection\"), \": We might pick a subset of features \", React.createElement(Latex, {\n    text: \"\\\\(X_1, X_2, \\\\dots\\\\)\"\n  }), \" such that they collectively maximize \", React.createElement(Latex, {\n    text: \"\\\\(I(X_1, X_2, \\\\ldots; Y)\\\\)\"\n  }), \" subject to constraints like cardinality or cost.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(Highlight, null, \"Redundancy minimization\"), \": If two features are highly correlated, their individual mutual information with \", React.createElement(Latex, {\n    text: \"\\\\(Y\\\\)\"\n  }), \" might be large but they do not jointly add as much incremental predictive value. This leads to advanced feature selection heuristics that balance relevance and redundancy.\"), \"\\n\"), \"\\n\", React.createElement(Code, {\n    text: `\nimport numpy as np\nfrom sklearn.feature_selection import mutual_info_classif\n\n# Suppose X is your feature matrix, y are the labels\nmi_scores = mutual_info_classif(X, y, discrete_features='auto')\nprint(\"Estimated mutual information scores per feature:\", mi_scores)\n`\n  }), \"\\n\", React.createElement(_components.p, null, \"The snippet above shows how one might compute approximate mutual information between features and a target using scikit-learn's \", React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<code class=\\\"language-text\\\">mutual_info_classif</code>\"\n    }\n  }), \". Behind the scenes, this uses a nearest-neighbor or kernel density approach to estimate the distributions needed.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"information-maximization-in-clustering\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#information-maximization-in-clustering\",\n    \"aria-label\": \"information maximization in clustering permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Information maximization in clustering\"), \"\\n\", React.createElement(_components.p, null, React.createElement(Highlight, null, \"Information maximization\"), \" often arises in unsupervised contexts. For example, in \", React.createElement(Highlight, null, \"InfoGAN\"), \" (Chen and gang, 2016, NeurIPS), the idea is to maximize the mutual information between latent variables and generated samples, encouraging the latent space to learn interpretable features. In clustering, if we treat cluster assignments as \", React.createElement(Latex, {\n    text: \"\\\\(C\\\\)\"\n  }), \" and data as \", React.createElement(Latex, {\n    text: \"\\\\(X\\\\)\"\n  }), \", we can design algorithms to maximize \", React.createElement(Latex, {\n    text: \"\\\\(I(C; X)\\\\)\"\n  }), \" so that the cluster labels are highly informative of the data distribution.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"multi-agent-information-sharing\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#multi-agent-information-sharing\",\n    \"aria-label\": \"multi agent information sharing permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Multi-agent information sharing\"), \"\\n\", React.createElement(_components.p, null, \"In distributed or multi-agent systems, \", React.createElement(Highlight, null, \"communication bandwidth\"), \" and \", React.createElement(Highlight, null, \"energy constraints\"), \" can be strict. Agents only want to transmit signals that reduce uncertainty in the receiver. \", React.createElement(Latex, {\n    text: \"\\\\(I(A; B)\\\\)\"\n  }), \" can measure how much the message from agent A reduces the uncertainty of agent B regarding its environment or internal state.\"), \"\\n\", React.createElement(_components.p, null, \"In cooperative tasks, designing communication protocols often involves maximizing mutual information subject to channel capacity constraints. The synergy between information theory and multi-agent RL is a rapidly evolving research frontier, with applications to decentralized robotics (where agents need to coordinate in real time) and sensor networks (where only partial local observations are available).\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.h2, {\n    id: \"the-information-bottleneck-principle\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#the-information-bottleneck-principle\",\n    \"aria-label\": \"the information bottleneck principle permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"The information bottleneck principle\"), \"\\n\", React.createElement(_components.p, null, \"The \", React.createElement(Highlight, null, \"information bottleneck principle\"), \" (Tishby, Pereira, and Bialek, 2000) proposes a trade-off between making a representation that is minimal yet maximally relevant. The classical formulation is:\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, \"We have an input variable \", React.createElement(Latex, {\n    text: \"\\\\(X\\\\)\"\n  }), \" and a target variable \", React.createElement(Latex, {\n    text: \"\\\\(Y\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(_components.li, null, \"We want to create an intermediate representation \", React.createElement(Latex, {\n    text: \"\\\\(T\\\\)\"\n  }), \" that captures all the necessary information in \", React.createElement(Latex, {\n    text: \"\\\\(X\\\\)\"\n  }), \" to predict \", React.createElement(Latex, {\n    text: \"\\\\(Y\\\\)\"\n  }), \", but discards everything else.\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, \"We formalize this by minimizing:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\mathcal{L}_{IB} = I(X; T) - \\\\beta \\\\, I(T; Y),\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"where:\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(Latex, {\n    text: \"\\\\(I(X; T)\\\\)\"\n  }), \" is the mutual information between input and representation, which we want to keep small (to compress unnecessary bits).\"), \"\\n\", React.createElement(_components.li, null, React.createElement(Latex, {\n    text: \"\\\\(I(T; Y)\\\\)\"\n  }), \" is the mutual information between representation and target, which we want to keep large (to preserve predictive power).\"), \"\\n\", React.createElement(_components.li, null, React.createElement(Latex, {\n    text: \"\\\\(\\\\beta\\\\)\"\n  }), \" trades off compression and prediction quality.\"), \"\\n\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"theory-of-optimal-representation\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#theory-of-optimal-representation\",\n    \"aria-label\": \"theory of optimal representation permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Theory of optimal representation\"), \"\\n\", React.createElement(_components.p, null, \"In deep learning, one can interpret each layer of a neural network as progressively refining the representation, discarding extraneous information about \", React.createElement(Latex, {\n    text: \"\\\\(X\\\\)\"\n  }), \" while retaining or amplifying the parts that help predict \", React.createElement(Latex, {\n    text: \"\\\\(Y\\\\)\"\n  }), \". Some results suggest that as neural networks train, they go through a \", React.createElement(Highlight, null, \"\\\"drift and diffusion\\\"\"), \" phase in which the hidden layers become more disentangled from the input (Shwartz-Ziv and Tishby, 2017).\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"applications-in-explainability\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#applications-in-explainability\",\n    \"aria-label\": \"applications in explainability permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Applications in explainability\"), \"\\n\", React.createElement(_components.p, null, React.createElement(Highlight, null, \"Information bottlenecks\"), \" can provide insights into \", React.createElement(Highlight, null, \"explainable AI\"), \" by revealing how much information about input features is retained at each hidden representation. If a hidden layer is extremely compressed, we can hypothesize that the network is ignoring large swaths of input data that it deems irrelevant for classification. This can be used to gauge how a neural network might generalize or fail on out-of-distribution data.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"dynamic-bottlenecks-in-reinforcement-learning\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#dynamic-bottlenecks-in-reinforcement-learning\",\n    \"aria-label\": \"dynamic bottlenecks in reinforcement learning permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Dynamic bottlenecks in reinforcement learning\"), \"\\n\", React.createElement(_components.p, null, \"RL problems can also incorporate the information bottleneck principle to compress the state into a minimal representation. This helps the agent focus on aspects of the environment that directly affect rewards while ignoring distractors. Some advanced RL algorithms incorporate a learnable bottleneck to adapt the state representation, improving performance in partially observable or noisy tasks.\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.h2, {\n    id: \"kl-divergence-and-model-adaptation\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#kl-divergence-and-model-adaptation\",\n    \"aria-label\": \"kl divergence and model adaptation permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"KL divergence and model adaptation\"), \"\\n\", React.createElement(_components.p, null, \"While mutual information quantifies how two distributions or variables relate to each other, \", React.createElement(Highlight, null, \"Kullback-Leibler divergence\"), \" quantifies how one distribution diverges from another. This is at the heart of many model adaptation techniques in machine learning.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"variational-inference\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#variational-inference\",\n    \"aria-label\": \"variational inference permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Variational inference\"), \"\\n\", React.createElement(_components.p, null, \"In Bayesian machine learning, we often want to compute \", React.createElement(Latex, {\n    text: \"\\\\(p(\\\\theta | D)\\\\)\"\n  }), \" (the posterior over parameters \", React.createElement(Latex, {\n    text: \"\\\\(\\\\theta\\\\)\"\n  }), \" given data \", React.createElement(Latex, {\n    text: \"\\\\(D\\\\)\"\n  }), \"), but direct computation can be intractable. \", React.createElement(Highlight, null, \"Variational inference\"), \" replaces the true posterior with a simpler distribution \", React.createElement(Latex, {\n    text: \"\\\\(q(\\\\theta)\\\\)\"\n  }), \" by minimizing\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\nD_{KL}\\\\bigl(q(\\\\theta)\\\\;\\\\|\\\\; p(\\\\theta|D)\\\\bigr).\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"This approach, widely used in \", React.createElement(Highlight, null, \"variational autoencoders (VAEs)\"), \", yields approximate posteriors that can be optimized with gradient methods, enabling large-scale Bayesian neural networks.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"domain-adaptation\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#domain-adaptation\",\n    \"aria-label\": \"domain adaptation permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Domain adaptation\"), \"\\n\", React.createElement(_components.p, null, \"In \", React.createElement(Highlight, null, \"domain adaptation\"), \", we have a source domain \", React.createElement(Latex, {\n    text: \"\\\\(p_s(x)\\\\)\"\n  }), \" and a target domain \", React.createElement(Latex, {\n    text: \"\\\\(p_t(x)\\\\)\"\n  }), \". The goal is to learn models robust to distribution shifts. Minimizing divergences like \", React.createElement(Latex, {\n    text: \"\\\\(D_{KL}(p_s \\\\parallel p_t)\\\\)\"\n  }), \" or \", React.createElement(Latex, {\n    text: \"\\\\(D_{KL}(p_t \\\\parallel p_s)\\\\)\"\n  }), \" can align the feature distributions. Alternative divergences, like \", React.createElement(Latex, {\n    text: \"\\\\(D_{\\\\mathrm{JS}}\\\\)\"\n  }), \" (Jensen-Shannon divergence), can also be used when the direct distributions are unknown or partially estimated.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"policy-alignment-in-multi-agent-systems\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#policy-alignment-in-multi-agent-systems\",\n    \"aria-label\": \"policy alignment in multi agent systems permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Policy alignment in multi-agent systems\"), \"\\n\", React.createElement(_components.p, null, \"In multi-agent settings, we may want multiple agents to converge to similar policies or to share a common policy, especially if they are cooperating. A straightforward approach is to penalize the difference between their policy distributions using a KL term:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\sum_{i, j} D_{KL}\\\\bigl(\\\\pi_i(\\\\cdot | s)\\\\;\\\\|\\\\;\\\\pi_j(\\\\cdot | s)\\\\bigr).\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"Such a penalty encourages the agents to remain close in policy space, facilitating coordinated behaviors, while still allowing for some variation.\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.h2, {\n    id: \"rate-distortion-theory-and-efficient-learning\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#rate-distortion-theory-and-efficient-learning\",\n    \"aria-label\": \"rate distortion theory and efficient learning permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Rate-distortion theory and efficient learning\"), \"\\n\", React.createElement(_components.p, null, React.createElement(Highlight, null, \"Rate-distortion theory\"), \" is a branch of information theory that studies the trade-off between the compression rate of a source and the distortion incurred by approximating that source. In machine learning, this is deeply connected to model compression, resource allocation, and trade-offs between fidelity to the original data and the capacity of the model.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"trade-offs-in-lossy-compression\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#trade-offs-in-lossy-compression\",\n    \"aria-label\": \"trade offs in lossy compression permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Trade-offs in lossy compression\"), \"\\n\", React.createElement(_components.p, null, \"A typical rate-distortion objective is:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\min_{p(\\\\hat{x}|x)} \\\\quad I(X; \\\\hat{X})\\n\\\\quad\\n\\\\text{subject to}\\n\\\\quad\\n\\\\mathbb{E}[d(X,\\\\hat{X})] \\\\leq D,\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"where:\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(Latex, {\n    text: \"\\\\(X\\\\)\"\n  }), \" is the original source.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(Latex, {\n    text: \"\\\\(\\\\hat{X}\\\\)\"\n  }), \" is the compressed representation.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(Latex, {\n    text: \"\\\\(d(\\\\cdot, \\\\cdot)\\\\)\"\n  }), \" is a distortion measure (e.g., mean squared error).\"), \"\\n\", React.createElement(_components.li, null, React.createElement(Latex, {\n    text: \"\\\\(D\\\\)\"\n  }), \" is a distortion budget.\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, \"This parallels the process of \", React.createElement(Highlight, null, \"model pruning\"), \" and \", React.createElement(Highlight, null, \"quantization\"), \", where we sacrifice some performance in exchange for smaller model footprints or faster inference.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"resource-constrained-agents\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#resource-constrained-agents\",\n    \"aria-label\": \"resource constrained agents permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Resource-constrained agents\"), \"\\n\", React.createElement(_components.p, null, \"On the edge or in IoT devices, \", React.createElement(Highlight, null, \"resource constraints\"), \" (like memory, power, or bandwidth) are real. Rate-distortion concepts guide how to represent data with minimal bits while retaining essential information for decision-making. For instance, an embedded sensor might compress images before sending them to the cloud for classification, balancing bandwidth usage and classification accuracy.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"perceptual-distortion-metrics\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#perceptual-distortion-metrics\",\n    \"aria-label\": \"perceptual distortion metrics permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Perceptual distortion metrics\"), \"\\n\", React.createElement(_components.p, null, \"In certain tasks like image compression, \", React.createElement(Highlight, null, \"perceptual quality\"), \" matters more than mean squared error. This has led to advanced \", React.createElement(Highlight, null, \"distortion metrics\"), \" that incorporate human visual system (HVS) models. Aligning these perceptual metrics with rate-distortion optimization can produce significantly better visual results, relevant in generative modeling or in tasks like super-resolution.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"bits-back-coding\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#bits-back-coding\",\n    \"aria-label\": \"bits back coding permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Bits-back coding\"), \"\\n\", React.createElement(_components.p, null, React.createElement(Highlight, null, \"Bits-back coding\"), \", used in some VAE-based compression schemes (Townsend and gang, 2019), allows near-lossless compression by leveraging the learned latent distribution. The scheme can effectively reduce the overhead cost of transmitting latent codes by recouping bits from the prior, linking deep generative modeling to practical compression architectures.\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.h2, {\n    id: \"information-theoretic-reinforcement-learning\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#information-theoretic-reinforcement-learning\",\n    \"aria-label\": \"information theoretic reinforcement learning permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Information-theoretic reinforcement learning\"), \"\\n\", React.createElement(_components.p, null, \"Reinforcement learning tasks often revolve around how an agent acquires information about its environment and how it uses that information to act optimally. Below are some advanced topics that combine RL and information theory.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"empowerment-and-intrinsic-motivation\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#empowerment-and-intrinsic-motivation\",\n    \"aria-label\": \"empowerment and intrinsic motivation permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Empowerment and intrinsic motivation\"), \"\\n\", React.createElement(_components.p, null, \"In some RL formulations, an agent is \\\"intrinsically motivated\\\" to maximize its ability to affect the environment. \", React.createElement(Highlight, null, \"Empowerment\"), \" is defined as the maximum mutual information between an agent's actions and its future states:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\mathrm{Empowerment}(s) = \\\\max_{p(a_{1:k})} I(A_{1:k}; S_{t+\\\\Delta} | S_t = s).\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"This encourages the agent to move to states where it has more potential influence on the environment, often leading to more robust exploration in complex or sparse-reward settings.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"curiosity-driven-exploration\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#curiosity-driven-exploration\",\n    \"aria-label\": \"curiosity driven exploration permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Curiosity-driven exploration\"), \"\\n\", React.createElement(_components.p, null, React.createElement(Highlight, null, \"Curiosity-based methods\"), \" reward an agent when its predictions about the next state (or observations) are incorrect (or high in \", React.createElement(Highlight, null, \"prediction error\"), \"), effectively encouraging it to seek out novel states that maximize information gain. This can be interpreted as maximizing \", React.createElement(Latex, {\n    text: \"\\\\(I(\\\\text{agent's model}; \\\\text{environment states})\\\\)\"\n  }), \". By exploring states that yield the largest model update or biggest reduction in uncertainty, the agent systematically uncovers new parts of the environment.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"information-directed-sampling\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#information-directed-sampling\",\n    \"aria-label\": \"information directed sampling permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Information-directed sampling\"), \"\\n\", React.createElement(_components.p, null, React.createElement(Highlight, null, \"Information-directed sampling\"), \" focuses on action selection by balancing the immediate reward of an action with its potential to reduce uncertainty about the environment model. This approach tries to unify exploration and exploitation by looking at the ratio of expected squared regret to information gain about the optimal action, sometimes leading to more sample-efficient RL than standard heuristics.\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.h2, {\n    id: \"multi-agent-systems-and-distributed-information\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#multi-agent-systems-and-distributed-information\",\n    \"aria-label\": \"multi agent systems and distributed information permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Multi-agent systems and distributed information\"), \"\\n\", React.createElement(_components.p, null, \"When multiple learning agents operate in the same environment, either competitively or cooperatively, the flow of information is more complex. Information theory can help quantify how agents communicate and how they converge on shared goals or strategies.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"emergent-communication-protocols\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#emergent-communication-protocols\",\n    \"aria-label\": \"emergent communication protocols permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Emergent communication protocols\"), \"\\n\", React.createElement(_components.p, null, \"In \", React.createElement(Highlight, null, \"emergent communication\"), \", multi-agent systems are trained to develop a communication channel from scratch. Agents might exchange messages that are discrete symbols, and the objective is often to maximize the team reward. Over time, the messages can become a discrete language. By analyzing \", React.createElement(Latex, {\n    text: \"\\\\(I(\\\\text{message}; \\\\text{environment state})\\\\)\"\n  }), \" or \", React.createElement(Latex, {\n    text: \"\\\\(I(\\\\text{message}; \\\\text{agent actions})\\\\)\"\n  }), \", researchers investigate whether the language is efficient, compositional, or correlated with specific task-relevant features.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"consensus-algorithms\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#consensus-algorithms\",\n    \"aria-label\": \"consensus algorithms permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Consensus algorithms\"), \"\\n\", React.createElement(_components.p, null, React.createElement(Highlight, null, \"Consensus\"), \" is a classic problem in distributed computing and robotics: multiple agents or nodes must agree on a certain value or state. From an information-theoretic perspective, consensus can be framed as minimizing the distributed KL divergence across the network. Communication overhead is minimized if the final, agreed-upon state is reached with minimal bits of information exchanged among the agents.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"information-asymmetry-in-game-theory\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#information-asymmetry-in-game-theory\",\n    \"aria-label\": \"information asymmetry in game theory permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Information asymmetry in game theory\"), \"\\n\", React.createElement(_components.p, null, \"In \", React.createElement(Highlight, null, \"game-theoretic\"), \" problems, players might not have the same information about the state of the world, leading to incomplete-information games. Real-world examples include negotiations, auctions, or complex multi-agent environments in robotics. The effect of \", React.createElement(Highlight, null, \"information asymmetry\"), \" can be studied through the lens of how some players hold \\\"private signals\\\" that others do not, and how rational strategies must account for that. This can lead to phenomena such as \", React.createElement(Highlight, null, \"information cascades\"), \" or \", React.createElement(Highlight, null, \"herding\"), \", where agents ignore their own signals and instead follow the majority, leading to suboptimal outcomes.\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.h2, {\n    id: \"advanced-topics\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#advanced-topics\",\n    \"aria-label\": \"advanced topics permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Advanced topics\"), \"\\n\", React.createElement(_components.p, null, \"Beyond the core building blocks of information theory, there are several advanced areas that shed more light on how these principles integrate with deep learning and AI research.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"information-bottleneck-in-deep-learning-recent-research\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#information-bottleneck-in-deep-learning-recent-research\",\n    \"aria-label\": \"information bottleneck in deep learning recent research permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Information bottleneck in deep learning: recent research\"), \"\\n\", React.createElement(_components.p, null, \"While the original \", React.createElement(Highlight, null, \"information bottleneck\"), \" paper dates back to 2000, recent investigations in deep learning have revived interest in how neural networks implicitly optimize compression of input data. Some researchers hypothesize that in the later stages of training, stochastic gradient descent drives hidden representations to discard high-frequency or irrelevant details, effectively implementing a form of the information bottleneck. This has been used to explain generalization phenomena in large-scale deep networks.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"neural-estimation-of-information-measures\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#neural-estimation-of-information-measures\",\n    \"aria-label\": \"neural estimation of information measures permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Neural estimation of information measures\"), \"\\n\", React.createElement(_components.p, null, \"In high-dimensional data, classical ways of estimating \", React.createElement(Latex, {\n    text: \"\\\\(H(X)\\\\)\"\n  }), \", \", React.createElement(Latex, {\n    text: \"\\\\(I(X;Y)\\\\)\"\n  }), \", or \", React.createElement(Latex, {\n    text: \"\\\\(D_{KL}(p \\\\parallel q)\\\\)\"\n  }), \" can be unreliable or computationally intractable. \", React.createElement(Highlight, null, \"Neural estimation\"), \" techniques, such as \", React.createElement(Highlight, null, \"Mutual Information Neural Estimation (MINE)\"), \" (Belghazi and gang, ICML 2018), use a trainable network to approximate these quantities. By framing mutual information estimation as a \", React.createElement(Latex, {\n    text: \"\\\\(f\\\\)\"\n  }), \"-divergence problem, one can learn a parameterized function that provides an unbiased or low-bias estimator for \", React.createElement(Latex, {\n    text: \"\\\\(I(X;Y)\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"integrated-information-theory-iit\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#integrated-information-theory-iit\",\n    \"aria-label\": \"integrated information theory iit permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Integrated information theory (IIT)\"), \"\\n\", React.createElement(_components.p, null, React.createElement(Highlight, null, \"Integrated information theory (IIT)\"), \" originated from consciousness studies, aiming to measure how integrated and differentiated the information is within a system. While controversial in its direct application to AI, there are ongoing discussions about whether certain complex neural architectures might exhibit forms of integrated information. Proposals exist for bridging concepts from IIT with emergent multi-agent coordination or advanced recurrent networks, but as of now, it remains a niche research domain.\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.h2, {\n    id: \"tools\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#tools\",\n    \"aria-label\": \"tools permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Tools\"), \"\\n\", React.createElement(_components.p, null, \"Given the variety of ways information-theoretic principles can be applied in machine learning, it is helpful to know the software tools and libraries that can facilitate computations.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"software-for-information-theoretic-ml\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#software-for-information-theoretic-ml\",\n    \"aria-label\": \"software for information theoretic ml permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Software for information-theoretic ML\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(Highlight, null, \"ITE toolbox\"), \" (Informational Toolkit) is a MATLAB/Python library providing a suite of estimators for Shannon entropy, Rnyi entropy, mutual information, and other divergences.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(Highlight, null, \"PyITLib\"), \" is a Python library offering a variety of \", React.createElement(Tooltip, {\n    text: \"Information Theoretic measures such as Shannon, Rnyi entropies, mutual information, partial information decomposition, etc.\"\n  }), \".\"), \"\\n\", React.createElement(_components.li, null, React.createElement(Highlight, null, \"scikit-learn\"), \" includes functions like \", React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<code class=\\\"language-text\\\">mutual_info_score</code>\"\n    }\n  }), \" and \", React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<code class=\\\"language-text\\\">mutual_info_classif</code>\"\n    }\n  }), \" for discrete variable mutual information estimation.\"), \"\\n\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"estimating-entropy-and-mi\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#estimating-entropy-and-mi\",\n    \"aria-label\": \"estimating entropy and mi permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Estimating entropy and MI\"), \"\\n\", React.createElement(_components.p, null, \"In high-dimensional spaces, direct methods (like naive binning) become infeasible. More advanced approaches include:\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(Highlight, null, \"K-nearest neighbor (KNN)\"), \" estimators (e.g., Kraskov and gang, 2004) that exploit local density estimates.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(Highlight, null, \"Kernel density\"), \" estimators that approximate the underlying density function.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(Highlight, null, \"Variational / neural approaches\"), \" (e.g., MINE) that learn function approximators to derive tight lower bounds on \", React.createElement(Latex, {\n    text: \"\\\\(I(X; Y)\\\\)\"\n  }), \".\"), \"\\n\"), \"\\n\", React.createElement(Code, {\n    text: `\nimport numpy as np\nfrom pyitlib import discrete_random_variable as drv\n\n# Example: Estimating the mutual information between two discrete arrays\nX = np.random.randint(0, 10, 1000)\nY = X + np.random.randint(0, 2, 1000)  # Y is correlated with X\nmi_est = drv.information_mutual(X, Y)\nprint(\"Estimated mutual information:\", mi_est)\n`\n  }), \"\\n\", React.createElement(_components.h3, {\n    id: \"case-study-mutual-information-for-feature-selection-in-healthcare\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#case-study-mutual-information-for-feature-selection-in-healthcare\",\n    \"aria-label\": \"case study mutual information for feature selection in healthcare permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Case study: mutual information for feature selection in healthcare\"), \"\\n\", React.createElement(_components.p, null, \"Imagine you are working with a healthcare dataset, containing demographic information (age, gender), clinical lab results (blood pressure, cholesterol levels, etc.), and a binary label (presence/absence of a particular disease). A typical workflow for mutual-information-based feature selection is:\"), \"\\n\", React.createElement(_components.ol, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Data collection\"), \": Gather patient data with relevant attributes and disease labels.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Preprocessing\"), \": Convert categorical features to numeric form, handle missing values, and ensure data is in a workable shape.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"MI estimation\"), \": For each feature \", React.createElement(Latex, {\n    text: \"\\\\(X_i\\\\)\"\n  }), \", estimate \", React.createElement(Latex, {\n    text: \"\\\\(I(X_i; Y)\\\\)\"\n  }), \". Rank features by this mutual information score.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Redundancy analysis\"), \": Avoid picking multiple features that carry essentially the same information about \", React.createElement(Latex, {\n    text: \"\\\\(Y\\\\)\"\n  }), \".\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Model building\"), \": Train classification models using the subset of high-MI features. Evaluate performance on validation sets.\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, \"By iteratively refining the feature set with an MI-based approach, you can isolate those patient attributes that truly matter for diagnosing the condition. This approach is robust even when the relationships are non-linear, a significant advantage over purely linear statistics (like correlation coefficients).\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.p, null, React.createElement(Highlight, null, \"Please note:\"), \" Because this article seeks to provide a thorough, wide-reaching exploration, I will now present additional expansions and elaborations to ensure an even deeper and more holistic coverage of information theory's impact on machine learning. The following sections dive further into advanced nuances, bridging theoretical frameworks with state-of-the-art research insights from leading conferences and journals, and offering more coding examples to solidify practical understanding.\"), \"\\n\", React.createElement(_components.p, null, \"This next section significantly extends the discussion, ensuring the article meets the depth and length necessary for advanced study.\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.h2, {\n    id: \"extended-elaborations-and-deep-theoretical-insights\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#extended-elaborations-and-deep-theoretical-insights\",\n    \"aria-label\": \"extended elaborations and deep theoretical insights permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Extended elaborations and deep theoretical insights\"), \"\\n\", React.createElement(_components.p, null, \"In the preceding sections, I have outlined the major constructs of classical information theory  from Shannon's foundational ideas of entropy and channel capacity to modern applications in ML like the information bottleneck, variational inference, and advanced multi-agent communications. However, the interplay of these constructs with ever-growing deep neural networks, high-dimensional data, and complex multi-modal tasks reveals a wealth of further subtleties.\"), \"\\n\", React.createElement(_components.p, null, \"Here, I expand on particular advanced themes, giving you an even deeper dive into contemporary research frontiers and theoretical refinements relevant to machine learning practitioners and researchers.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"1-alternative-divergences-and-f-divergences\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#1-alternative-divergences-and-f-divergences\",\n    \"aria-label\": \"1 alternative divergences and f divergences permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"1. Alternative divergences and f-divergences\"), \"\\n\", React.createElement(_components.p, null, \"While KL divergence \", React.createElement(Latex, {\n    text: \"\\\\(D_{KL}(p \\\\parallel q)\\\\)\"\n  }), \" is the classical measure of how one distribution \", React.createElement(Latex, {\n    text: \"\\\\(p\\\\)\"\n  }), \" differs from another \", React.createElement(Latex, {\n    text: \"\\\\(q\\\\)\"\n  }), \", we also have a broader family of \", React.createElement(Highlight, null, \"f-divergences\"), \":\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\nD_f(p \\\\parallel q) = \\\\sum_x q(x) f\\\\!\\\\Bigl(\\\\frac{p(x)}{q(x)}\\\\Bigr)\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"for some convex function \", React.createElement(Latex, {\n    text: \"\\\\(f\\\\)\"\n  }), \". The KL divergence is just one instance (with \", React.createElement(Latex, {\n    text: \"\\\\(f(u) = u \\\\log u\\\\)\"\n  }), \"). Others include \", React.createElement(Highlight, null, \"Jensen-Shannon\"), \" divergence, \", React.createElement(Highlight, null, \"Rnyi\"), \" divergences, and \", React.createElement(Highlight, null, \"total variation distance\"), \". In machine learning, these alternative divergences sometimes have better stability or interpretability properties than KL.\"), \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Jensen-Shannon divergence\"), \", for instance, is symmetrical and always defined (even if \", React.createElement(Latex, {\n    text: \"\\\\(p(x) = 0\\\\)\"\n  }), \" for some \", React.createElement(Latex, {\n    text: \"\\\\(x\\\\)\"\n  }), \"), making it popular for generative adversarial networks (GANs). Indeed, the original GAN used the JS divergence concept, though later it was recast in terms of a \", React.createElement(Latex, {\n    text: \"\\\\(\\\\log\\\\)\"\n  }), \" logistic loss interpretation.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"2-rnyi-entropy-and-divergences\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#2-r%C3%A9nyi-entropy-and-divergences\",\n    \"aria-label\": \"2 rnyi entropy and divergences permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"2. Rnyi entropy and divergences\"), \"\\n\", React.createElement(_components.p, null, React.createElement(Highlight, null, \"Rnyi entropy\"), \", a generalization of Shannon entropy, is defined as:\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\nH_\\\\alpha(X) = \\\\frac{1}{1-\\\\alpha} \\\\log \\\\sum_x p(x)^\\\\alpha,\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"where \", React.createElement(Latex, {\n    text: \"\\\\(\\\\alpha\\\\)\"\n  }), \" is a real parameter. When \", React.createElement(Latex, {\n    text: \"\\\\(\\\\alpha \\\\to 1\\\\)\"\n  }), \", Rnyi entropy converges to Shannon entropy. Rnyi's generalization also leads to the \", React.createElement(Highlight, null, \"Rnyi divergence\"), \" (or Rnyi's \", React.createElement(Latex, {\n    text: \"\\\\(D_\\\\alpha\\\\)\"\n  }), \"), which has found applications in specific ML contexts, for example in controlling the degree of penalization for distribution mismatch or for exploring theoretical bounds in generalized Bayesian approaches.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"3-maximum-entropy-principle-in-ml\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#3-maximum-entropy-principle-in-ml\",\n    \"aria-label\": \"3 maximum entropy principle in ml permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"3. Maximum entropy principle in ML\"), \"\\n\", React.createElement(_components.p, null, \"The \", React.createElement(Highlight, null, \"maximum entropy (MaxEnt)\"), \" principle states that subject to known constraints (e.g., expected values), the distribution that best represents our state of knowledge while assuming nothing unwarranted is the one with the greatest entropy. This principle underlies:\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(Highlight, null, \"Maximum entropy classifiers\"), \", which are logistic regression models generalized to multiclass scenarios.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(Highlight, null, \"Boltzmann machines\"), \", where the distribution over states is derived via an energy function and a partition function, ensuring maximum entropy subject to constraints.\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, \"In RL, the MaxEnt principle extends beyond entropy regularization. It fosters approaches like \", React.createElement(Highlight, null, \"Soft Q-learning\"), \" or \", React.createElement(Highlight, null, \"Soft Actor-Critic\"), \" that assume the optimal policy is the one that best balances reward maximization with maximum entropy. This perspective can be especially beneficial in continuous control tasks, where the action space is large.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"4-connections-between-compression-and-generalization\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#4-connections-between-compression-and-generalization\",\n    \"aria-label\": \"4 connections between compression and generalization permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"4. Connections between compression and generalization\"), \"\\n\", React.createElement(_components.p, null, \"A critical question in deep learning is \", React.createElement(Highlight, null, \"why large networks generalize so well\"), \" despite having more parameters than training examples. One explanation, grounded in information theory, posits that \", React.createElement(Highlight, null, \"compression\"), \" in the hidden layers helps the model throw away spurious details, focusing on robust features relevant to the target. Some lines of research argue that the implicit regularization of gradient-based training leads the system to solutions that compress the input in ways correlated with better generalization.\"), \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Open debate\"), \": The exact mechanics of how the network compresses, and to what degree this compression is essential, remains a lively debate. Some empirical investigations challenge the idea that compression always occurs, while others refine the argument about which layers do or do not compress. Regardless, the notion of neural networks as lossy compressors of input data remains a powerful conceptual framework.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"5-information-plane-analysis\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#5-information-plane-analysis\",\n    \"aria-label\": \"5 information plane analysis permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"5. Information plane analysis\"), \"\\n\", React.createElement(_components.p, null, React.createElement(Highlight, null, \"Information plane analysis\"), \" visualizes \", React.createElement(Latex, {\n    text: \"\\\\(I(X; T)\\\\)\"\n  }), \" versus \", React.createElement(Latex, {\n    text: \"\\\\(I(T; Y)\\\\)\"\n  }), \" over the course of network training. Observing how the hidden layer \", React.createElement(Latex, {\n    text: \"\\\\(T\\\\)\"\n  }), \" transitions from a high \", React.createElement(Latex, {\n    text: \"\\\\(I(X; T)\\\\)\"\n  }), \" state to a lower one while increasing \", React.createElement(Latex, {\n    text: \"\\\\(I(T; Y)\\\\)\"\n  }), \" can reveal how the model shapes representations. This approach has been used to study epochs of training in large-scale neural networks, linking network optimization to the notion of an \\\"information bottleneck.\\\"\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"6-influence-functions-and-sensitivity\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#6-influence-functions-and-sensitivity\",\n    \"aria-label\": \"6 influence functions and sensitivity permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"6. Influence functions and sensitivity\"), \"\\n\", React.createElement(_components.p, null, \"We briefly touched on \", React.createElement(Highlight, null, \"Fisher information\"), \". In modern ML, \", React.createElement(Highlight, null, \"influence functions\"), \" generalize the concept by measuring how changes in the training data affect the learned parameters and predictions. Influence functions approximate the effect of removing or modifying a single training example on the final model. This is related to the local geometry of the parameter space, often represented by the Hessian or an approximation to the Fisher information. For interpretability, these methods help debug or detect training outliers that disproportionately affect the model.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"7-differential-privacy-and-mutual-information\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#7-differential-privacy-and-mutual-information\",\n    \"aria-label\": \"7 differential privacy and mutual information permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"7. Differential privacy and mutual information\"), \"\\n\", React.createElement(_components.p, null, \"As data privacy regulations tighten, \", React.createElement(Highlight, null, \"differential privacy\"), \" (DP) provides a formal guarantee of privacy in ML algorithms by limiting the sensitivity of model outputs to any individual data point. Interestingly, there is a deep tie between DP and information theory  specifically, bounding the \", React.createElement(Latex, {\n    text: \"\\\\(I(X; \\\\text{output})\\\\)\"\n  }), \" can help ensure that the model's output does not reveal too much about \", React.createElement(Latex, {\n    text: \"\\\\(X\\\\)\"\n  }), \". Understanding this synergy is essential for building ML pipelines that respect user privacy while retaining valuable information for training.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"8-information-theoretic-bounds-on-generalization\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#8-information-theoretic-bounds-on-generalization\",\n    \"aria-label\": \"8 information theoretic bounds on generalization permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"8. Information-theoretic bounds on generalization\"), \"\\n\", React.createElement(_components.p, null, \"Techniques for bounding generalization error often rely on \", React.createElement(Highlight, null, \"VC dimension\"), \", \", React.createElement(Highlight, null, \"Rademacher complexity\"), \", or \", React.createElement(Highlight, null, \"uniform convergence\"), \" arguments. An alternative approach uses \", React.createElement(Highlight, null, \"information-theoretic generalization bounds\"), \":\"), \"\\n\", React.createElement(Latex, {\n    text: \"\\\\[\\n\\\\text{Generalization Error} \\\\leq \\\\sqrt{\\\\frac{I(\\\\theta; D)}{N}},\\n\\\\]\"\n  }), \"\\n\", React.createElement(_components.p, null, \"where \", React.createElement(Latex, {\n    text: \"\\\\(I(\\\\theta; D)\\\\)\"\n  }), \" is the mutual information between the model parameters and the training data, and \", React.createElement(Latex, {\n    text: \"\\\\(N\\\\)\"\n  }), \" is the sample size. This elegantly ties the notion of how many bits of information the model encodes about the specific training set to how well the model is expected to generalize. The smaller \", React.createElement(Latex, {\n    text: \"\\\\(I(\\\\theta; D)\\\\)\"\n  }), \", the better the generalization (under certain assumptions).\"), \"\\n\", React.createElement(_components.p, null, React.createElement(_components.strong, null, \"Practical insight\"), \": This aligns with the idea that if a model is too specialized to the training data (i.e., high \", React.createElement(Latex, {\n    text: \"\\\\(I(\\\\theta; D)\\\\)\"\n  }), \"), it might overfit. Conversely, if it learns more general, compressed abstractions, the mutual information with the exact training set is lower, enabling better generalization on unseen data.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"9-relating-mdl-and-bayesian-inference\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#9-relating-mdl-and-bayesian-inference\",\n    \"aria-label\": \"9 relating mdl and bayesian inference permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"9. Relating MDL and Bayesian inference\"), \"\\n\", React.createElement(_components.p, null, \"The \", React.createElement(Highlight, null, \"Minimum Description Length (MDL)\"), \" principle is closely related to Bayesian model selection. In essence, a model that can encode the data in fewer bits is preferable. Bayesian approaches implicitly do a similar trade-off through the posterior \", React.createElement(Latex, {\n    text: \"\\\\(p(\\\\theta|D)\\\\)\"\n  }), \", balancing model complexity and data fit. Specifically, the negative log of the evidence \", React.createElement(Latex, {\n    text: \"\\\\(-\\\\log p(D)\\\\)\"\n  }), \" can be seen as the coding cost of the data under the model. The synergy of these information-theoretic and Bayesian viewpoints gives a deeper understanding of how to balance complexity and fit in ML.\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.h2, {\n    id: \"additional-multi-agent-and-distributed-applications\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#additional-multi-agent-and-distributed-applications\",\n    \"aria-label\": \"additional multi agent and distributed applications permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Additional multi-agent and distributed applications\"), \"\\n\", React.createElement(_components.p, null, \"Because multi-agent and distributed learning are increasingly relevant in modern AI systems (e.g., robotics swarms, sensor networks, complex simulations), it is worthwhile to highlight advanced interplay with information theory.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"1-decentralized-partially-observable-mdps-dec-pomdps\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#1-decentralized-partially-observable-mdps-dec-pomdps\",\n    \"aria-label\": \"1 decentralized partially observable mdps dec pomdps permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"1. Decentralized partially observable MDPs (Dec-POMDPs)\"), \"\\n\", React.createElement(_components.p, null, \"When each agent observes only a part of the environment state, the question arises: \\\"What information should each agent share with others, and when?\\\" \", React.createElement(Latex, {\n    text: \"\\\\(I(A_i; A_j)\\\\)\"\n  }), \" quantifies how beneficial an agent's messages might be to the joint policy. Some algorithms explicitly optimize a communication policy by maximizing mutual information weighted by communication costs (e.g., if every message has a cost or a bandwidth limit).\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"2-graph-based-communication-topologies\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#2-graph-based-communication-topologies\",\n    \"aria-label\": \"2 graph based communication topologies permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"2. Graph-based communication topologies\"), \"\\n\", React.createElement(_components.p, null, \"Agents often reside in a network with edges representing communication links. Minimizing total KL divergence or maximizing mutual information subject to graph constraints leads to specialized \", React.createElement(Highlight, null, \"consensus\"), \" or \", React.createElement(Highlight, null, \"agreement\"), \" protocols. Tools from spectral graph theory can merge elegantly with information-theoretic frameworks, analyzing how quickly information spreads and consensus is reached given the structure of the graph.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"3-information-cascades\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#3-information-cascades\",\n    \"aria-label\": \"3 information cascades permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"3. Information cascades\"), \"\\n\", React.createElement(_components.p, null, \"In economics and social dynamics, \", React.createElement(Highlight, null, \"information cascades\"), \" occur when agents rely heavily on the actions of previous agents, ignoring their own private observations. This can lead to suboptimal herding behavior. From an information-theoretic lens, the system's effective \", React.createElement(Latex, {\n    text: \"\\\\(I(\\\\text{individual signals}; \\\\text{actions})\\\\)\"\n  }), \" might degrade quickly if individuals discard or override personal signals. ML approaches that mitigate cascades often reintroduce or highlight private signals, ensuring each agent's contribution is not overshadowed by the crowd.\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.h2, {\n    id: \"implementation-details-and-extended-code-samples\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#implementation-details-and-extended-code-samples\",\n    \"aria-label\": \"implementation details and extended code samples permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Implementation details and extended code samples\"), \"\\n\", React.createElement(_components.p, null, \"Below, I expand upon practical code examples illustrating how to compute or approximate key information-theoretic measures in typical ML workflows.\"), \"\\n\", React.createElement(_components.h3, {\n    id: \"estimating-shannon-entropy-in-python\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#estimating-shannon-entropy-in-python\",\n    \"aria-label\": \"estimating shannon entropy in python permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Estimating Shannon entropy in Python\"), \"\\n\", React.createElement(Code, {\n    text: `\nimport numpy as np\nfrom collections import Counter\nimport math\n\ndef shannon_entropy(data):\n    # data is a 1D list or NumPy array of discrete outcomes\n    counter = Counter(data)\n    total = len(data)\n    entropy = 0.0\n    for val, count in counter.items():\n        p = count / total\n        entropy -= p * math.log2(p)\n    return entropy\n\n# Example usage\nnp.random.seed(42)\ndata = np.random.randint(0, 5, 10000)  # 5 classes\nprint(\"Shannon Entropy:\", shannon_entropy(data))\n`\n  }), \"\\n\", React.createElement(_components.h3, {\n    id: \"approximate-mutual-information-for-continuous-variables-using-knn\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#approximate-mutual-information-for-continuous-variables-using-knn\",\n    \"aria-label\": \"approximate mutual information for continuous variables using knn permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Approximate mutual information for continuous variables using KNN\"), \"\\n\", React.createElement(Code, {\n    text: `\nimport numpy as np\nfrom sklearn.neighbors import NearestNeighbors\n\ndef knn_mi(x, y, k=5):\n    \"\"\"\n    A naive KNN-based estimate of mutual information between x and y,\n    both assumed to be 2D arrays of shape (n_samples, dim).\n    \"\"\"\n    x = np.asarray(x)\n    y = np.asarray(y)\n    \n    # Combined data\n    xy = np.hstack([x, y])\n    \n    n = x.shape[0]\n    \n    # Build KNN for combined space\n    nbrs_xy = NearestNeighbors(n_neighbors=k+1).fit(xy)\n    dist_xy, _ = nbrs_xy.kneighbors(xy)\n    # radius for each point is the distance to the k-th neighbor\n    radius = dist_xy[:, k]\n    \n    # Build KNN for x space\n    nbrs_x = NearestNeighbors(n_neighbors=k).fit(x)\n    count_x = []\n    for i in range(n):\n        # query the number of neighbors within 'radius[i]' in x-space\n        # minus 1 to exclude the point itself\n        count = nbrs_x.radius_neighbors([x[i]], radius[i], return_distance=False)\n        count_x.append(len(count[0]) - 1)\n    \n    # Build KNN for y space\n    nbrs_y = NearestNeighbors(n_neighbors=k).fit(y)\n    count_y = []\n    for i in range(n):\n        count = nbrs_y.radius_neighbors([y[i]], radius[i], return_distance=False)\n        count_y.append(len(count[0]) - 1)\n    \n    # Summation\n    import math\n    psi = lambda val: math.log(val)  # for simplicity, ignoring digamma for now\n    mi_est = psi(n) + psi(k) - np.mean([psi(count_xi+1) + psi(count_yi+1)\n                                        for count_xi, count_yi in zip(count_x, count_y)])\n    \n    return mi_est\n\n# Example usage:\nrng = np.random.RandomState(0)\nx = rng.normal(0, 1, (1000, 1))\ny = x + rng.normal(0, 0.1, (1000, 1))\nmi_val = knn_mi(x, y)\nprint(\"Approximate MI using KNN method:\", mi_val)\n`\n  }), \"\\n\", React.createElement(_components.p, null, \"The code above sketches a simplified approach to \", React.createElement(Highlight, null, \"KNN-based MI estimation\"), \", suitable for demonstration but not robust for all data distributions. In practice, more sophisticated or specialized libraries may handle edge cases and corrections to reduce bias.\"), \"\\n\", React.createElement(_components.hr), \"\\n\", React.createElement(_components.h2, {\n    id: \"practical-considerations-and-closing-thoughts\",\n    style: {\n      position: \"relative\"\n    }\n  }, React.createElement(_components.a, {\n    href: \"#practical-considerations-and-closing-thoughts\",\n    \"aria-label\": \"practical considerations and closing thoughts permalink\",\n    className: \"anchor before\"\n  }, React.createElement(_components.span, {\n    dangerouslySetInnerHTML: {\n      __html: \"<svg aria-hidden=\\\"true\\\" focusable=\\\"false\\\" height=\\\"16\\\" version=\\\"1.1\\\" viewBox=\\\"0 0 16 16\\\" width=\\\"16\\\"><path fill-rule=\\\"evenodd\\\" d=\\\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\\\"></path></svg>\"\n    }\n  })), \"Practical considerations and closing thoughts\"), \"\\n\", React.createElement(_components.p, null, \"Information theory, though originally built for communication systems, now stands as a pillar supporting numerous aspects of ML research and development:\"), \"\\n\", React.createElement(_components.ul, null, \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Theoretical clarity\"), \": Concepts like entropy, mutual information, and divergences unify the language of uncertainty and data. They highlight the fundamental limit of what a learning algorithm can achieve given a dataset and a hypothesis space.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Algorithmic design\"), \": Many modern algorithms, from \", React.createElement(Highlight, null, \"reinforcement learning\"), \" to \", React.createElement(Highlight, null, \"representation learning\"), \", incorporate these measures explicitly in their objectives or constraints (e.g., maximizing mutual information or minimizing KL divergence).\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Performance insights\"), \": Information-theoretic generalization bounds and the information bottleneck framework offer ways to think about how and why models generalize, providing alternative perspectives to more traditional statistical or computational complexity-based methods.\"), \"\\n\", React.createElement(_components.li, null, React.createElement(_components.strong, null, \"Implementation challenges\"), \": Despite their conceptual beauty, \", React.createElement(Highlight, null, \"estimating information-theoretic quantities\"), \" for high-dimensional, continuous data is non-trivial. This fosters the ongoing research into neural-based estimators, kernel-based methods, and novel bounds that are computationally tractable.\"), \"\\n\"), \"\\n\", React.createElement(_components.p, null, \"By weaving together theoretical principles, real-world applications, and practical implementation tips, this article has attempted to illustrate the depth and breadth of how \", React.createElement(Highlight, null, \"information theory\"), \" powers modern machine learning and AI systems. As you progress in your data science or ML endeavors, I encourage you to keep these information-theoretic concepts at your disposal: they are potent tools for diagnosing, designing, and understanding algorithms that must grapple with uncertainty, complexity, and high-dimensional spaces.\"), \"\\n\", React.createElement(_components.p, null, \"Whether you are building specialized feature selectors, harnessing curiosity-driven exploration in RL, or compressing a neural network for deployment on edge devices, the language of bits and uncertainty can guide you to more principled solutions. In the rapidly evolving AI landscape, \", React.createElement(Highlight, null, \"information theory\"), \" remains an enduring compass, continually pointing toward deeper insights and better algorithmic strategies.\"), \"\\n\", React.createElement(_components.p, null, \"In future parts of this course, you might encounter specific specialized domains  such as \", React.createElement(Highlight, null, \"Bayesian networks\"), \", \", React.createElement(Highlight, null, \"Gaussian processes\"), \", or \", React.createElement(Highlight, null, \"transformer-based large language models\"), \"  that further exploit these constructs to deal with complex statistical dependencies, uncertain priors, and the need for efficient representation learning. When you see references to cross-entropy or KL divergence in loss functions, or to mutual information in representation learning, I hope you'll now recognize them as powerful building blocks borrowed from a rich intellectual legacy, forming a cohesive approach to the fundamental problem of extracting, transmitting, and exploiting information in uncertain worlds.\"));\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? React.createElement(MDXLayout, props, React.createElement(_createMdxContent, props)) : _createMdxContent(props);\n}\nexport default MDXContent;\n","import GATSBY_COMPILED_MDX from \"/home/avrtt/Repos/avrtt.github.io/src/pages/posts/research/information_theory_for_ml.mdx\";\nimport React, {useState, useEffect} from 'react';\nimport {useSiteMetadata} from \"../hooks/useSiteMetadata\";\nimport RemoveMarkdown from 'remove-markdown';\nimport {ImageContext} from '../context/ImageContext';\nimport {MDXProvider} from '@mdx-js/react';\nimport Image from '../components/PostImage';\nimport {motion} from 'framer-motion';\nimport SEO from \"../components/seo\";\nimport PostBanner from '../components/PostBanner';\nimport PostBottom from '../components/PostBottom';\nimport {wordsPerMinuteAdventures, wordsPerMinuteResearch, wordsPerMinuteThoughts} from '../data/commonVariables';\nimport {graphql} from 'gatsby';\nimport PartOfCourseNotice from \"../components/PartOfCourseNotice\";\nimport * as stylesButtonsCommon from \"../styles/buttons_common.module.scss\";\nimport * as stylesCustomPostLayouts from \"../styles/custom_post_layouts.module.scss\";\nimport * as stylesTableOfContents from \"../styles/table_of_contents.module.scss\";\nimport * as stylesTagBadges from \"../styles/tag_badges.module.scss\";\nfunction formatReadTime(minutes) {\n  if (minutes <= 10) return '~10 min';\n  if (minutes <= 20) return '~20 min';\n  if (minutes <= 30) return '~30 min';\n  if (minutes <= 40) return '~40 min';\n  if (minutes <= 50) return '~50 min';\n  if (minutes <= 60) return '~1 h';\n  const hours = Math.floor(minutes / 60);\n  const remainder = minutes % 60;\n  if (remainder <= 30) {\n    return `~${hours}${remainder > 0 ? '.5' : ''} h`;\n  }\n  return `~${hours + 1} h`;\n}\nconst TableOfContents = ({toc}) => {\n  if (!toc || !toc.items) return null;\n  const handleClick = (e, url) => {\n    e.preventDefault();\n    const targetId = url.replace('#', '');\n    const targetElement = document.getElementById(targetId);\n    if (targetElement) {\n      targetElement.scrollIntoView({\n        behavior: 'smooth',\n        block: 'start'\n      });\n    }\n  };\n  return React.createElement(\"nav\", {\n    className: stylesTableOfContents.toc\n  }, React.createElement(\"ul\", null, toc.items.map((item, index) => React.createElement(\"li\", {\n    key: index\n  }, React.createElement(\"a\", {\n    href: item.url,\n    onClick: e => handleClick(e, item.url)\n  }, item.title), item.items && React.createElement(TableOfContents, {\n    toc: {\n      items: item.items\n    }\n  })))));\n};\nexport function PostTemplate({data: {mdx, allMdx, allPostImages}, children}) {\n  const {frontmatter, body, tableOfContents} = mdx;\n  const index = frontmatter.index;\n  const slug = frontmatter.slug;\n  const section = slug.split('/')[1];\n  const posts = allMdx.nodes.filter(post => post.frontmatter.slug.includes(`/${section}/`));\n  const sortedPosts = posts.sort((a, b) => a.frontmatter.index - b.frontmatter.index);\n  const currentIndex = sortedPosts.findIndex(post => post.frontmatter.index === index);\n  const nextPost = sortedPosts[currentIndex + 1];\n  const lastPost = sortedPosts[currentIndex - 1];\n  const trimmedSlug = frontmatter.slug.replace(/\\/$/, '');\n  const keyCurrent = (/[^/]*$/).exec(trimmedSlug)[0];\n  const basePath = `posts/${section}/content/${keyCurrent}/`;\n  const [isWideLayout, setIsWideLayout] = useState(frontmatter.flagWideLayoutByDefault);\n  const [isAnimating, setIsAnimating] = useState(false);\n  const toggleLayout = () => {\n    setIsWideLayout(!isWideLayout);\n  };\n  useEffect(() => {\n    setIsAnimating(true);\n    const timer = setTimeout(() => setIsAnimating(false), 340);\n    return () => clearTimeout(timer);\n  }, [isWideLayout]);\n  var wordsPerMinute;\n  if (section === \"adventures\") {\n    wordsPerMinute = wordsPerMinuteAdventures;\n  } else if (section === \"research\") {\n    wordsPerMinute = wordsPerMinuteResearch;\n  } else if (section === \"thoughts\") {\n    wordsPerMinute = wordsPerMinuteThoughts;\n  }\n  const plainTextBody = RemoveMarkdown(body).replace(/import .*? from .*?;/g, '').replace(/<.*?>/g, '').replace(/\\{\\/\\*[\\s\\S]*?\\*\\/\\}/g, '').trim();\n  const wordCount = plainTextBody.split(/\\s+/).length;\n  const baseReadTimeMinutes = Math.ceil(wordCount / wordsPerMinute);\n  const extraTime = frontmatter.extraReadTimeMin || 0;\n  const totalReadTime = baseReadTimeMinutes + extraTime;\n  const readTime = formatReadTime(totalReadTime);\n  const notices = [{\n    flag: frontmatter.flagDraft,\n    component: () => import(\"../components/NotFinishedNotice\")\n  }, {\n    flag: frontmatter.flagMindfuckery,\n    component: () => import(\"../components/MindfuckeryNotice\")\n  }, {\n    flag: frontmatter.flagRewrite,\n    component: () => import(\"../components/RewriteNotice\")\n  }, {\n    flag: frontmatter.flagOffensive,\n    component: () => import(\"../components/OffensiveNotice\")\n  }, {\n    flag: frontmatter.flagProfane,\n    component: () => import(\"../components/ProfanityNotice\")\n  }, {\n    flag: frontmatter.flagMultilingual,\n    component: () => import(\"../components/MultilingualNotice\")\n  }, {\n    flag: frontmatter.flagUnreliably,\n    component: () => import(\"../components/UnreliablyNotice\")\n  }, {\n    flag: frontmatter.flagPolitical,\n    component: () => import(\"../components/PoliticsNotice\")\n  }, {\n    flag: frontmatter.flagCognitohazard,\n    component: () => import(\"../components/CognitohazardNotice\")\n  }, {\n    flag: frontmatter.flagHidden,\n    component: () => import(\"../components/HiddenNotice\")\n  }];\n  const [loadedNotices, setLoadedNotices] = useState([]);\n  useEffect(() => {\n    notices.forEach(({flag, component}) => {\n      if (flag) {\n        component().then(module => {\n          setLoadedNotices(prev => [...prev, module.default]);\n        });\n      }\n    });\n  }, []);\n  return React.createElement(motion.div, {\n    initial: {\n      opacity: 0\n    },\n    animate: {\n      opacity: 1\n    },\n    exit: {\n      opacity: 0\n    },\n    transition: {\n      duration: 0.15\n    }\n  }, React.createElement(PostBanner, {\n    postNumber: frontmatter.index,\n    date: frontmatter.date,\n    updated: frontmatter.updated,\n    readTime: readTime,\n    difficulty: frontmatter.difficultyLevel,\n    title: frontmatter.title,\n    desc: frontmatter.desc,\n    banner: frontmatter.banner,\n    section: section,\n    postKey: keyCurrent,\n    isMindfuckery: frontmatter.flagMindfuckery,\n    mainTag: frontmatter.mainTag\n  }), React.createElement(\"div\", {\n    style: {\n      display: \"flex\",\n      justifyContent: \"flex-end\",\n      flexWrap: \"wrap\",\n      maxWidth: \"75%\",\n      marginLeft: \"auto\",\n      paddingRight: \"1vw\",\n      marginTop: \"-6vh\",\n      marginBottom: \"4vh\"\n    }\n  }, frontmatter.otherTags.map((tag, index) => React.createElement(\"span\", {\n    key: index,\n    className: `noselect ${stylesTagBadges.tagPosts}`,\n    style: {\n      margin: \"0 5px 5px 0\"\n    }\n  }, tag))), React.createElement(\"div\", {\n    class: \"postBody\"\n  }, React.createElement(TableOfContents, {\n    toc: tableOfContents\n  })), React.createElement(\"br\"), React.createElement(\"div\", {\n    style: {\n      margin: \"0 10% -2vh 30%\",\n      textAlign: \"right\"\n    }\n  }, React.createElement(motion.button, {\n    class: \"noselect\",\n    className: stylesCustomPostLayouts.postButton,\n    id: stylesCustomPostLayouts.postLayoutSwitchButton,\n    onClick: toggleLayout,\n    whileTap: {\n      scale: 0.93\n    }\n  }, React.createElement(motion.div, {\n    className: stylesButtonsCommon.buttonTextWrapper,\n    key: isWideLayout,\n    initial: {\n      opacity: 0\n    },\n    animate: {\n      opacity: 1\n    },\n    exit: {\n      opacity: 0\n    },\n    transition: {\n      duration: 0.3,\n      ease: \"easeInOut\"\n    }\n  }, isWideLayout ? \"Switch to default layout\" : \"Switch to wide layout\"))), React.createElement(\"br\"), React.createElement(\"div\", {\n    class: \"postBody\",\n    style: {\n      margin: isWideLayout ? \"0 -14%\" : \"\",\n      maxWidth: isWideLayout ? \"200%\" : \"\",\n      transition: \"margin 1s ease, max-width 1s ease, padding 1s ease\"\n    }\n  }, React.createElement(\"div\", {\n    className: `${stylesCustomPostLayouts.textContent} ${isAnimating ? stylesCustomPostLayouts.fadeOut : stylesCustomPostLayouts.fadeIn}`\n  }, loadedNotices.map((NoticeComponent, index) => React.createElement(NoticeComponent, {\n    key: index\n  })), frontmatter.indexCourse ? React.createElement(PartOfCourseNotice, {\n    index: frontmatter.indexCourse,\n    category: frontmatter.courseCategoryName\n  }) : \"\", React.createElement(ImageContext.Provider, {\n    value: {\n      images: allPostImages.nodes,\n      basePath: basePath.replace(/\\/$/, '') + '/'\n    }\n  }, React.createElement(MDXProvider, {\n    components: {\n      Image\n    }\n  }, children)))), React.createElement(PostBottom, {\n    nextPost: nextPost,\n    lastPost: lastPost,\n    keyCurrent: keyCurrent,\n    section: section\n  }));\n}\nPostTemplate\nexport default function GatsbyMDXWrapper(props) {\n  return React.createElement(PostTemplate, props, React.createElement(GATSBY_COMPILED_MDX, props));\n}\nexport function Head({data}) {\n  const {frontmatter} = data.mdx;\n  const title = frontmatter.titleSEO || frontmatter.title;\n  const titleOG = frontmatter.titleOG || title;\n  const titleTwitter = frontmatter.titleTwitter || title;\n  const description = frontmatter.descSEO || frontmatter.desc;\n  const descriptionOG = frontmatter.descOG || description;\n  const descriptionTwitter = frontmatter.descTwitter || description;\n  const schemaType = frontmatter.schemaType || \"BlogPosting\";\n  const keywords = frontmatter.keywordsSEO;\n  const datePublished = frontmatter.date;\n  const dateModified = frontmatter.updated || datePublished;\n  const imageOG = frontmatter.imageOG || frontmatter.banner?.childImageSharp?.gatsbyImageData?.images?.fallback?.src;\n  const imageAltOG = frontmatter.imageAltOG || descriptionOG;\n  const imageTwitter = frontmatter.imageTwitter || imageOG;\n  const imageAltTwitter = frontmatter.imageAltTwitter || descriptionTwitter;\n  const canonicalUrl = frontmatter.canonicalURL;\n  const flagHidden = frontmatter.flagHidden || false;\n  const mainTag = frontmatter.mainTag || \"Posts\";\n  const section = frontmatter.slug.split('/')[1] || \"posts\";\n  const type = \"article\";\n  const {siteUrl} = useSiteMetadata();\n  const breadcrumbJSON = {\n    \"@context\": \"https://schema.org\",\n    \"@type\": \"BreadcrumbList\",\n    \"itemListElement\": [{\n      \"@type\": \"ListItem\",\n      \"position\": 1,\n      \"name\": \"Home\",\n      \"item\": siteUrl\n    }, {\n      \"@type\": \"ListItem\",\n      \"position\": 2,\n      \"name\": mainTag,\n      \"item\": `${siteUrl}/${frontmatter.slug.split('/')[1]}`\n    }, {\n      \"@type\": \"ListItem\",\n      \"position\": 3,\n      \"name\": title,\n      \"item\": `${siteUrl}${frontmatter.slug}`\n    }]\n  };\n  return React.createElement(SEO, {\n    title: title + \" - avrtt.blog\",\n    titleOG: titleOG,\n    titleTwitter: titleTwitter,\n    description: description,\n    descriptionOG: descriptionOG,\n    descriptionTwitter: descriptionTwitter,\n    schemaType: schemaType,\n    keywords: keywords,\n    datePublished: datePublished,\n    dateModified: dateModified,\n    imageOG: imageOG,\n    imageAltOG: imageAltOG,\n    imageTwitter: imageTwitter,\n    imageAltTwitter: imageAltTwitter,\n    canonicalUrl: canonicalUrl,\n    flagHidden: flagHidden,\n    mainTag: mainTag,\n    section: section,\n    type: type\n  }, React.createElement(\"script\", {\n    type: \"application/ld+json\"\n  }, JSON.stringify(breadcrumbJSON)));\n}\nexport const query = graphql`\n  query($id: String!, $postsFilterRegex: String!, $imagePathRegex: String!) {\n    mdx(id: { eq: $id }) {\n      frontmatter {\n        index\n        indexCourse\n        title\n        titleSEO\n        titleOG\n        titleTwitter\n        courseCategoryName\n        desc\n        descSEO\n        descOG\n        descTwitter\n        date\n        updated\n        extraReadTimeMin\n        difficultyLevel\n        flagDraft\n        flagMindfuckery\n        flagRewrite\n        flagOffensive\n        flagProfane\n        flagMultilingual\n        flagUnreliably\n        flagPolitical\n        flagCognitohazard\n        flagHidden\n        flagWideLayoutByDefault\n        schemaType\n        mainTag\n        otherTags\n        keywordsSEO\n        banner {\n          childImageSharp {\n            gatsbyImageData(\n\t\t\t\t\t\t\tformats: [JPG, WEBP], \n\t\t\t\t\t\t\tplaceholder: BLURRED, \n\t\t\t\t\t\t\tquality: 100\n\t\t\t\t\t\t)\n          }\n        }\n        imageOG\n        imageAltOG\n        imageTwitter\n        imageAltTwitter\n        canonicalURL\n        slug\n      }\n      body\n      tableOfContents(maxDepth: 3)\n    }\n    allMdx(filter: {frontmatter: {slug: {regex: $postsFilterRegex}}}) {\n      nodes {\n        frontmatter {\n          index\n          slug\n          banner {\n            childImageSharp {\n              gatsbyImageData(\n                formats: [JPG, WEBP],\n                placeholder: BLURRED,\n                quality: 100\n              )\n            }\n          }\n        }\n      }\n    }\n    allPostImages: allFile(\n      filter: { \n        sourceInstanceName: { eq: \"images\" },\n        relativePath: { regex: $imagePathRegex }\n      }\n    ) {\n      nodes {\n        relativePath\n        childImageSharp {\n          gatsbyImageData(\n            layout: CONSTRAINED\n            placeholder: DOMINANT_COLOR\n            quality: 100\n          )\n        }\n      }\n    }\n  }\n`;\n","// extracted by mini-css-extract-plugin\nexport var info = \"styles-module--info--26c1f\";\nexport var infoBadge = \"styles-module--infoBadge--e3d66\";\nexport var tooltipWrapper = \"styles-module--tooltipWrapper--75ebf\";\nexport var tooltiptext = \"styles-module--tooltiptext--a263b\";\nexport var visible = \"styles-module--visible--c063c\";","import React, { useState, useEffect, useRef } from \"react\";\nimport info from \"../../images/goals/info.svg\"\nimport * as styles from \"./styles.module.scss\"\n\nconst Tooltip = ({ text, isBadge=false }) => {\n    const [isOpen, setIsOpen] = useState(false);\n    const tooltipRef = useRef(null);\n\n    const handleIconClick = (e) => {\n        e.stopPropagation();\n        setIsOpen((prev) => !prev);\n    };\n\n    useEffect(() => {\n        function handleClickOutside(event) {\n            if (tooltipRef.current && !tooltipRef.current.contains(event.target)) {\n                setIsOpen(false);\n            }\n        }\n        document.addEventListener(\"click\", handleClickOutside);\n        return () => {\n            document.removeEventListener(\"click\", handleClickOutside);\n        };\n    }, []);\n\n    return (\n        <span className={styles.tooltipWrapper} ref={tooltipRef}>\n            <img id={isBadge ? styles.infoBadge : styles.info} src={info} alt='info' onClick={handleIconClick}/>\n            <span className={isOpen ? `${styles.tooltiptext} ${styles.visible}` : styles.tooltiptext}>\n                {text}\n            </span>\n        </span>\n    );\n};\n\nexport default Tooltip;\n","import React from \"react\";\nimport Latex from 'react-latex-next';\nimport 'katex/dist/katex.min.css'; \n  \nconst L = ({ text }) => {\n  return (\n    <Latex>{text}</Latex>\n  );\n};\nexport default L;\n"],"names":["_createMdxContent","props","_components","Object","assign","p","h3","a","span","ul","li","hr","h2","strong","h4","ol","_provideComponents","components","React","id","style","position","href","className","dangerouslySetInnerHTML","__html","Highlight","Tooltip","text","Latex","Code","wrapper","MDXLayout","TableOfContents","_ref","toc","items","stylesTableOfContents","map","item","index","key","url","onClick","e","handleClick","preventDefault","targetId","replace","targetElement","document","getElementById","scrollIntoView","behavior","block","title","PostTemplate","_ref2","data","mdx","allMdx","allPostImages","children","frontmatter","body","tableOfContents","section","slug","split","sortedPosts","nodes","filter","post","includes","sort","b","currentIndex","findIndex","nextPost","lastPost","trimmedSlug","keyCurrent","exec","basePath","isWideLayout","setIsWideLayout","useState","flagWideLayoutByDefault","isAnimating","setIsAnimating","wordsPerMinute","useEffect","timer","setTimeout","clearTimeout","wordsPerMinuteAdventures","wordsPerMinuteResearch","wordsPerMinuteThoughts","wordCount","RemoveMarkdown","trim","length","readTime","minutes","hours","Math","floor","remainder","formatReadTime","ceil","extraReadTimeMin","notices","flag","flagDraft","component","flagMindfuckery","flagRewrite","flagOffensive","flagProfane","flagMultilingual","flagUnreliably","flagPolitical","flagCognitohazard","flagHidden","loadedNotices","setLoadedNotices","forEach","_ref3","then","module","prev","concat","_toConsumableArray","default","motion","div","initial","opacity","animate","exit","transition","duration","PostBanner","postNumber","date","updated","difficulty","difficultyLevel","desc","banner","postKey","isMindfuckery","mainTag","display","justifyContent","flexWrap","maxWidth","marginLeft","paddingRight","marginTop","marginBottom","otherTags","tag","stylesTagBadges","margin","class","textAlign","button","stylesCustomPostLayouts","toggleLayout","whileTap","scale","stylesButtonsCommon","ease","NoticeComponent","indexCourse","PartOfCourseNotice","category","courseCategoryName","ImageContext","Provider","value","images","MDXProvider","Image","PostBottom","GatsbyMDXWrapper","GATSBY_COMPILED_MDX","Head","_ref4","_frontmatter$banner","_frontmatter$banner$c","_frontmatter$banner$c2","_frontmatter$banner$c3","_frontmatter$banner$c4","titleSEO","titleOG","titleTwitter","description","descSEO","descriptionOG","descOG","descriptionTwitter","descTwitter","schemaType","keywords","keywordsSEO","datePublished","dateModified","imageOG","childImageSharp","gatsbyImageData","fallback","src","imageAltOG","imageTwitter","imageAltTwitter","canonicalUrl","canonicalURL","siteUrl","useSiteMetadata","breadcrumbJSON","SEO","type","JSON","stringify","tooltiptext","isBadge","isOpen","setIsOpen","tooltipRef","useRef","handleClickOutside","event","current","contains","target","addEventListener","removeEventListener","ref","info","alt","stopPropagation","styles"],"sourceRoot":""}