"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[7833],{64911:function(e,n,t){t.r(n),t.d(n,{Head:function(){return A},PostTemplate:function(){return k},default:function(){return H}});var a=t(54506),r=t(28453),i=t(96540),o=(t(16886),t(46295)),l=t(96098);function s(e){const n=Object.assign({p:"p",h3:"h3",a:"a",span:"span",ul:"ul",li:"li",strong:"strong",ol:"ol",h2:"h2",em:"em"},(0,r.RP)(),e.components),{Image:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),i.createElement(i.Fragment,null,"\n",i.createElement("br"),"\n","\n","\n","\n",i.createElement(n.p,null,"Autoencoders are neural network architectures designed to learn an efficient, compressed representation of input data in an unsupervised manner. Historically, this notion can be traced back decades, with early neural network research already hinting at the idea of encoding and decoding data through narrower hidden layers (Baldi and Hornik, 1989). However, it was only with improvements in training algorithms (e.g., backpropagation refinements and GPU-accelerated optimization) and larger datasets that autoencoders rose to prominence as powerful tools for representation learning, data compression, and a wide variety of generative or semi-supervised tasks."),"\n",i.createElement(n.p,null,"A hallmark characteristic of an autoencoder is its central bottleneck layer, where the dimensionality is typically far lower than that of the input. This compressed bottleneck, often called the latent space or code, forces the network to discover and learn salient features about the data. By instructing the network to reconstruct the original input from the code, we effectively train it to capture the underlying structure and distribution of the data with minimal loss (or in some cases, controlled loss)."),"\n",i.createElement(n.h3,{id:"definition-and-historical-context-of-autoencoder-models-in-machine-learning",style:{position:"relative"}},i.createElement(n.a,{href:"#definition-and-historical-context-of-autoencoder-models-in-machine-learning","aria-label":"definition and historical context of autoencoder models in machine learning permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Definition and historical context of autoencoder models in machine learning"),"\n",i.createElement(n.p,null,"An autoencoder is generally composed of two primary components: an encoder ",i.createElement(l.A,{text:"\\( f_\\theta \\)"})," that maps an input vector ",i.createElement(l.A,{text:"\\(x \\in \\mathbb{R}^D\\)"})," to a hidden representation ",i.createElement(l.A,{text:"\\(z \\in \\mathbb{R}^d\\)"})," (where typically ",i.createElement(l.A,{text:"\\(d < D\\)"}),"), and a decoder ",i.createElement(l.A,{text:"\\( g_\\phi \\)"})," that reconstructs the input from ",i.createElement(l.A,{text:"\\(z\\)"}),". In notation:"),"\n",i.createElement(l.A,{text:"\\[\nz = f_{\\theta}(x), \\quad \\hat{x} = g_{\\phi}(z).\n\\]"}),"\n",i.createElement(n.p,null,"The goal is to train ",i.createElement(l.A,{text:"\\( \\theta \\)"})," and ",i.createElement(l.A,{text:"\\( \\phi \\)"})," such that ",i.createElement(l.A,{text:"\\(\\hat{x}\\)"})," is as close to ",i.createElement(l.A,{text:"\\(x\\)"})," as possible, according to a chosen reconstruction loss. Historically, autoencoders came into focus alongside other neural approaches to unsupervised learning, with milestone works such as Hinton and Salakhutdinov (2006), which demonstrated how deep autoencoders could be used to find low-dimensional representations of data comparable to (and often surpassing) principal component analysis (PCA) on certain tasks."),"\n",i.createElement(n.h3,{id:"core-principle-learning-a-compressed-representation-latent-space-through-unsupervised-learning",style:{position:"relative"}},i.createElement(n.a,{href:"#core-principle-learning-a-compressed-representation-latent-space-through-unsupervised-learning","aria-label":"core principle learning a compressed representation latent space through unsupervised learning permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Core principle: learning a compressed representation (latent space) through unsupervised learning"),"\n",i.createElement(n.p,null,'Unlike supervised feed-forward networks that optimize a predictive mapping from inputs to labels, autoencoders optimize an internal representation that best reconstructs the original input. In other words, they learn from unlabeled data by simply taking the inputs as both "input" and "target." This approach is a key advantage in scenarios where labeled data is scarce or expensive, but unlabeled data is abundant.'),"\n",i.createElement(n.p,null,'The latent space (sometimes also called the "code") acts as a compressed, learned abstraction of the data. This abstraction can serve myriad purposes, such as:'),"\n",i.createElement(n.ul,null,"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Feature extraction"),": Downstream supervised tasks can benefit from autoencoder-derived features that capture the underlying structure of data in fewer dimensions."),"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Data visualization"),": By reducing data to two or three dimensions, one can visualize complex patterns or cluster structures in data."),"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Generative modeling"),": In some variants, e.g., variational autoencoders, the latent space is structured to enable random sampling and data generation."),"\n"),"\n",i.createElement(n.h3,{id:"difference-between-autoencoders-and-other-neural-network-architectures",style:{position:"relative"}},i.createElement(n.a,{href:"#difference-between-autoencoders-and-other-neural-network-architectures","aria-label":"difference between autoencoders and other neural network architectures permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Difference between autoencoders and other neural network architectures"),"\n",i.createElement(n.p,null,"Autoencoders differ significantly from supervised neural networks, in that they do not rely on explicit labels or targets separate from the input data. Furthermore, autoencoders can be contrasted with feed-forward classifiers in the sense that:"),"\n",i.createElement(n.ul,null,"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Objective function"),": Classification or regression networks minimize an error between predictions and known labels, whereas autoencoders minimize reconstruction error between reconstructed input and original input."),"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Architecture"),": Although they use many of the same building blocks (layers, activation functions, optimizers), autoencoders generally impose a bottleneck or some constraint (like sparsity) that encourages the network to learn a compressed internal representation."),"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Applications"),": While feed-forward classifiers aim to assign class labels, autoencoders revolve around representation learning, generative modeling, and data pre-processing or augmentation."),"\n"),"\n",i.createElement(n.h3,{id:"role-of-autoencoders-in-representation-learning-and-feature-extraction",style:{position:"relative"}},i.createElement(n.a,{href:"#role-of-autoencoders-in-representation-learning-and-feature-extraction","aria-label":"role of autoencoders in representation learning and feature extraction permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Role of autoencoders in representation learning and feature extraction"),"\n",i.createElement(n.p,null,"Representation learning is a cornerstone of modern machine learning research, focused on how to automatically learn features or embeddings from data. Autoencoders are a strong candidate for representation learning, as the process of compressing and reconstructing data inherently forces the model to learn salient, robust features that capture the essential characteristics of the input."),"\n",i.createElement(n.ul,null,"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Transfer learning"),": When data is limited in a target task, a pre-trained autoencoder from a similar domain can provide an insightful, lower-dimensional embedding."),"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Self-supervised learning"),": By focusing on a self-generated objective (reconstruction), autoencoders exemplify the notion of extracting signal from unlabeled data."),"\n"),"\n",i.createElement(n.h3,{id:"dimensionality-reduction-for-visualization-and-compact-feature-learning-with-autoencoders",style:{position:"relative"}},i.createElement(n.a,{href:"#dimensionality-reduction-for-visualization-and-compact-feature-learning-with-autoencoders","aria-label":"dimensionality reduction for visualization and compact feature learning with autoencoders permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Dimensionality reduction for visualization and compact feature learning with autoencoders"),"\n",i.createElement(n.p,null,"Dimensionality reduction is a vital task in data science, as it helps to mitigate the curse of dimensionality, reduce overfitting, and produce more interpretable visualizations. Classic methods like principal component analysis (PCA) provide a linear transformation to a lower-dimensional subspace. By contrast, autoencoders can learn ",i.createElement(n.strong,null,"non-linear")," transformations, thus capturing more complex patterns than linear methods."),"\n",i.createElement(n.p,null,"When used for visualization, a deep autoencoder might project high-dimensional data (such as images of 28x28=784 pixels from MNIST) onto a 2- or 3-dimensional manifold, enabling us to see interesting clusters, outliers, or latent groupings in the data."),"\n",i.createElement(n.h3,{id:"anomaly-detection-with-autoencoders",style:{position:"relative"}},i.createElement(n.a,{href:"#anomaly-detection-with-autoencoders","aria-label":"anomaly detection with autoencoders permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Anomaly detection with autoencoders"),"\n",i.createElement(n.p,null,'Autoencoders are a popular method for anomaly detection, particularly when anomalies are defined as "uncommon patterns" that deviate from the training distribution. The reasoning is:'),"\n",i.createElement(n.ol,null,"\n",i.createElement(n.li,null,'Train an autoencoder on "normal" data.'),"\n",i.createElement(n.li,null,"Because the model only sees normal data, it learns to reconstruct typical patterns well."),"\n",i.createElement(n.li,null,"When presented with an anomalous input that deviates significantly from normal patterns, the reconstruction error should (in theory) spike, as the autoencoder is not accustomed to such an input."),"\n",i.createElement(n.li,null,"A threshold on reconstruction error can then be used to flag anomalies."),"\n"),"\n",i.createElement(n.p,null,"This approach finds use in fraud detection, manufacturing defect detection, medical image anomaly spotting, and more."),"\n",i.createElement(n.h3,{id:"image-denoising-super-resolution-and-inpainting-through-specialized-decoders",style:{position:"relative"}},i.createElement(n.a,{href:"#image-denoising-super-resolution-and-inpainting-through-specialized-decoders","aria-label":"image denoising super resolution and inpainting through specialized decoders permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Image denoising, super-resolution, and inpainting through specialized decoders"),"\n",i.createElement(n.p,null,'A compelling property of many autoencoders is their ability to "fill in" missing information or remove noise by design. Specifically:'),"\n",i.createElement(n.ul,null,"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Denoising autoencoders")," (Vincent and gang, 2008) are trained on data corrupted by random noise, with the reconstruction target being the uncorrupted input. This forces the network to learn robust representations that filter out the noise."),"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Super-resolution")," can be viewed as a specialized form of autoencoder, where the encoder takes a low-resolution image, and the decoder is trained to produce a higher-resolution version."),"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Inpainting")," tasks set parts of an image to missing or corrupted, and the autoencoder must reconstruct those missing regions based on learned context from the training set."),"\n"),"\n",i.createElement(n.h3,{id:"data-augmentation-and-generative-modeling-for-downstream-tasks",style:{position:"relative"}},i.createElement(n.a,{href:"#data-augmentation-and-generative-modeling-for-downstream-tasks","aria-label":"data augmentation and generative modeling for downstream tasks permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Data augmentation and generative modeling for downstream tasks"),"\n",i.createElement(n.p,null,"Beyond straightforward reconstruction, autoencoders can enrich data with variations or help in generative contexts. For instance, once an autoencoder has learned a latent space, one can manipulate or randomly sample points in that latent space to generate new data. This is especially prevalent with ",i.createElement(n.strong,null,"variational autoencoders")," (VAEs), which impose probabilistic constraints on latent variables."),"\n",i.createElement(n.p,null,"Such generative capabilities can supply additional synthetic training examples for downstream tasks, help the model capture richer data distributions, and sometimes yield creative outputs in domains like image synthesis or text generation."),"\n",i.createElement("br"),"\n",i.createElement(n.h2,{id:"2-theoretical-foundations",style:{position:"relative"}},i.createElement(n.a,{href:"#2-theoretical-foundations","aria-label":"2 theoretical foundations permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2. Theoretical foundations"),"\n",i.createElement(n.p,null,"At the heart of autoencoders lie mathematical underpinnings that define their training objective, connect them to traditional dimensionality reduction methods, and structure them for robust generalization."),"\n",i.createElement(n.h3,{id:"concept-of-reconstruction-loss-mean-squared-error-cross-entropy-and-other-metrics",style:{position:"relative"}},i.createElement(n.a,{href:"#concept-of-reconstruction-loss-mean-squared-error-cross-entropy-and-other-metrics","aria-label":"concept of reconstruction loss mean squared error cross entropy and other metrics permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Concept of reconstruction loss: mean squared error, cross-entropy, and other metrics"),"\n",i.createElement(n.p,null,"The ",i.createElement(n.strong,null,"reconstruction loss")," (a.k.a. reconstruction error) drives autoencoder training. Common loss functions include:"),"\n",i.createElement(n.ul,null,"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Mean Squared Error (MSE)"),":"),"\n"),"\n",i.createElement(l.A,{text:"\\[\n\\mathcal{L}_{\\text{MSE}}(x,\\hat{x}) = \\frac{1}{n}\\sum_{i=1}^{n} (x_i - \\hat{x}_i)^2,\n\\]"}),"\n",i.createElement(n.p,null,"where ",i.createElement(l.A,{text:"\\(x_i\\)"})," is the ",i.createElement(l.A,{text:"\\(i\\)"}),"-th component of the input, and ",i.createElement(l.A,{text:"\\(\\hat{x}_i\\)"})," is the reconstructed value. MSE is typical for continuous data such as grayscale image intensities."),"\n",i.createElement(n.ul,null,"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Cross-Entropy Loss"),":"),"\n"),"\n",i.createElement(l.A,{text:"\\[\n\\mathcal{L}_{\\text{CE}}(x,\\hat{x}) = -\\sum_{i=1}^{n} \\left[ x_i \\log(\\hat{x}_i) + (1-x_i)\\log(1-\\hat{x}_i) \\right],\n\\]"}),"\n",i.createElement(n.p,null,"often used for binary or probabilistic outputs (e.g., for Bernoulli distributions, or images scaled to [0,1])."),"\n",i.createElement(n.ul,null,"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Other Metrics"),": Depending on the data type (e.g., images, text, audio), one might employ L1 loss, perceptual losses (using features from a pre-trained network to measure reconstruction quality), or even specialized domain-specific metrics."),"\n"),"\n",i.createElement(n.h3,{id:"dimensionality-reduction-and-manifold-learning-as-a-key-objective-of-autoencoders",style:{position:"relative"}},i.createElement(n.a,{href:"#dimensionality-reduction-and-manifold-learning-as-a-key-objective-of-autoencoders","aria-label":"dimensionality reduction and manifold learning as a key objective of autoencoders permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Dimensionality reduction and manifold learning as a key objective of autoencoders"),"\n",i.createElement(n.p,null,'As autoencoders learn to encode inputs into a latent space, one can view them as a form of non-linear manifold learning. If the high-dimensional data lie on or near some lower-dimensional manifold, the encoder attempts to map data points onto that manifold in latent space, and the decoder attempts to "unfold" them back into the original input space.'),"\n",i.createElement(n.p,null,"This perspective is linked to ideas from manifold learning, such as locally linear embedding or isomap, but autoencoders are more flexible because they rely on fully parametric neural architectures rather than purely geometric approaches."),"\n",i.createElement(n.h3,{id:"relationship-to-pca-principal-component-analysis-and-linear-vs-non-linear-transformations",style:{position:"relative"}},i.createElement(n.a,{href:"#relationship-to-pca-principal-component-analysis-and-linear-vs-non-linear-transformations","aria-label":"relationship to pca principal component analysis and linear vs non linear transformations permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Relationship to PCA (principal component analysis) and linear vs. non-linear transformations"),"\n",i.createElement(n.p,null,i.createElement(n.strong,null,"Principal Component Analysis (PCA)")," is the classic linear technique for dimensionality reduction, finding orthogonal directions (principal components) of maximum variance. There is a direct connection: a single-layer linear autoencoder with MSE loss and no activation function in the hidden layer will essentially learn the same subspace as PCA."),"\n",i.createElement(n.p,null,"However, deeper autoencoders with non-linear activations can capture more complex, curved manifolds, surpassing linear methods in capturing intricate data structures. The difference is:"),"\n",i.createElement(n.ul,null,"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Linear autoencoder")," <=> ",i.createElement(n.strong,null,"PCA")," (same solution space)."),"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Deep non-linear autoencoder")," <=> ",i.createElement(n.strong,null,"Non-linear PCA")," (no closed-form solution, typically more expressive)."),"\n"),"\n",i.createElement(n.h3,{id:"regularization-mechanisms-weight-decay-dropout-to-improve-generalization",style:{position:"relative"}},i.createElement(n.a,{href:"#regularization-mechanisms-weight-decay-dropout-to-improve-generalization","aria-label":"regularization mechanisms weight decay dropout to improve generalization permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Regularization mechanisms (weight decay, dropout) to improve generalization"),"\n",i.createElement(n.p,null,"Autoencoders, like other neural networks, risk overfitting. Regularization strategies:"),"\n",i.createElement(n.ul,null,"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Weight decay"),": Adds an L2 penalty on network weights, encouraging smaller magnitude weights."),"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Dropout"),": Randomly zeros out neurons during training, forcing the model to distribute learned features and improve generalization."),"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Sparsity constraints"),": Encourages many hidden units to remain near zero activation, effectively limiting the capacity of the latent representation."),"\n"),"\n",i.createElement(n.p,null,"These mechanisms help autoencoders learn robust, generalizable embeddings rather than merely memorizing training examples."),"\n",i.createElement("br"),"\n",i.createElement(n.h2,{id:"3-encoder-decoder-topology",style:{position:"relative"}},i.createElement(n.a,{href:"#3-encoder-decoder-topology","aria-label":"3 encoder decoder topology permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3. Encoder-decoder topology"),"\n",i.createElement(n.p,null,"The fundamental structure of an autoencoder is straightforward: an ",i.createElement(n.strong,null,"encoder")," compresses input data, and a ",i.createElement(n.strong,null,"decoder")," reconstructs it back. Yet, design choices around layer width, depth, activation functions, and symmetrical or asymmetrical connectivity can have significant impact on performance."),"\n",i.createElement(n.h3,{id:"encoder-mapping-the-input-to-a-lower-dimensional-latent-representation-bottleneck",style:{position:"relative"}},i.createElement(n.a,{href:"#encoder-mapping-the-input-to-a-lower-dimensional-latent-representation-bottleneck","aria-label":"encoder mapping the input to a lower dimensional latent representation bottleneck permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Encoder: mapping the input to a lower-dimensional latent representation (bottleneck)"),"\n",i.createElement(n.p,null,"The encoder is typically a stack of layers that reduce dimension step by step (or, in the case of convolutional layers, reduce spatial resolution in image tasks). Formally:"),"\n",i.createElement(l.A,{text:"\\(z = f_{\\theta}(x)\\)"}),"\n",i.createElement(n.ul,null,"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Layer sizes")," often funnel from a dimension close to that of the input (e.g., 784 for MNIST) down to a smaller dimension ",i.createElement(l.A,{text:"\\(d\\)"})," for the bottleneck (e.g., 32 or 64)."),"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Non-linear activations")," (e.g., ReLU, sigmoid, tanh) enable the encoder to capture more complex features than a purely linear transform."),"\n"),"\n",i.createElement(n.h3,{id:"decoder-reconstructing-the-original-input-from-the-latent-representation",style:{position:"relative"}},i.createElement(n.a,{href:"#decoder-reconstructing-the-original-input-from-the-latent-representation","aria-label":"decoder reconstructing the original input from the latent representation permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Decoder: reconstructing the original input from the latent representation"),"\n",i.createElement(n.p,null,"Mirroring the encoder, the decoder expands from the latent space dimension ",i.createElement(l.A,{text:"\\(d\\)"})," back to the original dimension ",i.createElement(l.A,{text:"\\(D\\)"}),". In mathematical terms:"),"\n",i.createElement(l.A,{text:"\\( \\hat{x} = g_{\\phi}(z) \\)"}),"\n",i.createElement(n.ul,null,"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Symmetry"),": A common design is to make the decoder symmetrical to the encoder. For instance, if the encoder is five layers with widths decreasing from 512 to 64, the decoder might be five layers with widths increasing from 64 back to 512."),"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Asymmetry"),": Certain tasks benefit from a narrower or deeper decoder. For instance, in generative tasks, the decoder may be more complex than the encoder to capture all the small details needed for high-fidelity reconstruction."),"\n"),"\n",i.createElement(n.h3,{id:"symmetry-in-design-and-its-rationale-possible-asymmetries-for-specialized-tasks",style:{position:"relative"}},i.createElement(n.a,{href:"#symmetry-in-design-and-its-rationale-possible-asymmetries-for-specialized-tasks","aria-label":"symmetry in design and its rationale possible asymmetries for specialized tasks permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Symmetry in design and its rationale; possible asymmetries for specialized tasks"),"\n",i.createElement(n.p,null,'The rationale behind a symmetrical autoencoder is partly intuitive: we are applying a reverse transformation, so "mirroring" the architecture can make sense. Another rationale is that symmetrical networks can simplify design considerations and, in some cases, produce balanced gradient flows. Nonetheless, advanced tasks (super-resolution, generative modeling with skip connections, etc.) often break strict symmetry to incorporate domain-specific knowledge or more flexible decoding strategies.'),"\n",i.createElement(n.h3,{id:"activation-functions-and-considerations-for-stable-training",style:{position:"relative"}},i.createElement(n.a,{href:"#activation-functions-and-considerations-for-stable-training","aria-label":"activation functions and considerations for stable training permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Activation functions and considerations for stable training"),"\n",i.createElement(n.p,null,"Activation functions like ReLU (",i.createElement(l.A,{text:"\\( \\max(0, x)\\)"}),") are widely used in modern architectures thanks to their simplicity and reduced vanishing gradient issues. Sigmoid or tanh might still be used for the output layer if the input data is normalized to [0,1] or [-1,1]. Alternative activation strategies include ELU, SELU, or leaky ReLU, each with its pros and cons related to training stability and expressiveness."),"\n",i.createElement(n.p,null,"Autoencoders can face the vanishing or exploding gradient problem in deeper architectures. Techniques such as skip connections (as in residual networks), careful weight initialization, and normalization layers help mitigate these issues."),"\n",i.createElement("br"),"\n",i.createElement(n.h2,{id:"4-training-methodologies",style:{position:"relative"}},i.createElement(n.a,{href:"#4-training-methodologies","aria-label":"4 training methodologies permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4. Training methodologies"),"\n",i.createElement(n.p,null,"Training an autoencoder parallels training most neural networks â€” typically via stochastic gradient descent (SGD) or its variants. However, certain subtleties arise from the unsupervised nature of the task and the interplay between encoder and decoder."),"\n",i.createElement(n.h3,{id:"unsupervised-training-objective-minimizing-reconstruction-error",style:{position:"relative"}},i.createElement(n.a,{href:"#unsupervised-training-objective-minimizing-reconstruction-error","aria-label":"unsupervised training objective minimizing reconstruction error permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Unsupervised training objective: minimizing reconstruction error"),"\n",i.createElement(n.p,null,"Because autoencoders do not rely on external labels, their primary objective is the reconstruction loss:"),"\n",i.createElement(l.A,{text:"\\[\n\\min_{\\theta,\\phi} \\sum_{x \\in \\mathcal{D}} \\mathcal{L} \\bigl(x, g_\\phi(f_\\theta(x))\\bigr),\n\\]"}),"\n",i.createElement(n.p,null,"where ",i.createElement(l.A,{text:"\\( \\mathcal{D} \\)"})," is the training dataset and ",i.createElement(l.A,{text:"\\( \\mathcal{L}\\)"})," might be MSE, cross-entropy, or another appropriate measure. One collects a large batch of unlabeled data, feeds each sample forward, computes the reconstruction error, and backpropagates gradients to update encoder and decoder weights."),"\n",i.createElement(n.h3,{id:"batch-normalization-and-layer-normalization-to-stabilize-and-accelerate-training",style:{position:"relative"}},i.createElement(n.a,{href:"#batch-normalization-and-layer-normalization-to-stabilize-and-accelerate-training","aria-label":"batch normalization and layer normalization to stabilize and accelerate training permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Batch normalization and layer normalization to stabilize and accelerate training"),"\n",i.createElement(n.p,null,i.createElement(n.strong,null,"Batch normalization")," (BN) normalizes activations by their mean and standard deviation within a mini-batch, accelerating training convergence and reducing internal covariate shift. ",i.createElement(n.strong,null,"Layer normalization")," (LN) instead normalizes across each data instance's features. Both can be employed in autoencoders to improve stability, especially in deeper networks or when the data distribution is complex."),"\n",i.createElement(n.h3,{id:"initialization-strategies-for-weights-eg-xavier-he-initialization",style:{position:"relative"}},i.createElement(n.a,{href:"#initialization-strategies-for-weights-eg-xavier-he-initialization","aria-label":"initialization strategies for weights eg xavier he initialization permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Initialization strategies for weights (e.g., Xavier, He initialization)"),"\n",i.createElement(n.p,null,"Since reconstruction is an intricate process, poor weight initialization can hamper training. Common strategies:"),"\n",i.createElement(n.ul,null,"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Xavier (Glorot) Initialization"),": Scales weights according to the size of the incoming and outgoing layers, helping keep signals in a reasonable range."),"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"He Initialization"),": Tailored to ReLU-like activations, scaling by the number of input units to preserve variance through layers."),"\n"),"\n",i.createElement(n.h3,{id:"common-optimizers-sgd-adam-rmsprop-and-learning-rate-scheduling",style:{position:"relative"}},i.createElement(n.a,{href:"#common-optimizers-sgd-adam-rmsprop-and-learning-rate-scheduling","aria-label":"common optimizers sgd adam rmsprop and learning rate scheduling permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Common optimizers (SGD, Adam, RMSProp) and learning rate scheduling"),"\n",i.createElement(n.p,null,i.createElement(n.strong,null,"Gradient-based")," optimizers remain the mainstay for training autoencoders:"),"\n",i.createElement(n.ul,null,"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"SGD"),": Often with momentum, can perform well on large-scale problems but may converge slowly if learning rates are not tuned carefully."),"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Adam"),": Adapts learning rates per parameter, often delivering fast convergence."),"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"RMSProp"),": Similar to Adam in that it scales gradients by a running average of their magnitude, can be beneficial for certain tasks."),"\n"),"\n",i.createElement(n.p,null,"Learning rate schedules (step decay, exponential decay, or cyclical learning rates) can further refine performance and help avoid local minima or stall in training."),"\n",i.createElement(n.h3,{id:"handling-overfitting-early-stopping-validation-based-hyperparameter-tuning",style:{position:"relative"}},i.createElement(n.a,{href:"#handling-overfitting-early-stopping-validation-based-hyperparameter-tuning","aria-label":"handling overfitting early stopping validation based hyperparameter tuning permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Handling overfitting: early stopping, validation-based hyperparameter tuning"),"\n",i.createElement(n.p,null,"Even in unsupervised settings, one can apply standard ",i.createElement(n.strong,null,"early stopping")," by monitoring the reconstruction error on a hold-out validation set. If the error stops decreasing or starts to rise (indicative of overfitting), training can be halted. Further hyperparameter tuning can revolve around the size of the bottleneck, weight decay parameters, or the learning rate schedule."),"\n",i.createElement("br"),"\n",i.createElement(n.h2,{id:"5-denoising-autoencoders",style:{position:"relative"}},i.createElement(n.a,{href:"#5-denoising-autoencoders","aria-label":"5 denoising autoencoders permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5. Denoising autoencoders"),"\n",i.createElement(n.p,null,"Denoising autoencoders (DAEs) (Vincent and gang, 2008) are a cornerstone extension that enhances the robustness of learned representations. The approach:"),"\n",i.createElement(n.ol,null,"\n",i.createElement(n.li,null,"Corrupt the original input ",i.createElement(l.A,{text:"\\(x\\)"})," by adding noise or randomly setting some inputs to zero. Call this noisy version ",i.createElement(l.A,{text:"\\( \\tilde{x} \\)"}),"."),"\n",i.createElement(n.li,null,"Feed ",i.createElement(l.A,{text:"\\( \\tilde{x} \\)"})," into the encoder. Let ",i.createElement(l.A,{text:"\\(z = f_\\theta(\\tilde{x})\\)"}),"."),"\n",i.createElement(n.li,null,"Reconstruct the ",i.createElement(n.em,null,"uncorrupted")," target ",i.createElement(l.A,{text:"\\( x\\)"})," from ",i.createElement(l.A,{text:"\\(z\\)"})," with the decoder: ",i.createElement(l.A,{text:"\\(\\hat{x} = g_\\phi(z)\\)"}),"."),"\n",i.createElement(n.li,null,"Minimize the reconstruction loss ",i.createElement(l.A,{text:"\\( \\mathcal{L}(x, \\hat{x})\\)"}),"."),"\n"),"\n",i.createElement(n.p,null,"Because the network sees corrupted data but is tasked to reconstruct the clean version, it learns features that are invariant to small perturbations or missing components. This yields a more robust latent representation that can prove useful for a variety of downstream applications."),"\n",i.createElement("br"),"\n",i.createElement(n.h2,{id:"6-sparse-autoencoders",style:{position:"relative"}},i.createElement(n.a,{href:"#6-sparse-autoencoders","aria-label":"6 sparse autoencoders permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6. Sparse autoencoders"),"\n",i.createElement(n.p,null,"Sparse autoencoders introduce a constraint that the hidden units in the bottleneck (or in other layers) should mostly remain inactive, except for a small subset. This sparsity can be enforced in multiple ways:"),"\n",i.createElement(n.ul,null,"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"L1 penalty on activations"),": Encourages many neurons to stay near zero by adding ",i.createElement(l.A,{text:"\\( \\lambda \\sum_{j} |z_j| \\)"})," to the loss, where ",i.createElement(l.A,{text:"\\(z_j\\)"})," are hidden activations."),"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"KL divergence penalty"),": One can specify a desired activation level ",i.createElement(l.A,{text:"\\( \\rho \\)"})," and penalize deviations from that via ",i.createElement(l.A,{text:"\\( \\sum_j \\mathrm{KL}(\\rho || \\hat{\\rho_j})\\)"}),", where ",i.createElement(l.A,{text:"\\( \\hat{\\rho_j} \\)"})," is the average activation of neuron ",i.createElement(l.A,{text:"\\(j\\)"}),"."),"\n"),"\n",i.createElement(n.p,null,'Sparsity can yield representations that are more interpretable and can reflect more localized "features," akin to the way some neurons in the visual cortex respond strongly to specific patterns and remain dormant otherwise.'),"\n",i.createElement("br"),"\n",i.createElement(n.h2,{id:"7-contractive-autoencoders",style:{position:"relative"}},i.createElement(n.a,{href:"#7-contractive-autoencoders","aria-label":"7 contractive autoencoders permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7. Contractive autoencoders"),"\n",i.createElement(n.p,null,i.createElement(n.strong,null,"Contractive autoencoders")," (CAE) explicitly penalize the sensitivity of the encoder mapping to small changes in input. Formally, this can be done by adding to the loss a term proportional to the Frobenius norm of the Jacobian of the encoder activations with respect to the input:"),"\n",i.createElement(l.A,{text:"\\[\n\\lambda \\sum_x \\left \\|\\frac{\\partial f_\\theta(x)}{\\partial x}\\right\\|_F^2.\n\\]"}),"\n",i.createElement(n.p,null,'This penalty encourages the learned representation to be locally invariant to perturbations, thus "contracting" the manifold around each training point. This can improve robustness and lead to learning of smoother latent manifolds.'),"\n",i.createElement("br"),"\n",i.createElement(n.h2,{id:"8-variational-autoencoders-vaes",style:{position:"relative"}},i.createElement(n.a,{href:"#8-variational-autoencoders-vaes","aria-label":"8 variational autoencoders vaes permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8. Variational autoencoders (VAEs)"),"\n",i.createElement(n.p,null,i.createElement(n.strong,null,"Variational autoencoders")," (VAEs) (Kingma and Welling, 2014) reframe autoencoders through a Bayesian lens, making them generative models capable of sampling from the latent space to create new data points. The key ideas:"),"\n",i.createElement(n.ol,null,"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Latent variables"),": Instead of learning a deterministic code ",i.createElement(l.A,{text:"\\(z\\)"}),", the encoder learns parameters of a distribution ",i.createElement(l.A,{text:"\\( q_\\theta(z|x) \\)"}),", typically a Gaussian with mean ",i.createElement(l.A,{text:"\\(\\mu\\)"})," and variance ",i.createElement(l.A,{text:"\\(\\sigma^2\\)"}),"."),"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Reparameterization trick"),": A random variable ",i.createElement(l.A,{text:"\\(z\\)"})," is sampled via ",i.createElement(l.A,{text:"\\(z = \\mu + \\sigma \\odot \\epsilon\\)"}),", where ",i.createElement(l.A,{text:"\\(\\epsilon \\sim \\mathcal{N}(0,I)\\)"}),". This allows gradients to backpropagate through the random sampling."),"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Decoder"),": The decoder defines a distribution ",i.createElement(l.A,{text:"\\( p_\\phi(x|z) \\)"})," from which the reconstructed sample ",i.createElement(l.A,{text:"\\(\\hat{x}\\)"})," is drawn."),"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Loss"),": The loss function has two parts:","\n",i.createElement(n.ul,null,"\n",i.createElement(n.li,null,"A ",i.createElement(n.strong,null,"reconstruction term")," (e.g., log-likelihood under ",i.createElement(l.A,{text:"\\( p_\\phi(x|z)\\)"}),") ensuring that generated samples match the real data."),"\n",i.createElement(n.li,null,"A ",i.createElement(n.strong,null,"regularization term")," ensuring ",i.createElement(l.A,{text:"\\( q_\\theta(z|x) \\)"})," remains close to a prior ",i.createElement(l.A,{text:"\\( p(z) \\)"})," (commonly a standard normal), typically measured via the KL divergence ",i.createElement(l.A,{text:"\\( \\mathrm{KL}[q_\\theta(z|x) || p(z)]\\)"}),"."),"\n"),"\n"),"\n"),"\n",i.createElement(n.p,null,"Hence, the VAE tries to ",i.createElement(n.em,null,"both")," reconstruct data and learn a latent space distribution from which one can sample novel points that look like the original dataset. This is a major leap beyond classical autoencoders, facilitating deep generative modeling across images, text, and more."),"\n",i.createElement("br"),"\n",i.createElement(n.h2,{id:"9-convolutional-autoencoders-caes",style:{position:"relative"}},i.createElement(n.a,{href:"#9-convolutional-autoencoders-caes","aria-label":"9 convolutional autoencoders caes permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9. Convolutional autoencoders (CAEs)"),"\n",i.createElement(n.p,null,i.createElement(n.strong,null,"Convolutional autoencoders")," harness convolutional layers in both encoder and decoder, making them well-suited for image or grid-like data. Convolutional layers reduce spatial dimensionality (via strides or pooling) and can detect local features. Inversely, the decoder uses transposed convolutions (a.k.a. deconvolutions) or upsampling to reconstruct the original spatial resolution."),"\n",i.createElement(n.p,null,"For instance, an encoder might:"),"\n",i.createElement(n.ol,null,"\n",i.createElement(n.li,null,"Take an ",i.createElement(l.A,{text:"\\(H\\times W\\)"})," image."),"\n",i.createElement(n.li,null,"Apply a series of conv layers with strides, reducing the spatial dimension."),"\n",i.createElement(n.li,null,"Arrive at a bottleneck ",i.createElement(l.A,{text:"\\( z \\)"})," which is smaller in height and width."),"\n"),"\n",i.createElement(n.p,null,"The decoder:"),"\n",i.createElement(n.ol,null,"\n",i.createElement(n.li,null,"Takes ",i.createElement(l.A,{text:"\\( z \\)"})," and uses transposed conv or upsampling layers to get back to an ",i.createElement(l.A,{text:"\\(H\\times W\\)"})," image."),"\n",i.createElement(n.li,null,"Outputs a reconstructed image ",i.createElement(l.A,{text:"\\(\\hat{x}\\)"}),"."),"\n"),"\n",i.createElement(n.p,null,"Such CAEs are widely used in image denoising, super-resolution, image-to-image translation, and various other computer vision tasks where local spatial coherence is crucial."),"\n",i.createElement("br"),"\n",i.createElement(n.h2,{id:"10-recurrent-autoencoders-raes",style:{position:"relative"}},i.createElement(n.a,{href:"#10-recurrent-autoencoders-raes","aria-label":"10 recurrent autoencoders raes permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"10. Recurrent autoencoders (RAEs)"),"\n",i.createElement(n.p,null,"When dealing with sequential data (e.g., text, time series, speech), ",i.createElement(n.strong,null,"recurrent autoencoders")," employ recurrent neural network (RNN) cells like LSTM or GRU for both encoding and decoding. The encoder RNN reads a sequence ",i.createElement(l.A,{text:"\\(\\{x_1, x_2, \\ldots, x_T\\}\\)"})," and produces a hidden state that serves as the compressed representation. The decoder RNN attempts to reconstruct the sequence from that hidden state."),"\n",i.createElement(n.p,null,"Recurrent autoencoders can capture temporal dependencies in sequences and are often used for tasks such as:"),"\n",i.createElement(n.ul,null,"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Time series anomaly detection"),": By reconstructing normal sequences, anomalies produce higher reconstruction errors."),"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Text-based representation"),": When used for textual data, a recurrent autoencoder can learn a latent representation that captures semantic or syntactic structures."),"\n"),"\n",i.createElement("br"),"\n",i.createElement(n.h2,{id:"11-residual-and-ladder-autoencoders",style:{position:"relative"}},i.createElement(n.a,{href:"#11-residual-and-ladder-autoencoders","aria-label":"11 residual and ladder autoencoders permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"11. Residual and ladder autoencoders"),"\n",i.createElement(n.p,null,i.createElement(n.strong,null,"Residual autoencoders")," adopt skip connections reminiscent of ResNets, where the input of a layer is added to its output. This helps mitigate vanishing gradients and allows deeper autoencoder architectures to train effectively."),"\n",i.createElement(n.p,null,i.createElement(n.strong,null,"Ladder networks")," (Rasmus and gang, 2015) are a more complex approach, combining denoising objectives with skip connections that link encoder and decoder at every layer. Each decoder layer tries to denoise the encoder's corresponding latent representation, leading to improved unsupervised feature extraction, even in semi-supervised contexts."),"\n",i.createElement("br"),"\n",i.createElement(n.h2,{id:"12-generative-adversarial-autoencoders-aaes",style:{position:"relative"}},i.createElement(n.a,{href:"#12-generative-adversarial-autoencoders-aaes","aria-label":"12 generative adversarial autoencoders aaes permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"12. Generative adversarial autoencoders (AAEs)"),"\n",i.createElement(n.p,null,i.createElement(n.strong,null,"Adversarial autoencoders")," (Makhzani and gang, 2015) bridge autoencoders with the adversarial framework introduced by generative adversarial networks (GANs). Essentially:"),"\n",i.createElement(n.ol,null,"\n",i.createElement(n.li,null,"One trains an autoencoder (encoder + decoder) to minimize reconstruction error."),"\n",i.createElement(n.li,null,"Simultaneously, one imposes a constraint on the latent space by using an adversary (a discriminator) that forces the encoder's latent distribution to match some prior ",i.createElement(l.A,{text:"\\(p(z)\\)"})," (e.g., a standard Gaussian)."),"\n",i.createElement(n.li,null,"The discriminator tries to distinguish real samples from the prior distribution vs. encoded samples from data. The encoder aims to fool the discriminator, effectively aligning the latent space with ",i.createElement(l.A,{text:"\\(p(z)\\)"}),"."),"\n"),"\n",i.createElement(n.p,null,"Like VAEs, adversarial autoencoders allow direct sampling from a prior in the latent space to generate new data, while also leveraging the flexible power of adversarial training to produce sharper or more detailed reconstructions."),"\n",i.createElement("br"),"\n",i.createElement(n.h2,{id:"13-hyperparameters-and-model-selection",style:{position:"relative"}},i.createElement(n.a,{href:"#13-hyperparameters-and-model-selection","aria-label":"13 hyperparameters and model selection permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"13. Hyperparameters and model selection"),"\n",i.createElement(n.p,null,"Designing an autoencoder involves many hyperparameter choices. The interplay among these choices can significantly affect reconstruction fidelity, latent space interpretability, and training stability."),"\n",i.createElement(n.h3,{id:"bottleneck-size-and-its-impact-on-information-capacity-vs-reconstruction-fidelity",style:{position:"relative"}},i.createElement(n.a,{href:"#bottleneck-size-and-its-impact-on-information-capacity-vs-reconstruction-fidelity","aria-label":"bottleneck size and its impact on information capacity vs reconstruction fidelity permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Bottleneck size and its impact on information capacity vs. reconstruction fidelity"),"\n",i.createElement(n.p,null,"One of the most crucial design decisions is the dimensionality ",i.createElement(l.A,{text:"\\( d \\)"})," of the latent bottleneck:"),"\n",i.createElement(n.ul,null,"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Smaller ",i.createElement(l.A,{text:"\\( d \\)"})),": Forces strong compression. Useful for dimensionality reduction and robust feature learning, but might cause under-representation or high reconstruction errors if the data is very complex."),"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Larger ",i.createElement(l.A,{text:"\\( d \\)"})),": Allows the network to encode more information, often lowering reconstruction error but risking learning trivial identity mappings (especially if ",i.createElement(l.A,{text:"\\( d \\geq D \\)"}),")."),"\n"),"\n",i.createElement(n.h3,{id:"depth-of-the-encoder-and-decoder-balancing-model-complexity-and-computational-cost",style:{position:"relative"}},i.createElement(n.a,{href:"#depth-of-the-encoder-and-decoder-balancing-model-complexity-and-computational-cost","aria-label":"depth of the encoder and decoder balancing model complexity and computational cost permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Depth of the encoder and decoder: balancing model complexity and computational cost"),"\n",i.createElement(n.p,null,"Deeper architectures can capture more complex patterns, but they also:"),"\n",i.createElement(n.ul,null,"\n",i.createElement(n.li,null,"Demand more computation and memory."),"\n",i.createElement(n.li,null,"Risk overfitting unless carefully regularized."),"\n",i.createElement(n.li,null,"May require advanced techniques (skip connections, normalization) to ensure stable training."),"\n"),"\n",i.createElement(n.h3,{id:"choices-of-activation-function-optimizer-and-loss-function-for-specific-data-modalities",style:{position:"relative"}},i.createElement(n.a,{href:"#choices-of-activation-function-optimizer-and-loss-function-for-specific-data-modalities","aria-label":"choices of activation function optimizer and loss function for specific data modalities permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Choices of activation function, optimizer, and loss function for specific data modalities"),"\n",i.createElement(n.ul,null,"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Activation"),": ReLU for hidden layers, sigmoid or tanh for the output layer if the data are normalized, or no activation for purely linear decoders in certain tasks."),"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Optimizer"),": Adam is a common default, but certain tasks might see benefits from RMSProp or even plain SGD with momentum."),"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Loss"),": MSE, cross-entropy, or domain-specific metrics (perceptual losses in computer vision, for example)."),"\n"),"\n",i.createElement(n.h3,{id:"use-of-skip-connections-attention-mechanisms-or-gating-for-enhanced-expressivity",style:{position:"relative"}},i.createElement(n.a,{href:"#use-of-skip-connections-attention-mechanisms-or-gating-for-enhanced-expressivity","aria-label":"use of skip connections attention mechanisms or gating for enhanced expressivity permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Use of skip connections, attention mechanisms, or gating for enhanced expressivity"),"\n",i.createElement(n.p,null,"Autoencoders have been extended in many ways to be more expressive:"),"\n",i.createElement(n.ul,null,"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Skip connections"),": Passing low-level feature maps or embeddings directly from encoder layers to the corresponding decoder layers (U-Net style)."),"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Attention layers"),": Providing the model with the capacity to highlight specific parts of the input during encoding or decoding (common in sequence-to-sequence tasks)."),"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Gating mechanisms"),": Allow the model to learn to dynamically weigh or combine different latent features."),"\n"),"\n",i.createElement(n.p,null,"Such extensions can substantially improve reconstruction quality and produce more informative latent representations."),"\n",i.createElement("br"),"\n",i.createElement(n.h2,{id:"14-implementations",style:{position:"relative"}},i.createElement(n.a,{href:"#14-implementations","aria-label":"14 implementations permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"14. Implementations"),"\n",i.createElement(n.p,null,"Below, I provide general guidance on building and training various autoencoder variants in Python, focusing primarily on frameworks such as PyTorch or TensorFlow/Keras. These examples are meant to be illustrative rather than fully optimized for any specific dataset."),"\n",i.createElement(n.h3,{id:"data-preprocessing-normalization-resizing-for-images-tokenization-for-text",style:{position:"relative"}},i.createElement(n.a,{href:"#data-preprocessing-normalization-resizing-for-images-tokenization-for-text","aria-label":"data preprocessing normalization resizing for images tokenization for text permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Data preprocessing: normalization, resizing for images, tokenization for text"),"\n",i.createElement(n.p,null,"When preparing data for an autoencoder:"),"\n",i.createElement(n.ol,null,"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Images"),": Typically resized to a consistent resolution, then normalized to a known range (e.g., [0,1] or [-1,1])."),"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Text"),": Tokenize and possibly embed the tokens (e.g., via word embeddings). The autoencoder might work directly on embeddings or treat them as sequences of discrete tokens (for more advanced discrete autoencoders)."),"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Time series"),": Might be z-normalized (subtract mean, divide by std) to stabilize training."),"\n"),"\n",i.createElement(n.h3,{id:"gpu-vs-cpu-training-considerations-scalability-to-large-datasets",style:{position:"relative"}},i.createElement(n.a,{href:"#gpu-vs-cpu-training-considerations-scalability-to-large-datasets","aria-label":"gpu vs cpu training considerations scalability to large datasets permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"GPU vs. CPU training considerations, scalability to large datasets"),"\n",i.createElement(n.p,null,"Autoencoders can easily be trained on GPUs to speed up matrix operations, especially for large images or deep networks. For extremely large datasets, one might:"),"\n",i.createElement(n.ul,null,"\n",i.createElement(n.li,null,"Use distributed training or frameworks like PyTorch Lightning."),"\n",i.createElement(n.li,null,"Employ data streaming or sharding to handle data that does not fit in memory."),"\n"),"\n",i.createElement(n.h3,{id:"monitoring-reconstruction-loss-on-training-vs-validation-sets-for-early-stopping",style:{position:"relative"}},i.createElement(n.a,{href:"#monitoring-reconstruction-loss-on-training-vs-validation-sets-for-early-stopping","aria-label":"monitoring reconstruction loss on training vs validation sets for early stopping permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Monitoring reconstruction loss on training vs. validation sets for early stopping"),"\n",i.createElement(n.p,null,"Even though autoencoders are unsupervised, I recommend splitting the dataset into training and validation sets. By tracking reconstruction loss on both, you can detect overfitting or diminishing returns. Stopping early often yields better generalization in the learned representations."),"\n",i.createElement(n.h3,{id:"debugging-strategies-gradient-checking-analyzing-latent-space-distributions",style:{position:"relative"}},i.createElement(n.a,{href:"#debugging-strategies-gradient-checking-analyzing-latent-space-distributions","aria-label":"debugging strategies gradient checking analyzing latent space distributions permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Debugging strategies: gradient checking, analyzing latent space distributions"),"\n",i.createElement(n.ul,null,"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Gradient checking"),": Ensure gradients are not exploding or vanishing. Tools like hooking onto the gradient in PyTorch or gradient summaries in TensorFlow can reveal anomalies."),"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Latent space analysis"),": Periodically visualize the latent space or project it onto 2D using t-SNE or PCA to see if the autoencoder is separating different data clusters meaningfully."),"\n"),"\n",i.createElement(n.h3,{id:"building-sparse-autoencoder-in-python-code-snippets-with-comments",style:{position:"relative"}},i.createElement(n.a,{href:"#building-sparse-autoencoder-in-python-code-snippets-with-comments","aria-label":"building sparse autoencoder in python code snippets with comments permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Building sparse autoencoder in Python: code snippets with comments"),"\n",i.createElement(n.p,null,"Below is a simplified PyTorch example of a sparse autoencoder. The focus is on implementing an L1 penalty on the hidden activations:"),"\n",i.createElement(o.A,{text:'\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass SparseAutoencoder(nn.Module):\n    def __init__(self, input_dim=784, hidden_dim=128):\n        super(SparseAutoencoder, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(True)\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(hidden_dim, input_dim),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        z = self.encoder(x)\n        x_hat = self.decoder(z)\n        return x_hat, z\n\ndef train_sparse_ae(model, data_loader, num_epochs=10, l1_lambda=1e-5, lr=1e-3):\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.MSELoss()\n\n    model.train()\n    for epoch in range(num_epochs):\n        total_loss = 0.0\n        for inputs, _ in data_loader:  # ignoring labels\n            inputs = inputs.view(inputs.size(0), -1)  # flatten\n            optimizer.zero_grad()\n            x_hat, z = model(inputs)\n            mse_loss = criterion(x_hat, inputs)\n            l1_loss = l1_lambda * torch.mean(torch.abs(z))\n            loss = mse_loss + l1_loss\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * inputs.size(0)\n\n        avg_loss = total_loss / len(data_loader.dataset)\n        print(f"Epoch {epoch+1}, Loss: {avg_loss:.6f}")\n'}),"\n",i.createElement(n.p,null,"Here, ",i.createElement(n.strong,null,"l1_loss")," is added to the MSE reconstruction loss. ",i.createElement(l.A,{text:"\\( l1\\_lambda \\)"})," controls the importance of sparse regularization."),"\n",i.createElement(n.h3,{id:"building-contractive-autoencoder-in-python-code-snippets-with-comments",style:{position:"relative"}},i.createElement(n.a,{href:"#building-contractive-autoencoder-in-python-code-snippets-with-comments","aria-label":"building contractive autoencoder in python code snippets with comments permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Building contractive autoencoder in Python: code snippets with comments"),"\n",i.createElement(n.p,null,"Below is a toy contractive autoencoder where we add a Jacobian penalty for each mini-batch:"),"\n",i.createElement(o.A,{text:'\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass ContractiveAutoencoder(nn.Module):\n    def __init__(self, input_dim=784, hidden_dim=64):\n        super(ContractiveAutoencoder, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(True)\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(hidden_dim, input_dim),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        z = self.encoder(x)\n        x_hat = self.decoder(z)\n        return x_hat, z\n\ndef compute_jacobian_penalty(model, inputs, z, lambda_cae=1e-3):\n    # z is (batch_size x hidden_dim)\n    # We sum over all hidden units\n    # For contractive autoencoder, we compute the norm of d(z)/d(x).\n    # We\'ll do a naive approach by summing partial derivatives for each dimension.\n    batch_size = inputs.size(0)\n    hidden_dim = z.size(1)\n    J = 0.0\n    for i in range(hidden_dim):\n        grad = torch.autograd.grad(z[:, i].sum(), inputs, create_graph=True)[0]\n        J += torch.sum(grad**2)\n    return lambda_cae * J / batch_size\n\ndef train_contractive_ae(model, data_loader, num_epochs=10, lambda_cae=1e-3, lr=1e-3):\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.MSELoss()\n\n    model.train()\n    for epoch in range(num_epochs):\n        total_loss = 0.0\n        for inputs, _ in data_loader:\n            inputs = inputs.view(inputs.size(0), -1)\n            optimizer.zero_grad()\n            x_hat, z = model(inputs)\n            mse_loss = criterion(x_hat, inputs)\n\n            # Contractive penalty\n            contractive_loss = compute_jacobian_penalty(model, inputs, z, lambda_cae)\n            loss = mse_loss + contractive_loss\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item() * inputs.size(0)\n\n        avg_loss = total_loss / len(data_loader.dataset)\n        print(f"Epoch {epoch+1}, Loss: {avg_loss:.6f}")\n'}),"\n",i.createElement(n.h3,{id:"building-variational-autoencoder-in-python-code-snippets-with-comments",style:{position:"relative"}},i.createElement(n.a,{href:"#building-variational-autoencoder-in-python-code-snippets-with-comments","aria-label":"building variational autoencoder in python code snippets with comments permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Building variational autoencoder in Python: code snippets with comments"),"\n",i.createElement(n.p,null,"Below, I demonstrate a minimal ",i.createElement(n.strong,null,"VAE"),". The key distinction is that the encoder outputs a mean and log-variance, from which we sample a latent vector using the reparameterization trick:"),"\n",i.createElement(o.A,{text:"\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass VAE(nn.Module):\n    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20):\n        super(VAE, self).__init__()\n        # Encoder\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2_mean = nn.Linear(hidden_dim, latent_dim)\n        self.fc2_logvar = nn.Linear(hidden_dim, latent_dim)\n        # Decoder\n        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n        self.fc4 = nn.Linear(hidden_dim, input_dim)\n        self.relu = nn.ReLU()\n\n    def encode(self, x):\n        h = self.relu(self.fc1(x))\n        mu = self.fc2_mean(h)\n        logvar = self.fc2_logvar(h)\n        return mu, logvar\n\n    def reparameterize(self, mu, logvar):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def decode(self, z):\n        h = self.relu(self.fc3(z))\n        return torch.sigmoid(self.fc4(h))\n\n    def forward(self, x):\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        x_hat = self.decode(z)\n        return x_hat, mu, logvar\n\ndef vae_loss_function(x_hat, x, mu, logvar):\n    BCE = nn.functional.binary_cross_entropy(x_hat, x, reduction='sum')\n    # KL divergence term\n    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    return BCE + KLD\n\ndef train_vae(model, data_loader, num_epochs=10, lr=1e-3):\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    model.train()\n    for epoch in range(num_epochs):\n        total_loss = 0.0\n        for inputs, _ in data_loader:\n            inputs = inputs.view(inputs.size(0), -1)\n            optimizer.zero_grad()\n            x_hat, mu, logvar = model(inputs)\n            loss = vae_loss_function(x_hat, inputs, mu, logvar)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        avg_loss = total_loss / len(data_loader.dataset)\n        print(f\"Epoch {epoch+1}, VAE loss: {avg_loss:.6f}\")\n"}),"\n",i.createElement(n.p,null,"Here, ",i.createElement(n.strong,null,"vae_loss_function")," includes both the reconstruction term (BCE) and the KL divergence term. The KL divergence pushes ",i.createElement(l.A,{text:"\\(q_\\theta(z|x)\\)"})," to align with the standard Gaussian prior."),"\n",i.createElement(n.h3,{id:"building-generative-adversarial-autoencoder-in-python-code-snippets-with-comments",style:{position:"relative"}},i.createElement(n.a,{href:"#building-generative-adversarial-autoencoder-in-python-code-snippets-with-comments","aria-label":"building generative adversarial autoencoder in python code snippets with comments permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Building generative adversarial autoencoder in Python: code snippets with comments"),"\n",i.createElement(n.p,null,"A simplified version of an ",i.createElement(n.strong,null,"adversarial autoencoder")," (AAE) combines a reconstruction loss with an adversarial loss on the latent distribution:"),"\n",i.createElement(o.A,{text:'\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass AAE_Encoder(nn.Module):\n    def __init__(self, input_dim=784, hidden_dim=256, latent_dim=64):\n        super(AAE_Encoder, self).__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(True),\n            nn.Linear(hidden_dim, latent_dim)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nclass AAE_Decoder(nn.Module):\n    def __init__(self, latent_dim=64, hidden_dim=256, output_dim=784):\n        super(AAE_Decoder, self).__init__()\n        self.net = nn.Sequential(\n            nn.Linear(latent_dim, hidden_dim),\n            nn.ReLU(True),\n            nn.Linear(hidden_dim, output_dim),\n            nn.Sigmoid()\n        )\n    def forward(self, z):\n        return self.net(z)\n\nclass AAE_Discriminator(nn.Module):\n    def __init__(self, latent_dim=64, hidden_dim=256):\n        super(AAE_Discriminator, self).__init__()\n        self.net = nn.Sequential(\n            nn.Linear(latent_dim, hidden_dim),\n            nn.ReLU(True),\n            nn.Linear(hidden_dim, 1),\n            nn.Sigmoid()\n        )\n    def forward(self, z):\n        return self.net(z)\n\ndef train_aae(encoder, decoder, discriminator, data_loader, \n              num_epochs=10, lr=1e-3, batch_size=64):\n    # Setup\n    recon_opt = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=lr)\n    disc_opt = optim.Adam(discriminator.parameters(), lr=lr)\n    criterion = nn.MSELoss()\n    bce_criterion = nn.BCELoss()\n\n    for epoch in range(num_epochs):\n        for inputs, _ in data_loader:\n            inputs = inputs.view(inputs.size(0), -1)\n            batch_len = inputs.size(0)\n\n            # ================== RECONSTRUCTION PHASE ==================\n            # Train encoder+decoder to minimize reconstruction error\n            recon_opt.zero_grad()\n            z_fake = encoder(inputs)\n            x_hat = decoder(z_fake)\n            recon_loss = criterion(x_hat, inputs)\n            recon_loss.backward()\n            recon_opt.step()\n\n            # ================== REGULARIZATION PHASE ==================\n            # Match q(z|x) to p(z) with adversarial training\n            # Sample from prior (e.g., standard normal)\n            z_real = torch.randn(batch_len, z_fake.size(1))\n            # Forward pass to get real/fake labels\n            disc_opt.zero_grad()\n            # Discriminator on real z\n            d_real = discriminator(z_real)\n            # Discriminator on fake z\n            z_fake = encoder(inputs)  # re-encode after recon step\n            d_fake = discriminator(z_fake.detach())\n\n            real_labels = torch.ones(batch_len, 1)\n            fake_labels = torch.zeros(batch_len, 1)\n\n            d_real_loss = bce_criterion(d_real, real_labels)\n            d_fake_loss = bce_criterion(d_fake, fake_labels)\n            d_loss = d_real_loss + d_fake_loss\n            d_loss.backward()\n            disc_opt.step()\n\n            # ================== ENCODER ADVERSARIAL LOSS ==================\n            # Now train encoder so that z_fake fools the discriminator\n            recon_opt.zeroGrad()\n            z_fake = encoder(inputs)\n            d_fake2 = discriminator(z_fake)\n            # we want to trick disc, so labels=1 for these\n            gen_loss = bce_criterion(d_fake2, real_labels)\n            gen_loss.backward()\n            recon_opt.step()\n\n        print(f"Epoch {epoch+1}, recon loss: {recon_loss.item():.6f}, d loss: {d_loss.item():.6f}, gen loss: {gen_loss.item():.6f}")\n'}),"\n",i.createElement(n.p,null,"In this schematic code, the training loop is divided into:"),"\n",i.createElement(n.ol,null,"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Reconstruction phase")," â€” the encoder and decoder minimize the MSE between input and reconstruction."),"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Regularization phase")," â€” the discriminator tries to distinguish real latent samples from the prior vs. fake samples from the encoder."),"\n",i.createElement(n.li,null,i.createElement(n.strong,null,"Encoder adversarial loss")," â€” the encoder tries to fool the discriminator."),"\n"),"\n",i.createElement(n.p,null,"This yields an autoencoder whose latent space is constrained to match a chosen prior distribution."),"\n",i.createElement("br"),"\n",i.createElement(n.h2,{id:"15-misc-notes",style:{position:"relative"}},i.createElement(n.a,{href:"#15-misc-notes","aria-label":"15 misc notes permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"15. Misc notes"),"\n",i.createElement(n.p,null,"Autoencoders unlock a rich panorama of advanced topics, challenges, and continuing research directions. Below are a few additional insights to keep in mind as you incorporate or extend autoencoders in your own work."),"\n",i.createElement(n.h3,{id:"mode-collapse-in-certain-autoencoder-variants-and-difficulty-in-capturing-multi-modal-distributions",style:{position:"relative"}},i.createElement(n.a,{href:"#mode-collapse-in-certain-autoencoder-variants-and-difficulty-in-capturing-multi-modal-distributions","aria-label":"mode collapse in certain autoencoder variants and difficulty in capturing multi modal distributions permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Mode collapse in certain autoencoder variants and difficulty in capturing multi-modal distributions"),"\n",i.createElement(n.p,null,"While autoencoders can learn powerful representations, certain variants (especially adversarially trained ones) can suffer from ",i.createElement(n.strong,null,"mode collapse"),", where they fail to represent all the modes of a complex distribution. This can manifest as producing only a small subset of possible reconstructions or ignoring certain data modes. Techniques like minibatch discrimination, multi-discriminator setups, or additional regularization can partially mitigate these issues."),"\n",i.createElement(n.h3,{id:"balancing-reconstruction-fidelity-with-useful-latent-representations-for-downstream-tasks",style:{position:"relative"}},i.createElement(n.a,{href:"#balancing-reconstruction-fidelity-with-useful-latent-representations-for-downstream-tasks","aria-label":"balancing reconstruction fidelity with useful latent representations for downstream tasks permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Balancing reconstruction fidelity with useful latent representations for downstream tasks"),"\n",i.createElement(n.p,null,"A potential tension arises in autoencoder design:"),"\n",i.createElement(n.ul,null,"\n",i.createElement(n.li,null,"Minimizing reconstruction error may cause the autoencoder to memorize or store direct identity-like mappings."),"\n",i.createElement(n.li,null,"On the other hand, imposing heavy constraints (small bottleneck, strong sparsity, etc.) can hamper reconstruction quality."),"\n"),"\n",i.createElement(n.p,null,"The sweet spot often depends on your downstream objective. For instance, if you mainly want robust features for classification, you can reduce the bottleneck and enforce more constraints. If you are primarily interested in high-fidelity reconstruction or generative capabilities, you might opt for a larger latent dimension and gentler constraints."),"\n",i.createElement(n.h3,{id:"integrating-autoencoders-with-self-supervised-or-contrastive-learning-paradigms",style:{position:"relative"}},i.createElement(n.a,{href:"#integrating-autoencoders-with-self-supervised-or-contrastive-learning-paradigms","aria-label":"integrating autoencoders with self supervised or contrastive learning paradigms permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Integrating autoencoders with self-supervised or contrastive learning paradigms"),"\n",i.createElement(n.p,null,"Autoencoders can serve as a stepping stone to ",i.createElement(n.strong,null,"self-supervised learning"),". For example, one can combine reconstruction losses with ",i.createElement(n.strong,null,"contrastive")," objectives, encouraging the latent representation of different views (augmentations) of the same input to be similar. This synergy can dramatically improve representation quality compared to an autoencoder alone."),"\n",i.createElement(n.h3,{id:"ongoing-research-in-novel-architectures-eg-transformers-as-encoders-decoders-and-improvements-in-optimization-methods",style:{position:"relative"}},i.createElement(n.a,{href:"#ongoing-research-in-novel-architectures-eg-transformers-as-encoders-decoders-and-improvements-in-optimization-methods","aria-label":"ongoing research in novel architectures eg transformers as encoders decoders and improvements in optimization methods permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Ongoing research in novel architectures (e.g., transformers as encoders-decoders) and improvements in optimization methods"),"\n",i.createElement(n.p,null,"Recent years have seen the adaptation of ",i.createElement(n.strong,null,"transformer"),' architectures into autoencoder-like frameworks, particularly in natural language and vision tasks. For instance, "masked autoencoders" for vision (He and gang, 2022) randomly mask patches in an image and train the network to reconstruct the masked regions, showing impressive self-supervised learning capabilities.'),"\n",i.createElement(n.p,null,"Furthermore, advanced optimization methods and normalization strategies (e.g., weight normalization, group normalization) continue to improve the stability and speed of training deeper autoencoders on large-scale datasets."),"\n",i.createElement("br"),"\n",i.createElement(n.h2,{id:"putting-it-all-together",style:{position:"relative"}},i.createElement(n.a,{href:"#putting-it-all-together","aria-label":"putting it all together permalink",className:"anchor before"},i.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Putting it all together"),"\n",i.createElement(n.p,null,"In summary, autoencoders exemplify the power of unsupervised neural networks to learn compressed and meaningful representations of data. They play a pivotal role in representation learning, generative modeling, dimensionality reduction, anomaly detection, image denoising, and more. From classical, single-layer approaches that closely resemble PCA, to complex, deep or recurrent architectures, and from specialized variants like denoising autoencoders to sophisticated generative models like variational autoencoders and adversarial autoencoders, the autoencoder family continues to expand rapidly."),"\n",i.createElement(n.p,null,"When designing an autoencoder, the crucial balance lies in deciding how much capacity to allocate (depth, width), how small or structured the bottleneck should be, which loss functions to use, and what regularization or constraints best suit your end goal. The breadth and richness of the autoencoder framework make it an essential chapter in any advanced machine learning curriculum. Whether your domain of interest is computer vision, text processing, time series modeling, or beyond, an appreciation for autoencoder architecture and theory can open up vast possibilities for creative and robust representation learning â€” a cornerstone of modern data science and deep learning."),"\n",i.createElement(t,{alt:"schematic-of-encoder-decoder-structure",path:"",caption:"A schematic showing the encoder, latent space, and decoder in a generic autoencoder.",zoom:"false"}),"\n",i.createElement("br"),"\n",i.createElement(n.p,null,"That concludes the comprehensive exploration of autoencoder architectures, including numerous variants and their theoretical foundations. By understanding the nuances of each method, one can tailor autoencoders to a wide range of tasks, from denoising to full-blown generative modeling, all while harnessing the power of unsupervised learning."))}var c=function(e){void 0===e&&(e={});const{wrapper:n}=Object.assign({},(0,r.RP)(),e.components);return n?i.createElement(n,e,i.createElement(s,e)):s(e)};var d=t(36710),m=t(58481),h=t.n(m),u=t(36310),p=t(87245),g=t(27042),f=t(59849),v=t(5591),E=t(61122),y=t(9219),b=t(33203),_=t(95751),w=t(94328),z=t(80791),S=t(78137);const x=e=>{let{toc:n}=e;if(!n||!n.items)return null;return i.createElement("nav",{className:z.R},i.createElement("ul",null,n.items.map(((e,n)=>i.createElement("li",{key:n},i.createElement("a",{href:e.url,onClick:n=>((e,n)=>{e.preventDefault();const t=n.replace("#",""),a=document.getElementById(t);a&&a.scrollIntoView({behavior:"smooth",block:"start"})})(n,e.url)},e.title),e.items&&i.createElement(x,{toc:{items:e.items}}))))))};function k(e){let{data:{mdx:n,allMdx:o,allPostImages:l},children:s}=e;const{frontmatter:c,body:d,tableOfContents:m}=n,f=c.index,z=c.slug.split("/")[1],k=o.nodes.filter((e=>e.frontmatter.slug.includes(`/${z}/`))).sort(((e,n)=>e.frontmatter.index-n.frontmatter.index)),H=k.findIndex((e=>e.frontmatter.index===f)),A=k[H+1],T=k[H-1],C=c.slug.replace(/\/$/,""),M=/[^/]*$/.exec(C)[0],L=`posts/${z}/content/${M}/`,{0:V,1:I}=(0,i.useState)(c.flagWideLayoutByDefault),{0:B,1:N}=(0,i.useState)(!1);var P;(0,i.useEffect)((()=>{N(!0);const e=setTimeout((()=>N(!1)),340);return()=>clearTimeout(e)}),[V]),"adventures"===z?P=y.cb:"research"===z?P=y.Qh:"thoughts"===z&&(P=y.T6);const R=h()(d).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,D=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const n=Math.floor(e/60),t=e%60;return t<=30?`~${n}${t>0?".5":""} h`:`~${n+1} h`}(Math.ceil(R/P)+(c.extraReadTimeMin||0)),q=[{flag:c.flagDraft,component:()=>Promise.all([t.e(3231),t.e(8809)]).then(t.bind(t,28809))},{flag:c.flagMindfuckery,component:()=>Promise.all([t.e(3231),t.e(2471)]).then(t.bind(t,67709))},{flag:c.flagRewrite,component:()=>Promise.all([t.e(3231),t.e(6764)]).then(t.bind(t,62002))},{flag:c.flagOffensive,component:()=>Promise.all([t.e(3231),t.e(2443)]).then(t.bind(t,17681))},{flag:c.flagProfane,component:()=>Promise.all([t.e(3231),t.e(8048)]).then(t.bind(t,53286))},{flag:c.flagMultilingual,component:()=>Promise.all([t.e(3231),t.e(4069)]).then(t.bind(t,78831))},{flag:c.flagUnreliably,component:()=>Promise.all([t.e(3231),t.e(3417)]).then(t.bind(t,8179))},{flag:c.flagPolitical,component:()=>Promise.all([t.e(3231),t.e(5195)]).then(t.bind(t,30433))},{flag:c.flagCognitohazard,component:()=>Promise.all([t.e(3231),t.e(3175)]).then(t.bind(t,8413))},{flag:c.flagHidden,component:()=>Promise.all([t.e(3231),t.e(9556)]).then(t.bind(t,14794))}],{0:O,1:j}=(0,i.useState)([]);return(0,i.useEffect)((()=>{q.forEach((e=>{let{flag:n,component:t}=e;n&&t().then((e=>{j((n=>[].concat((0,a.A)(n),[e.default])))}))}))}),[]),i.createElement(g.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},i.createElement(v.A,{postNumber:c.index,date:c.date,updated:c.updated,readTime:D,difficulty:c.difficultyLevel,title:c.title,desc:c.desc,banner:c.banner,section:z,postKey:M,isMindfuckery:c.flagMindfuckery,mainTag:c.mainTag}),i.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},c.otherTags.map(((e,n)=>i.createElement("span",{key:n,className:`noselect ${S.MW}`,style:{margin:"0 5px 5px 0"}},e)))),i.createElement("div",{class:"postBody"},i.createElement(x,{toc:m})),i.createElement("br"),i.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},i.createElement(g.P.button,{class:"noselect",className:w.pb,id:w.xG,onClick:()=>{I(!V)},whileTap:{scale:.93}},i.createElement(g.P.div,{className:_.DJ,key:V,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},V?"Switch to default layout":"Switch to wide layout"))),i.createElement("br"),i.createElement("div",{class:"postBody",style:{margin:V?"0 -14%":"",maxWidth:V?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},i.createElement("div",{className:`${w.P_} ${B?w.Xn:w.qG}`},O.map(((e,n)=>i.createElement(e,{key:n}))),c.indexCourse?i.createElement(b.A,{index:c.indexCourse,category:c.courseCategoryName}):"",i.createElement(u.Z.Provider,{value:{images:l.nodes,basePath:L.replace(/\/$/,"")+"/"}},i.createElement(r.xA,{components:{Image:p.A}},s)))),i.createElement(E.A,{nextPost:A,lastPost:T,keyCurrent:M,section:z}))}function H(e){return i.createElement(k,e,i.createElement(c,e))}function A(e){var n,t,a,r,o;let{data:l}=e;const{frontmatter:s}=l.mdx,c=s.titleSEO||s.title,m=s.titleOG||c,h=s.titleTwitter||c,u=s.descSEO||s.desc,p=s.descOG||u,g=s.descTwitter||u,v=s.schemaType||"BlogPosting",E=s.keywordsSEO,y=s.date,b=s.updated||y,_=s.imageOG||(null===(n=s.banner)||void 0===n||null===(t=n.childImageSharp)||void 0===t||null===(a=t.gatsbyImageData)||void 0===a||null===(r=a.images)||void 0===r||null===(o=r.fallback)||void 0===o?void 0:o.src),w=s.imageAltOG||p,z=s.imageTwitter||_,S=s.imageAltTwitter||g,x=s.canonicalURL,k=s.flagHidden||!1,H=s.mainTag||"Posts",A=s.slug.split("/")[1]||"posts",{siteUrl:T}=(0,d.Q)(),C={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:T},{"@type":"ListItem",position:2,name:H,item:`${T}/${s.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${T}${s.slug}`}]};return i.createElement(f.A,{title:c+" - avrtt.blog",titleOG:m,titleTwitter:h,description:u,descriptionOG:p,descriptionTwitter:g,schemaType:v,keywords:E,datePublished:y,dateModified:b,imageOG:_,imageAltOG:w,imageTwitter:z,imageAltTwitter:S,canonicalUrl:x,flagHidden:k,mainTag:H,section:A,type:"article"},i.createElement("script",{type:"application/ld+json"},JSON.stringify(C)))}},96098:function(e,n,t){var a=t(96540),r=t(7978);n.A=e=>{let{text:n}=e;return a.createElement(r.A,null,n)}}}]);
//# sourceMappingURL=component---src-templates-post-js-content-file-path-src-pages-posts-research-autoencoder-architecture-mdx-e12ef32c85c09e3a5648.js.map