"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[2305],{3962:function(e,t){t.A="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iaXNvLTg4NTktMSI/Pgo8IS0tIEdlbmVyYXRvcjogQWRvYmUgSWxsdXN0cmF0b3IgMTYuMC4wLCBTVkcgRXhwb3J0IFBsdWctSW4gLiBTVkcgVmVyc2lvbjogNi4wMCBCdWlsZCAwKSAgLS0+CjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+CjxzdmcgdmVyc2lvbj0iMS4xIiBpZD0iQ2FwYV8xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB4PSIwcHgiIHk9IjBweCIKCSB3aWR0aD0iNDE2Ljk3OXB4IiBoZWlnaHQ9IjQxNi45NzlweCIgdmlld0JveD0iMCAwIDQxNi45NzkgNDE2Ljk3OSIgc3R5bGU9ImVuYWJsZS1iYWNrZ3JvdW5kOm5ldyAwIDAgNDE2Ljk3OSA0MTYuOTc5OyIKCSB4bWw6c3BhY2U9InByZXNlcnZlIj4KPGc+Cgk8cGF0aCBkPSJNMzU2LjAwNCw2MS4xNTZjLTgxLjM3LTgxLjQ3LTIxMy4zNzctODEuNTUxLTI5NC44NDgtMC4xODJjLTgxLjQ3LDgxLjM3MS04MS41NTIsMjEzLjM3OS0wLjE4MSwyOTQuODUKCQljODEuMzY5LDgxLjQ3LDIxMy4zNzgsODEuNTUxLDI5NC44NDksMC4xODFDNDM3LjI5MywyNzQuNjM2LDQzNy4zNzUsMTQyLjYyNiwzNTYuMDA0LDYxLjE1NnogTTIzNy42LDM0MC43ODYKCQljMCwzLjIxNy0yLjYwNyw1LjgyMi01LjgyMiw1LjgyMmgtNDYuNTc2Yy0zLjIxNSwwLTUuODIyLTIuNjA1LTUuODIyLTUuODIyVjE2Ny44ODVjMC0zLjIxNywyLjYwNy01LjgyMiw1LjgyMi01LjgyMmg0Ni41NzYKCQljMy4yMTUsMCw1LjgyMiwyLjYwNCw1LjgyMiw1LjgyMlYzNDAuNzg2eiBNMjA4LjQ5LDEzNy45MDFjLTE4LjYxOCwwLTMzLjc2Ni0xNS4xNDYtMzMuNzY2LTMzLjc2NQoJCWMwLTE4LjYxNywxNS4xNDctMzMuNzY2LDMzLjc2Ni0zMy43NjZjMTguNjE5LDAsMzMuNzY2LDE1LjE0OCwzMy43NjYsMzMuNzY2QzI0Mi4yNTYsMTIyLjc1NSwyMjcuMTA3LDEzNy45MDEsMjA4LjQ5LDEzNy45MDF6Ii8+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPC9zdmc+Cg=="},9360:function(e,t,a){a.d(t,{A:function(){return l}});var n=a(96540),i=a(3962),r="styles-module--tooltiptext--a263b";var l=e=>{let{text:t,isBadge:a=!1}=e;const{0:l,1:s}=(0,n.useState)(!1),o=(0,n.useRef)(null);return(0,n.useEffect)((()=>{function e(e){o.current&&e.target instanceof Node&&!o.current.contains(e.target)&&s(!1)}return document.addEventListener("click",e),()=>{document.removeEventListener("click",e)}}),[]),n.createElement("span",{className:"styles-module--tooltipWrapper--75ebf",ref:o},n.createElement("img",{id:a?"styles-module--infoBadge--e3d66":"styles-module--info--26c1f",src:i.A,alt:"info",onClick:e=>{e.stopPropagation(),s((e=>!e))}}),n.createElement("span",{className:l?`${r} styles-module--visible--c063c`:r},t))}},35636:function(e,t,a){a.r(t),a.d(t,{Head:function(){return C},PostTemplate:function(){return M},default:function(){return A}});var n=a(28453),i=a(96540),r=a(9360),l=a(61992),s=a(62087),o=a(90548);function c(e){const t=Object.assign({p:"p",ul:"ul",li:"li",h2:"h2",a:"a",span:"span",h3:"h3",h4:"h4",strong:"strong"},(0,n.RP)(),e.components),{Image:a}=t;return a||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),i.createElement(i.Fragment,null,"\n",i.createElement("br"),"\n","\n","\n",i.createElement(t.p,null,"Welcome to this comprehensive exploration of neural ordinary differential equations (",i.createElement(l.A,null,"neural ODEs"),"), a cutting-edge paradigm at the intersection of deep learning and dynamical systems. My goal is to offer a thorough, in-depth discussion spanning from fundamental principles of ordinary differential equations (",i.createElement(r.A,{text:"ODEs are equations relating a function and its derivatives; solutions typically represent how a system evolves over time."}),"), all the way to advanced neural ODE frameworks that have sparked significant excitement in the machine learning community. By taking the continuous-time perspective, neural ODEs have opened up a realm of novel insights into how we parameterize and train deep models."),"\n",i.createElement(t.p,null,"This text, while advanced, remains explanatory in nature. I will assume you have a decent working knowledge of:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"Linear algebra (vectors, matrices, transformations)."),"\n",i.createElement(t.li,null,"Basic calculus (differentiation, integration, partial derivatives, chain rule)."),"\n",i.createElement(t.li,null,"Fundamentals of machine learning (familiarity with feed-forward neural networks, gradient-based optimization, cost functions)."),"\n",i.createElement(t.li,null,"Python programming (e.g., using PyTorch, NumPy, or similar libraries)."),"\n"),"\n",i.createElement(t.p,null,"Additionally, a grasp of concepts like residual networks (ResNets) and normalizing flows will allow you to draw parallels more easily to neural ODEs and their continuous transformations. Throughout the article, I will also allude to research papers and advanced developments, so some familiarity with reading scientific literature (like from NeurIPS, ICML, or JMLR) may be beneficial, but it is certainly not mandatory to gain value from this material."),"\n",i.createElement(t.p,null,'Why do we care about continuous-time formulations in modern machine learning? One reason is that many real-world systems are inherently governed by continuous or near-continuous evolution rules (think of physical, biological, ecological, or chemical processes). Another is that building neural networks that effectively treat model "depth" as a continuous variable often leads to improved memory efficiency when backpropagating through time, and can reveal novel insights about how deep representations evolve.'),"\n",i.createElement(t.p,null,"In essence, neural ODEs offer:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"A perspective that unifies discrete deep neural architectures (like ResNets) with classical dynamical systems."),"\n",i.createElement(t.li,null,'The possibility to adapt step sizes in "depth" during inference, and potentially more flexible function approximations.'),"\n",i.createElement(t.li,null,"Connections to continuous normalizing flows, invertible transformations, and generative modeling."),"\n"),"\n",i.createElement(t.p,null,"Given these motivations, let us embark on a journey that starts from first principles of dynamical systems, ensuring the conceptual foundations are solid, then progresses into the numerical methods for solving ODEs, culminating in an extensive treatment of neural ODE models, their limitations, augmentations, and practical considerations."),"\n",i.createElement(t.h2,{id:"dynamical-systems-primer",style:{position:"relative"}},i.createElement(t.a,{href:"#dynamical-systems-primer","aria-label":"dynamical systems primer permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Dynamical systems primer"),"\n",i.createElement(t.h3,{id:"formal-definition-of-a-dynamical-system",style:{position:"relative"}},i.createElement(t.a,{href:"#formal-definition-of-a-dynamical-system","aria-label":"formal definition of a dynamical system permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Formal definition of a dynamical system"),"\n",i.createElement(t.p,null,"A ",i.createElement(l.A,null,"dynamical system")," generally describes how a state vector evolves over time according to some rule. In the simplest deterministic continuous-time formulation, we write:"),"\n",i.createElement(o.A,{text:"\\[\n\\frac{d\\mathbf{x}}{dt} = f(\\mathbf{x}(t), t; \\boldsymbol{\\theta}),\n\\]"}),"\n",i.createElement(t.p,null,"where ",i.createElement(o.A,{text:"\\( \\mathbf{x}(t) \\)"})," in ",i.createElement(o.A,{text:"\\( \\mathbb{R}^n \\)"})," is the state vector at time ",i.createElement(o.A,{text:"\\( t \\)"}),", ",i.createElement(o.A,{text:"\\( f \\)"})," is a function that governs the time evolution (sometimes referred to as a vector field), and ",i.createElement(o.A,{text:"\\( \\boldsymbol{\\theta} \\)"})," are parameters. The solution to such an equation, assuming well-posedness, is a trajectory ",i.createElement(o.A,{text:"\\( \\mathbf{x}(t) \\)"})," tracing out how the system changes over the continuous variable ",i.createElement(o.A,{text:"\\( t \\)"}),"."),"\n",i.createElement(t.p,null,"Dynamical systems can also be discrete, in which case time is indexed by steps, ",i.createElement(o.A,{text:"\\( k = 0, 1, 2, \\ldots \\)"}),". Then, we might write:"),"\n",i.createElement(o.A,{text:"\\[\n\\mathbf{x}_{k+1} = g(\\mathbf{x}_k, k; \\boldsymbol{\\theta}).\n\\]"}),"\n",i.createElement(t.p,null,"Though many real-world processes are intrinsically continuous, discrete dynamical systems can be an excellent approximation (e.g., sampling every second, minute, or day). The classification into deterministic vs. stochastic further refines whether there is random noise or uncertainty in how states evolve. Stochastic differential equations are also highly relevant in certain contexts (like finance and physics), but for neural ODEs in this article, we will primarily focus on deterministic continuous ODE settings."),"\n",i.createElement(t.h3,{id:"state-space-parameter-space-and-evolution",style:{position:"relative"}},i.createElement(t.a,{href:"#state-space-parameter-space-and-evolution","aria-label":"state space parameter space and evolution permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"State space, parameter space, and evolution"),"\n",i.createElement(t.p,null,"When we solve an ODE, the solution ",i.createElement(o.A,{text:"\\( \\mathbf{x}(t) \\)"})," lives in the ",i.createElement(l.A,null,"state space"),". This is the domain of possible states the system might occupy. The parameters ",i.createElement(o.A,{text:"\\( \\boldsymbol{\\theta} \\)"})," (if present) typically reside in a ",i.createElement(l.A,null,"parameter space"),", which can influence the shape of the function ",i.createElement(o.A,{text:"\\( f \\)"}),". As ",i.createElement(o.A,{text:"\\( t \\)"})," changes, the continuous trajectory ",i.createElement(o.A,{text:"\\( \\mathbf{x}(t) \\)"})," in state space is determined by ",i.createElement(o.A,{text:"\\( f \\)"}),"."),"\n",i.createElement(t.p,null,"In machine learning terms, for neural ODEs, the ",i.createElement(o.A,{text:"\\( \\boldsymbol{\\theta} \\)"})," typically represent the trainable weights of a neural network defining the vector field ",i.createElement(o.A,{text:"\\( f \\)"}),". Instead of layering a network in discrete blocks, the idea is that we are specifying a continuous transformation by integrating a learned vector field over time."),"\n",i.createElement(t.h3,{id:"flows-in-continuous-time",style:{position:"relative"}},i.createElement(t.a,{href:"#flows-in-continuous-time","aria-label":"flows in continuous time permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Flows in continuous time"),"\n",i.createElement(t.p,null,"Consider a continuous dynamical system ",i.createElement(o.A,{text:"\\( d\\mathbf{x}/dt = f(\\mathbf{x}(t)) \\)"}),". A ",i.createElement(l.A,null,"flow")," can be understood as a function ",i.createElement(o.A,{text:"\\( \\Phi_t(\\mathbf{x}_0) \\)"})," describing the solution at time ",i.createElement(o.A,{text:"\\( t \\)"})," given the initial condition ",i.createElement(o.A,{text:"\\( \\mathbf{x}_0 \\)"})," at ",i.createElement(o.A,{text:"\\( t=0 \\)"}),". Thus,"),"\n",i.createElement(o.A,{text:"\\[\n\\mathbf{x}(t) = \\Phi_t(\\mathbf{x}_0).\n\\]"}),"\n",i.createElement(t.p,null,"Flows can be invertible if ",i.createElement(o.A,{text:"\\( f \\)"})," is well-behaved (satisfying the right conditions), meaning ",i.createElement(o.A,{text:"\\( \\Phi_{-t}(\\Phi_t(\\mathbf{x}_0)) = \\mathbf{x}_0 \\)"}),". This invertibility property is central to continuous normalizing flows, which are a class of generative models leveraging neural ODE frameworks."),"\n",i.createElement(t.h3,{id:"relevance-in-machine-learning-and-data-science",style:{position:"relative"}},i.createElement(t.a,{href:"#relevance-in-machine-learning-and-data-science","aria-label":"relevance in machine learning and data science permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Relevance in machine learning and data science"),"\n",i.createElement(t.p,null,'Dynamical systems theory has influenced many areas of ML. Recurrent neural networks (RNNs) can be interpreted as discrete dynamical systems with hidden states updated at each time step. More recently, continuous normalizing flows and neural ODEs further unify the viewpoint that "depth" can be treated as a continuous variable.'),"\n",i.createElement(t.p,null,"The synergy between ODEs and ML also manifests in advanced generative modeling, time-series analysis (continuous-time RNN analogs), and physically inspired neural networks that incorporate known scientific structure. If you are studying complex processes — from climate modeling to neuron spiking to chemical kinetics — a deep understanding of dynamical systems can help incorporate domain knowledge into ML models or, conversely, interpret neural networks from a physics-based perspective."),"\n",i.createElement(t.h3,{id:"discrete-vs-continuous-systems",style:{position:"relative"}},i.createElement(t.a,{href:"#discrete-vs-continuous-systems","aria-label":"discrete vs continuous systems permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Discrete vs. continuous systems"),"\n",i.createElement(t.p,null,"A discrete system with update ",i.createElement(o.A,{text:"\\( \\mathbf{x}_{k+1} = \\mathbf{x}_k + h \\, f(\\mathbf{x}_k) \\)"})," can approximate a continuous system ",i.createElement(o.A,{text:"\\( d\\mathbf{x}/dt = f(\\mathbf{x}(t)) \\)"})," using some finite step ",i.createElement(o.A,{text:"\\( h \\)"}),". In deep learning, one can draw a direct analogy between:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(l.A,null,"Discrete layers")," in a feed-forward or residual network."),"\n",i.createElement(t.li,null,i.createElement(l.A,null,"Continuous layers")," in a neural ODE."),"\n"),"\n",i.createElement(t.p,null,"Discrete systems can exhibit drastically different behavior if ",i.createElement(o.A,{text:"\\( h \\)"})," changes or if the system is chaotic. Continuous systems, by contrast, have a smooth time parameter, but can also be chaotic depending on ",i.createElement(o.A,{text:"\\( f \\)"}),". Neural ODEs let us switch from the discrete viewpoint of repeated function compositions to a smooth integral-based perspective."),"\n",i.createElement(t.h3,{id:"bifurcations-and-stability",style:{position:"relative"}},i.createElement(t.a,{href:"#bifurcations-and-stability","aria-label":"bifurcations and stability permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Bifurcations and stability"),"\n",i.createElement(t.p,null,"A ",i.createElement(l.A,null,"bifurcation")," occurs when tiny changes in parameters cause a qualitative change in the long-term behavior of the system (e.g., changing from a stable fixed point to oscillatory behavior or chaos). Stability addresses whether solutions to the ODE remain bounded and converge to particular states, or diverge under small perturbations."),"\n",i.createElement(t.p,null,"In ML, while we do not typically talk about ",i.createElement(r.A,{text:"Bifurcations in neural networks are not as common as in classical dynamic systems, but the concept of abrupt transitions can appear in training."})," in the same sense as classical nonlinear dynamics, understanding that small parameter changes can drastically shift model behavior is conceptually important — especially in advanced architectures that harness continuous transformations."),"\n",i.createElement(t.h2,{id:"ordinary-differential-equations-odes",style:{position:"relative"}},i.createElement(t.a,{href:"#ordinary-differential-equations-odes","aria-label":"ordinary differential equations odes permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Ordinary differential equations (ODEs)"),"\n",i.createElement(t.h3,{id:"initial-value-problems-ivps",style:{position:"relative"}},i.createElement(t.a,{href:"#initial-value-problems-ivps","aria-label":"initial value problems ivps permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Initial value problems (IVPs)"),"\n",i.createElement(t.p,null,"The typical ODE we solve in ML or engineering is given by:"),"\n",i.createElement(o.A,{text:"\\[\n\\begin{cases}\n\\frac{d\\mathbf{x}}{dt} = f(\\mathbf{x}(t), t), \\\\\n\\mathbf{x}(t_0) = \\mathbf{x}_0.\n\\end{cases}\n\\]"}),"\n",i.createElement(t.p,null,"Here, ",i.createElement(o.A,{text:"\\( \\mathbf{x}_0 \\)"})," is the initial condition at time ",i.createElement(o.A,{text:"\\( t_0 \\)"}),". The entire problem of finding ",i.createElement(o.A,{text:"\\( \\mathbf{x}(t) \\)"})," for ",i.createElement(o.A,{text:"\\( t > t_0 \\)"})," is what we call an ",i.createElement(l.A,null,"initial value problem")," (IVP). Neural ODEs essentially solve IVPs where ",i.createElement(o.A,{text:"\\( f \\)"})," is parameterized by a neural network. The training process modifies the neural network's weights so that the solution ",i.createElement(o.A,{text:"\\( \\mathbf{x}(t) \\)"})," fits data or solves a classification/regression objective."),"\n",i.createElement(t.h3,{id:"existence-and-uniqueness-picardlindelöf-theorem",style:{position:"relative"}},i.createElement(t.a,{href:"#existence-and-uniqueness-picardlindel%C3%B6f-theorem","aria-label":"existence and uniqueness picardlindelöf theorem permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Existence and uniqueness (Picard–Lindelöf theorem)"),"\n",i.createElement(t.p,null,"The ",i.createElement(l.A,null,"Picard–Lindelöf theorem")," states that if ",i.createElement(o.A,{text:"\\( f(\\mathbf{x}, t) \\)"})," is Lipschitz continuous in ",i.createElement(o.A,{text:"\\( \\mathbf{x} \\)"})," and continuous in ",i.createElement(o.A,{text:"\\( t \\)"})," on some domain, then there exists a unique local solution to the IVP. This is important in neural ODEs because we need to ensure the forward pass (solving the ODE) yields a well-defined trajectory. If ",i.createElement(o.A,{text:"\\( f \\)"})," is not Lipschitz continuous, solutions could be non-unique or might blow up in finite time."),"\n",i.createElement(t.h3,{id:"lipschitz-continuity-and-its-role-in-odes",style:{position:"relative"}},i.createElement(t.a,{href:"#lipschitz-continuity-and-its-role-in-odes","aria-label":"lipschitz continuity and its role in odes permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Lipschitz continuity and its role in ODEs"),"\n",i.createElement(t.p,null,"A function ",i.createElement(o.A,{text:"\\( f \\)"})," is Lipschitz continuous if there exists a constant ",i.createElement(o.A,{text:"\\( L \\)"})," such that for any two points ",i.createElement(o.A,{text:"\\( \\mathbf{x}_1, \\mathbf{x}_2 \\)"}),":"),"\n",i.createElement(o.A,{text:"\\[\n\\| f(\\mathbf{x}_1) - f(\\mathbf{x}_2) \\| \\leq L \\|\\mathbf{x}_1 - \\mathbf{x}_2\\|.\n\\]"}),"\n",i.createElement(t.p,null,"A smaller Lipschitz constant ",i.createElement(o.A,{text:"\\( L \\)"})," implies ",i.createElement(o.A,{text:"\\( f \\)"}),' does not change "too abruptly", facilitating stable solutions. In neural ODEs, certain activation functions and weight constraints can help keep ',i.createElement(o.A,{text:"\\( f \\)"})," Lipschitz, thereby ensuring stable training and well-posedness."),"\n",i.createElement(t.h3,{id:"boundary-value-problems-bvps-vs-ivps",style:{position:"relative"}},i.createElement(t.a,{href:"#boundary-value-problems-bvps-vs-ivps","aria-label":"boundary value problems bvps vs ivps permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Boundary value problems (BVPs) vs. IVPs"),"\n",i.createElement(t.p,null,"In boundary value problems, we specify conditions at two points in time (e.g., ",i.createElement(o.A,{text:"\\( \\mathbf{x}(t_0) = \\mathbf{x}_0 \\)"})," and ",i.createElement(o.A,{text:"\\( \\mathbf{x}(t_1) = \\mathbf{x}_1 \\)"}),"), rather than just an initial condition. BVPs arise in applications like heat conduction or structural mechanics. They are less common in standard neural ODE scenarios, but appear in some advanced or specialized contexts (e.g., certain trajectory optimization tasks)."),"\n",i.createElement(t.h3,{id:"analytical-solutions-and-feasibility",style:{position:"relative"}},i.createElement(t.a,{href:"#analytical-solutions-and-feasibility","aria-label":"analytical solutions and feasibility permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Analytical solutions and feasibility"),"\n",i.createElement(t.p,null,"Many ODEs do not have closed-form solutions that can be written down using elementary functions. Even seemingly simple ODEs can require special functions for solutions. Hence, in practice — especially for neural ODEs — we rely heavily on ",i.createElement(l.A,null,"numerical methods"),"."),"\n",i.createElement(t.h3,{id:"applications-and-examples",style:{position:"relative"}},i.createElement(t.a,{href:"#applications-and-examples","aria-label":"applications and examples permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Applications and examples"),"\n",i.createElement(t.p,null,"Ordinary differential equations arise in ",i.createElement(r.A,{text:"Newtonian mechanics: Force = mass * acceleration."})," physics (e.g., motion under forces), chemical reactions, epidemiological models (like SIR), population dynamics (predator-prey, logistic growth), circuit dynamics, and beyond. Within ML, they appear in recurrent modeling, generative flows, and bridging the gap between continuous transformations and deep nets."),"\n",i.createElement(t.h2,{id:"numerical-methods-for-solving-odes",style:{position:"relative"}},i.createElement(t.a,{href:"#numerical-methods-for-solving-odes","aria-label":"numerical methods for solving odes permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Numerical methods for solving ODEs"),"\n",i.createElement(t.h3,{id:"the-euler-method",style:{position:"relative"}},i.createElement(t.a,{href:"#the-euler-method","aria-label":"the euler method permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"The Euler method"),"\n",i.createElement(t.p,null,"The simplest solver is the ",i.createElement(l.A,null,"forward Euler method"),". We discretize time into steps ",i.createElement(o.A,{text:"\\( t_n = t_0 + n\\,h \\)"})," with step size ",i.createElement(o.A,{text:"\\( h \\)"}),". We approximate:"),"\n",i.createElement(o.A,{text:"\\[\n\\mathbf{x}_{n+1} = \\mathbf{x}_n + h\\, f(\\mathbf{x}_n, t_n).\n\\]"}),"\n",i.createElement(t.p,null,"Although easy to implement, Euler's method can be inaccurate for larger ",i.createElement(o.A,{text:"\\( h \\)"})," and can be unstable for stiff problems."),"\n",i.createElement(t.h4,{id:"example-code-snippet",style:{position:"relative"}},i.createElement(t.a,{href:"#example-code-snippet","aria-label":"example code snippet permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Example code snippet:"),"\n",i.createElement(s.A,{text:"\nimport numpy as np\n\ndef euler_step(x, t, h, f):\n    # x: current state\n    # t: current time\n    # h: step size\n    # f: function f(x, t)\n    return x + h * f(x, t)\n\n# Example usage:\ndef lotka_volterra(x, t, alpha=1.1, beta=0.4, delta=0.1, gamma=0.4):\n    # x: [prey, predator]\n    dxdt = alpha*x[0] - beta*x[0]*x[1]\n    dydt = delta*x[0]*x[1] - gamma*x[1]\n    return np.array([dxdt, dydt])\n\nt0, tf = 0.0, 20.0\nh = 0.1\nnum_steps = int((tf - t0)/h)\ntimes = np.linspace(t0, tf, num_steps)\nx = np.array([10.0, 5.0])  # initial state\n\ntrajectory = [x]\nfor i in range(num_steps-1):\n    x = euler_step(x, times[i], h, lotka_volterra)\n    trajectory.append(x)\n"}),"\n",i.createElement(t.h3,{id:"rungekutta-methods-rk-family",style:{position:"relative"}},i.createElement(t.a,{href:"#rungekutta-methods-rk-family","aria-label":"rungekutta methods rk family permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Runge–Kutta methods (RK family)"),"\n",i.createElement(t.p,null,"Runge–Kutta (RK) methods achieve higher accuracy by sampling ",i.createElement(o.A,{text:"\\( f \\)"})," at multiple points within each step. The classic ",i.createElement(l.A,null,"RK4")," method, for instance, is:"),"\n",i.createElement(o.A,{text:"\\[\n\\mathbf{k}_1 = f(\\mathbf{x}_n, t_n),\n\\]"}),"\n",i.createElement(o.A,{text:"\\[\n\\mathbf{k}_2 = f\\Bigl(\\mathbf{x}_n + \\frac{h}{2}\\mathbf{k}_1, t_n + \\frac{h}{2}\\Bigr),\n\\]"}),"\n",i.createElement(o.A,{text:"\\[\n\\mathbf{k}_3 = f\\Bigl(\\mathbf{x}_n + \\frac{h}{2}\\mathbf{k}_2, t_n + \\frac{h}{2}\\Bigr),\n\\]"}),"\n",i.createElement(o.A,{text:"\\[\n\\mathbf{k}_4 = f(\\mathbf{x}_n + h\\,\\mathbf{k}_3, t_n + h),\n\\]"}),"\n",i.createElement(o.A,{text:"\\[\n\\mathbf{x}_{n+1} = \\mathbf{x}_n + \\frac{h}{6}\\bigl(\\mathbf{k}_1 + 2\\mathbf{k}_2 + 2\\mathbf{k}_3 + \\mathbf{k}_4\\bigr).\n\\]"}),"\n",i.createElement(t.p,null,"Here, the intermediate ",i.createElement(o.A,{text:"\\( \\mathbf{k}_i \\)"})," values are essentially estimates of the slope at different points in the interval ",i.createElement(o.A,{text:"\\( [t_n, t_n + h] \\)"}),". This approach usually yields much better accuracy than Euler for the same step size."),"\n",i.createElement(t.h3,{id:"local-vs-total-truncation-errors",style:{position:"relative"}},i.createElement(t.a,{href:"#local-vs-total-truncation-errors","aria-label":"local vs total truncation errors permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Local vs. total truncation errors"),"\n",i.createElement(t.p,null,i.createElement(l.A,null,"Local error")," is the error made in one step, assuming perfect knowledge of ",i.createElement(o.A,{text:"\\( \\mathbf{x}_n \\)"}),". ",i.createElement(l.A,null,"Total error")," (or global error) accumulates over many steps. Higher-order methods like RK4 reduce local error more rapidly, thus improving overall accuracy at the cost of additional function evaluations per step."),"\n",i.createElement(t.h3,{id:"adaptive-step-size-methods",style:{position:"relative"}},i.createElement(t.a,{href:"#adaptive-step-size-methods","aria-label":"adaptive step size methods permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Adaptive step-size methods"),"\n",i.createElement(t.p,null,"Sophisticated solvers (Dormand–Prince, Bogacki–Shampine, etc.) dynamically adjust ",i.createElement(o.A,{text:"\\( h \\)"})," depending on local error estimates. This is extremely useful in neural ODE contexts, since the solver can take large steps in regions where ",i.createElement(o.A,{text:"\\( f \\)"})," does not change much and smaller steps where ",i.createElement(o.A,{text:"\\( f \\)"})," is more rapidly varying, thereby balancing speed and accuracy."),"\n",i.createElement(t.h3,{id:"implicit-vs-explicit-methods",style:{position:"relative"}},i.createElement(t.a,{href:"#implicit-vs-explicit-methods","aria-label":"implicit vs explicit methods permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Implicit vs. explicit methods"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(l.A,null,"Explicit")," methods compute ",i.createElement(o.A,{text:"\\( \\mathbf{x}_{n+1} \\)"})," directly from known quantities. They can fail or require very small step sizes for stiff problems."),"\n",i.createElement(t.li,null,i.createElement(l.A,null,"Implicit")," methods solve an equation involving ",i.createElement(o.A,{text:"\\( \\mathbf{x}_{n+1} \\)"})," (like backward Euler), which is more stable for stiff systems but can be more computationally expensive per step."),"\n"),"\n",i.createElement(t.h3,{id:"example-lotkavolterra-equations",style:{position:"relative"}},i.createElement(t.a,{href:"#example-lotkavolterra-equations","aria-label":"example lotkavolterra equations permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Example: Lotka–Volterra equations"),"\n",i.createElement(t.p,null,"The Lotka–Volterra predator–prey model is a classic demonstration of ODE solvers. You can see how different methods (Euler vs. RK4 vs. adaptive) capture the oscillatory nature of the predator and prey populations. By adjusting step sizes, you can explore how numerical stability and accuracy vary."),"\n",i.createElement(t.h2,{id:"phase-space-and-phase-portraits",style:{position:"relative"}},i.createElement(t.a,{href:"#phase-space-and-phase-portraits","aria-label":"phase space and phase portraits permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Phase space and phase portraits"),"\n",i.createElement(t.h3,{id:"phase-space-variables-and-dimensionality",style:{position:"relative"}},i.createElement(t.a,{href:"#phase-space-variables-and-dimensionality","aria-label":"phase space variables and dimensionality permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Phase space variables and dimensionality"),"\n",i.createElement(t.p,null,"A system with ",i.createElement(o.A,{text:"\\( n \\)"})," state variables can be viewed as evolving in an ",i.createElement(o.A,{text:"\\( n \\)"}),"-dimensional phase space. Each point in that space corresponds to a unique configuration of the system. As time progresses, a trajectory is traced out in this space."),"\n",i.createElement(t.h3,{id:"example-simple-pendulum-system",style:{position:"relative"}},i.createElement(t.a,{href:"#example-simple-pendulum-system","aria-label":"example simple pendulum system permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Example: Simple pendulum system"),"\n",i.createElement(t.p,null,"A simple pendulum's state can be described by angle ",i.createElement(o.A,{text:"\\( \\theta \\)"})," and angular velocity ",i.createElement(o.A,{text:"\\( \\omega = d\\theta/dt \\)"}),". That makes a 2D phase space (",i.createElement(o.A,{text:"\\( \\theta \\)"}),", ",i.createElement(o.A,{text:"\\( \\omega \\)"}),"). Since energy is conserved (assuming no friction), trajectories form closed loops in phase space."),"\n",i.createElement(a,{alt:"Simple pendulum phase portrait example",path:"",caption:"Idealized simple pendulum phase portrait with energy contours.",zoom:"false"}),"\n",i.createElement(t.h3,{id:"equilibrium-points-and-stability",style:{position:"relative"}},i.createElement(t.a,{href:"#equilibrium-points-and-stability","aria-label":"equilibrium points and stability permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Equilibrium points and stability"),"\n",i.createElement(t.p,null,"An ",i.createElement(l.A,null,"equilibrium point")," (or fixed point) is where ",i.createElement(o.A,{text:"\\( d\\mathbf{x}/dt = \\mathbf{0} \\)"}),". If small perturbations decay over time, the equilibrium is stable; if they grow, it is unstable. In neural ODEs, stable equilibria can sometimes correspond to attractors in the function space that the neural network learns."),"\n",i.createElement(t.h3,{id:"plotting-phase-portraits-for-various-initial-conditions",style:{position:"relative"}},i.createElement(t.a,{href:"#plotting-phase-portraits-for-various-initial-conditions","aria-label":"plotting phase portraits for various initial conditions permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Plotting phase portraits for various initial conditions"),"\n",i.createElement(t.p,null,"Plotting a 2D system can be done by picking a grid of points and drawing arrows or lines that represent ",i.createElement(o.A,{text:"\\( f(\\mathbf{x}, t) \\)"}),". By overlaying multiple trajectories from different initial conditions, you gain insight into the global behavior (e.g., whether solutions converge to attractors or revolve around cycles)."),"\n",i.createElement(t.h3,{id:"lotkavolterra-phase-portrait",style:{position:"relative"}},i.createElement(t.a,{href:"#lotkavolterra-phase-portrait","aria-label":"lotkavolterra phase portrait permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Lotka–Volterra phase portrait"),"\n",i.createElement(t.p,null,"If you visualize the predator–prey populations in a 2D plane (",i.createElement(o.A,{text:"\\( x \\)"})," for prey, ",i.createElement(o.A,{text:"\\( y \\)"})," for predator), the system's cyclical nature becomes readily apparent. Orbits revolve around a central point, representing out-of-phase population oscillations."),"\n",i.createElement(t.h3,{id:"chaotic-dynamics-and-strange-attractors",style:{position:"relative"}},i.createElement(t.a,{href:"#chaotic-dynamics-and-strange-attractors","aria-label":"chaotic dynamics and strange attractors permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Chaotic dynamics and strange attractors"),"\n",i.createElement(t.p,null,"Systems like the ",i.createElement(l.A,null,"Lorenz system")," exhibit chaos, meaning sensitive dependence on initial conditions. Neural ODEs can theoretically approximate chaotic flows as well, but training them to do so reliably remains an open research challenge in many contexts."),"\n",i.createElement(t.h2,{id:"neural-odes",style:{position:"relative"}},i.createElement(t.a,{href:"#neural-odes","aria-label":"neural odes permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Neural ODEs"),"\n",i.createElement(t.h3,{id:"introduction-and-intuition",style:{position:"relative"}},i.createElement(t.a,{href:"#introduction-and-intuition","aria-label":"introduction and intuition permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Introduction and intuition"),"\n",i.createElement(t.p,null,"Neural ODEs were popularized by (Chen and gang, NeurIPS 2018). The main idea is that instead of stacking discrete layers ",i.createElement(o.A,{text:"\\( h_{k+1} = h_k + f(h_k) \\)"})," (think of a ResNet's skip connection), we parameterize a continuous transformation of a hidden state ",i.createElement(o.A,{text:"\\( \\mathbf{z}(t) \\)"})," via an ODE:"),"\n",i.createElement(o.A,{text:"\\[\n\\frac{d\\mathbf{z}}{dt} = f(\\mathbf{z}(t), t; \\theta).\n\\]"}),"\n",i.createElement(t.p,null,"Then, we use a black-box ODE solver to integrate from ",i.createElement(o.A,{text:"\\( t = 0 \\)"})," to ",i.createElement(o.A,{text:"\\( t = 1 \\)"})," (or any interval we choose). This effectively replaces discrete layers with a continuous-depth network. The advantage is that we only evaluate as many intermediate steps as needed for the desired accuracy (controlled by the ODE solver), and we can train the parameters ",i.createElement(o.A,{text:"\\( \\theta \\)"})," end-to-end to perform classification or other tasks."),"\n",i.createElement(t.h3,{id:"comparison-to-resnet-architectures",style:{position:"relative"}},i.createElement(t.a,{href:"#comparison-to-resnet-architectures","aria-label":"comparison to resnet architectures permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Comparison to ResNet architectures"),"\n",i.createElement(t.p,null,"A ResNet block with a small step size ",i.createElement(o.A,{text:"\\( h \\)"})," can be viewed as:"),"\n",i.createElement(o.A,{text:"\\[\n\\mathbf{z}_{k+1} = \\mathbf{z}_k + h \\, f(\\mathbf{z}_k),\n\\]"}),"\n",i.createElement(t.p,null,"where if ",i.createElement(o.A,{text:"\\( h \\)"})," is small and ",i.createElement(o.A,{text:"\\( k \\)"})," is large, the transformations approximate a continuous evolution. Neural ODEs push this concept to the limit: they literally treat the transformation as an ODE in continuous time, removing the notion of a finite number of layers altogether. Depth becomes a variable that the ODE solver can adapt dynamically."),"\n",i.createElement(t.h3,{id:"backpropagation-through-ode-solvers",style:{position:"relative"}},i.createElement(t.a,{href:"#backpropagation-through-ode-solvers","aria-label":"backpropagation through ode solvers permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Backpropagation through ODE solvers"),"\n",i.createElement(t.p,null,"A naive approach might record all intermediate states to perform standard backpropagation, but that can become very memory-intensive. Instead, (Chen and gang) introduced the ",i.createElement(l.A,null,"adjoint sensitivity method"),", which solves another ODE backwards in time to compute gradients, removing the need to store the entire forward pass. This can reduce memory usage dramatically."),"\n",i.createElement(t.h4,{id:"naive-backprop-vs-adjoint-sensitivity-method",style:{position:"relative"}},i.createElement(t.a,{href:"#naive-backprop-vs-adjoint-sensitivity-method","aria-label":"naive backprop vs adjoint sensitivity method permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Naive backprop vs. adjoint sensitivity method"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(l.A,null,"Naive backprop"),": The solver obtains states ",i.createElement(o.A,{text:"\\( \\mathbf{z}(t_0), \\ldots, \\mathbf{z}(t_N) \\)"}),". Then a standard backprop pass is done, requiring memory of each state."),"\n",i.createElement(t.li,null,i.createElement(l.A,null,"Adjoint method"),": We keep only the final state, then integrate a specially derived equation for the adjoint variable ",i.createElement(o.A,{text:"\\( \\frac{\\partial L}{\\partial \\mathbf{z}(t)} \\)"})," backwards in time to find the gradient with respect to parameters and the initial state."),"\n"),"\n",i.createElement(t.h3,{id:"memory-vs-accuracy-trade-off",style:{position:"relative"}},i.createElement(t.a,{href:"#memory-vs-accuracy-trade-off","aria-label":"memory vs accuracy trade off permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Memory vs. accuracy trade-off"),"\n",i.createElement(t.p,null,"Using the adjoint method can lead to ",i.createElement(r.A,{text:"Reverse-mode integration can have numerical issues, e.g., requiring fine step sizes if the forward trajectory is sensitive."})," concerns about numeric stability and accuracy. In some tasks, standard backprop might be simpler if memory permits. In large-scale problems, the memory savings of adjoint-based approaches can be critical."),"\n",i.createElement(t.h3,{id:"implementation-with-torchdiffeq",style:{position:"relative"}},i.createElement(t.a,{href:"#implementation-with-torchdiffeq","aria-label":"implementation with torchdiffeq permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Implementation with Torchdiffeq"),"\n",i.createElement(t.p,null,"The ",i.createElement(l.A,null,"torchdiffeq")," library provides ",i.createElement(s.A,{text:"\npip install torchdiffeq\n"})," a simple way to define neural ODEs in PyTorch:"),"\n",i.createElement(s.A,{text:"\nimport torch\nfrom torch import nn\nfrom torchdiffeq import odeint\n\nclass ODEFunc(nn.Module):\n    def __init__(self):\n        super(ODEFunc, self).__init__()\n        self.net = nn.Sequential(\n            nn.Linear(2, 50),\n            nn.Tanh(),\n            nn.Linear(50, 2),\n        )\n    \n    def forward(self, t, z):\n        return self.net(z)\n\n# Usage:\nfunc = ODEFunc()\nz0 = torch.tensor([1.0, 0.0])\nt = torch.linspace(0, 1, 100)\nz_t = odeint(func, z0, t)\n"}),"\n",i.createElement(t.p,null,"Here, ",i.createElement(o.A,{text:"\\( ODEFunc \\)"})," defines the vector field ",i.createElement(o.A,{text:"\\( f(\\mathbf{z}, t) \\)"}),", and ",i.createElement(o.A,{text:"\\( odeint \\)"})," integrates from ",i.createElement(o.A,{text:"\\( t=0 \\)"})," to ",i.createElement(o.A,{text:"\\( t=1 \\)"}),"."),"\n",i.createElement(t.h3,{id:"classification-example-half-moons-dataset",style:{position:"relative"}},i.createElement(t.a,{href:"#classification-example-half-moons-dataset","aria-label":"classification example half moons dataset permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Classification example: Half Moons dataset"),"\n",i.createElement(t.p,null,"A classic 2D classification test problem is the half-moons dataset. One can specify ",i.createElement(o.A,{text:"\\( \\mathbf{z}(0) = \\text{input features} \\)"})," and integrate to ",i.createElement(o.A,{text:"\\( \\mathbf{z}(1) \\)"}),". Then feed ",i.createElement(o.A,{text:"\\( \\mathbf{z}(1) \\)"})," into a linear layer or simply interpret the integrated features as a final representation for classification. By training the parameters of the ODE function, we aim to separate the half-moons in the final space."),"\n",i.createElement(s.A,{text:'\nimport torch\nimport numpy as np\nfrom torchdiffeq import odeint_adjoint as odeint  # Using adjoint version for memory efficiency\nimport matplotlib.pyplot as plt\n\n# Generate half moons data\nN = 2000\n# We\'ll keep it small for demonstration, but can be bigger\nfrom sklearn.datasets import make_moons\nX, y = make_moons(n_samples=N, noise=0.1)\n\nX = torch.from_numpy(X).float()\ny = torch.from_numpy(y).long()\n\nclass ODEFunc(nn.Module):\n    def __init__(self, hidden_dim=16):\n        super(ODEFunc, self).__init__()\n        self.net = nn.Sequential(\n            nn.Linear(2, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, 2)\n        )\n    def forward(self, t, z):\n        return self.net(z)\n\nclass ODEBlock(nn.Module):\n    def __init__(self, odefunc, solver=\'rk4\'):\n        super(ODEBlock, self).__init__()\n        self.odefunc = odefunc\n        self.solver = solver\n    \n    def forward(self, x, t=torch.tensor([0., 1.])):\n        # Solve from t=0 to t=1\n        z = odeint(self.odefunc, x, t, method=self.solver)\n        # z shape: [time, batch, features]\n        return z[-1]\n\nclass ODEModel(nn.Module):\n    def __init__(self, hidden_dim=16):\n        super(ODEModel, self).__init__()\n        self.odeblock = ODEBlock(ODEFunc(hidden_dim=hidden_dim))\n        self.classifier = nn.Linear(2, 2)\n    \n    def forward(self, x):\n        z = self.odeblock(x)\n        return self.classifier(z)\n\nmodel = ODEModel()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nloss_fn = nn.CrossEntropyLoss()\n\n# Simple training loop\nfor epoch in range(50):\n    optimizer.zero_grad()\n    pred = model(X)\n    loss = loss_fn(pred, y)\n    loss.backward()\n    optimizer.step()\n    if epoch % 10 == 0:\n        print(f"Epoch {epoch} - Loss: {loss.item():.4f}")\n\n# Evaluate\nwith torch.no_grad():\n    preds = model(X).argmax(dim=1)\n    acc = (preds == y).float().mean().item()\n    print(f"Train Accuracy: {acc*100:.2f}%")\n'}),"\n",i.createElement(t.p,null,"As you train, the ODE-based model learns a flow that separates the classes in the integrated representation."),"\n",i.createElement(t.h3,{id:"visualizing-learned-trajectories",style:{position:"relative"}},i.createElement(t.a,{href:"#visualizing-learned-trajectories","aria-label":"visualizing learned trajectories permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Visualizing learned trajectories"),"\n",i.createElement(t.p,null,"You can sample ",i.createElement(o.A,{text:"\\( \\mathbf{z}(0) \\)"})," from a grid over the 2D plane (spanning the half moons) and plot ",i.createElement(o.A,{text:"\\( \\mathbf{z}(t) \\)"})," at intermediate time points. This yields a continuous flow field that warps the plane so as to pull apart the classes."),"\n",i.createElement(t.h2,{id:"topological-constraints-and-limitations",style:{position:"relative"}},i.createElement(t.a,{href:"#topological-constraints-and-limitations","aria-label":"topological constraints and limitations permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Topological constraints and limitations"),"\n",i.createElement(t.h3,{id:"why-neural-odes-can-only-describe-homeomorphisms",style:{position:"relative"}},i.createElement(t.a,{href:"#why-neural-odes-can-only-describe-homeomorphisms","aria-label":"why neural odes can only describe homeomorphisms permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Why neural ODEs can only describe homeomorphisms"),"\n",i.createElement(t.p,null,"The continuous flows they generate are invertible (assuming certain regularity conditions). This implies that in 2D, a neural ODE map cannot ",i.createElement(l.A,null,"tear apart")," a connected region of space into two disconnected regions. In topological terms, the transformation is a homeomorphism. Consequently, if your dataset classes are topologically distinct (like two concentric circles with empty space between them), a single continuous flow that starts as a single connected region cannot easily separate them unless it does so in a continuous manner that does not violate invertibility."),"\n",i.createElement(t.h3,{id:"concentric-circles-dataset-example",style:{position:"relative"}},i.createElement(t.a,{href:"#concentric-circles-dataset-example","aria-label":"concentric circles dataset example permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Concentric circles dataset example"),"\n",i.createElement(t.p,null,"Suppose you have an inner circle labeled 0 and an outer ring labeled 1. A neural ODE that is purely 2D can have trouble learning a function that linearly separates the classes after time ",i.createElement(o.A,{text:"\\( T \\)"}),". Intuitively, an invertible continuous transformation cannot create or remove holes in the domain."),"\n",i.createElement(t.h3,{id:"the-cheating-effect-in-practice",style:{position:"relative"}},i.createElement(t.a,{href:"#the-cheating-effect-in-practice","aria-label":"the cheating effect in practice permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),'The "cheating" effect in practice'),"\n",i.createElement(t.p,null,"Empirically, neural ODEs can sometimes ",i.createElement(r.A,{text:"One possibility is that the solver can revolve or stretch the data near singular boundaries, effectively 'threading a needle' topologically."})," appear to circumvent strict topological constraints. Still, these illusions often rely on subtle distortions or numerical approximations. True topological obstructions remain a key limitation."),"\n",i.createElement(t.h3,{id:"discussion-on-generalization-pitfalls",style:{position:"relative"}},i.createElement(t.a,{href:"#discussion-on-generalization-pitfalls","aria-label":"discussion on generalization pitfalls permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Discussion on generalization pitfalls"),"\n",i.createElement(t.p,null,"Because of these constraints, neural ODEs might not generalize well to problems requiring non-invertible transformations or intricate topological changes. They can also be susceptible to stiff dynamics, requiring specialized solvers."),"\n",i.createElement(t.h3,{id:"potential-solutions-and-open-research-problems",style:{position:"relative"}},i.createElement(t.a,{href:"#potential-solutions-and-open-research-problems","aria-label":"potential solutions and open research problems permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Potential solutions and open research problems"),"\n",i.createElement(t.p,null,"Researchers have proposed ways to address these topological constraints, such as:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Augmented neural ODEs.")),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Adding diffusion or noise")," (leading to neural SDEs)."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Piecewise defined ODE fields")," that can incorporate boundary conditions, though these remain complex in practice."),"\n"),"\n",i.createElement(t.h2,{id:"augmented-neural-odes",style:{position:"relative"}},i.createElement(t.a,{href:"#augmented-neural-odes","aria-label":"augmented neural odes permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Augmented Neural ODEs"),"\n",i.createElement(t.h3,{id:"augmenting-dimensions-to-overcome-topological-limits",style:{position:"relative"}},i.createElement(t.a,{href:"#augmenting-dimensions-to-overcome-topological-limits","aria-label":"augmenting dimensions to overcome topological limits permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Augmenting dimensions to overcome topological limits"),"\n",i.createElement(t.p,null,"Augmented Neural ODEs (ANODEs) (Dupont and gang, NeurIPS 2019) introduce extra channels or dimensions, effectively embedding the data in a higher-dimensional space where the flow can separate classes more easily. This can circumvent the topological constraints in lower dimensions."),"\n",i.createElement(o.A,{text:"\\[\n\\frac{d}{dt}\n\\begin{pmatrix}\n\\mathbf{z}(t) \\\\\n\\mathbf{a}(t)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nf(\\mathbf{z}(t), \\mathbf{a}(t), t; \\theta) \\\\\ng(\\mathbf{z}(t), \\mathbf{a}(t), t; \\theta)\n\\end{pmatrix},\n\\]"}),"\n",i.createElement(t.p,null,"where ",i.createElement(o.A,{text:"\\( \\mathbf{a}(t) \\)"})," is the augmented component with arbitrary dimension. This new dimension can effectively provide new degrees of freedom to warp data."),"\n",i.createElement(t.h3,{id:"formulating-the-augmented-ode-problem",style:{position:"relative"}},i.createElement(t.a,{href:"#formulating-the-augmented-ode-problem","aria-label":"formulating the augmented ode problem permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Formulating the augmented ODE problem"),"\n",i.createElement(t.p,null,"One simple approach is to initialize ",i.createElement(o.A,{text:"\\( \\mathbf{a}(0) \\)"})," to zeros (or random values) of dimension ",i.createElement(o.A,{text:"\\( d_{aug} \\)"}),", then let it evolve alongside ",i.createElement(o.A,{text:"\\( \\mathbf{z}(t) \\)"}),". The final readout might ignore these extra dimensions or incorporate them into the classification layer."),"\n",i.createElement(t.h3,{id:"implementation-details-and-training",style:{position:"relative"}},i.createElement(t.a,{href:"#implementation-details-and-training","aria-label":"implementation details and training permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Implementation details and training"),"\n",i.createElement(t.p,null,"The training loop is largely the same, except your ODE function sees a concatenated vector ",i.createElement(o.A,{text:"\\( [\\mathbf{z}(t), \\mathbf{a}(t)] \\)"}),". You might also tie or freeze parts of ",i.createElement(o.A,{text:"\\( g \\)"})," to force the model to learn specific behaviors or to reduce complexity. This can help the model break the homeomorphism restriction in the original low-dimensional space."),"\n",i.createElement(t.h3,{id:"visualizing-trajectories-in-higher-dimensions",style:{position:"relative"}},i.createElement(t.a,{href:"#visualizing-trajectories-in-higher-dimensions","aria-label":"visualizing trajectories in higher dimensions permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Visualizing trajectories in higher dimensions"),"\n",i.createElement(t.p,null,"Although direct visualization beyond 3D is trickier, you can still visualize the final 2D projection. Or use dimensionality reduction techniques (like PCA) to see how the augmented dimension modifies the data's manifold structure."),"\n",i.createElement(t.h3,{id:"theoretical-considerations-on-expressivity",style:{position:"relative"}},i.createElement(t.a,{href:"#theoretical-considerations-on-expressivity","aria-label":"theoretical considerations on expressivity permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Theoretical considerations on expressivity"),"\n",i.createElement(t.p,null,"By embedding data into a higher-dimensional manifold, the model can represent transformations that were previously impossible. This can significantly enhance expressivity but may also introduce more parameters and potential overfitting."),"\n",i.createElement(t.h2,{id:"practical-considerations",style:{position:"relative"}},i.createElement(t.a,{href:"#practical-considerations","aria-label":"practical considerations permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Practical considerations"),"\n",i.createElement(t.h3,{id:"choosing-activation-functions-lipschitz-constraints",style:{position:"relative"}},i.createElement(t.a,{href:"#choosing-activation-functions-lipschitz-constraints","aria-label":"choosing activation functions lipschitz constraints permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Choosing activation functions (Lipschitz constraints)"),"\n",i.createElement(t.p,null,"To ensure stability, it is often recommended to use smoother, carefully bounded activation functions (e.g., ",i.createElement(l.A,null,"tanh"),", ",i.createElement(l.A,null,"softplus"),", ",i.createElement(l.A,null,"swish"),"). Harsh nonlinearities like ReLU can cause large Lipschitz constants. Some advanced neural ODE frameworks explicitly parameterize Lipschitz-constrained layers to guarantee well-behaved flows."),"\n",i.createElement(t.h3,{id:"handling-stiff-odes-and-advanced-solvers",style:{position:"relative"}},i.createElement(t.a,{href:"#handling-stiff-odes-and-advanced-solvers","aria-label":"handling stiff odes and advanced solvers permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Handling stiff ODEs and advanced solvers"),"\n",i.createElement(t.p,null,"Stiffness arises when different components of ",i.createElement(o.A,{text:"\\( \\mathbf{z}(t) \\)"})," evolve on very different time scales, forcing the solver to use very small steps. Implicit solvers or specialized methods can handle stiffness but at a higher computational cost. In neural ODE training, if you notice extremely small step sizes, that may hint at stiff behavior in your learned vector field. Techniques like spectral normalization or restricting layer depth can help mitigate stiffness."),"\n",i.createElement(t.h3,{id:"gpu-vs-cpu-usage-and-performance-tips",style:{position:"relative"}},i.createElement(t.a,{href:"#gpu-vs-cpu-usage-and-performance-tips","aria-label":"gpu vs cpu usage and performance tips permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"GPU vs. CPU usage and performance tips"),"\n",i.createElement(t.p,null,"Because ODE solvers might require many function evaluations (especially higher-order methods), GPU acceleration can be extremely beneficial, but your mileage may vary depending on how many times ",i.createElement(o.A,{text:"\\( f \\)"})," is called and how large your batch sizes are. For CPU-based systems, consider simpler solvers or smaller batch sizes. Also, caching repeated operations or using specialized libraries like ",i.createElement(l.A,null,"torchdiffeq")," can help."),"\n",i.createElement(t.h3,{id:"hyperparameter-tuning",style:{position:"relative"}},i.createElement(t.a,{href:"#hyperparameter-tuning","aria-label":"hyperparameter tuning permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Hyperparameter tuning"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Step size")," or ",i.createElement(l.A,null,"solver tolerance"),": In adaptive solvers, you typically set a relative and absolute tolerance. Tighter tolerances produce more accurate solutions but require more function evaluations."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Regularization"),": Weight decay or other forms of regularization can help keep ",i.createElement(o.A,{text:"\\( f \\)"})," from becoming too large or too rapidly changing."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Network architecture"),": The dimension of hidden layers, the number of layers in ",i.createElement(o.A,{text:"\\( f \\)"})," itself, and activation choices can drastically affect performance."),"\n"),"\n",i.createElement(t.h3,{id:"working-with-real-world-data-and-noise",style:{position:"relative"}},i.createElement(t.a,{href:"#working-with-real-world-data-and-noise","aria-label":"working with real world data and noise permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Working with real-world data and noise"),"\n",i.createElement(t.p,null,"Real data is seldom noise-free. The ODE viewpoint assumes a certain level of continuity or smoothness. If your data is heavily corrupted or sampling times are irregular, specialized approaches (like neural CDEs, which handle continuous-time data streams) or robust solvers might be needed. Preprocessing steps — smoothing or filtering your data — can also help."),"\n",i.createElement(t.h3,{id:"debugging-and-troubleshooting-common-issues",style:{position:"relative"}},i.createElement(t.a,{href:"#debugging-and-troubleshooting-common-issues","aria-label":"debugging and troubleshooting common issues permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Debugging and troubleshooting common issues"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"If the solver takes tiny steps or runs extremely slowly, suspect stiffness in your learned ",i.createElement(o.A,{text:"\\( f \\)"}),"."),"\n",i.createElement(t.li,null,"If training fails to converge, try simpler architectures or limit the range of time integration."),"\n",i.createElement(t.li,null,"If you see unbounded blow-ups in ",i.createElement(o.A,{text:"\\( \\mathbf{z}(t) \\)"}),", check for extremely large parameter values or extremely steep activation functions, which might break Lipschitz continuity."),"\n"),"\n",i.createElement(t.h2,{id:"further-reading-and-resources",style:{position:"relative"}},i.createElement(t.a,{href:"#further-reading-and-resources","aria-label":"further reading and resources permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Further reading and resources"),"\n",i.createElement(t.h3,{id:"extensions-neural-sdes-cdes-hamiltonian-neural-networks-symplectic-odes-and-other-variants",style:{position:"relative"}},i.createElement(t.a,{href:"#extensions-neural-sdes-cdes-hamiltonian-neural-networks-symplectic-odes-and-other-variants","aria-label":"extensions neural sdes cdes hamiltonian neural networks symplectic odes and other variants permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Extensions: Neural SDEs, CDEs, Hamiltonian neural networks, symplectic ODEs, and other variants"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Neural SDEs"),": Add stochastic terms to handle random processes."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Neural CDEs"),": Model data coming in as continuous streams (Kidger and gang)."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Hamiltonian neural networks"),": Enforce Hamiltonian structure for systems with conserved energy (e.g., in physics)."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Symplectic ODEs"),": For systems with symplectic structure, specialized integrators preserve volume in phase space."),"\n"),"\n",i.createElement(t.h3,{id:"recommended-libraries",style:{position:"relative"}},i.createElement(t.a,{href:"#recommended-libraries","aria-label":"recommended libraries permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Recommended libraries"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"torchdiffeq"),": The go-to library in PyTorch for neural ODE functionality."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Torchdyn"),": Offers a higher-level interface with additional features like neural SDEs."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"JAX-based")," frameworks: For automatic differentiation with HPC-friendly backends."),"\n",i.createElement(t.li,null,"Domain-specific frameworks: E.g., for computational physics or biology where ODE solutions and ML converge."),"\n"),"\n",i.createElement(t.h3,{id:"blogs-tutorials-and-code-repositories",style:{position:"relative"}},i.createElement(t.a,{href:"#blogs-tutorials-and-code-repositories","aria-label":"blogs tutorials and code repositories permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Blogs, tutorials, and code repositories"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(l.A,null,"Neural ODEs: The Technical Details")," (blog by David Duvenaud's group)."),"\n",i.createElement(t.li,null,i.createElement(l.A,null,"GitHub: DiffEqFlux.jl")," (Julia-based library for neural differential equations)."),"\n",i.createElement(t.li,null,i.createElement(l.A,null,"Arxiv papers")," from Chen and gang and follow-up works by Dupont and gang on ANODEs."),"\n"),"\n",i.createElement(t.h3,{id:"closing-remarks-and-future-directions",style:{position:"relative"}},i.createElement(t.a,{href:"#closing-remarks-and-future-directions","aria-label":"closing remarks and future directions permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Closing remarks and future directions"),"\n",i.createElement(t.p,null,"Neural ODEs bring an elegant continuous-time viewpoint to deep learning, bridging classical numerical analysis with cutting-edge ML. They open interesting avenues for memory-efficient training, invertible transformations, and physically informed modeling. Ongoing research explores deeper theoretical underpinnings (such as well-posedness in high dimensions, better handling of stiffness, and topological constraints) and broader applications across domains like finance, signal processing, climate science, and generative modeling."),"\n",i.createElement(t.h3,{id:"collaboration-and-community-development",style:{position:"relative"}},i.createElement(t.a,{href:"#collaboration-and-community-development","aria-label":"collaboration and community development permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Collaboration and community development"),"\n",i.createElement(t.p,null,"I encourage you to share your own experiments and open-source code with the broader community. Many neural ODE enthusiasts gather in specialized forums and GitHub repositories, where best practices and solver innovations are constantly evolving. By collaborating, we can refine these methods, discover their limitations, and push the frontiers of modern machine learning in a direction that harmonizes with the timeless beauty of continuous dynamical systems."),"\n",i.createElement(t.p,null,"I hope this extensive journey has provided both breadth and depth to your understanding of neural ordinary differential equations. If you are new to the area, try replicating the half-moons classification experiment. If you are experienced, explore augmented ODEs or neural SDEs. In all cases, stay curious about the interplay of continuous mathematics, numerical methods, and neural architectures — the synergy is likely to continue advancing the field in remarkable ways!"))}var h=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,n.RP)(),e.components);return t?i.createElement(t,e,i.createElement(c,e)):c(e)};var m=a(54506),d=a(88864),u=a(58481),p=a.n(u),f=a(5984),v=a(43672),g=a(27042),y=a(72031),E=a(81817),b=a(27105),w=a(17265),x=a(2043),S=a(95751),z=a(94328),H=a(80791),k=a(78137);const _=e=>{let{toc:t}=e;if(!t||!t.items)return null;return i.createElement("nav",{className:H.R},i.createElement("ul",null,t.items.map(((e,t)=>i.createElement("li",{key:t},i.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const a=t.replace("#",""),n=document.getElementById(a);n&&n.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&i.createElement(_,{toc:{items:e.items}}))))))};function M(e){let{data:{mdx:t,allMdx:r,allPostImages:l},children:s}=e;const{frontmatter:o,body:c,tableOfContents:h}=t,d=o.index,u=o.slug.split("/")[1],y=r.nodes.filter((e=>e.frontmatter.slug.includes(`/${u}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),H=y.findIndex((e=>e.frontmatter.index===d)),M=y[H+1],A=y[H-1],C=o.slug.replace(/\/$/,""),L=/[^/]*$/.exec(C)[0],T=`posts/${u}/content/${L}/`,{0:N,1:D}=(0,i.useState)(o.flagWideLayoutByDefault),{0:I,1:V}=(0,i.useState)(!1);var O;(0,i.useEffect)((()=>{V(!0);const e=setTimeout((()=>V(!1)),340);return()=>clearTimeout(e)}),[N]),"adventures"===u?O=w.cb:"research"===u?O=w.Qh:"thoughts"===u&&(O=w.T6);const j=p()(c).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,B=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),a=e%60;return a<=30?`~${t}${a>0?".5":""} h`:`~${t+1} h`}(Math.ceil(j/O)+(o.extraReadTimeMin||0)),P=[{flag:o.flagDraft,component:()=>Promise.all([a.e(5850),a.e(9833)]).then(a.bind(a,49833))},{flag:o.flagMindfuckery,component:()=>Promise.all([a.e(5850),a.e(7805)]).then(a.bind(a,27805))},{flag:o.flagRewrite,component:()=>Promise.all([a.e(5850),a.e(8916)]).then(a.bind(a,78916))},{flag:o.flagOffensive,component:()=>Promise.all([a.e(5850),a.e(6731)]).then(a.bind(a,49112))},{flag:o.flagProfane,component:()=>Promise.all([a.e(5850),a.e(3336)]).then(a.bind(a,83336))},{flag:o.flagMultilingual,component:()=>Promise.all([a.e(5850),a.e(2343)]).then(a.bind(a,62343))},{flag:o.flagUnreliably,component:()=>Promise.all([a.e(5850),a.e(6865)]).then(a.bind(a,11627))},{flag:o.flagPolitical,component:()=>Promise.all([a.e(5850),a.e(4417)]).then(a.bind(a,24417))},{flag:o.flagCognitohazard,component:()=>Promise.all([a.e(5850),a.e(8669)]).then(a.bind(a,18669))},{flag:o.flagHidden,component:()=>Promise.all([a.e(5850),a.e(8124)]).then(a.bind(a,48124))}],{0:q,1:R}=(0,i.useState)([]);return(0,i.useEffect)((()=>{P.forEach((e=>{let{flag:t,component:a}=e;t&&a().then((e=>{R((t=>[].concat((0,m.A)(t),[e.default])))}))}))}),[]),i.createElement(g.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},i.createElement(E.A,{postNumber:o.index,date:o.date,updated:o.updated,readTime:B,difficulty:o.difficultyLevel,title:o.title,desc:o.desc,banner:o.banner,section:u,postKey:L,isMindfuckery:o.flagMindfuckery,mainTag:o.mainTag}),i.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},o.otherTags.map(((e,t)=>i.createElement("span",{key:t,className:`noselect ${k.MW}`,style:{margin:"0 5px 5px 0"}},e)))),i.createElement("div",{className:"postBody"},i.createElement(_,{toc:h})),i.createElement("br",null),i.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},i.createElement(g.P.button,{className:`noselect ${z.pb}`,id:z.xG,onClick:()=>{D(!N)},whileTap:{scale:.93}},i.createElement(g.P.div,{className:S.DJ,key:N,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},N?"Switch to default layout":"Switch to wide layout"))),i.createElement("br",null),i.createElement("div",{className:"postBody",style:{margin:N?"0 -14%":"",maxWidth:N?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},i.createElement("div",{className:`${z.P_} ${I?z.Xn:z.qG}`},q.map(((e,t)=>i.createElement(e,{key:t}))),o.indexCourse?i.createElement(x.A,{index:o.indexCourse,category:o.courseCategoryName}):"",i.createElement(f.Z.Provider,{value:{images:l.nodes,basePath:T.replace(/\/$/,"")+"/"}},i.createElement(n.xA,{components:{Image:v.A}},s)))),i.createElement(b.A,{nextPost:M,lastPost:A,keyCurrent:L,section:u}))}function A(e){return i.createElement(M,e,i.createElement(h,e))}function C(e){var t,a,n,r,l;let{data:s}=e;const{frontmatter:o}=s.mdx,c=o.titleSEO||o.title,h=o.titleOG||c,m=o.titleTwitter||c,u=o.descSEO||o.desc,p=o.descOG||u,f=o.descTwitter||u,v=o.schemaType||"BlogPosting",g=o.keywordsSEO,E=o.date,b=o.updated||E,w=o.imageOG||(null===(t=o.banner)||void 0===t||null===(a=t.childImageSharp)||void 0===a||null===(n=a.gatsbyImageData)||void 0===n||null===(r=n.images)||void 0===r||null===(l=r.fallback)||void 0===l?void 0:l.src),x=o.imageAltOG||p,S=o.imageTwitter||w,z=o.imageAltTwitter||f,H=o.canonicalURL,k=o.flagHidden||!1,_=o.mainTag||"Posts",M=o.slug.split("/")[1]||"posts",{siteUrl:A}=(0,d.Q)(),C={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:A},{"@type":"ListItem",position:2,name:_,item:`${A}/${o.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${A}${o.slug}`}]};return i.createElement(y.A,{title:c+" - avrtt.blog",titleOG:h,titleTwitter:m,description:u,descriptionOG:p,descriptionTwitter:f,schemaType:v,keywords:g,datePublished:E,dateModified:b,imageOG:w,imageAltOG:x,imageTwitter:S,imageAltTwitter:z,canonicalUrl:H,flagHidden:k,mainTag:_,section:M,type:"article"},i.createElement("script",{type:"application/ld+json"},JSON.stringify(C)))}},90548:function(e,t,a){var n=a(96540),i=a(7978);t.A=e=>{let{text:t}=e;return n.createElement(i.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-tsx-content-file-path-src-pages-posts-research-neural-odes-mdx-45081b0b27ddad756d77.js.map