"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[3347],{46256:function(e,t,n){n.r(t),n.d(t,{Head:function(){return H},PostTemplate:function(){return S},default:function(){return A}});var a=n(28453),i=n(96540),l=n(61992),r=(n(62087),n(90548));function o(e){const t=Object.assign({p:"p",ol:"ol",li:"li",strong:"strong",h3:"h3",a:"a",span:"span",ul:"ul",h2:"h2",hr:"hr",em:"em"},(0,a.RP)(),e.components),{Image:n}=t;return n||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),i.createElement(i.Fragment,null,"\n",i.createElement("br"),"\n","\n","\n",i.createElement(t.p,null,"Gaussian mixture models (GMMs) are a core tool in modern statistics and machine learning for representing complex, multimodal data distributions using a superposition of multiple Gaussian (normal) density components. In essence, a GMM posits that each data point is generated from one of several latent Gaussian distributions, each having its own mean and covariance parameters, and the probability of sampling from any particular component is given by a set of mixture weights that sum to one."),"\n",i.createElement(t.p,null,"The concept of mixture modeling is not limited to Gaussians, but Gaussian mixtures have proven especially popular due to their mathematical tractability, interpretability, and strong ties to the central limit theorem. Mixture of Gaussians also arises naturally in many real-world data scenarios where multiple underlying (approximately normal) processes combine to produce observed samples."),"\n",i.createElement(t.p,null,"Although a single Gaussian distribution may fail to capture complicated shapes in data (for instance, if the distribution is clearly multimodal or strongly skewed), a mixture of Gaussians can approximate a large variety of densities. This adaptability and flexibility has led to widespread use of GMMs in clustering, density estimation, anomaly detection, computer vision, speech processing, and numerous other fields. The capacity to model multiple, potentially overlapping subpopulations within a dataset gives GMMs a strong advantage over simpler parametric approaches."),"\n",i.createElement(t.p,null,"Early references to mixture models in general, and mixture-of-Gaussian approaches specifically, date back decades, but the concept soared in popularity in the statistical community in large part due to two major factors:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"The expectationâ€“maximization (EM) algorithm")," became recognized as a standard procedure for maximum likelihood parameter estimation in latent-variable models. While EM algorithms have been discovered many times in specialized settings, their general exposition in the classic paper by Dempster, Laird, and Rubin (1977) brought them into the mainstream. GMMs are perhaps the most famous example of an EM application, as the unknown membership of each data point to a particular mixture component can be treated as the latent variable."),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Widespread computational resources")," became available, which significantly lowered the barrier to fitting computationally non-trivial models, including iterative algorithms like EM. With high-powered servers and frameworks, even large datasets can be processed efficiently, making GMM training feasible in many commercial and research settings."),"\n"),"\n"),"\n",i.createElement(t.h3,{id:"11-the-basics-of-mixture-models",style:{position:"relative"}},i.createElement(t.a,{href:"#11-the-basics-of-mixture-models","aria-label":"11 the basics of mixture models permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"1.1 The basics of mixture models"),"\n",i.createElement(t.p,null,"A mixture model is a probabilistic framework in which the data distribution ",i.createElement(r.A,{text:"\\( p(\\mathbf{x}) \\)"})," is expressed as a finite or infinite weighted sum of component distributions:"),"\n",i.createElement(r.A,{text:"\\[\np(\\mathbf{x}) = \\sum_{k=1}^K \\pi_k \\, p(\\mathbf{x} \\mid \\theta_k),\n\\]"}),"\n",i.createElement(t.p,null,"where:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(r.A,{text:"\\( \\pi_k \\)"})," are the ",i.createElement(t.strong,null,"mixture weights")," or ",i.createElement(t.strong,null,"mixing coefficients"),", subject to constraints:\n",i.createElement(r.A,{text:"\\( \\pi_k \\ge 0 \\)"})," for all ",i.createElement(r.A,{text:"\\(k\\)"}),", and"),"\n",i.createElement(r.A,{text:"\\(\\sum_{k=1}^K \\pi_k = 1.\\)"}),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(r.A,{text:"\\( p(\\mathbf{x}\\mid\\theta_k) \\)"})," represents the ",i.createElement(r.A,{text:"\\(k\\)"}),"-th component distribution (for instance, a Gaussian with parameters ",i.createElement(r.A,{text:"\\(\\theta_k\\)"}),")."),"\n"),"\n"),"\n",i.createElement(t.p,null,"Hence, to generate a data point ",i.createElement(r.A,{text:"\\(\\mathbf{x}\\)"}),", one would first choose a mixture component (indexed by ",i.createElement(r.A,{text:"\\(k\\)"}),") according to probabilities ",i.createElement(r.A,{text:"\\(\\pi_k\\)"}),", then draw ",i.createElement(r.A,{text:"\\(\\mathbf{x}\\)"})," from that component's distribution. This latent process is typically unknown to us, so we do not observe which component each point came from. The ",i.createElement(t.strong,null,"Gaussian mixture model")," is a special case in which each component is a Gaussian distribution with its own mean and covariance matrix."),"\n",i.createElement(t.h3,{id:"12-why-use-gaussian-mixture-models",style:{position:"relative"}},i.createElement(t.a,{href:"#12-why-use-gaussian-mixture-models","aria-label":"12 why use gaussian mixture models permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"1.2 Why use Gaussian mixture models?"),"\n",i.createElement(t.p,null,"Gaussian mixtures serve as a sort of universal approximator for continuous densities, especially if one allows a large number of mixture components. Even with a moderate number of components, GMMs can represent complicated shapes that would be difficult to capture with a single parametric family (like a single Gaussian)."),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Common applications")," of GMMs in machine learning include:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Clustering:")," GMM-based clustering can be seen as a soft or probabilistic alternative to ",i.createElement(r.A,{text:"\\(k\\)"}),"-means, where each point is assigned membership probabilities for each cluster instead of a single discrete label. This allows for more nuanced cluster boundaries and can accommodate overlapping subpopulations."),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Anomaly or outlier detection:")," Fitting a mixture to normal data and then looking for points that have low posterior membership in all mixture components is a practical approach in anomaly detection tasks."),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Data generation and density estimation:")," GMMs can produce synthetic samples that mirror the distribution of real data (helpful in simulation, data augmentation, and other generative tasks). They are also used in iterative processes like some versions of the EM-based approaches for incomplete data."),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Model-based clustering in high dimensions:")," When the covariance matrices of the mixture components are appropriately constrained (e.g., diagonal or spherical), GMMs may remain tractable in moderate or even high-dimensional problems."),"\n"),"\n"),"\n",i.createElement(t.h3,{id:"13-a-brief-history-of-gaussian-distributions-and-mixture-modeling",style:{position:"relative"}},i.createElement(t.a,{href:"#13-a-brief-history-of-gaussian-distributions-and-mixture-modeling","aria-label":"13 a brief history of gaussian distributions and mixture modeling permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"1.3 A brief history of Gaussian distributions and mixture modeling"),"\n",i.createElement(t.p,null,"Gauss introduced the normal distribution in the context of astronomical observations. As the normal distribution took on a central role in statistics, it naturally began to appear in mixture contexts as well. Mixture modeling can be traced to Karl Pearson (1894), who attempted to fit a mixture of two normal distributions to biological data (the ratio of forehead-chin measurements, to be exact). Pearson's approach can be considered one of the earliest forms of mixture modeling."),"\n",i.createElement(t.p,null,"The broader acceptance of mixture modeling came with the formal introduction and analysis of the EM algorithm. Arthur Dempster, Nan Laird, and Donald Rubin introduced EM in 1977 and demonstrated how to use it for maximum likelihood estimation in incomplete-data problems â€” including, as a showcase, mixture-of-Gaussian models. Since then, GMMs have become a textbook example for illustrating the EM procedure."),"\n",i.createElement(t.h2,{id:"2-mathematical-foundations",style:{position:"relative"}},i.createElement(t.a,{href:"#2-mathematical-foundations","aria-label":"2 mathematical foundations permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2. Mathematical foundations"),"\n",i.createElement(t.p,null,"To fully grasp GMMs and how to estimate their parameters, it's useful to recall fundamental ideas from probability and statistics. We'll highlight those relevant to mixture modeling in general, and to Gaussian mixture models in particular."),"\n",i.createElement(t.h3,{id:"21-refresher-of-related-statistics-concepts",style:{position:"relative"}},i.createElement(t.a,{href:"#21-refresher-of-related-statistics-concepts","aria-label":"21 refresher of related statistics concepts permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.1 Refresher of related statistics concepts"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Probability density function (PDF):")," A continuous random variable ",i.createElement(r.A,{text:"\\( \\mathbf{x} \\in \\mathbb{R}^d \\)"})," has a PDF ",i.createElement(r.A,{text:"\\( p(\\mathbf{x}) \\)"})," if, for any region ",i.createElement(r.A,{text:"\\( R \\subset \\mathbb{R}^d \\)"}),","),"\n",i.createElement(r.A,{text:"\\[\n\\Pr(\\mathbf{x} \\in R) = \\int_{R} p(\\mathbf{x}) \\, d\\mathbf{x}.\n\\]"}),"\n",i.createElement(t.p,null,"In the mixture model context, ",i.createElement(r.A,{text:"\\( p(\\mathbf{x}) \\)"})," is decomposed into a sum of component PDFs."),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Multivariate Gaussian distribution:")," A random variable ",i.createElement(r.A,{text:"\\( \\mathbf{x} \\in \\mathbb{R}^d \\)"})," is normally distributed with mean vector ",i.createElement(r.A,{text:"\\(\\boldsymbol{\\mu}\\)"})," and covariance matrix ",i.createElement(r.A,{text:"\\(\\Sigma\\)"})," if"),"\n",i.createElement(r.A,{text:"\\[\np(\\mathbf{x} \\mid \\boldsymbol{\\mu}, \\Sigma) \\;=\\; \\frac{1}{\\sqrt{(2\\pi)^d \\det(\\Sigma)}} \\exp\\!\\Bigl(-\\tfrac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^\\top \\Sigma^{-1} (\\mathbf{x}-\\boldsymbol{\\mu})\\Bigr).\n\\]"}),"\n",i.createElement(t.p,null,"Gaussian mixture models make use of multiple such Gaussian distributions, each with potentially distinct ",i.createElement(r.A,{text:"\\(\\boldsymbol{\\mu}_k\\)"})," and ",i.createElement(r.A,{text:"\\(\\Sigma_k\\)"}),"."),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Log-likelihood:")," Given i.i.d. data ",i.createElement(r.A,{text:"\\(\\{\\mathbf{x}_1,\\ldots,\\mathbf{x}_n\\}\\)"}),", the likelihood of parameters ",i.createElement(r.A,{text:"\\(\\theta\\)"})," is"),"\n",i.createElement(r.A,{text:"\\[\nL(\\theta) = \\prod_{i=1}^n p(\\mathbf{x}_i \\mid \\theta),\n\\]"}),"\n",i.createElement(t.p,null,"and the log-likelihood is ",i.createElement(r.A,{text:"\\(\\log L(\\theta)\\)"}),". The mixture model log-likelihood typically involves ",i.createElement(r.A,{text:"\\(\\log\\bigl(\\sum_k \\pi_k\\,p(\\mathbf{x}_i \\mid \\theta_k)\\bigr)\\)"}),"."),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Expectation of a log-likelihood"),": In latent variable models, we often look at the expectation of the complete-data log-likelihood, computed under some distribution over the latent variables (in the GMM setting, the latent variable is the index of the component from which each point was drawn)."),"\n"),"\n"),"\n",i.createElement(t.h3,{id:"22-components-means-and-covariances-in-gmm",style:{position:"relative"}},i.createElement(t.a,{href:"#22-components-means-and-covariances-in-gmm","aria-label":"22 components means and covariances in gmm permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.2 Components, means, and covariances in GMM"),"\n",i.createElement(t.p,null,"In a GMM, each component ",i.createElement(r.A,{text:"\\(k\\)"})," is specified by:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"A mean vector ",i.createElement(r.A,{text:"\\( \\boldsymbol{\\mu}_k \\in \\mathbb{R}^d\\)"}),"."),"\n",i.createElement(t.li,null,"A covariance matrix ",i.createElement(r.A,{text:"\\( \\Sigma_k \\in \\mathbb{R}^{d \\times d}\\)"}),", which must be positive semi-definite (and typically assumed positive-definite in practice)."),"\n",i.createElement(t.li,null,"A mixture weight ",i.createElement(r.A,{text:"\\( \\pi_k \\ge 0\\)"})," such that ",i.createElement(r.A,{text:"\\(\\sum_{k=1}^K \\pi_k = 1\\)"}),"."),"\n"),"\n",i.createElement(t.p,null,"Thus, the GMM can be written as:"),"\n",i.createElement(r.A,{text:"\\[\np(\\mathbf{x}) \\;=\\; \\sum_{k=1}^K \\pi_k \\,\\mathcal{N}(\\mathbf{x}\\mid \\boldsymbol{\\mu}_k,\\Sigma_k).\n\\]"}),"\n",i.createElement(t.p,null,"This is the basis for modeling data with multiple subpopulations. It's easy to see that as ",i.createElement(r.A,{text:"\\(K\\to\\infty\\)"}),", a mixture of Gaussians can approximate a very large family of densities (though in practice we choose ",i.createElement(r.A,{text:"\\(K\\)"})," by model selection criteria or domain knowledge)."),"\n",i.createElement(t.h3,{id:"221-mixture-weights-and-log-likelihood",style:{position:"relative"}},i.createElement(t.a,{href:"#221-mixture-weights-and-log-likelihood","aria-label":"221 mixture weights and log likelihood permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.2.1 Mixture weights and log-likelihood"),"\n",i.createElement(t.p,null,"Given a dataset ",i.createElement(r.A,{text:"\\(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_n\\}\\)"}),", the log-likelihood of a GMM with parameters ",i.createElement(r.A,{text:"\\(\\{\\pi_k,\\boldsymbol{\\mu}_k,\\Sigma_k\\}_{k=1}^K\\)"})," is:"),"\n",i.createElement(r.A,{text:"\\[\n\\log L(\\{\\pi_k,\\boldsymbol{\\mu}_k,\\Sigma_k\\}) \n\\;=\\; \\sum_{i=1}^n \\log \\Bigl(\\sum_{k=1}^K \\pi_k\\, \\mathcal{N}(\\mathbf{x}_i \\mid \\boldsymbol{\\mu}_k,\\Sigma_k)\\Bigr).\n\\]"}),"\n",i.createElement(t.p,null,"Maximizing this log-likelihood w.r.t. ",i.createElement(r.A,{text:"\\(\\pi_k, \\boldsymbol{\\mu}_k,\\Sigma_k\\)"})," is not straightforward because of the logarithm of a sum. The standard approach is to use the EM algorithm, which provides an efficient iterative solution."),"\n",i.createElement(t.h3,{id:"222-convergence-criteria-in-mixture-models",style:{position:"relative"}},i.createElement(t.a,{href:"#222-convergence-criteria-in-mixture-models","aria-label":"222 convergence criteria in mixture models permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.2.2 Convergence criteria in mixture models"),"\n",i.createElement(t.p,null,'When one speaks of "convergence criteria," it can refer to:'),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Convergence of the EM algorithm")," to a local maximum of the log-likelihood (or more generally to a stationary point)."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Stopping criteria")," used in practice, such as the relative change in log-likelihood between iterations, or a maximum number of iterations, or small changes in parameters, or the difference in posterior membership probabilities across iterations."),"\n"),"\n",i.createElement(t.p,null,"In many mixture modeling contexts, we typically define a tolerance ",i.createElement(r.A,{text:"\\(\\varepsilon\\)"})," such that we stop the iterations once"),"\n",i.createElement(r.A,{text:"\\[\n\\frac{\\bigl|\\log L(\\theta^{(t+1)}) - \\log L(\\theta^{(t)})\\bigr|}{1 + |\\log L(\\theta^{(t)})|} \\;<\\; \\varepsilon,\n\\]"}),"\n",i.createElement(t.p,null,"or a similar condition based on parameter changes or posterior membership changes."),"\n",i.createElement(t.h2,{id:"3-parameter-estimation-using-the-em-algorithm",style:{position:"relative"}},i.createElement(t.a,{href:"#3-parameter-estimation-using-the-em-algorithm","aria-label":"3 parameter estimation using the em algorithm permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3. Parameter estimation using the EM algorithm"),"\n",i.createElement(t.p,null,"The ",i.createElement(t.strong,null,"expectationâ€“maximization (EM)")," algorithm is the canonical method for finding maximum likelihood or maximum a posteriori (MAP) estimates in latent variable models such as GMMs. EM iterates two main steps until convergence: an E-step (where we compute or update the distribution over latent variables) and an M-step (where we maximize with respect to parameters given the updated latent distribution)."),"\n",i.createElement(t.h3,{id:"31-overview-of-the-em-algorithm-relation-to-k-means",style:{position:"relative"}},i.createElement(t.a,{href:"#31-overview-of-the-em-algorithm-relation-to-k-means","aria-label":"31 overview of the em algorithm relation to k means permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.1 Overview of the EM algorithm, relation to k-means"),"\n",i.createElement(t.p,null,"At a high level:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"E-step:")," Estimate the probability that each data point ",i.createElement(r.A,{text:"\\(\\mathbf{x}_i\\)"})," belongs to each component ",i.createElement(r.A,{text:"\\(k\\)"}),", given the current parameters. In GMM terms, this is the posterior probability of the latent variable ",i.createElement(r.A,{text:"\\(z_i = k\\)"}),":","\n",i.createElement(r.A,{text:"\\[\n\\gamma_{ik} \\;=\\; P\\bigl(z_i = k \\mid \\mathbf{x}_i, \\{\\pi_k,\\boldsymbol{\\mu}_k,\\Sigma_k\\}\\bigr).\n\\]"}),"\n","Concretely:","\n",i.createElement(r.A,{text:"\\[\n\\gamma_{ik} \\;=\\; \\frac{\\,\\pi_k\\,\\mathcal{N}(\\mathbf{x}_i \\mid \\boldsymbol{\\mu}_k,\\Sigma_k)\\,}\n{\\sum_{j=1}^K \\,\\pi_j\\, \\mathcal{N}(\\mathbf{x}_i \\mid \\boldsymbol{\\mu}_j,\\Sigma_j)\\,}.\n\\]"}),"\n"),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"M-step:")," Given those posterior probabilities (or membership responsibilities), update the parameters ",i.createElement(r.A,{text:"\\(\\{\\pi_k,\\boldsymbol{\\mu}_k,\\Sigma_k\\}\\)"})," to maximize the expected complete-data log-likelihood. This yields:","\n",i.createElement(r.A,{text:"\\[\n\\pi_k \\;=\\; \\frac{1}{n}\\sum_{i=1}^n \\gamma_{ik}, \\quad\n\\boldsymbol{\\mu}_k \\;=\\; \\frac{\\sum_{i=1}^n \\gamma_{ik}\\,\\mathbf{x}_i}{\\sum_{i=1}^n \\gamma_{ik}}, \\quad\n\\Sigma_k \\;=\\; \\frac{\\sum_{i=1}^n \\gamma_{ik} \\; (\\mathbf{x}_i - \\boldsymbol{\\mu}_k)(\\mathbf{x}_i - \\boldsymbol{\\mu}_k)^\\top}{\\sum_{i=1}^n \\gamma_{ik}}.\n\\]"}),"\n"),"\n"),"\n",i.createElement(t.p,null,"This procedure is reminiscent of ",i.createElement(t.strong,null,i.createElement(r.A,{text:"\\(k\\)"}),"-means clustering")," in the sense that ",i.createElement(r.A,{text:"\\(k\\)"}),"-means has two analogous steps: (1) assign each point to its nearest centroid; (2) recompute centroids as the mean of assigned points. However, in ",i.createElement(r.A,{text:"\\(k\\)"}),"-means each point has a hard assignment to a cluster, while in GMM each point has a soft assignment (the ",i.createElement(r.A,{text:"\\(\\gamma_{ik}\\)"})," probabilities). This tends to make GMM more flexible, though the computations are more expensive."),"\n",i.createElement(t.h3,{id:"32-the-expectation-step",style:{position:"relative"}},i.createElement(t.a,{href:"#32-the-expectation-step","aria-label":"32 the expectation step permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.2 The expectation step"),"\n",i.createElement(t.p,null,"In more detail, let ",i.createElement(r.A,{text:"\\(\\theta^{(t)}\\)"})," denote the current parameter estimates at iteration ",i.createElement(r.A,{text:"\\(t\\)"}),". The E-step sets:"),"\n",i.createElement(r.A,{text:"\\[\n\\gamma_{ik}^{(t)} \\;=\\; \\frac{\\;\\pi_k^{(t)}\\,\\mathcal{N}(\\mathbf{x}_i \\mid \\boldsymbol{\\mu}_k^{(t)}, \\Sigma_k^{(t)})\\;}\n{\\sum_{j=1}^K \\pi_j^{(t)}\\,\\mathcal{N}(\\mathbf{x}_i \\mid \\boldsymbol{\\mu}_j^{(t)}, \\Sigma_j^{(t)})}.\n\\]"}),"\n",i.createElement(t.p,null,"These posterior membership probabilities are sometimes called ",i.createElement(t.strong,null,"responsibilities"),', because they quantify the "responsibility" that each component ',i.createElement(r.A,{text:"\\(k\\)"})," takes for generating the data point ",i.createElement(r.A,{text:"\\(\\mathbf{x}_i\\)"}),"."),"\n",i.createElement(t.p,null,"Conceptually:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"If ",i.createElement(r.A,{text:"\\(\\mathbf{x}_i\\)"})," is closer (in Mahalanobis distance) to ",i.createElement(r.A,{text:"\\(\\boldsymbol{\\mu}_k\\)"})," and if ",i.createElement(r.A,{text:"\\(\\Sigma_k\\)"})," is not too large, then ",i.createElement(r.A,{text:"\\(\\gamma_{ik}\\)"})," will be relatively large."),"\n",i.createElement(t.li,null,"If ",i.createElement(r.A,{text:"\\(\\mathbf{x}_i\\)"})," is far from that component's center, or if the mixture weight ",i.createElement(r.A,{text:"\\(\\pi_k\\)"})," is small, ",i.createElement(r.A,{text:"\\(\\gamma_{ik}\\)"})," becomes smaller."),"\n"),"\n",i.createElement(t.h3,{id:"33-the-maximization-step",style:{position:"relative"}},i.createElement(t.a,{href:"#33-the-maximization-step","aria-label":"33 the maximization step permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.3 The maximization step"),"\n",i.createElement(t.p,null,'Next, the M-step uses these responsibilities to re-estimate the parameters. Consider the "expected complete-data log-likelihood" (sometimes called the Q-function in EM discussions). The solution for the new parameters ',i.createElement(r.A,{text:"\\(\\theta^{(t+1)}\\)"})," is found by setting partial derivatives to zero, yielding:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Mixing coefficients"),":"),"\n",i.createElement(r.A,{text:"\\[\n\\pi_k^{(t+1)} \n= \\frac{1}{n}\\,\\sum_{i=1}^n \\gamma_{ik}^{(t)}.\n\\]"}),"\n",i.createElement(t.p,null,"This is effectively the fraction of data points for which component ",i.createElement(r.A,{text:"\\(k\\)"})," is responsible."),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Means"),":"),"\n",i.createElement(r.A,{text:"\\[\n\\boldsymbol{\\mu}_k^{(t+1)} \n= \\frac{\\sum_{i=1}^n \\gamma_{ik}^{(t)} \\,\\mathbf{x}_i}{\\sum_{i=1}^n \\gamma_{ik}^{(t)}}.\n\\]"}),"\n",i.createElement(t.p,null,"This is the weighted average of the data points, with weights ",i.createElement(r.A,{text:"\\(\\gamma_{ik}^{(t)}\\)"}),"."),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Covariances"),":"),"\n",i.createElement(r.A,{text:"\\[\n\\Sigma_k^{(t+1)} \n= \\frac{\\sum_{i=1}^n \\gamma_{ik}^{(t)} \\;(\\mathbf{x}_i - \\boldsymbol{\\mu}_k^{(t+1)})(\\mathbf{x}_i - \\boldsymbol{\\mu}_k^{(t+1)})^\\top}{\\sum_{i=1}^n \\gamma_{ik}^{(t)}}.\n\\]"}),"\n",i.createElement(t.p,null,"This is the weighted sample covariance of the data assigned to component ",i.createElement(r.A,{text:"\\(k\\)"}),"."),"\n"),"\n"),"\n",i.createElement(t.h3,{id:"34-convergence-and-stopping-conditions",style:{position:"relative"}},i.createElement(t.a,{href:"#34-convergence-and-stopping-conditions","aria-label":"34 convergence and stopping conditions permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.4 Convergence and stopping conditions"),"\n",i.createElement(t.p,null,"Each EM iteration is guaranteed not to decrease the observed-data log-likelihood, but the algorithm can get stuck in a local maximum or a saddle point. In practice, we repeat E- and M-steps until:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"The improvement in log-likelihood is below some threshold, or"),"\n",i.createElement(t.li,null,"A maximum number of iterations is reached, or"),"\n",i.createElement(t.li,null,"The parameter estimates are no longer changing significantly."),"\n"),"\n",i.createElement(t.p,null,"Because the log-likelihood for mixture models can have many local maxima, running EM multiple times from different randomized initial parameters is common. Then one chooses the best solution (highest log-likelihood) among the restarts, or uses a suitable model-selection approach."),"\n",i.createElement(t.h3,{id:"35-numerical-stability-considerations",style:{position:"relative"}},i.createElement(t.a,{href:"#35-numerical-stability-considerations","aria-label":"35 numerical stability considerations permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.5 Numerical stability considerations"),"\n",i.createElement(t.p,null,"When implementing GMMs and EM in numerical code, one must beware of potential instabilities:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Log-sum-exp")," computations: The expression ",i.createElement(r.A,{text:"\\(\\log \\sum_k \\pi_k \\mathcal{N}(\\mathbf{x}\\mid\\boldsymbol{\\mu}_k,\\Sigma_k)\\)"}),' can lead to overflow or underflow for large or small values. A stable approach is to use the "log-sum-exp trick," which involves factoring out the maximum exponent inside the sum.'),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Covariance singularities"),": It's possible for the EM algorithm to produce a near-singular covariance matrix if a component collapses onto a single data point. Various regularization strategies exist (e.g., adding a small diagonal term ",i.createElement(r.A,{text:"\\(\\epsilon I\\)"})," to each ",i.createElement(r.A,{text:"\\(\\Sigma_k\\)"}),")."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Empty or tiny clusters"),": If ",i.createElement(r.A,{text:"\\(\\pi_k\\)"})," becomes extremely small, floating-point precision may cause that component's parameters to degrade. Some frameworks remove or merge such tiny clusters into others."),"\n"),"\n",i.createElement(t.h2,{id:"4-model-selection-and-evaluation",style:{position:"relative"}},i.createElement(t.a,{href:"#4-model-selection-and-evaluation","aria-label":"4 model selection and evaluation permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4. Model selection and evaluation"),"\n",i.createElement(t.p,null,"Determining an appropriate number of mixture components ",i.createElement(r.A,{text:"\\(K\\)"})," and assessing how well a fitted GMM describes the data can be just as important as the parameter estimation itself."),"\n",i.createElement(t.h3,{id:"41-choosing-the-number-of-components",style:{position:"relative"}},i.createElement(t.a,{href:"#41-choosing-the-number-of-components","aria-label":"41 choosing the number of components permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.1 Choosing the number of components"),"\n",i.createElement(t.p,null,"A fundamental question in mixture modeling is how many components to use. Approaches include:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Domain insight"),": In some applications, prior knowledge might indicate how many subpopulations are expected."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Heuristics"),": One might train GMMs for a range of ",i.createElement(r.A,{text:"\\(K\\)"})," values and visually inspect solutions or measure cluster compactness, out-of-sample likelihood, etc."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Automated model selection criteria"),": The Akaike information criterion (AIC) or Bayesian information criterion (BIC) are widely used to balance model fit with complexity."),"\n"),"\n",i.createElement(t.h3,{id:"42-aic-bic-and-other-criteria",style:{position:"relative"}},i.createElement(t.a,{href:"#42-aic-bic-and-other-criteria","aria-label":"42 aic bic and other criteria permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.2 AIC, BIC, and other criteria"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"AIC")," is given by"),"\n",i.createElement(r.A,{text:"\\[\n\\text{AIC} = 2p - 2\\log L(\\hat{\\theta}),\n\\]"}),"\n",i.createElement(t.p,null,"where ",i.createElement(r.A,{text:"\\(p\\)"})," is the number of free parameters in the model, and ",i.createElement(r.A,{text:"\\(\\hat{\\theta}\\)"})," is the parameter estimate that maximizes the likelihood."),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"BIC")," is given by"),"\n",i.createElement(r.A,{text:"\\[\n\\text{BIC} = p\\,\\log n - 2\\log L(\\hat{\\theta}),\n\\]"}),"\n",i.createElement(t.p,null,"with ",i.createElement(r.A,{text:"\\(n\\)"})," the number of data points."),"\n"),"\n"),"\n",i.createElement(t.p,null,"BIC tends to penalize complex models (large ",i.createElement(r.A,{text:"\\(p\\)"}),") more harshly than AIC, often favoring simpler models if the sample size is not large. In practice, BIC is popular for selecting ",i.createElement(r.A,{text:"\\(K\\)"})," for GMMs because it often yields good real-world performance and has a direct connection to a Bayesian viewpoint (approximation to the marginal likelihood)."),"\n",i.createElement(t.h3,{id:"43-cross-validation-for-mixture-models",style:{position:"relative"}},i.createElement(t.a,{href:"#43-cross-validation-for-mixture-models","aria-label":"43 cross validation for mixture models permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.3 Cross-validation for mixture models"),"\n",i.createElement(t.p,null,"Another approach for model selection is ",i.createElement(t.strong,null,"cross-validation (CV)"),". The idea is to:"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,"Partition the data into training and validation sets."),"\n",i.createElement(t.li,null,"Fit a GMM with a certain ",i.createElement(r.A,{text:"\\(K\\)"})," on the training set."),"\n",i.createElement(t.li,null,"Evaluate the log-likelihood of the hold-out set under the fitted model."),"\n",i.createElement(t.li,null,"Repeat for multiple folds and multiple choices of ",i.createElement(r.A,{text:"\\(K\\)"}),"."),"\n",i.createElement(t.li,null,"Choose ",i.createElement(r.A,{text:"\\(K\\)"})," that yields the best average validation log-likelihood (or some similar metric)."),"\n"),"\n",i.createElement(t.p,null,"Although more computationally expensive than single-shot metrics like AIC/BIC, cross-validation is often more robust in evaluating predictive performance."),"\n",i.createElement(t.h3,{id:"44-initialization-strategies-and-random-restarts",style:{position:"relative"}},i.createElement(t.a,{href:"#44-initialization-strategies-and-random-restarts","aria-label":"44 initialization strategies and random restarts permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.4 Initialization strategies and random restarts"),"\n",i.createElement(t.p,null,"GMM parameter estimation with EM is sensitive to initialization. Common strategies:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,i.createElement(r.A,{text:"\\(k\\)"}),"-means-based initialization"),": Start by running ",i.createElement(r.A,{text:"\\(k\\)"}),"-means clustering to get cluster assignments, then set ",i.createElement(r.A,{text:"\\(\\boldsymbol{\\mu}_k\\)"})," to the cluster centroids, ",i.createElement(r.A,{text:"\\(\\Sigma_k\\)"})," to the within-cluster covariance, and ",i.createElement(r.A,{text:"\\(\\pi_k\\)"})," to cluster sizes / ",i.createElement(r.A,{text:"\\(n\\)"}),"."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Random initialization"),": Randomly choose ",i.createElement(r.A,{text:"\\(\\boldsymbol{\\mu}_k\\)"})," from the data, maybe add random noise."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Hierarchical or distance-based")," methods: If the dimension is manageable, we can do hierarchical clustering first, then pick centers to initialize the GMM."),"\n"),"\n",i.createElement(t.p,null,"Because of local maxima, it's common to run EM multiple times with different initializations, picking the solution with the best final log-likelihood or best BIC."),"\n",i.createElement(t.h3,{id:"45-avoiding-overfitting-and-underfitting",style:{position:"relative"}},i.createElement(t.a,{href:"#45-avoiding-overfitting-and-underfitting","aria-label":"45 avoiding overfitting and underfitting permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.5 Avoiding overfitting and underfitting"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Overfitting")," can happen when ",i.createElement(r.A,{text:"\\(K\\)"})," is too large. The model can place components on small sets of points or even single points, artificially boosting the likelihood but not generalizing well."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Underfitting")," is the opposite scenario: too few components hamper the model's representational power, leading to systematically poor fits."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Regularization"),": You can constrain or regularize the covariance matrices, for instance, requiring them to be diagonal, or adding a penalty term. This mitigates overfitting in high-dimensional spaces."),"\n"),"\n",i.createElement(t.h2,{id:"5-implementation",style:{position:"relative"}},i.createElement(t.a,{href:"#5-implementation","aria-label":"5 implementation permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5. Implementation"),"\n",i.createElement(t.p,null,"To illustrate a straightforward Python-based implementation, we can rely on libraries like NumPy for data manipulation and scikit-learn for GMM or we can craft a minimal EM from scratch. Below is a very simplified version of an EM for GMM, written in Python-like pseudocode (using the scikit-learn style as an example). Note that this is ",i.createElement(t.strong,null,"not")," production-level code. In real usage, it's better to rely on well-tested implementations such as ",i.createElement(l.A,null,"GaussianMixture")," in scikit-learn."),"\n",i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">&lt;Code text={`\nimport numpy as np\n\ndef gaussian_pdf(x, mu, Sigma):\n    d = len(x)\n    det_Sigma = np.linalg.det(Sigma)\n    inv_Sigma = np.linalg.inv(Sigma)\n    norm_const = 1.0 / np.sqrt((2*np.pi)**d * det_Sigma)\n    diff = x - mu\n    return norm_const * np.exp(-0.5 * diff.T @ inv_Sigma @ diff)\n\nclass GMM_EM:\n    def __init__(self, n_components, max_iter=100, tol=1e-4):\n        self.n_components = n_components\n        self.max_iter = max_iter\n        self.tol = tol\n\n    def fit(self, X):\n        n, d = X.shape\n\n        # 1) Initialize mixture weights, means, and covariances\n        # Simple random init for demonstration\n        np.random.seed(42)\n        shuffle_idx = np.random.permutation(n)\n        self.means_ = X[shuffle_idx[:self.n_components]]  # pick random points as means\n        self.weights_ = np.ones(self.n_components) / self.n_components\n        self.covs_ = np.array([np.cov(X, rowvar=False) for _ in range(self.n_components)])\n        \n        log_likelihood_old = None\n\n        for iteration in range(self.max_iter):\n            # E-step: compute responsibilities\n            resp = np.zeros((n, self.n_components))\n            for i in range(n):\n                for k in range(self.n_components):\n                    resp[i, k] = self.weights_[k] * gaussian_pdf(X[i], self.means_[k], self.covs_[k])\n                # Normalize row i\n                resp[i, :] /= np.sum(resp[i, :])\n\n            # M-step: update weights, means, and covariances\n            Nk = np.sum(resp, axis=0)  # sum responsibilities per component\n\n            # Update weights\n            self.weights_ = Nk / n\n\n            # Update means\n            self.means_ = np.zeros((self.n_components, d))\n            for k in range(self.n_components):\n                for i in range(n):\n                    self.means_[k] += resp[i, k] * X[i]\n                self.means_[k] /= Nk[k]\n\n            # Update covariances\n            self.covs_ = np.zeros((self.n_components, d, d))\n            for k in range(self.n_components):\n                for i in range(n):\n                    diff = X[i] - self.means_[k]\n                    self.covs_[k] += resp[i, k] * np.outer(diff, diff)\n                self.covs_[k] /= Nk[k]\n\n            # Check convergence via log-likelihood\n            log_likelihood = 0\n            for i in range(n):\n                val = 0\n                for k in range(self.n_components):\n                    val += self.weights_[k] * gaussian_pdf(X[i], self.means_[k], self.covs_[k])\n                log_likelihood += np.log(val + 1e-15)  # add small constant to avoid log(0)\n\n            if log_likelihood_old is not None:\n                if abs(log_likelihood - log_likelihood_old) &lt; self.tol:\n                    break\n            log_likelihood_old = log_likelihood\n\n    def predict_proba(self, X):\n        n, d = X.shape\n        resp = np.zeros((n, self.n_components))\n        for i in range(n):\n            for k in range(self.n_components):\n                resp[i, k] = self.weights_[k] * gaussian_pdf(X[i], self.means_[k], self.covs_[k])\n            resp[i, :] /= np.sum(resp[i, :])\n        return resp\n\n    def predict(self, X):\n        resp = self.predict_proba(X)\n        return np.argmax(resp, axis=1)\n`}/></code></pre></div>'}}),"\n",i.createElement(t.h3,{id:"example-usage",style:{position:"relative"}},i.createElement(t.a,{href:"#example-usage","aria-label":"example usage permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Example usage"),"\n",i.createElement(t.p,null,"To use the above class:"),"\n",i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">&lt;Code text={`\nimport numpy as np\n\n# Generate synthetic data from a mixture of 2 Gaussians\nnp.random.seed(0)\nn = 300\nmean1, mean2 = np.array([0, 0]), np.array([5, 5])\ncov1 = np.eye(2)\ncov2 = np.eye(2)\n\nX1 = np.random.multivariate_normal(mean1, cov1, n//2)\nX2 = np.random.multivariate_normal(mean2, cov2, n//2)\nX = np.vstack([X1, X2])\n\n# Fit GMM\nmodel = GMM_EM(n_components=2, max_iter=100)\nmodel.fit(X)\n\n# Inspect fitted parameters\nprint("Fitted mixture weights:", model.weights_)\nprint("Fitted means:", model.means_)\nprint("Fitted covariances:", model.covs_)\n\n# Predict membership\nlabels = model.predict(X)\nprint("Predicted cluster labels (first 10):", labels[:10])\n`}/></code></pre></div>'}}),"\n",i.createElement(t.p,null,"In practice, scikit-learn's ",i.createElement(l.A,null,"GaussianMixture")," is more robust and efficient, and includes advanced initialization, multiple covariance parameterization options, and built-in methods for BIC and AIC."),"\n",i.createElement(t.h2,{id:"6-advanced-topics",style:{position:"relative"}},i.createElement(t.a,{href:"#6-advanced-topics","aria-label":"6 advanced topics permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6. Advanced topics"),"\n",i.createElement(t.p,null,'While the "vanilla" GMM is widely used, many advanced variants and related ideas have been explored in recent research. We touch on some particularly interesting directions here.'),"\n",i.createElement(t.h3,{id:"61-variational-inference-for-gmms",style:{position:"relative"}},i.createElement(t.a,{href:"#61-variational-inference-for-gmms","aria-label":"61 variational inference for gmms permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.1 Variational inference for GMMs"),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Variational inference (VI)")," is an alternative to EM for approximate posterior inference in Bayesian models. In a standard GMM context, EM yields the maximum likelihood (or MAP, if priors are introduced) parameters. If we want full posterior distributions over means, covariances, and mixing weights, VI methods can be employed to approximate the intractable posterior with a factorized distribution ",i.createElement(r.A,{text:"\\(q(\\theta)\\)"}),". Instead of the standard E and M steps, we optimize a variational lower bound. The result is often called a ",i.createElement(t.strong,null,"Variational Bayesian Gaussian mixture"),"."),"\n",i.createElement(t.p,null,"Key benefits of VI-based GMMs:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"We get a distribution over parameters, not just point estimates, which can capture parameter uncertainty."),"\n",i.createElement(t.li,null,"Automatic regularization from Bayesian priors can mitigate overfitting."),"\n"),"\n",i.createElement(t.p,null,"Although the details are more complex than classical EM, implementations can be found in popular frameworks, as VI has gained traction for large-scale Bayesian problems."),"\n",i.createElement(t.h3,{id:"62-bayesian-gaussian-mixture-models",style:{position:"relative"}},i.createElement(t.a,{href:"#62-bayesian-gaussian-mixture-models","aria-label":"62 bayesian gaussian mixture models permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.2 Bayesian Gaussian mixture models"),"\n",i.createElement(t.p,null,"A ",i.createElement(t.strong,null,"Bayesian Gaussian mixture")," typically places prior distributions on the mixture weights ",i.createElement(r.A,{text:"\\(\\boldsymbol{\\pi}\\)"})," (e.g., a Dirichlet prior) and on each component's parameters ",i.createElement(r.A,{text:"\\((\\boldsymbol{\\mu}_k, \\Sigma_k)\\)"}),". One might use a Normalâ€“Inverse-Wishart prior or Normalâ€“Inverse-Gamma prior, for example. The posterior distribution is then a complicated function. Markov Chain Monte Carlo (MCMC) methods or variational approximations are used to sample or approximate the posterior."),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Conjugate priors"),": If the prior is conjugate to the likelihood, some steps in MCMC or VI are analytically simpler."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Practical usage"),': Bayesian GMM can automatically handle the complexity vs. fit trade-off by letting the posterior concentrate more mass on fewer components if the data do not support more. This can help with "automatic" model selection.'),"\n"),"\n",i.createElement(t.h3,{id:"63-infinite-mixture-models-and-dirichlet-processes",style:{position:"relative"}},i.createElement(t.a,{href:"#63-infinite-mixture-models-and-dirichlet-processes","aria-label":"63 infinite mixture models and dirichlet processes permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.3 Infinite mixture models and Dirichlet processes"),"\n",i.createElement(t.p,null,'A "finite" GMM must specify a fixed ',i.createElement(r.A,{text:"\\(K\\)"}),". But one can define an ",i.createElement(t.strong,null,"infinite mixture model")," using a ",i.createElement(t.strong,null,"Dirichlet process Gaussian mixture")," (DP-GMM). Here, the DP is a prior on the mixing measures, allowing for a random number of components. In practice, algorithms like ",i.createElement(t.strong,null,"Gibbs sampling")," or ",i.createElement(t.strong,null,"Variational Inference"),' for DP-GMMs can yield an effectively inferred number of active components, providing a solution to the "How many clusters?" question in an elegant Bayesian framework.'),"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Dirichlet process mixture")," models have become a major field of research (e.g., Neal, 2000, and subsequent papers in top-tier conferences). They're quite popular in nonparametric Bayesian approaches to clustering."),"\n",i.createElement(t.h3,{id:"64-mixture-models-with-non-gaussian-components",style:{position:"relative"}},i.createElement(t.a,{href:"#64-mixture-models-with-non-gaussian-components","aria-label":"64 mixture models with non gaussian components permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.4 Mixture models with non-Gaussian components"),"\n",i.createElement(t.p,null,"Though Gaussian mixtures are the most widely studied, the mixture modeling technique applies to any family of distributions:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Mixture of Bernoulli distributions")," for binary data."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Mixture of Poisson distributions")," for count data."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Mixture of factor analyzers")," (each component is a factor analyzer)."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Mixture of exponentials"),", ",i.createElement(t.strong,null,"gamma")," distributions, or even ",i.createElement(t.strong,null,"mixture of t-distributions")," for heavy-tailed data."),"\n"),"\n",i.createElement(t.p,null,"In each case, the model is typically estimated by an EM-like procedure (or some other method if the log-likelihood is suitable). The mathematics is analogous, though the formula for each component's PDF changes."),"\n",i.createElement(t.h3,{id:"65-mixture-of-bernoulli-distributions",style:{position:"relative"}},i.createElement(t.a,{href:"#65-mixture-of-bernoulli-distributions","aria-label":"65 mixture of bernoulli distributions permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.5 Mixture of Bernoulli distributions"),"\n",i.createElement(t.p,null,'A "mixture of Bernoulli distributions" is relevant in binary data scenarios. Each dimension is a Bernoulli random variable with parameter ',i.createElement(r.A,{text:"\\(\\theta_{k,j}\\)"})," in component ",i.createElement(r.A,{text:"\\(k\\)"}),". The mixture log-likelihood sums up ",i.createElement(r.A,{text:"\\(\\sum_{j=1}^d [ x_{i,j} \\log \\theta_{k,j} + (1-x_{i,j}) \\log (1-\\theta_{k,j}) ]\\)"}),". This approach is used in applications such as modeling presence/absence features or bag-of-words with binary indicators."),"\n",i.createElement(t.h3,{id:"66-em-for-bayesian-linear-regression",style:{position:"relative"}},i.createElement(t.a,{href:"#66-em-for-bayesian-linear-regression","aria-label":"66 em for bayesian linear regression permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.6 EM for Bayesian linear regression"),"\n",i.createElement(t.p,null,"Interestingly, the EM algorithm has broad usage beyond just mixture modeling. For instance, it can be applied to certain formulations of Bayesian linear regression where some parameters are integrated out or considered latent. The unstructured text in the prompt references that EM can solve multiple linear regression as well (Kwon and gang, 2020). However, in the standard GMM context, the usage of EM is more direct, focusing on the mixture membership as latent variables."),"\n",i.createElement(t.h3,{id:"67-handling-missing-data-and-outliers",style:{position:"relative"}},i.createElement(t.a,{href:"#67-handling-missing-data-and-outliers","aria-label":"67 handling missing data and outliers permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.7 Handling missing data and outliers"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Missing data"),": If some entries of ",i.createElement(r.A,{text:"\\(\\mathbf{x}_i\\)"})," are missing, EM can be adapted to marginalize them out. This is consistent with the original vision of EM: handle incomplete data by treating missing values as latent variables. The E-step then uses the conditional distribution of missing values given the observed part to fill in or weight the likelihood."),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Outliers"),": GMM performance can degrade if outliers significantly shift the means and inflate the covariances. Various robust alternatives exist:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"Heavy-tailed mixtures (e.g., a mixture of Student-t distributions)."),"\n",i.createElement(t.li,null,"Regularization or constraints on covariance estimates."),"\n",i.createElement(t.li,null,"Bayesian approaches that place robust priors on parameters, or outlier detection prior to GMM fitting."),"\n"),"\n"),"\n"),"\n",i.createElement(n,{alt:"Illustration of Gaussian mixture components",path:"",caption:"A hypothetical 2D dataset fitted by a GMM with three elliptical Gaussian components. Each ellipse is a contour of one component's covariance.",zoom:"false"}),"\n",i.createElement(t.hr),"\n",i.createElement(t.p,null,"Below, we present a thorough elaboration on each major concept introduced, ensuring that even advanced readers can deepen their understanding of the intricacies of GMMs. The next sections will dive deeper into theoretical aspects, additional computational strategies (like partial E and M steps), and more specialized references."),"\n",i.createElement(t.h2,{id:"7-further-theoretical-perspectives-extended-discussion",style:{position:"relative"}},i.createElement(t.a,{href:"#7-further-theoretical-perspectives-extended-discussion","aria-label":"7 further theoretical perspectives extended discussion permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7. Further theoretical perspectives (extended discussion)"),"\n",i.createElement(t.h3,{id:"71-the-complete-data-likelihood-and-the-q-function",style:{position:"relative"}},i.createElement(t.a,{href:"#71-the-complete-data-likelihood-and-the-q-function","aria-label":"71 the complete data likelihood and the q function permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.1 The complete-data likelihood and the Q-function"),"\n",i.createElement(t.p,null,"The central quantity behind the EM algorithm is the ",i.createElement(t.strong,null,"complete-data likelihood"),", which treats the cluster memberships ",i.createElement(r.A,{text:"\\(z_i\\in\\{1,\\dots,K\\}\\)"})," as observed. If we define indicator variables"),"\n",i.createElement(r.A,{text:"\\[\nr_{ik} \\;=\\; \n\\begin{cases}\n1 & \\text{if } z_i = k,\\\\\n0 & \\text{otherwise},\n\\end{cases}\n\\]"}),"\n",i.createElement(t.p,null,"the complete-data log-likelihood for GMM is:"),"\n",i.createElement(r.A,{text:"\\[\n\\log p(\\mathbf{X}, \\mathbf{z} \\mid \\theta)\n\\;=\\;\\sum_{i=1}^n \\sum_{k=1}^K r_{ik} \\bigl[\\log \\pi_k + \\log \\mathcal{N}(\\mathbf{x}_i \\mid \\boldsymbol{\\mu}_k, \\Sigma_k)\\bigr].\n\\]"}),"\n",i.createElement(t.p,null,"The Q-function is the expectation of this complete-data log-likelihood w.r.t. the conditional distribution of ",i.createElement(r.A,{text:"\\(\\mathbf{z}\\)"})," given the current parameter estimates ",i.createElement(r.A,{text:"\\(\\theta^{(t)}\\)"}),":"),"\n",i.createElement(r.A,{text:"\\[\nQ(\\theta \\mid \\theta^{(t)}) \\;=\\; \n\\mathbb{E}_{\\mathbf{z}\\mid\\mathbf{X},\\theta^{(t)}} \\bigl[\\log p(\\mathbf{X}, \\mathbf{z} \\mid \\theta)\\bigr].\n\\]"}),"\n",i.createElement(t.p,null,"Maximizing ",i.createElement(r.A,{text:"\\(Q(\\theta \\mid \\theta^{(t)})\\)"})," in the M-step leads precisely to the formulas described earlier."),"\n",i.createElement(t.h3,{id:"72-monotonicity-and-convergence-proofs",style:{position:"relative"}},i.createElement(t.a,{href:"#72-monotonicity-and-convergence-proofs","aria-label":"72 monotonicity and convergence proofs permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.2 Monotonicity and convergence proofs"),"\n",i.createElement(t.p,null,"It can be shown that each EM iteration will not decrease the observed-data log-likelihood:"),"\n",i.createElement(r.A,{text:"\\(\n\\log L(\\theta^{(t+1)}) \\;\\ge\\; \\log L(\\theta^{(t)}).\n\\)"}),"\n",i.createElement(t.p,null,"C. F. Jeff Wu's 1983 proof established the convergence properties outside of the exponential family setting as well. However, the iteration can converge to local maxima or saddle points. Deterministic annealing or random restarts are common strategies to mitigate this issue."),"\n",i.createElement(t.h3,{id:"73-coordinate-ascent-viewpoint",style:{position:"relative"}},i.createElement(t.a,{href:"#73-coordinate-ascent-viewpoint","aria-label":"73 coordinate ascent viewpoint permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.3 Coordinate ascent viewpoint"),"\n",i.createElement(t.p,null,"EM can be interpreted as a two-block coordinate ascent on a certain objective function (the ",i.createElement(t.strong,null,"evidence lower bound"),", or ELBO), though it's a special case of the more general ",i.createElement(t.strong,null,"majorizationâ€“minimization (MM)")," algorithm. This viewpoint clarifies how partial or incremental updates can still improve or maintain a lower bound on the log-likelihood."),"\n",i.createElement(t.h2,{id:"8-broader-research-context-and-advanced-references",style:{position:"relative"}},i.createElement(t.a,{href:"#8-broader-research-context-and-advanced-references","aria-label":"8 broader research context and advanced references permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8. Broader research context and advanced references"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Robust mixtures"),': Various works (e.g., "Robust Clustering Methods" in JMLR, 2018) adapt GMMs to be robust to outliers by employing heavier-tailed distributions or trimming strategies.'),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"High-dimensional data"),": Methods that impose structure on ",i.createElement(r.A,{text:"\\(\\Sigma_k\\)"}),' (like factor analysis, diagonal covariance, or shared covariance) help GMMs scale to high-dimensional data. In this domain, see "Parsimonious Gaussian mixture models" by Celeux and Govaert, Journal of Classification (1995).'),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Online/Incremental EM"),": For streaming data, one can use incremental or online versions of the EM algorithm that update the parameters in small batches or single points at a time. This is especially relevant for big data."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Parallelization"),': To handle large datasets, parallel and distributed variants of EM for GMM have been proposed (for instance, "Accelerating Expectationâ€“Maximization with Frequent Updates" in Cluster 2012).'),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Statistical theory"),": The asymptotic properties of EM estimates in mixture models, such as consistency and rate of convergence, can be subtle. Researchers have studied the identifiability conditions under which mixture parameters can be uniquely identified."),"\n"),"\n",i.createElement(t.h2,{id:"9-practical-tips-and-pitfalls",style:{position:"relative"}},i.createElement(t.a,{href:"#9-practical-tips-and-pitfalls","aria-label":"9 practical tips and pitfalls permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9. Practical tips and pitfalls"),"\n",i.createElement(t.ol,null,"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Cluster degeneracy"),": If you see that one or more covariance matrices are collapsing (becoming singular) during training, you might fix it by:"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,"Setting a lower bound on the determinant, effectively ",i.createElement(r.A,{text:"\\(\\Sigma_k \\leftarrow \\Sigma_k + \\epsilon I\\)"}),"."),"\n",i.createElement(t.li,null,"Re-initializing that component's parameters if it degenerates too severely."),"\n"),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Interpretability"),': GMM is sometimes used for clustering, but the mixture components do not necessarily correspond to "well-separated" clusters. Overlaps can be large. Evaluate membership probabilities ',i.createElement(r.A,{text:"\\(\\gamma_{ik}\\)"})," carefully."),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Scaling and standardization"),": In many real datasets, different dimensions of ",i.createElement(r.A,{text:"\\(\\mathbf{x}\\)"})," vary at different scales. Standardizing or normalizing the data is often recommended before GMM fitting, to avoid numerical or interpretational confusion in the covariance estimates."),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Dimensionality"),": With dimension ",i.createElement(r.A,{text:"\\(d\\)"}),", each covariance matrix has ",i.createElement(r.A,{text:"\\(\\frac{d(d+1)}{2}\\)"})," parameters, which can become large for bigger ",i.createElement(r.A,{text:"\\(d\\)"}),". You might reduce dimensionality with PCA or adopt diagonal/spherical covariances."),"\n"),"\n",i.createElement(t.li,null,"\n",i.createElement(t.p,null,i.createElement(t.strong,null,"Hyperparameter tuning"),": Model selection via BIC or cross-validation is generally recommended. If your final solution ends up with many very small mixture weights, try a smaller ",i.createElement(r.A,{text:"\\(K\\)"}),"."),"\n"),"\n"),"\n",i.createElement(t.h2,{id:"10-additional-code-example-scikit-learn-usage",style:{position:"relative"}},i.createElement(t.a,{href:"#10-additional-code-example-scikit-learn-usage","aria-label":"10 additional code example scikit learn usage permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"10. Additional code example: scikit-learn usage"),"\n",i.createElement(t.p,null,"Here is a short snippet using scikit-learn's ",i.createElement(l.A,null,"GaussianMixture")," class for demonstration:"),"\n",i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">&lt;Code text={`\nimport numpy as np\nfrom sklearn.mixture import GaussianMixture\nimport matplotlib.pyplot as plt\n\n# Synthetic data\nnp.random.seed(123)\nn_samples = 500\nmean_a = [2, 2]\ncov_a = [[1, 0.2], [0.2, 1]]\ndata_a = np.random.multivariate_normal(mean_a, cov_a, n_samples//2)\n\nmean_b = [-2, -3]\ncov_b = [[1, -0.1], [-0.1, 1]]\ndata_b = np.random.multivariate_normal(mean_b, cov_b, n_samples//2)\n\nX = np.vstack((data_a, data_b))\n\n# Fit GMM\ngmm = GaussianMixture(n_components=2, covariance_type=\'full\', random_state=42)\ngmm.fit(X)\n\nprint("Means:")\nprint(gmm.means_)\nprint("Covariances:")\nprint(gmm.covariances_)\nprint("Weights:", gmm.weights_)\n\n# Predict cluster membership\nlabels = gmm.predict(X)\n\n# Plot\nplt.figure(figsize=(6,6))\nplt.scatter(X[:, 0], X[:, 1], c=labels, cmap=\'viridis\', s=30)\nplt.title("GMM Clustering with scikit-learn")\nplt.show()\n`}/></code></pre></div>'}}),"\n",i.createElement(t.p,null,"This snippet trains a GMM with two components and then plots the data points with color-coded cluster assignments (based on the maximum posterior responsibility). Notice how easy it is to retrieve the model's means, covariances, and weights."),"\n",i.createElement(t.h2,{id:"11-final-remarks-optional",style:{position:"relative"}},i.createElement(t.a,{href:"#11-final-remarks-optional","aria-label":"11 final remarks optional permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"11. Final remarks (optional)"),"\n",i.createElement(t.p,null,"Gaussian mixture models, while conceptually straightforward, continue to be a mainstay in unsupervised learning and distribution modeling tasks. They are a natural extension of the single Gaussian approach and can handle multi-modal, overlapping distributions elegantly. Despite potential challenges such as local maxima, cluster degeneracy, or dimensional explosion, GMMs remain favored due to their flexibility, interpretability, and a long history of theoretical grounding."),"\n",i.createElement(t.p,null,"With the advent of advanced Bayesian and nonparametric approaches (like Dirichlet process mixtures) and faster optimization methods (like stochastic or parallel EM), GMMs are more capable than ever of scaling to large datasets. On the experimental side, scikit-learn's ",i.createElement(l.A,null,"GaussianMixture")," or similarly robust libraries in languages like R (mclust) or Julia (Clustering.jl) can be used for quick and reliable modeling."),"\n",i.createElement(t.p,null,"I encourage the reader to experiment with real data, tune the number of components, and get a feel for how GMMs partition data in a soft, probabilistic manner. The synergy of GMMs with other areas â€” like hidden Markov models for time series, mixture of experts for advanced regression tasks, or as building blocks in generative adversarial networks â€” further highlights their central role in modern data science."),"\n",i.createElement(t.p,null,"Below are extended references and short notes on specialized topics, providing a launching point for further study."),"\n",i.createElement(t.h2,{id:"12-extra-references-and-advanced-reading",style:{position:"relative"}},i.createElement(t.a,{href:"#12-extra-references-and-advanced-reading","aria-label":"12 extra references and advanced reading permalink",className:"anchor before"},i.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"12. Extra references and advanced reading"),"\n",i.createElement(t.ul,null,"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977).")," Maximum likelihood from incomplete data via the EM algorithm. ",i.createElement(t.em,null,"Journal of the Royal Statistical Society: Series B (Methodological)"),", ",i.createElement(t.strong,null,"39"),"(1), 1â€“38."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"McLachlan, G., & Peel, D. (2000).")," ",i.createElement(t.em,null,"Finite Mixture Models.")," Wiley. A comprehensive treatment, including theoretical and practical aspects."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Bishop, C. M. (2006).")," ",i.createElement(t.em,null,"Pattern Recognition and Machine Learning.")," Springer. Chapter on mixture models, EM, and related ideas."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Neal, R. M. (1992).")," Bayesian mixture modeling for the Dirichlet process. ",i.createElement(t.em,null,"Journal of the Royal Statistical Society: Series B"),", conceptual introduction to infinite mixture models."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Fraley, C., & Raftery, A. E. (2002).")," Model-based clustering, discriminant analysis, and density estimation. ",i.createElement(t.em,null,"Journal of the American Statistical Association"),", 97(458), 611â€“631."),"\n",i.createElement(t.li,null,i.createElement(t.strong,null,"Stephens, M. (2000).")," Dealing with label switching in mixture models. ",i.createElement(t.em,null,"Journal of the Royal Statistical Society: Series B (Statistical Methodology)"),". On dealing with the label ambiguity in mixture components."),"\n"),"\n",i.createElement(t.p,null,"I hope this detailed exploration of Gaussian mixture models will help you see why they remain a fundamental building block of advanced machine learning and data science pipelines, especially in unsupervised or semi-supervised contexts. Whether you use them for clustering, density estimation, or as submodules in more elaborate pipelines, GMMs will likely continue to be relevant for years to come."))}var s=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,a.RP)(),e.components);return t?i.createElement(t,e,i.createElement(o,e)):o(e)};var c=n(54506),m=n(88864),h=n(58481),d=n.n(h),u=n(5984),p=n(43672),g=n(27042),f=n(72031),v=n(81817),E=n(27105),b=n(17265),x=n(2043),y=n(95751),_=n(94328),k=n(80791),M=n(78137);const w=e=>{let{toc:t}=e;if(!t||!t.items)return null;return i.createElement("nav",{className:k.R},i.createElement("ul",null,t.items.map(((e,t)=>i.createElement("li",{key:t},i.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const n=t.replace("#",""),a=document.getElementById(n);a&&a.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&i.createElement(w,{toc:{items:e.items}}))))))};function S(e){let{data:{mdx:t,allMdx:l,allPostImages:r},children:o}=e;const{frontmatter:s,body:m,tableOfContents:h}=t,f=s.index,k=s.slug.split("/")[1],S=l.nodes.filter((e=>e.frontmatter.slug.includes(`/${k}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),A=S.findIndex((e=>e.frontmatter.index===f)),H=S[A+1],C=S[A-1],z=s.slug.replace(/\/$/,""),T=/[^/]*$/.exec(z)[0],I=`posts/${k}/content/${T}/`,{0:G,1:B}=(0,i.useState)(s.flagWideLayoutByDefault),{0:V,1:N}=(0,i.useState)(!1);var L;(0,i.useEffect)((()=>{N(!0);const e=setTimeout((()=>N(!1)),340);return()=>clearTimeout(e)}),[G]),"adventures"===k?L=b.cb:"research"===k?L=b.Qh:"thoughts"===k&&(L=b.T6);const P=d()(m).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,R=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),n=e%60;return n<=30?`~${t}${n>0?".5":""} h`:`~${t+1} h`}(Math.ceil(P/L)+(s.extraReadTimeMin||0)),D=[{flag:s.flagDraft,component:()=>Promise.all([n.e(5850),n.e(9833)]).then(n.bind(n,49833))},{flag:s.flagMindfuckery,component:()=>Promise.all([n.e(5850),n.e(7805)]).then(n.bind(n,27805))},{flag:s.flagRewrite,component:()=>Promise.all([n.e(5850),n.e(8916)]).then(n.bind(n,78916))},{flag:s.flagOffensive,component:()=>Promise.all([n.e(5850),n.e(6731)]).then(n.bind(n,49112))},{flag:s.flagProfane,component:()=>Promise.all([n.e(5850),n.e(3336)]).then(n.bind(n,83336))},{flag:s.flagMultilingual,component:()=>Promise.all([n.e(5850),n.e(2343)]).then(n.bind(n,62343))},{flag:s.flagUnreliably,component:()=>Promise.all([n.e(5850),n.e(6865)]).then(n.bind(n,11627))},{flag:s.flagPolitical,component:()=>Promise.all([n.e(5850),n.e(4417)]).then(n.bind(n,24417))},{flag:s.flagCognitohazard,component:()=>Promise.all([n.e(5850),n.e(8669)]).then(n.bind(n,18669))},{flag:s.flagHidden,component:()=>Promise.all([n.e(5850),n.e(8124)]).then(n.bind(n,48124))}],{0:j,1:X}=(0,i.useState)([]);return(0,i.useEffect)((()=>{D.forEach((e=>{let{flag:t,component:n}=e;t&&n().then((e=>{X((t=>[].concat((0,c.A)(t),[e.default])))}))}))}),[]),i.createElement(g.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},i.createElement(v.A,{postNumber:s.index,date:s.date,updated:s.updated,readTime:R,difficulty:s.difficultyLevel,title:s.title,desc:s.desc,banner:s.banner,section:k,postKey:T,isMindfuckery:s.flagMindfuckery,mainTag:s.mainTag}),i.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},s.otherTags.map(((e,t)=>i.createElement("span",{key:t,className:`noselect ${M.MW}`,style:{margin:"0 5px 5px 0"}},e)))),i.createElement("div",{className:"postBody"},i.createElement(w,{toc:h})),i.createElement("br",null),i.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},i.createElement(g.P.button,{className:`noselect ${_.pb}`,id:_.xG,onClick:()=>{B(!G)},whileTap:{scale:.93}},i.createElement(g.P.div,{className:y.DJ,key:G,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},G?"Switch to default layout":"Switch to wide layout"))),i.createElement("br",null),i.createElement("div",{className:"postBody",style:{margin:G?"0 -14%":"",maxWidth:G?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},i.createElement("div",{className:`${_.P_} ${V?_.Xn:_.qG}`},j.map(((e,t)=>i.createElement(e,{key:t}))),s.indexCourse?i.createElement(x.A,{index:s.indexCourse,category:s.courseCategoryName}):"",i.createElement(u.Z.Provider,{value:{images:r.nodes,basePath:I.replace(/\/$/,"")+"/"}},i.createElement(a.xA,{components:{Image:p.A}},o)))),i.createElement(E.A,{nextPost:H,lastPost:C,keyCurrent:T,section:k}))}function A(e){return i.createElement(S,e,i.createElement(s,e))}function H(e){var t,n,a,l,r;let{data:o}=e;const{frontmatter:s}=o.mdx,c=s.titleSEO||s.title,h=s.titleOG||c,d=s.titleTwitter||c,u=s.descSEO||s.desc,p=s.descOG||u,g=s.descTwitter||u,v=s.schemaType||"BlogPosting",E=s.keywordsSEO,b=s.date,x=s.updated||b,y=s.imageOG||(null===(t=s.banner)||void 0===t||null===(n=t.childImageSharp)||void 0===n||null===(a=n.gatsbyImageData)||void 0===a||null===(l=a.images)||void 0===l||null===(r=l.fallback)||void 0===r?void 0:r.src),_=s.imageAltOG||p,k=s.imageTwitter||y,M=s.imageAltTwitter||g,w=s.canonicalURL,S=s.flagHidden||!1,A=s.mainTag||"Posts",H=s.slug.split("/")[1]||"posts",{siteUrl:C}=(0,m.Q)(),z={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:C},{"@type":"ListItem",position:2,name:A,item:`${C}/${s.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${C}${s.slug}`}]};return i.createElement(f.A,{title:c+" - avrtt.blog",titleOG:h,titleTwitter:d,description:u,descriptionOG:p,descriptionTwitter:g,schemaType:v,keywords:E,datePublished:b,dateModified:x,imageOG:y,imageAltOG:_,imageTwitter:k,imageAltTwitter:M,canonicalUrl:w,flagHidden:S,mainTag:A,section:H,type:"article"},i.createElement("script",{type:"application/ld+json"},JSON.stringify(z)))}},90548:function(e,t,n){var a=n(96540),i=n(7978);t.A=e=>{let{text:t}=e;return a.createElement(i.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-tsx-content-file-path-src-pages-posts-research-gaussian-mixture-models-mdx-380c031d29286125daf3.js.map