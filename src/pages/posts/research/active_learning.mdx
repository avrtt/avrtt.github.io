---
index: 100
indexCourse: 20
indexFavorites:
title: "Active learning"
titleDetailed: ""
titleSEO: ""
titleOG: ""
titleTwitter: ""
titleCourse: "Active learning"
courseCategoryName: "Basic ML theory & techniques"
desc: "Curiosity-driven algorithms"
descSEO: ""
descOG: ""
descTwitter: ""
date: "10.04.2024"
updated:
prioritySitemap: 0.6
changefreqSitemap: "monthly"
extraReadTimeMin: 30
difficultyLevel: 1
flagDraft: true
flagMindfuckery: false
flagRewrite: false
flagOffensive: false
flagProfane: false
flagMultilingual: false
flagUnreliably: false
flagPolitical: false
flagCognitohazard: false
flagHidden: false
flagWideLayoutByDefault: true
schemaType: "Article"
mainTag: ""
otherTags: [""]
keywordsSEO: [""]
banner: "../../../images/posts/research/banners/active_learning.jpg"
imageOG: ""
imageAltOG: ""
imageTwitter: ""
imageAltTwitter: ""
canonicalURL: "https://avrtt.github.io/research/active_learning"
slug: "/research/active_learning"
---

import Tooltip from "../../../components/Tooltip"
import Highlight from "../../../components/Highlight"
import Code from "../../../components/Code"
import Latex from "../../../components/Latex"


{/* *(intro: a quote, catchphrase, joke, etc.)* */}

<br/>


{/*

Активное обучение
https://neerc.ifmo.ru/wiki/index.php?title=%D0%90%D0%BA%D1%82%D0%B8%D0%B2%D0%BD%D0%BE%D0%B5_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5

*/}


{/*

1. Introduction
- Definition of Active Learning  
  - Focus on scenarios where a learning algorithm can interactively query an oracle (e.g., a human annotator) to obtain labels.  
  - Emphasis on reducing labeling effort while maintaining or improving model performance.
- Historical Background  
  - Early theoretical foundations and the concept of query-based learning.  
  - Milestones in active learning research and key papers.  
  - Shifts from purely academic research to industrial applications.
- Relevance in Modern Machine Learning  
  - Growing demand for efficiently labeling large-scale datasets.  
  - Cost and time savings in data annotation.  
  - Use in data-scarce or specialized domains (e.g., medical imaging).
- Comparison with Traditional Supervised Learning  
  - Typical supervised learning requires a large labeled dataset upfront.  
  - Active learning iteratively selects which data points to label, potentially using fewer labels overall.
- Benefits of Iterative Label Acquisition  
  - Direct control over the labeling process.  
  - Potential for rapid convergence on high-performance models.  
  - More efficient usage of domain expert time and resources.
- Common Pitfalls in Data Collection  
  - Over-reliance on random sampling leading to inefficient coverage of edge cases.  
  - Poorly defined labeling guidelines causing inconsistent labels.  
  - Ignoring representativeness and focusing only on the "uncertain" samples.
- Cost-Benefit Considerations  
  - Trade-off between labeling cost and model performance gains.  
  - Strategies for optimizing budget usage across labeling iterations.
2. Core strategies of active learning
- Pool-Based Sampling  
  - Large unlabeled dataset from which the most "informative" samples are chosen.  
  - Implementation details (batch query vs. one-by-one queries).  
  - Handling high-dimensional data in pool-based settings.
- Query Synthesis  
  - Generation of synthetic examples rather than selecting from an existing pool.  
  - Use cases in reinforcement learning or when unlabeled data is scarce.  
  - Practical complexities and risks of generating unrealistic or unhelpful samples.
- Stream-Based (Online) Learning  
  - Real-time (or near real-time) assessment of incoming data points.  
  - Decision-making on whether to query a label immediately or discard.  
  - Applicable in domains with continuous data streams (e.g., sensor data).
- Hybrid approaches  
  - Combining aspects of pool-based and stream-based sampling.  
  - Mixing query synthesis with pool-based selection for improved coverage.  
  - Use cases where domain constraints demand flexible sampling strategies.
3. Techniques and algorithms
- Uncertainty Sampling  
  - Using model confidence (e.g., margin, entropy) to select the least certain predictions.  
  - Pros and cons of different uncertainty metrics (e.g., softmax-based, Bayesian methods).  
  - Susceptibility to outliers and mislabeled data.
- Query-by-Committee  
  - Maintaining multiple models or hypotheses.  
  - Selecting samples where committee members disagree the most.  
  - Balancing model diversity and computational complexity.
- Expected Error Reduction  
  - Estimating the expected decrease in generalization error when labeling a specific data point.  
  - Complexity of calculating these expectations in high-dimensional settings.  
  - Approximation methods and heuristics.
- Density-Weighted Methods  
  - Incorporating data distribution information to ensure selected samples are representative.  
  - Trade-off between uncertainty and representativeness.  
  - Handling imbalanced distributions or multi-modal data.
- Active Transfer Learning  
  - Adapting a pre-trained model to a new domain with minimal labeled data.  
  - Strategizing query selection to address domain shift.  
  - Relevance in low-resource NLP tasks and specialized imaging domains.
- Ensemble-Based Approaches  
  - Using multiple models (ensembles, bagging, boosting) to refine query strategies.  
  - Robustness through model diversity.  
  - Practical considerations in computational overhead.
4. Implementation considerations
- Data Representation and Feature Engineering  
  - Importance of feature selection for meaningful uncertainty estimates.  
  - Effect of dimensionality reduction (e.g., PCA, autoencoders) on active query decisions.  
  - Data preprocessing and cleaning for reliable model feedback.
- Labeling Costs and Budget Constraints  
  - Monetary vs. time costs in human annotation.  
  - Strategies for setting up iterative labeling budgets.  
  - Dynamic budget allocation during the active learning process.
- Model Retraining Frequency  
  - Balancing model update intervals with labeling cycles.  
  - Incremental vs. full retraining.  
  - Use of warm-start strategies to reduce training overhead.
- Dealing with Class Imbalance  
  - Sampling approaches that target underrepresented classes.  
  - Cost-sensitive selection methods.  
  - Impact of imbalance on uncertainty measures.
- Annotation Quality Control  
  - Techniques to ensure label consistency (e.g., multi-annotator consensus).  
  - Handling noisy or adversarial annotators.  
  - Reinforcement of labeling guidelines and best practices.
5. Evaluation of active learning systems
- Metrics for Measuring Performance  
  - Accuracy, precision, recall, and F1-score in iterative settings.  
  - Label efficiency metrics (e.g., performance vs. number of queries).  
  - Tracking model improvement curve over time.
- Stopping Criteria and Validation  
  - When to stop querying for additional labels.  
  - Use of a validation or hold-out set to estimate performance gains.  
  - Automated vs. manual strategies for deciding "enough" labeling.
- Comparison with Passive Learning Baselines  
  - Benchmarking against random sampling or fully labeled datasets.  
  - Statistical tests for significance (e.g., t-tests, bootstrap confidence intervals).  
  - Visualizing learning curves and cost-effectiveness.
- Experimental Protocols and Best Practices  
  - Cross-validation for robust performance estimates.  
  - Reproducibility concerns in iterative experiments.  
  - Common pitfalls in setup and reporting.
6. Tools and frameworks
- Popular Python Libraries (e.g., modAL, ALiPy)  
  - Core functionalities: uncertainty sampling, query-by-committee, etc.  
  - Integration with scikit-learn pipelines.  
  - Community support and example notebooks.
- Integrating Active Learning in Scikit-learn and TensorFlow  
  - Adapting standard ML pipelines for iterative queries.  
  - Custom callback functions for active labeling in TensorFlow/Keras.  
  - Handling large-scale neural network training with active selection.
- Cloud-Based Solutions and APIs  
  - Managed services for data annotation (e.g., Amazon SageMaker Ground Truth).  
  - Platform-specific active learning features (e.g., Microsoft Azure, Google Cloud).  
  - Pros and cons of outsourced labeling services.
- Labeling Tools and Platforms  
  - Specialized annotation interfaces (e.g., CVAT, Labelbox).  
  - Integration with active sampling to reduce annotation time.  
  - Collaborative annotation workflows and version control.
7. Challenges and the future
- Scalability for Large Datasets  
  - Efficient query strategies for multi-million sample pools.  
  - Distributed and parallelizable active learning algorithms.  
  - Cloud-based or GPU-accelerated approaches.
- Adapting to Evolving Data Streams  
  - Non-stationary environments (concept drift).  
  - Continuous monitoring and dynamic model updating.  
  - Real-time query decisions with streaming data.
- Interpretability of Active Learning Decisions  
  - Explaining why certain samples are queried.  
  - Techniques for improving transparency and trust (LIME, SHAP).  
  - Trade-offs between interpretability and model complexity.
8. Extended Topics in Active Learning
- MLOps and Production Integration  
  - Lifecycle management of models that incorporate active learning loops.  
  - Continuous deployment and monitoring (CI/CD for active learning).  
  - Tracking metrics and triggering re-labeling or re-training events.
- Hybrid Human-AI Annotation Workflows  
  - Combining crowdsourcing platforms with in-house experts.  
  - Automatic quality assurance checks and consensus labeling.  
  - Designing user-friendly interfaces for non-technical annotators.
- Cost-Sensitive Active Learning  
  - Assigning different costs to labeling certain classes or data types.  
  - Optimizing for minimal total annotation cost.  
  - Use cases in high-stakes domains (e.g., healthcare, legal).
- Cross-Domain Active Learning  
  - Strategies for leveraging labeled data in one domain to jumpstart learning in another.  
  - Evaluating domain similarity and representativeness.  
  - Risk of negative transfer if domains diverge significantly.
9. Conclusion and references
  - Importance of strategic sample selection to reduce labeling effort.  
  - Balance between model performance, domain expertise, and label costs.  
  - Future innovations likely to address scalability, interpretability, and ethical use.  
  - Implementation guides and open-source project links.

*/}


Active learning is a subfield of machine learning dedicated to scenarios where a learning algorithm can query a so-called oracle — for instance, a human labeler or domain expert — to label specific data points of interest. In contrast to classic supervised learning methods, which rely on collecting large labeled datasets upfront, active learning is interactive and iterative. Instead of labeling everything in bulk, the system pinpoints which unlabeled samples seem most valuable for improving model performance and then requests labels for only those, attempting to achieve higher accuracy with fewer labeled instances overall. This approach addresses a critical bottleneck in modern data science: labeling can be costly, time-consuming, or even hazardous (for instance, when labels require special expertise, as in medical diagnostics or chemical experiments). By judiciously selecting the most "informative" points to label, active learning seeks to reduce the annotation burden without sacrificing the quality of the trained model.

### definition of active learning

In active learning, the learner is not strictly passive in receiving labeled data. Rather, it actively chooses what to label next in a guided manner. Formally, we consider a pool of unlabeled data points <Latex text="\(X = \{ x_1, x_2, \ldots, x_n \}\)"/> and a set of possible labels <Latex text="\(Y = \{y_1, \ldots, y_m\}\)"/>. There is an oracle <Latex text="\(O: X \rightarrow Y\)"/> capable of providing a ground-truth label <Latex text="\(y \in Y\)"/> for any queried data point <Latex text="\(x\)"/>. The active learner's goal is to find a model <Latex text="\(a: X \rightarrow Y\)"/> that is sufficiently accurate on the distribution of interest, while at the same time querying as few samples as possible from the oracle. Each time the active learner queries the oracle, it incurs a cost — perhaps monetary, time-based, or otherwise. Minimizing the total cost of these queries while still converging to a high-performance model is the essence of active learning.

The learning loop typically proceeds in rounds:
1. The model is trained on whatever labeled set the learner currently has (<Latex text="\(X_{labeled}\)"/>).
2. Based on the current state of the model, a strategy (or multiple strategies) identifies which samples from <Latex text="\(X_{unlabeled}\)"/> are the most valuable to label next.
3. The selected points (<Latex text="\(X_{query}\)"/>) are sent to the oracle, which returns the correct labels.
4. The newly labeled points are moved into <Latex text="\(X_{labeled}\)"/>, and the model is retrained or updated.
5. Steps 2–4 repeat until a stopping condition is met (e.g., the labeling budget is exhausted, or the performance plateaus).

### historical background

Active learning traces its origins to the 1980s and early 1990s, influenced by research on query-based learning in the computational learning theory community (e.g., the concept of "queries" in the PAC learning model). A pivotal early work was the research on selective sampling and query synthesis, highlighting the possibility that carefully chosen queries could accelerate the rate of learning. Over the years, theoretical results established upper and lower bounds on the label complexity — the number of labels needed to learn a hypothesis to a certain level of accuracy (e.g., in the Probably Approximately Correct framework).

In subsequent decades, interest ballooned as researchers from institutions worldwide published empirical successes of active learning in domains like text classification, image recognition, bioinformatics, and more. Early seminal papers include works that introduced uncertainty sampling, query-by-committee, and expected error reduction. More recent active learning research trends (seen in top AI conferences such as NeurIPS, ICML, and ICLR) focus on handling large-scale data, addressing deep neural networks, dealing with adversarial label noise, and integrating active learning with other paradigms such as transfer learning and reinforcement learning.

Industry applications of active learning have followed suit. Tech companies with massive annotation costs (ranging from content moderation to self-driving car sensor labeling) and specialized fields like radiology that rely on expert judgments have all embraced the core ideas of active learning. Over time, the synergy between new annotation tools, cloud-based labeling pipelines, and advanced active selection strategies has become an increasingly visible trend.

### relevance in modern machine learning

With big data's rise, labeling remains a bottleneck in many real-world machine learning projects. Datasets in areas like autonomous driving, natural language processing (especially in low-resource languages), or rare-event detection (e.g., in medical images or scientific instrumentation) can be extremely difficult to label at scale. Active learning addresses this challenge by focusing labeling resources on exactly the data that matters most to the model's decision boundaries.

Even when abundant unlabeled data is available, there may be only a limited time window or budget to manually annotate samples. Active learning not only reduces costs but can also boost performance by prioritizing corner cases and borderline situations that typical random sampling might miss. This leads to improved coverage in "tricky" regions of the data space and faster discovery of novel phenomena in specialized domains.

### comparison with traditional supervised learning

Traditional supervised learning methods typically assume a dataset <Latex text="\(D\)"/> where each <Latex text="\(x\)"/> already has an associated label <Latex text="\(y\)"/>. The model trains on this labeled dataset in one shot or with standard cross-validation. In active learning, by contrast, the labeling process is dynamic and iterative. The model starts with a small labeled subset or none at all, then interacts with the unlabeled pool to decide which examples to annotate next.

A key difference is label efficiency. In pure supervised learning, label usage can be highly inefficient if many labeled points are redundant or come from "easy" parts of the input space that don't inform the decision boundary. In active learning, the hope is that each label requested significantly contributes to improving the model. Of course, the overhead of the active querying strategy itself must be managed — if the approach for selecting queries is too complex, the computational cost might undermine the benefits of fewer labels.

### benefits of iterative label acquisition

Adopting an active learning pipeline can bring significant gains:

1. **Reduced labeling cost**: By only querying the most relevant samples, projects can save money and time on large-scale annotation tasks.
2. **Faster convergence**: Models may reach a satisfactory performance level in fewer training iterations if the queries pinpoint edge-case regions or uncertain areas.
3. **Adaptive data collection**: As the model evolves, the active learning strategy refines its selection of data to label, focusing on newly discovered weaknesses in the hypothesis space.
4. **Better coverage of rare events**: If certain classes or phenomena are scarce, an active learner can actively hunt down those minority samples, addressing class imbalance and capturing potentially critical outliers.

### common pitfalls in data collection

While active learning aims to be more efficient than random sampling, improper implementation can lead to problems:

1. **Over-reliance on uncertainty**: Some strategies pick points purely based on uncertainty, which might cause the model to chase random noise or outliers that do not help generalization.
2. **Inconsistent labeling guidelines**: If the labeling task is subjective or poorly specified, the chosen queries might receive ambiguous or incorrect labels. This can be disastrous for the iterative learning loop.
3. **Ignoring representativeness**: In some tasks, focusing only on uncertain points can bias the labeled dataset away from the overall data distribution. A balanced approach that accounts for data density is often important.
4. **Batch-mode complexities**: In practical scenarios, queries might be selected in batches rather than one at a time. Poorly designed batch selection can result in redundant or overly correlated samples.

### cost-benefit considerations

In real-world setups, there is always a tension between spending resources on labeling more data versus investing in alternative ways to improve performance (e.g., model architecture enhancements, hyperparameter tuning, data augmentation). Active learning is not a panacea for labeling overhead; it is one possible strategy. One must measure potential gains by systematically running pilot experiments to see if a given active strategy provides enough performance benefit to justify its complexity.

Moreover, there are situations where random sampling (or an alternative approach) might suffice — particularly if labeling is extremely cheap or if the data distribution is uniform and well-behaved. Nonetheless, in many high-cost or domain-specific labeling tasks (like collecting ground truths in radiology, chemistry, or large-scale image classification with bounding boxes), a careful active learning pipeline can yield large cost savings.

## core strategies of active learning

Various formalizations of active learning exist, but most approaches can be categorized into a few main paradigms. In broad strokes, these paradigms define how the learner obtains unlabeled data and how it decides which data points to query.

### pool-based sampling

Pool-based sampling is perhaps the most common scenario in practical applications. We assume there is a large pool (or reservoir) of unlabeled data, <Latex text="\(X_{unlabeled}\)"/>, from which the learning algorithm can iteratively sample points to label. Each round, the active learner ranks these unlabeled points according to an informativeness criterion (e.g., uncertainty, disagreement, density). Depending on the implementation, it can pick a single data point or a batch of points to query.

Common steps:
1. Train the model on the current labeled set.
2. Compute a scoring function (e.g., an uncertainty measure) for all points in the unlabeled pool.
3. Select the top <Latex text="\(k\)"/> points that are deemed most informative.
4. Query the oracle for those labels, add them to the training set, retrain the model, and repeat.

The advantage is clear interpretability and ease of implementation. The challenge is scalability — iterating over a massive unlabeled pool can be computationally expensive, especially if the scoring function is not trivially parallelizable or if the model is large (e.g., a deep neural network).

### query synthesis

In this strategy, the learner actively synthesizes or generates new data points rather than drawing from a fixed unlabeled pool. Early theoretical research in active learning considered a perfect noise-free scenario where one might craft points near the decision boundary to refine classification performance. This approach is often referred to as "query synthesis," and it can be useful if unlabeled data is scarce or if we can cheaply generate new candidate points (e.g., in certain simulation-based environments).

Query synthesis is particularly relevant in settings like reinforcement learning or robotics, where an agent can gather new experiences by exploring certain states or actions. However, designing generative processes that create realistic and helpful data for labeling can be tricky. Synthetic points that are out-of-distribution or physically impossible might not help model generalization in real-world tasks.

### stream-based (online) learning

Stream-based active learning is a setting where data arrives in a streaming fashion (for instance, sensor outputs, real-time user logs, or live social media posts). The learner must decide — typically on the fly — for each data point in the stream whether to request a label from the oracle or to discard it. Unlike pool-based sampling, the system does not necessarily store all unlabeled samples, especially in environments with memory constraints or massive data throughput.

Key considerations:
1. **Decision rules**: Must be simple enough to handle in near real-time, such as deciding to query the oracle when the model's confidence is below a certain threshold.
2. **Concept drift**: In streaming contexts, the data distribution may shift over time, so a strategy must detect and adapt to such drifts by sampling new data more aggressively.
3. **Budget constraints**: The labeling budget must be allocated wisely, given that the system may never revisit past data once it has been discarded.

### hybrid approaches

Some contexts warrant hybrid active learning solutions that combine elements of pool-based sampling, stream-based selection, and query synthesis. Such approaches can adapt to complex operational constraints — imagine an environment where you have a large historical pool, but also a continuous flow of new data. The system might do offline batch selection from the pool as well as real-time screening of the incoming stream, occasionally synthesizing new points in particular areas of interest.

Hybrid approaches can be beneficial in domains where partial data is labeled offline, but new, potentially more relevant examples become available over time (e.g., updating product catalogs in e-commerce or monitoring sensor networks in industrial setups). Additionally, mixing query synthesis with pool-based selection might help explore corners of the feature space not well-represented in the unlabeled pool.

## techniques and algorithms

Within these broader active learning frameworks, several specific query strategies have been proposed. The effectiveness of each strategy depends on data distribution, task complexity, and the type of model being trained.

### uncertainty sampling

One of the most intuitive and widely used strategies is uncertainty sampling. The model picks samples about which it is the least confident. Different metrics can gauge uncertainty:

- **Least confidence**: <Latex text="\(\Phi_{\text{LC}}(x) = 1 - P(y_1 \mid x)\)"/> where <Latex text="\(y_1\)"/> is the most probable class. If <Latex text="\(P(y_1 \mid x)\)"/> is small, it indicates the model is unsure about how to label <Latex text="\(x\)"/>.
- **Smallest margin**: <Latex text="\(\Phi_{\text{M}}(x) = P(y_1 \mid x) - P(y_2 \mid x)\)"/> where <Latex text="\(y_1\)"/> and <Latex text="\(y_2\)"/> are the top two most probable classes. A small margin implies a tie between the two top predictions, signaling high confusion.
- **Maximum entropy**: <Latex text="\(\Phi_{\text{ENT}}(x) = - \sum_{y \in Y} P(y \mid x) \log P(y \mid x)\)"/>. A higher entropy indicates higher uncertainty over the label distribution.

Uncertainty sampling is straightforward to implement, especially if you already have probability estimates from a classifier (e.g., logistic regression, neural networks with softmax outputs). However, it can lead to querying many outliers or noisy samples if they are systematically confusing to the model but do not help generalization. Density-aware or representativeness-aware methods are often used in tandem to mitigate this.

### query-by-committee

In query-by-committee, instead of maintaining a single model, the learner manages a committee (a set) of models. Each model is trained on the same labeled data (or a slightly varied subset, as in bagging ensembles). The unlabeled pool is then screened for points on which these models disagree the most.

The disagreement measure can be:
- **Vote entropy**: <Latex text="\(\Phi_{\text{VE}}(x) = - \sum_{y \in Y} \frac{V(y)}{T} \log \frac{V(y)}{T}\)"/> where <Latex text="\(V(y)\)"/> is the number of committee members who voted for class <Latex text="\(y\)"/> and <Latex text="\(T\)"/> is the total number of models in the committee.
- **Pairwise disagreement** or **Kullback-Leibler divergence** among the predictive distributions of committee members.

Query-by-committee can yield robust active learning policies by focusing on data points that the ensemble cannot resolve. However, training and maintaining multiple models increases computational overhead. It is also crucial to ensure committee diversity; if all models are too similar, the committee will not exhibit the healthy levels of disagreement necessary to identify truly informative samples.

### expected error reduction

The expected error reduction technique tries to directly estimate how much labeling a particular unlabeled sample will reduce the model's future error or loss on the entire distribution. Conceptually, it involves:
1. Hypothesizing each possible label <Latex text="\(y \in Y\)"/> for a point <Latex text="\(x\)"/>.
2. Re-training or updating the model with the new label assumption.
3. Computing the expected reduction in generalization error (or some proxy) weighted by <Latex text="\(P(y \mid x)\)"/>.

Because exact re-training for each unlabeled point can be expensive, practitioners often rely on approximations such as only partially updating the model parameters or using simpler metrics. Despite the computational cost, expected error reduction is appealing because it directly targets the ultimate goal of lowering overall classification/regression error, rather than focusing on narrower heuristics like uncertainty.

### density-weighted methods

One notable shortcoming of pure uncertainty-based selection is the risk of fixating on outliers. Density-weighted methods attempt to account for how representative a point is within the unlabeled pool. A typical approach is to multiply an uncertainty score by an estimated density <Latex text="\(p(x)\)"/>:

<Latex text="\[
x_{\text{informative}} = \arg \max_x \left(\Phi(x) \times p(x)\right)
\]"/>

where <Latex text="\(\Phi(x)\)"/> is an uncertainty measure (like margin or entropy) and <Latex text="\(p(x)\)"/> is a local density estimate. Points that are uncertain but also occur in regions of high density are more likely to be representative of the overall data manifold. This yields a better trade-off: label queries that not only help refine uncertain decisions but also more broadly cover the data distribution.

### active transfer learning

Active transfer learning unites two powerful ideas: starting with a pre-trained model (or one trained on a related domain) and selectively querying labels in the target domain. The central question is which points to label to best adapt a model from source domain knowledge to the new domain distribution. For instance, in low-resource NLP tasks, one might have a robust model trained on a high-resource language, then apply active learning to quickly adapt it to a similar but under-resourced language with minimal labeling.

Challenges include domain shift and partial mismatch in label semantics. Researchers have proposed various heuristics that combine standard active learning strategies (like uncertainty sampling) with domain discrepancy estimations (e.g., maximum mean discrepancy or other measures of distribution alignment) to pick points where the model is uncertain due to domain differences.

### ensemble-based approaches

Beyond query-by-committee, there is a broader class of ensemble-based active learning methods. These can involve random forests, gradient boosting machines, or any aggregator of multiple hypotheses. The principle remains that disagreement among ensemble members is a proxy for the model's uncertainty about a particular data region. Another angle is using ensembles to estimate predictive variance (especially in Bayesian neural networks or Monte Carlo dropout), which can guide an uncertainty-based selection.

In production environments, one can harness ensembles that already exist for reliability or performance reasons (e.g., in Kaggle competition winners, ensembles are common) and repurpose them for active query selection. However, the extra computational expense of maintaining an ensemble is something to consider carefully.

## implementation considerations

### data representation and feature engineering

High-quality input representations can heavily influence the effectiveness of active learning. If the model struggles to produce reliable uncertainty or disagreement measures — perhaps because features are noisy or uninformative — it cannot accurately identify which unlabeled points are most valuable. Feature engineering techniques, such as dimensionality reduction (PCA, autoencoders) or specialized embeddings (for text or images), can enhance the model's capacity to differentiate between "easy" and "hard" samples.

For example, if working with image data, employing a pretrained convolutional neural network to extract embeddings can significantly improve the subsequent active learner's selection strategy. Instead of doing direct uncertainty sampling on raw pixels, the system would base it on a latent representation that is more semantically meaningful.

### labeling costs and budget constraints

Active learning is particularly relevant when annotation is expensive or time-consuming. The user (or the system) often has a fixed "budget" of labels <Latex text="\(B\)"/> to spend. One must design the active learning loop to ensure it uses <Latex text="\(B\)"/> effectively. Some strategies:
- **Fixed batch size**: Query <Latex text="\(k\)"/> samples per iteration until the total budget is reached.
- **Adaptive batch size**: Stop earlier if the marginal improvement from each additional label is too small.
- **Cost-sensitive strategies**: Different labels might have different costs. For example, labeling an X-ray might require a radiologist's time, which is costlier than labeling a simple text snippet.

### model retraining frequency

Each active learning iteration typically involves re-training or updating the model after adding newly labeled points. If the model is large (e.g., a deep neural network), re-training from scratch each round can be prohibitively expensive. Possible workarounds:
- **Incremental or warm-start training**: Continue from previous model parameters, only fine-tuning with new labels. This can be done, for example, in scikit-learn's `partial_fit` methods or in neural network frameworks with checkpointing.
- **Lower-fidelity models for query selection**: Use a computationally cheaper surrogate model (e.g., logistic regression) to guide queries, then label them and eventually re-train the more expensive "final" model once enough labels are gathered. This approach is sometimes referred to as "active learning with a model mismatch."

### dealing with class imbalance

In many real-world problems, certain classes are underrepresented. Relying solely on uncertainty sampling could fail to acquire enough positive examples of a rare class. For instance, a fraud detection system may rarely see examples of fraudulent transactions. Active learning strategies might require explicit class-balancing or cost-sensitive query selection:
- **Weighted uncertainty**: Increase the importance (weight) of identifying uncertain points that likely belong to the rare class.
- **Stratified query selection**: Force the model to occasionally query data from minority clusters or from borderline examples that show signals of belonging to an underrepresented class.

### annotation quality control

Active learning's iterative nature amplifies the consequences of labeling errors. If an incorrectly labeled example leads the model astray, the subsequent query strategy may further reinforce misguided areas of the decision space. Quality control is thus paramount:
1. **Redundancy or consensus**: Send certain queries to multiple annotators and merge the labels (majority vote or confidence weighting).
2. **Expert verification**: Have domain experts periodically review uncertain or ambiguous examples.
3. **Regular label audits**: Even if the initial labeling pipeline is well-defined, keep track of label consistency metrics over time and refine instructions as needed.

## evaluation of active learning systems

### metrics for measuring performance

When assessing an active learning system, it is not enough to measure final accuracy alone. Additional key performance indicators include:

1. **Label efficiency**: How many labels does the system need to reach a certain performance threshold? This can be expressed as "performance vs. number of queries" or "area under the accuracy curve" with respect to queries.
2. **Standard classification metrics**: Accuracy, precision, recall, F1 score, etc. Over the course of iterative labeling, you might plot how these metrics evolve after each batch of queries.
3. **Query distribution analysis**: Analyze which classes or regions of the input space are being queried frequently. This can help confirm whether the strategy is effectively covering the distribution.

### stopping criteria and validation

Determining when to stop is crucial: the system might continue querying labels indefinitely, even after diminishing returns set in. Common heuristics or strategies for stopping include:

- **Performance plateau**: If performance on a validation set does not improve meaningfully over a certain number of labeling rounds, stop querying.
- **Budget limit**: Cease queries once you've hit the label budget <Latex text="\(B\)"/>.
- **Confidence threshold**: If the model's confidence on most unlabeled data surpasses a threshold, it might be considered "good enough" in practice.

### comparison with passive learning baselines

An active learning method should be compared to a baseline (such as random sampling or a purely passive approach using all labels up front). Relevant comparisons:

1. **Label usage**: Did active learning require significantly fewer labels to achieve the same performance?
2. **Statistical significance**: Use t-tests, bootstrap intervals, or other robust methods to ensure improvements are not due to chance.
3. **Learning curves**: Plot performance against the number of queries for both active and passive approaches.

### experimental protocols and best practices

To ensure reliable, reproducible results in active learning research:

1. **Multiple runs**: Because the unlabeled data selection depends on the current model, randomness in initialization can cause significant variance in outcomes.
2. **Hold-out or cross-validation**: Keep a separate test set or use cross-validation to fairly assess the model's generalization during each labeling round.
3. **Open-source pipelines**: Tools like `modAL` (Python) or custom frameworks built on scikit-learn's `BaseEstimator` help standardize evaluations and reduce errors in the iterative loop.

## tools and frameworks

### popular python libraries

Several libraries aim to simplify the implementation of active learning strategies:
- **modAL** (Python): Offers a flexible framework for pool-based active learning. It supports uncertainty sampling, query-by-committee, and integrates with scikit-learn models.
- **ALiPy**: Another active learning tool with modular design and multiple query strategies, batch selection, and experiment management features.

These libraries typically provide convenience functions to rank unlabeled samples by an informativeness metric and to update the training set accordingly. Additionally, they integrate well with scikit-learn, allowing you to leverage scikit-learn's classifiers, pipelines, and cross-validation.

### integrating active learning in scikit-learn and tensorflow

While scikit-learn does not natively implement active learning, you can build a custom loop around any estimator that provides a method for obtaining decision function values or probability estimates. For instance, with a logistic regression model:

<Code text={`
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

def uncertainty_sampling(model, X_pool, n_queries=5):
    # model.predict_proba(X) returns an array of probabilities of shape (n_samples, n_classes)
    probs = model.predict_proba(X_pool)
    # compute least confidence
    max_conf = np.max(probs, axis=1)
    uncertainty = 1 - max_conf
    # select the top n_queries most uncertain samples
    query_indices = np.argsort(uncertainty)[-n_queries:]
    return query_indices

# Example usage:
X_labeled = ...  # initially labeled data
y_labeled = ...
X_pool = ...     # unlabeled pool
model = LogisticRegression()
model.fit(X_labeled, y_labeled)

query_indices = uncertainty_sampling(model, X_pool, n_queries=10)
# ask the oracle for labels on those indices
# oracle_labels = ...
# augment the labeled dataset
X_labeled = np.concatenate((X_labeled, X_pool[query_indices]), axis=0)
y_labeled = np.concatenate((y_labeled, oracle_labels), axis=0)
# remove queried samples from the pool
X_pool = np.delete(X_pool, query_indices, axis=0)
`}/>

In deep learning frameworks like TensorFlow or PyTorch, you can similarly implement custom active learning loops. The model's forward pass can yield predicted class probabilities or embeddings for uncertainty or representativeness-based selection. After each round of labeling, you can fine-tune the model on the expanded labeled set. For advanced strategies — like Bayesian neural networks — Monte Carlo dropout can approximate uncertainty, and you can rank unlabeled samples by their predictive variance.

### cloud-based solutions and apis

Major cloud providers offer annotation services, some of which include built-in active learning functionality:
- **Amazon SageMaker Ground Truth**: Provides a labeling platform with an "automated data labeling" feature that uses active learning behind the scenes.
- **Microsoft Azure Machine Learning**: Offers labeling tasks and partial active learning functionalities, though more advanced or customized strategies often require your own code.
- **Google Cloud AutoML**: While known for automating model building, it also has features for managing labeled datasets, though it may not natively support a wide range of active learning query strategies.

If using such services, be mindful of data security, label costs, and the ability to customize or refine the selection strategies. Some platforms abstract away the strategy details, offering limited control over query selection logic.

### labeling tools and platforms

There are platforms specifically designed for building efficient annotation pipelines:
- **CVAT (Computer Vision Annotation Tool)**: An open-source tool for annotating images and video. You can integrate custom active sampling logic by interfacing with CVAT's backend.
- **Labelbox, Supervise.ly**: Commercial platforms that support collaborative annotation, version control for labeled data, and can be extended with active learning hooks (e.g., webhooks that fetch new tasks from an active learner).

Providing annotators with a user-friendly environment is crucial for productivity and label quality. Integrations that allow you to directly feed newly selected unlabeled data for annotation — and then retrieve updated labels — are vital to the iterative nature of active learning.

## challenges and the future

### scalability for large datasets

As machine learning moves into the realm of tens or hundreds of millions of samples, naive pool-based strategies face scalability issues. Iterating through the entire unlabeled dataset to compute an uncertainty score can be prohibitively expensive. Strategies to address these challenges include:
- **Sampling approximation**: Instead of scoring all unlabeled samples, sample from the pool and only compute uncertainly for that subset.
- **Clustering or partition-based indexing**: Pre-group unlabeled data, quickly identify which clusters are uncertain, and query representative samples from those clusters.
- **Distributed architecture**: Parallelize query scoring across multiple machines or GPUs. Large-scale frameworks like Apache Spark can be adapted for distributed active learning loops.

### adapting to evolving data streams

In streaming scenarios or real-time applications (e.g., IoT sensor networks), the data distribution can shift over time — a phenomenon known as concept drift. Active learning solutions in these contexts must continuously monitor the model's performance and uncertainty levels. If signs of drift appear (e.g., performance on a recent validation batch drops, or uncertainty for newly arrived data becomes abnormally high), the learner must increase queries from the new region of the data distribution. Handling concept drift elegantly remains an area of active research, with solutions that often combine active learning with online learning or incremental update techniques.

### interpretability of active learning decisions

When an active learning strategy chooses specific samples for labeling, domain experts might ask "why were these data points selected?" In safety-critical or regulated domains (e.g., medical diagnostics, finance), having an interpretable rationale helps build trust. Techniques such as:
- **Feature attribution**: Indicate which features or partial dependence contributed most to the uncertainty of a query.
- **Explainer methods**: Use LIME or SHAP to generate local explanations for uncertain samples.
- **Visual prototypes**: For image-based tasks, show saliency maps or highlight the pixel regions driving the confusion.

Increasing interpretability can come at a cost to computational efficiency. There is a trade-off between the complexity of the query selection method and how easily one can communicate its workings to stakeholders.  

## extended topics in active learning

### mlops and production integration

Deploying an active learning system into a continuous production environment requires addressing:
1. **Monitoring**: Tracking query volume, label accuracy, model performance drift.  
2. **Automation**: Setting triggers that automatically open labeling tasks when certain conditions are met (e.g., a certain fraction of newly arrived data is flagged as uncertain).  
3. **CI/CD**: The model may be re-trained iteratively as new labels arrive, so continuous integration and deployment pipelines must handle repeated updates.

These challenges relate to MLOps, the broader discipline of managing machine learning systems at scale. Tools like Kubeflow, MLflow, or DVC can be adapted to store labeling events, track model versions, and orchestrate the repeated training cycles. Logging queries and associated label outcomes is essential for reproducibility and auditing.

### hybrid human-ai annotation workflows

Many organizations combine crowd-sourced labeling with internal expert labeling. This can be beneficial for tasks that require specialized knowledge but also have subtasks that are relatively straightforward. Active learning can dynamically route samples to the appropriate labeler type:
- **High-uncertainty or domain-specific**: Route these to in-house experts.
- **Low-uncertainty or simpler**: Route these to crowd workers, which is cheaper and faster in bulk.

Such hierarchical or multi-tier labeling approaches can dramatically reduce overall costs, especially if the system does a good job at triaging. Interfaces that highlight specific instructions for the crowd or for experts also help maintain label consistency.

### cost-sensitive active learning

Not all labeling tasks cost the same. For instance, in medical imaging, labeling an MRI might require a radiologist's time, whereas labeling a standard X-ray is cheaper. Cost-sensitive active learning methods incorporate a cost function <Latex text="\(C(x)\)"/> that depends on the type of data point <Latex text="\(x\)"/> or the labeling process. The goal shifts from "maximize accuracy per label" to "maximize accuracy per cost." If some queries are more expensive, the system might pick them less frequently unless their expected contribution to accuracy is proportionally larger.

In high-stakes domains, cost might also include potential errors — i.e., the cost of mislabeling or misclassifying an example, which can be integrated into a risk-based active learning framework.  

### cross-domain active learning

Cross-domain active learning extends the standard framework to situations where you have labeled data in one domain (source domain) but want to label and learn in a different, potentially related domain (target domain). If the source and target domains share some structure or feature representation, the model can leverage the source-labeled examples as partial prior knowledge. The active strategy helps identify which target-domain points are critical to label for bridging the gap between domains.

This approach is increasingly relevant in multi-lingual NLP, cross-lingual speech recognition, or adapting from synthetic simulations (where labeled data is easy to generate) to real-world data (where labeling is expensive). One must be careful about negative transfer: if the domains are too dissimilar, the model's prior assumptions might be misleading.

## conclusion and references

Active learning is a powerful approach to reducing the labeling burden, enabling iterative refinement of machine learning models through targeted queries to an oracle. By focusing on the most "informative" samples — whether measured by uncertainty, disagreement among committee models, expected reduction in error, or representativeness — an active learner can achieve excellent performance with fewer labeled examples. Real-world constraints like labeling cost, domain expertise, class imbalance, and evolving data streams complicate the design of active learning pipelines, but also underscore their practical value.

Looking ahead, we can expect continued progress in scaling active learning techniques to massive datasets, integrating them more deeply with MLOps pipelines, and refining strategies that adapt to shifting data distributions. Greater emphasis on interpretable query selection and cost-sensitive or risk-aware methods will undoubtedly emerge, reflecting the growing demand for transparency and efficiency in industrial and scientific machine learning applications.

**References and further reading**:
1. Settles, Burr. "Active Learning Literature Survey." Computer Sciences Technical Report 1648, University of Wisconsin–Madison, 2009.
2. Sener, Ozan, and Silvio Savarese. "Active Learning for Convolutional Neural Networks: A Core-Set Approach." International Conference on Learning Representations (ICLR), 2018.
3. Kirsch, Andreas, Joost van Amersfoort, and Yarin Gal. "BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning." Advances in Neural Information Processing Systems (NeurIPS), 2019.
4. Bouneffouf, Djallel, and Ritesh Noothigattu. "Survey on Applications of Multi-Armed and Contextual Bandits." arXiv:1707.00424, which also discusses active exploration approaches.
5. Zhang, Yi. "Active Learning (Lecture Notes)." Carnegie Mellon University, 2011 <Tooltip text="Slides available online"/>.
6. Valiant, Leslie G. "A Theory of the Learnable." Communications of the ACM, 1984, which laid foundations for query-based learning in the PAC framework.
7. Contributions from the domain of query-by-committee: Seung, H.S., Opper, M., and Sompolinsky, H. "Query by Committee." COLT, 1992.
8. <Highlight>MachineLearning.ru resources</Highlight>: Various lectures from K.V. Vorontsov on active learning strategies (in Russian).
9. [Official modAL documentation](https://modal-python.readthedocs.io/en/latest/)
10. [ALiPy GitHub repository](https://github.com/datamllab/ALiPy)

I recommend exploring these works and trying out code examples with a small pilot project to confirm that active learning provides significant advantages in your specific application context. By combining the right strategy, an appropriate model, and well-designed annotation interfaces, you can drastically reduce labeling efforts and still achieve robust performance in a fraction of the time.