---
index: 62
indexCourse: 98
indexFavorites:
title: "Intro to Computer Vision"
titleDetailed: ""
titleSEO: ""
titleOG: ""
titleTwitter: ""
titleCourse: "Intro to Computer Vision"
courseCategoryName: "Computer vision"
desc: "The mathematics of motion"
descSEO: ""
descOG: ""
descTwitter: ""
date: "17.07.2023"
updated:
prioritySitemap: 0.6
changefreqSitemap: "monthly"
extraReadTimeMin: 30
difficultyLevel: 1
flagDraft: true
flagMindfuckery: false
flagRewrite: false
flagOffensive: false
flagProfane: false
flagMultilingual: false
flagUnreliably: false
flagPolitical: false
flagCognitohazard: false
flagHidden: false
flagWideLayoutByDefault: true
schemaType: "Article"
mainTag: ""
otherTags: [""]
keywordsSEO: [""]
banner: "../../../images/posts/research/banners/introduction_to_computer_vision.jpg"
imageOG: ""
imageAltOG: ""
imageTwitter: ""
imageAltTwitter: ""
canonicalURL: "https://avrtt.github.io/research/intro_to_computer_vision"
slug: "/research/intro_to_computer_vision"
---

import Highlight from "../../../components/Highlight"
import Code from "../../../components/Code"
import Latex from "../../../components/Latex"


{/* *(intro: a quote, catchphrase, joke, etc.)* */}

<br/>


{/*

Unit 1:
https://huggingface.co/learn/computer-vision-course/en/unit0/welcome/welcome
Компьютерное зрение
https://neerc.ifmo.ru/wiki/index.php?title=%D0%9A%D0%BE%D0%BC%D0%BF%D1%8C%D1%8E%D1%82%D0%B5%D1%80%D0%BD%D0%BE%D0%B5_%D0%B7%D1%80%D0%B5%D0%BD%D0%B8%D0%B5
[https://www.youtube.com/playlist?list=PLzMcBGfZo4-lUA8uGjeXhBUUzPYc6vZRn](https://www.youtube.com/playlist?list=PLzMcBGfZo4-lUA8uGjeXhBUUzPYc6vZRn)  
[https://www.youtube.com/watch?v=oXlwWbU8l2o](https://www.youtube.com/watch?v=oXlwWbU8l2o)  
[https://www.youtube.com/watch?v=01sAkU_NvOY](https://www.youtube.com/watch?v=01sAkU_NvOY)  
[https://jakevdp.github.io/PythonDataScienceHandbook/05.14-image-features.html](https://jakevdp.github.io/PythonDataScienceHandbook/05.14-image-features.html)
Kaggle Course: Computer Vision (не забыть выложить задания на GitHub)
Artificial Intelligence: A Modern Approach, Global Edition, 4ed: chapter 27

Вопросы разобрать и запостить в ML-канал:
- Что такое и для чего нужен слой batchnorm
- Что такое Focal Loss?
- Attention в компьютерном зрении
- Разница между сингл шот сетками и 2х стадийными
- RetinaNet, Faster R-CNN, Effdet, DETR, их особенности
- DW-свертки
- Основное отличие GAN от VAE

*/}


{/*

1. Introduction to Computer Vision  
- Definition of computer vision: Involves enabling machines to interpret and understand visual information (images, videos). Explores how to mimic aspects of human sight and perception using algorithms and deep learning methods.  
- Historical context: Early pattern recognition research, the role of edge detection in the 1970s, the emergence of neural networks in the late 1980s, and breakthroughs in deep learning since the 2010s.  
- Common real-world applications: Autonomous driving (object detection, lane detection), facial recognition systems, medical imaging diagnostics, robotics (navigation, grasping), and numerous industrial inspection tasks.  
2. Fundamentals of Image Processing and Representation  
- Image formation: Basic pinhole camera model, intrinsic and extrinsic camera parameters, pixel-based representation, and concepts like resolution and aspect ratio.  
- Thresholding, filtering, edge detection: Essential preprocessing operations used to enhance or extract specific features, such as binary thresholding, Gaussian blurring, Sobel edges, and Canny edge detection.  
- Color spaces and transformations: Understanding RGB, HSV, Lab, and YCrCb color models, along with conversions for better feature extraction or segmentation tasks.  
- Additional techniques: Morphological operations (erosion, dilation), geometric transformations (rotation, scaling), and histogram equalization for contrast enhancement.
3. Essential Tools and Libraries  
- OpenCV basics: Installation and setup (via pip or source compilation), reading and writing images, loading videos, and camera interfacing. Covers commonly used functions such as resizing, cropping, and image manipulation.  
- Other popular Python libraries: NumPy for numerical computations (matrix operations on images), Matplotlib for data visualization and plotting results, and scikit-image for additional image processing techniques.  
- Additional frameworks: PyTorch and TensorFlow for deep learning workflows, offering tools like automatic differentiation, large-scale training optimizations, and model deployment options.
4. Deep Learning for Computer Vision  
- Convolution and pooling layers: Mechanisms of local receptive fields, parameter sharing, stride, padding, and the use of pooling (max or average) for dimensionality reduction.  
- Batch normalization: Technique to normalize activations within a mini-batch, stabilizing and speeding up training.  
- Depthwise separable (DW) convolution: Lightweight convolution approach used in architectures like MobileNet to reduce model size and computational cost.  
- Popular CNN architectures: LeNet for handwritten digits, AlexNet as a breakthrough in ImageNet competition, VGG for deeper networks, and ResNet with skip connections to combat vanishing gradients.  
- Training basics (optimization, backpropagation): Gradient descent, stochastic gradient descent, momentum, Adam optimizer, and details on gradient flow through multilayer networks.  
- Regularization and data augmentation: Dropout, L2 regularization, random flips, rotations, and other strategies to reduce overfitting.  
- Transfer learning: Fine-tuning large pre-trained models on new tasks to leverage knowledge from vast datasets such as ImageNet.
5. Object Detection and Beyond  
- Single-shot networks vs two-stage networks: YOLO and SSD as single-stage detectors for real-time performance, contrasted with two-stage methods (Faster R-CNN) that use region proposals for improved accuracy.  
- Focal loss: A specialized loss function used in class-imbalanced detection tasks, helping models focus on hard, misclassified examples.  
- RetinaNet, Faster R-CNN, EffDet, and DETR: Key detection architectures, including transformer-based approaches (DETR) and efficient design paradigms (EffDet).  
- Image segmentation extensions: Semantic vs. instance segmentation, the role of Mask R-CNN in combining detection with segmentation.
6. Attention-Based Methods  
- Attention mechanism in computer vision: Self-attention to capture global relationships in an image without relying solely on local receptive fields.  
- Vision Transformers (ViT) and related architectures: Splitting images into patches and applying Transformer blocks, leading to state-of-the-art results in various benchmarks.  
- Hybrid approaches: Combining CNNs with attention modules or partially replacing convolution layers with attention blocks for improved performance in tasks like classification and detection.  
- Future directions: Scaling vision transformers with sparse attention, exploring hierarchical transformers (Swin Transformer), and leveraging multimodal data (e.g., text + vision).
7. Generative Models in Computer Vision  
- Overview of generative models: Techniques that learn underlying data distributions to synthesize realistic samples, handle missing data, or create novel content.  
- The main difference between GAN and VAE: Adversarial training in GANs versus reconstruction-based probabilistic approach in VAEs.  
- Advanced generative techniques: WGAN for stable training, DCGAN for large-scale image generation, and normalizing flows for exact likelihood-based modeling.  
- Applications of generative models: Image synthesis, style transfer, super-resolution, image inpainting, and domain adaptation (e.g., converting sketches to realistic images).
8. Advanced Topics and Future Outlook  
- 3D computer vision and depth estimation: LIDAR-based perception, structure from motion (SfM), stereo vision, and SLAM for robotics and AR/VR.  
- Reinforcement learning for visual tasks: Learning policies directly from pixel input for control and navigation.  
- Multimodal learning: Combining text and image inputs (e.g., image captioning, visual question answering) or integrating audio, sensors, and other data sources.  

*/}


Computer vision is a subfield of artificial intelligence and machine learning dedicated to enabling machines to interpret and derive meaningful information from visual data, such as images and videos. At its core, computer vision seeks to mimic aspects of human visual perception and understanding, but it also aims to surpass our capabilities by virtue of speed, consistency, and the ability to leverage massive datasets. In practice, this involves the development of algorithms, models, and pipelines that can detect, classify, localize, segment, and track objects or regions of interest within a visual scene. By transforming raw pixel intensities into higher-level insights, computers can then automate tasks — from recognizing faces to guiding autonomous vehicles — that traditionally required direct human visual expertise.

While early theoretical work in computer vision focused on simple edge detection and feature extraction, rapid advances in both hardware (GPUs, specialized AI accelerators) and software (deep learning frameworks, optimized libraries for high-throughput computation) have propelled the field to new frontiers. Today, computer vision technology underpins numerous industrial and consumer applications, driving innovation across multiple sectors and domains.

### historical context
The development of computer vision spans several decades, with its roots in the early pattern recognition and artificial intelligence research of the 1960s and 1970s. During the 1970s, classical edge detection research (e.g., the Sobel filter, Canny edge operator) laid the groundwork for methods that transformed images into sets of meaningful features. Various researchers realized that robust image segmentation and feature extraction were vital to any form of higher-level recognition or scene understanding.

In the 1980s, the introduction of neural networks — particularly the work on the Neocognitron and early multilayer perceptrons — began to show promise for visual pattern recognition. However, hardware limitations and the scarcity of large labeled datasets made it challenging to train these models to handle complex real-world data. Over time, new techniques emerged to address these limitations, such as Support Vector Machines (SVMs) for image classification and various feature descriptor methods (SIFT, SURF, HOG) for object detection and recognition. During the 2000s, these methods became standard in practical computer vision pipelines, especially for tasks that required robust matching and recognition in real-world conditions.

A watershed moment occurred in 2012 when a deep convolutional neural network (AlexNet) trained on the massive ImageNet dataset achieved a dramatic improvement in image classification performance (Krizhevsky and gang, NIPS 2012). This victory in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) ignited a frenzy of research and innovation, ushering in what is often called the "deep learning era" of computer vision. Successive breakthroughs — VGG, GoogLeNet, ResNet, DenseNet, EfficientNet, and Vision Transformers (ViTs) — have since led to steady improvements in accuracy, speed, and robustness.

### common real-world applications
Computer vision's influence spans a vast array of fields, reflecting the ubiquity of visual data in modern life:

1. **Autonomous driving**: Self-driving cars rely on object detection and tracking to identify vehicles, pedestrians, traffic lights, and road signs. Lane detection and free-space segmentation help guide steering and ensure safety.

2. **Facial recognition**: Used extensively in security systems, personal device access, surveillance, and social media platforms for tagging and identity verification. 

3. **Medical imaging**: Radiology, pathology, and other medical disciplines increasingly leverage computer vision to detect abnormalities such as tumors in MRI or CT scans. Automated systems can assist doctors in diagnosis and treatment planning, reducing human error and improving patient outcomes.

4. **Robotics**: In industrial settings, computer vision assists robots in tasks like pick-and-place, assembly, and inspection. Mobile robots utilize vision-based simultaneous localization and mapping (SLAM) for navigation and obstacle avoidance.

5. **Agriculture**: Drones and remote sensing cameras identify plant diseases, estimate crop yield, and monitor field conditions to optimize resource usage.

6. **Industrial inspection**: High-throughput camera systems rapidly detect defects in manufacturing lines, ensuring product quality. 

7. **Augmented reality (AR) and virtual reality (VR)**: Vision-based understanding of the user's surroundings allows seamless overlay of virtual objects (AR) or immersive environment creation (VR). 

From broad societal applications like retail checkout systems and traffic management, to specialized fields like marine biology (underwater exploration and species identification), the significance of computer vision continues to grow. The field now stands at the forefront of AI research, with new breakthroughs poised to reshape our interaction with machines.


## Fundamentals of image processing and representation

### image formation
Understanding how images are formed is essential to computer vision, as it lays the groundwork for how machines interpret visual data. A common simplified model of image formation is the <Highlight>pinhole camera model</Highlight>, which conceptualizes how 3D scenes are projected onto a 2D plane (the image sensor).

A pinhole camera, in idealized form, consists of a small aperture through which light rays pass, projecting an inverted view of the scene onto a planar surface. In modern cameras, a lens system replaces the single pinhole, but the geometry of projection remains conceptually similar. 

We describe the mapping from a 3D point <Latex text="\(P = (X, Y, Z)\)"/> to a 2D image coordinate <Latex text="\(p = (x, y)\)"/> often with the equation:

<Latex text="\[
\begin{bmatrix}
x \\
y \\
1
\end{bmatrix}
= K \, [R \mid t] \, 
\begin{bmatrix}
X \\
Y \\
Z \\
1
\end{bmatrix}
\]"/>

Where:
- <Latex text="\(K\)"/> is the camera intrinsic matrix (focal length, principal point, and skew).
- <Latex text="\(R\)"/> is the rotation matrix representing the camera's orientation.
- <Latex text="\(t\)"/> is the translation vector representing the camera's position in the world.

These intrinsic and extrinsic parameters define how the camera captures light and projects it onto the image plane. Conceptually, <Latex text="\(K\)"/> captures the geometry of the camera itself, while <Latex text="\(R\)"/> and <Latex text="\(t\)"/> represent how the camera is oriented and placed in the environment.

Images themselves are often stored as matrices of pixel values, with each pixel encoding color or intensity information. The pixel array has dimensions of resolution (width and height), and each pixel might store color channels (e.g., red, green, and blue) or a single grayscale value. The resolution and aspect ratio (the ratio of width to height) are fundamental descriptors of an image's geometry, influencing both visual quality and computational cost in processing.

### thresholding, filtering, edge detection
Before applying complex recognition algorithms, computer vision pipelines commonly employ classic image processing techniques. These methods can enhance features, suppress noise, or facilitate straightforward object segmentation.

1. **Thresholding**: A simple but powerful segmentation technique that turns a grayscale or color image into a binary image based on a threshold value. For instance, you might convert each pixel <Latex text="\(I(x, y)\)"/> into:

<Latex text="\[
I_{\text{binary}}(x, y) = 
\begin{cases}
1 & \text{if } I(x, y) \ge \tau\\
0 & \text{otherwise}
\end{cases}
\]"/>

where <Latex text="\(\tau\)"/> is a predefined threshold. Otsu's method (Otsu, IEEE Trans. SMC, 1979) automatically selects an optimal threshold by minimizing intra-class intensity variance.

2. **Filtering**: Convolving an image with specific kernels allows enhancement or suppression of certain spatial frequencies. Common examples include:
   - **Gaussian blur**: Uses a Gaussian kernel to smooth the image, typically to reduce noise or detail.
   - **Median filter**: Replaces each pixel with the median of neighboring pixel values, effective against salt-and-pepper noise.
   - **Sharpening filters**: Enhance edges and fine details by emphasizing high-frequency components.

3. **Edge detection**: Detecting boundaries in an image is crucial for feature extraction. Examples include:
   - **Sobel operator**: Computes an approximation of the gradient of intensity, highlighting regions of rapid intensity change.
   - **Canny edge detector** (Canny, IEEE Trans. PAMI, 1986): A multi-stage algorithm involving gradient calculation, non-maximum suppression, and hysteresis thresholding for robust and thin edges.

Edge detection is significant in tasks like object boundary extraction, shape recognition, and image registration. Combined with thresholding or morphological operations, edges provide geometric cues about the underlying structures in an image.

### color spaces and transformations
In computer vision, it is often beneficial to transform from the standard RGB color space into alternative representations that might simplify or improve specific tasks (e.g., segmentation or color-based feature extraction). 

- **HSV (Hue, Saturation, Value)**: Separates color into hue (the "color" component), saturation (the intensity of the color), and value (brightness). This is often more aligned with how humans perceive color, making thresholding or segmentation by color more intuitive.
- **Lab color space**: Decomposes a color into L (lightness) and two chromaticity components a (green-red) and b (blue-yellow). One advantage is its approximate perceptual uniformity — a small Euclidean distance in Lab often corresponds to a small perceived difference in color.
- **YCrCb**: Common in video compression, separates luminance (Y) from chrominance (Cr and Cb) components, which can be processed or compressed differently according to human vision's varying sensitivity.

Sometimes, tasks like skin detection, fruit ripeness assessment, or specialized segmentation benefit greatly from converting an image to a color space where the relevant features (e.g., color hue) appear more distinct.

### additional techniques
Further essential image manipulations serve as building blocks or pre-processing steps:

- **Morphological operations**: In binary images, morphological transformations (e.g., erosion and dilation) can refine or correct the structure of segmented objects. 
  - **Erosion** shrinks the foreground by stripping away boundary pixels, helpful for removing isolated noise.
  - **Dilation** expands foreground regions, bridging gaps or holes within objects.
  - **Opening** (erosion followed by dilation) and **closing** (dilation followed by erosion) can correct small artifacts or join disconnected parts of objects.

- **Geometric transformations**: Rotation, scaling, translation, or more complex transformations like perspective warping. These allow for dataset augmentation or correction of geometric misalignment.

- **Histogram equalization**: Adjusts contrast by redistributing the intensity histogram. A well-known approach is Contrast Limited Adaptive Histogram Equalization (CLAHE), which can locally normalize contrast in different regions of an image without over-amplifying noise.

All these techniques represent valuable steps in a computer vision workflow, often used in tandem with more sophisticated methods for classification or detection. Even in the era of deep learning, traditional image processing remains highly relevant for data preprocessing, augmentation, and interpretability.


## Essential tools and libraries

### OpenCV basics
OpenCV (Open Source Computer Vision Library) is a foundational library for real-time computer vision. With interfaces in C++, Python, Java, and more, it offers numerous functions for image processing, feature detection, video analysis, and machine learning. 

A typical Python installation with OpenCV might look like:

<Code text={`
pip install opencv-python
`}/>

Once installed, you can read, display, and write images:

<Code text={`
import cv2

# Read an image from disk (file path)
image = cv2.imread("my_image.jpg")  

# Display the image in a window
cv2.imshow("Window Name", image)
cv2.waitKey(0)  
cv2.destroyAllWindows()

# Write an image to disk
cv2.imwrite("output_image.png", image)
`}/>

OpenCV also supports capturing frames from webcams or video files, plus a rich array of transformations (resizing, cropping, rotating, morphological operations), thresholding, edge detection, and so on.

### other popular python libraries
- **NumPy**: The fundamental package for scientific computing with Python. Images in OpenCV are typically stored as NumPy arrays, making it easy to perform low-level array manipulations or to integrate OpenCV with other data science or ML libraries.
- **Matplotlib**: A versatile library for plotting and visualizations. Often used in Jupyter notebooks for displaying images inline and generating charts to visualize intermediate results, such as loss curves or detection bounding boxes.
- **scikit-image**: A collection of algorithms for image processing, including advanced techniques for restoration, segmentation, and feature extraction. It sometimes complements or extends what OpenCV offers, especially in areas like morphological filtering and advanced transforms (e.g., Hough transforms).

### additional frameworks
Many tasks in modern computer vision workflows rely on deep learning. Two of the most widespread frameworks are:

1. **PyTorch**: Developed primarily by Meta AI (formerly Facebook AI Research), PyTorch provides a flexible, Pythonic interface for building neural networks. Its dynamic computation graph has made it extremely popular for research, while support for accelerated training on GPUs or specialized hardware ensures it remains efficient for large-scale production tasks.

2. **TensorFlow**: An end-to-end open-source platform for machine learning created by Google. TensorFlow also has high-level APIs like Keras for streamlined model prototyping, plus TensorBoard for visualization of training metrics. TensorFlow Lite targets mobile and embedded applications.

Using these frameworks for computer vision tasks typically involves the following steps:
- Loading or generating datasets (images, bounding boxes, segmentation masks).
- Constructing a neural network architecture (Convolutional Neural Networks, Transformers, etc.).
- Defining a loss function and optimization strategy.
- Training the model on GPUs or specialized accelerators.
- Evaluating performance on validation and test sets.
- Deploying the trained model to production (e.g., using TensorFlow Serving, TorchServe, or exporting to an ONNX format).

In practice, combining domain-specific libraries (OpenCV, scikit-image) with deep learning frameworks (PyTorch, TensorFlow) yields a powerful environment for tackling a wide range of computer vision challenges.


## deep learning for computer vision

### convolution and pooling layers
Convolutional neural networks (CNNs) are central to modern computer vision. Their core operation, the 2D convolution, serves as a feature extractor that learns spatial hierarchies of patterns directly from data. The convolution operation for a single 2D feature map can be expressed as:

<Latex text="\[
y_{k, l} = \sum_{i}\sum_{j} X_{k+i, l+j}\, W_{i, j} + b
\]"/>

where:
- <Latex text="\(X_{k+i, l+j}\)"/> is the input feature map's pixel (or neuron) value at position <Latex text="(k+i, l+j)"/>.  
- <Latex text="\(W_{i, j}\)"/> is the weight of the kernel filter at offset <Latex text="(i, j)"/>.  
- <Latex text="\(b\)"/> is the bias term, often included in the operation.  

Key concepts:
- **Local receptive fields**: Each filter covers only a small spatial area of the input. 
- **Stride**: How many pixels the filter window moves each step.
- **Padding**: Adding zero (or other) values around the input edges, controlling the spatial dimensionality of the output.

Pooling layers (max or average pooling) reduce the spatial resolution by aggregating information, thereby reducing the parameter count and controlling overfitting. A max pooling layer with size 2×2 and stride 2, for example, splits the feature map into non-overlapping 2×2 blocks, taking the maximum value in each block.

### batch normalization
Batch normalization normalizes the activations of each layer within a mini-batch. By preventing large shifts in the distribution of intermediate layer outputs, batch normalization:

- Accelerates training by allowing larger learning rates.
- Stabilizes gradient flow.
- Acts as a regularizer, often reducing the need for other forms of regularization.

It can be expressed as:

<Latex text="\[
\hat{x} = \frac{x - \mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^{2} + \epsilon}}, \quad
y = \gamma \hat{x} + \beta
\]"/>

Where <Latex text="\(x\)"/> is an activation within the mini-batch <Latex text="(\mathcal{B})"/>; <Latex text="\(\mu_{\mathcal{B}}\)"/> and <Latex text="\(\sigma_{\mathcal{B}}^{2}\)"/> are the mean and variance within that mini-batch; <Latex text="\(\gamma\)"/> and <Latex text="\(\beta\)"/> are trainable parameters that allow the normalized activations to scale and shift.

### depthwise separable (DW) convolution
A standard 2D convolution simultaneously learns filters for both spatial and cross-channel mixing, leading to a large computational overhead for wide or deep networks. Depthwise separable convolution breaks down the convolution into two parts:
1. **Depthwise convolution**: Applies a single filter to each input channel separately.
2. **Pointwise convolution**: Combines the separate channel outputs with a 1×1 convolution that mixes them.

This factorization drastically reduces the number of parameters and multiplications. Used in architectures like MobileNet (Howard and gang, arXiv 2017) and Xception (Chollet, CVPR 2017), depthwise separable convolutions are particularly handy for mobile and embedded vision applications.

### popular CNN architectures
1. **LeNet** (LeCun and gang, 1990s): Pioneered convolutional networks for digit recognition on the MNIST dataset. Uses a few convolution layers followed by fully connected layers.

2. **AlexNet** (Krizhevsky and gang, NIPS 2012): Set a new state of the art on ImageNet classification with deeper architecture and GPU training. Introduced ReLU activations for faster training.

3. **VGG** (Simonyan and Zisserman, ICLR 2015): Demonstrated that deeper networks (16 or 19 layers) with small 3×3 convolutions significantly improve accuracy. However, it was computationally expensive.

4. **GoogLeNet / Inception** (Szegedy and gang, CVPR 2015): Proposed the Inception module that computes multiple filter sizes in parallel and concatenates them, improving efficiency and multi-scale feature extraction.

5. **ResNet** (He and gang, CVPR 2016): Introduced residual connections that allow gradients to flow unimpeded through deeper networks, tackling the vanishing gradient problem. Some ResNet variants exceed 100 layers in depth.

6. **DenseNet** (Huang and gang, CVPR 2017): Connects each layer to every other layer in a dense connectivity pattern, reducing parameter count and improving feature reuse.

7. **EfficientNet** (Tan and Le, ICML 2019): Scales width, depth, and resolution in a principled way to find more efficient model families.

### training basics
Deep learning relies heavily on efficient optimization methods to adjust network parameters:

- **Gradient descent** and **Stochastic Gradient Descent (SGD)** are the standard. 
- **Momentum**-based methods help accelerate training and escape local minima by adding a fraction of the previous update to the current update.
- **Adam** blends momentum and RMSProp ideas, adapting the learning rate for each parameter based on first and second moments of gradients.

During **backpropagation**, each layer's parameters are updated by computing the gradient of the loss function with respect to those parameters. The process is repeated over many epochs (full passes through the dataset), ideally converging to a local (or global) minimum in the parameter space.

### regularization and data augmentation
Deep neural networks are prone to overfitting, especially if the training data is limited. Common techniques to address overfitting:

1. **Dropout**: Randomly drops a fraction of neurons (and their connections) during training, preventing co-adaptation of features.
2. **L2 regularization**: Penalizes large weights by adding a term <Latex text="\(\lambda ||W||_2^2\)"/> to the loss function, encouraging smaller parameter values.
3. **Data augmentation**: Artificially expands the dataset with label-preserving transformations (random crops, flips, rotations, color jitter). Networks exposed to these variants generalize better.

### transfer learning
Modern deep networks typically require large datasets and extensive compute resources. Transfer learning addresses this by taking a network pre-trained on a large dataset like ImageNet and fine-tuning its weights on a new, typically smaller dataset. This approach often yields strong performance with significantly reduced training time. You typically freeze the early layers that contain general feature extractors (edges, textures) and adapt the latter layers for the new classification or detection task.


## object detection and beyond

### single-shot networks vs two-stage networks
Object detection extends image classification by localizing objects within the image. Two principal design paradigms exist:

1. **Single-stage detectors**: Predict bounding boxes and class probabilities directly from the feature map in a single pass. Examples:
   - **YOLO (You Only Look Once)** (Redmon and gang, CVPR 2016, later variants like YOLOv3, YOLOv5) 
   - **SSD (Single Shot Detector)** (Liu and gang, ECCV 2016)

   Single-stage detectors can be very fast and are suitable for real-time applications, though they sometimes trade off accuracy for speed.

2. **Two-stage detectors**: Use a region proposal mechanism (e.g., RPN in Faster R-CNN) to suggest candidate object regions, and then classify and refine these proposals in a second step. This typically yields higher accuracy but at a computational cost. 
   - **Faster R-CNN** (Ren and gang, NeurIPS 2015) remains a popular benchmark for many detection tasks.

### focal loss
In problems with class imbalance or tasks where many examples belong to the "background" class (e.g., small objects vs. large background), standard cross-entropy can overemphasize the majority classes. **Focal loss** (Lin and gang, ICCV 2017) modifies cross-entropy by introducing a factor <Latex text="\((1 - p_t)^\gamma\)"/> that down-weights easy examples so that the model focuses more on the hard, misclassified ones:

<Latex text="\[
\text{FL}(p_t) = - \alpha_t (1 - p_t)^\gamma \log(p_t)
\]"/>

where:
- <Latex text="\(p_t\)"/> is the model's estimated probability for the correct class.
- <Latex text="\(\alpha_t\)"/> is a weighting factor for class imbalance.
- <Latex text="\(\gamma\)"/> is the focusing parameter that adjusts how heavily the loss penalizes well-classified examples.

### retinaNet, faster r-cnn, effdet, and detr
Beyond YOLO and SSD, several object detection architectures illustrate the evolution of the field:

- **RetinaNet**: A single-stage detector that uses focal loss to handle class imbalance effectively, achieving competitive accuracy with two-stage methods.
- **Faster R-CNN**: Often considered the standard two-stage approach, balancing speed and accuracy. 
- **EffDet** (EfficientDet, Tan and gang, CVPR 2020): Builds on EfficientNet backbones and a new BiFPN feature pyramid to achieve strong accuracy-speed tradeoffs.
- **DETR** (Carion and gang, ECCV 2020): Introduces transformer-based object detection, removing the need for many hand-crafted components like non-maximum suppression and anchor generation. DETR directly predicts bounding boxes as sets, using the attention mechanism to capture global relationships.

### image segmentation extensions
**Semantic segmentation** classifies each pixel into a semantic class (e.g., road, building, car). **Instance segmentation** goes further, distinguishing individual object instances of the same class. 

- **Mask R-CNN** (He and gang, ICCV 2017) extends Faster R-CNN by adding a parallel branch that outputs segmentation masks for each detected instance. This architecture is widely used in medical imaging and tasks that need pixel-level understanding, such as robotic grasping of objects with complex shapes.


## attention-based methods

### attention mechanism in computer vision
Attention mechanisms were first popularized in natural language processing. However, the concept of letting a model learn "what to focus on" in the input has proven highly useful in computer vision as well. Unlike convolution filters that capture local patterns, self-attention mechanisms can capture long-range dependencies across an entire image or feature map. This ability helps a model learn relationships between distant parts of an image, potentially leading to more robust representations.

### vision transformers (vit) and related architectures
**Vision Transformers (ViT)** (Dosovitskiy and gang, ICLR 2021) adapt the original transformer architecture from NLP to image recognition. They split the image into a sequence of patches (e.g., 16×16 pixels each), flatten them, and embed them with positional encodings. The transformer blocks then apply multi-head self-attention to these patch embeddings, effectively modeling the entire image as a sequence of tokens.

ViT-like architectures have rapidly gained popularity due to their ability to scale with model size and training data. Notable improvements and variants include:
- **DeiT** (Touvron and gang, ICML 2021): Demonstrates data-efficient training strategies for Vision Transformers.
- **Swin Transformer** (Liu and gang, ICCV 2021): Uses a hierarchical design with shifted windows, capturing local relationships efficiently while enabling a global receptive field.

### hybrid approaches
Some architectures combine CNNs and attention modules, benefiting from the local inductive biases of convolutions and the global modeling power of attention. For example:
- **CNN + SE (Squeeze-and-Excitation) blocks** (Hu and gang, CVPR 2018): Weights channels adaptively based on global context.
- **ConViT** merges convolutional tokens with transformer blocks, seeking an architecture that is both robust to local distortions and capable of capturing long-range interactions.

These hybrids often appear in detection or segmentation contexts, where multi-scale feature representations (typical of CNNs) merge well with the expressive capacity of attention.

### future directions
Ongoing research explores:
- **Sparse attention** patterns to reduce computational cost in large images.
- **Hierarchical transformers** that can handle higher resolutions more efficiently.
- **Multimodal transformers** integrating language, audio, or other sensors into vision tasks (e.g., text + image for image captioning or visual question answering).

The promise of attention-based methods is vast, especially as compute resources grow and training strategies become more refined.


## generative models in computer vision

### overview of generative models
Generative models learn to capture the distribution of data. If a model <Latex text="\(p_{\theta}(x)\)"/> is a good approximation of the real data distribution <Latex text="\(p_{\text{data}}(x)\)"/>, then sampling from <Latex text="\(p_{\theta}(x)\)"/> will produce new data points that resemble the real examples. In computer vision, these models enable:
- **Synthesis of new, realistic images**.
- **Interpolation** between data points (e.g., generating novel faces).
- **Filling in missing data** (inpainting).
- **Domain adaptation** (sketch to realistic image, summer to winter scenes).

### the main difference between gan and vae
Two popular generative approaches are **GANs** (Generative Adversarial Networks) and **VAEs** (Variational Autoencoders).

1. **GAN** (Goodfellow and gang, NeurIPS 2014): Trains two networks in an adversarial game. A generator <Latex text="G"/> tries to produce realistic samples from random noise, while a discriminator <Latex text="D"/> attempts to distinguish real samples from generated ones. Training aims to converge when <Latex text="G"/> can consistently fool <Latex text="D"/> while <Latex text="D"/> remains accurate on real vs. fake samples. 
   - Typically produce sharper images but can be challenging to stabilize during training.

2. **VAE**: Uses a probabilistic encoder-decoder structure. The encoder maps inputs <Latex text="\(x\)"/> to a latent distribution <Latex text="\(q_{\phi}(z|x)\)"/>, while the decoder reconstructs <Latex text="\(x\)"/> from latent variable <Latex text="\(z\)"/>. Optimization involves maximizing the Evidence Lower BOund (ELBO):
   <Latex text="\[
   \mathcal{L}(\theta, \phi; x) = \mathbb{E}_{z \sim q_{\phi}(z|x)}[\log p_{\theta}(x|z)] 
   - D_{\text{KL}}\bigl(q_{\phi}(z|x) \parallel p(z)\bigr)
   \]"/>
   VAEs tend to produce more "blurry" outputs but have a principled probabilistic foundation and stable training dynamics.

### advanced generative techniques
- **WGAN (Wasserstein GAN)** (Arjovsky and gang, ICML 2017): Uses the Earth Mover's distance to improve training stability and measure the quality of samples more meaningfully.
- **DCGAN (Deep Convolutional GAN)** (Radford and gang, ICLR 2016): Applies CNNs in both generator and discriminator, enabling large-scale stable training.
- **Normalizing flows**: Models (e.g., RealNVP, Glow) that transform a simple distribution (like a Gaussian) to match the target distribution exactly, allowing exact likelihood calculation.

### applications of generative models
Generative models have shown remarkable potential in various tasks:
1. **Image synthesis**: Generating entirely new images, such as faces or artwork. 
2. **Style transfer**: Combining the content of one image with the style of another. This was popularized by neural style transfer methods, which are partially generative in nature.
3. **Super-resolution**: Enhancing the resolution of low-quality images (e.g., photo restoration).
4. **Inpainting**: Filling in missing or corrupted regions of an image in a visually plausible way.
5. **Domain adaptation**: CycleGAN, for instance, can map images between two domains (summer ↔ winter landscapes) without needing paired training data.

Generative modeling continues to evolve rapidly, intersecting with vision transformers, diffusion models, and advanced adversarial setups, which push the frontier of image realism and creative AI outputs.


## advanced topics and future outlook

### 3d computer vision and depth estimation
Moving beyond 2D images, 3D computer vision deals with reconstructing, understanding, and manipulating 3D information from multiple images or specialized sensors. Common approaches include:
- **LIDAR-based perception**: Used in autonomous driving to get precise depth measurements. 
- **Structure from Motion (SfM)** and **multi-view stereo**: Recovers 3D structures by analyzing correspondences in overlapping images.
- **Stereo vision**: Exploits two parallel cameras to estimate disparity maps, which correlate to depth.
- **SLAM (Simultaneous Localization and Mapping)**: Used heavily in robotics and AR/VR. A device or robot incrementally builds a map of an unknown environment while keeping track of its own location.

3D understanding also includes pose estimation of objects, scene flow (the 3D extension of optical flow), and 3D shape reconstruction, enabling applications in robotics, augmented reality, and digital content creation.

### reinforcement learning for visual tasks
Reinforcement learning (RL) addresses sequential decision-making, where an agent interacts with an environment to maximize cumulative reward. For visual tasks, the agent often receives raw pixel data as input. Achieving robust behavior from high-dimensional visual signals is challenging but has led to successes:
- **Atari games**: Agents trained directly from pixel frames can outperform human players (Mnih and gang, Nature 2015).
- **Robotics**: RL can optimize policies for grasping and object manipulation based on camera feeds.
- **Autonomous navigation**: Combining RL with computer vision (such as obstacle detection) for mobile robot path planning.

### multimodal learning
Modern AI systems increasingly integrate multiple data modalities:
- **Vision + language**: E.g., image captioning, visual question answering (VQA). Models must understand an image while also parsing the textual query or generating textual output.
- **Vision + audio**: For instance, cross-modal tasks in robotics, where visual cues are combined with auditory signals.
- **Vision + sensor data**: Depth sensors, thermal cameras, or radar can supplement RGB cameras to improve perception.

The synergy between different modalities can yield richer, more robust representations, reflecting the multi-sensory nature of real-world perception.

### additional areas of active research
Computer vision is a vibrant field, continually pushing boundaries in domains like:
- **Advanced domain generalization**: Models that adapt to new domains without fine-tuning.
- **Long-tail recognition**: Handling rare classes or examples that appear infrequently in training data.
- **Federated learning for edge devices**: Training computer vision models on distributed datasets without centralizing data (protecting privacy).
- **Active learning**: Strategies for selecting the most informative data samples to label, to reduce annotation costs in large-scale vision datasets.

As computational power grows and new paradigms (e.g., foundation models, large-scale multimodal transformers) become ubiquitous, the future of computer vision promises ever more sophisticated capabilities — from real-time scene understanding to creative synthesis of new visual worlds.

<Image alt="Image illustrating a pipeline of modern computer vision tasks" path="" caption="A conceptual diagram showing a pipeline of modern computer vision tasks, including image classification, object detection, segmentation, and 3D reconstruction." zoom="false" />

---

I hope this long-form introduction to computer vision clarifies not only the fundamental image processing steps and key deep learning architectures, but also highlights the remarkable breadth of tasks and ongoing research in the field. By understanding these foundational elements and tracking cutting-edge developments, readers can confidently navigate the ever-evolving landscape of computer vision.