"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[3694],{89445:function(e,t,a){a.r(t),a.d(t,{Head:function(){return z},PostTemplate:function(){return C},default:function(){return _}});var n=a(54506),i=a(28453),r=a(96540),l=a(16886),o=a(46295),s=a(96098);function c(e){const t=Object.assign({p:"p",h3:"h3",a:"a",span:"span",br:"br",h2:"h2",ol:"ol",li:"li",strong:"strong"},(0,i.RP)(),e.components),{Image:a}=t;return a||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),r.createElement(r.Fragment,null,"\n",r.createElement("br"),"\n","\n",r.createElement(t.p,null,'Dimensionality reduction stands as one of the foundational pillars of modern data analysis, serving myriad purposes that range from improved visualization of high-dimensional datasets to facilitating faster and more robust machine learning models. When one is confronted with datasets that have an overwhelming number of features — potentially in the hundreds, thousands, or even millions — performing certain types of computations or building effective predictive models can quickly become unwieldy. This phenomenon, often referred to as the "curse of dimensionality", motivates the creation of methods and algorithms to meaningfully reduce the dimension of the data space, preserving as much relevant information as possible.'),"\n",r.createElement(t.p,null,"Principal Component Analysis (PCA) is historically and practically one of the most widely used linear dimensionality reduction methods. Dating back to the work of Karl Pearson (Pearson, Philosophical Magazine, 1901) and further formalized by Harold Hotelling (Hotelling, 1933), PCA transforms the original features into new, uncorrelated features known as principal components, each of which captures a certain portion of the variance present in the data. These principal components, ordered by descending variance, provide a powerful way to discard noise and redundancy while capturing the main trends in the data."),"\n",r.createElement(t.p,null,"While PCA is linear in nature, it remains highly relevant in contemporary machine learning practices, not least because it is relatively straightforward to implement, computationally tractable (with the help of singular value decomposition implementations), and conceptually intuitive. In combination with more advanced or nonlinear dimensionality reduction algorithms, PCA continues to serve as a baseline technique, a pre-processing method, and a benchmark for comparing new approaches. In this article, I dive into the theoretical underpinnings of PCA, detail its uses, and chart a path through the broader landscape of dimensionality reduction. Along the way, I highlight practical implementation details, advanced modifications, and meaningful real-world applications."),"\n",r.createElement(t.h3,{id:"11-the-curse-of-dimensionality",style:{position:"relative"}},r.createElement(t.a,{href:"#11-the-curse-of-dimensionality","aria-label":"11 the curse of dimensionality permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"1.1 The curse of dimensionality"),"\n",r.createElement(t.p,null,'The phrase "curse of dimensionality" appears frequently in discussions about high-dimensional datasets. At an intuitive level, when the dimensionality of data grows, the volume of the space grows so fast that the available data become sparse, making many density- or distance-based methods lose their effectiveness. Points that seemed close in a couple of dimensions suddenly appear far apart, and classification boundaries might become more complex to discern. Neighborhood-based algorithms such as k-nearest neighbors tend to suffer severely from the curse of dimensionality, because every point is far from every other point in a very high-dimensional space.'),"\n",r.createElement(t.p,null,"Furthermore, a large number of features significantly increases the risk of overfitting. If you feed a machine learning model a massive number of features with limited data, the model can latch onto spurious correlations that do not generalize. Consequently, the need for methods to systematically reduce or select features, typically followed by robust forms of validation, becomes paramount."),"\n",r.createElement(t.h3,{id:"12-motivations-for-dimensionality-reduction-in-machine-learning",style:{position:"relative"}},r.createElement(t.a,{href:"#12-motivations-for-dimensionality-reduction-in-machine-learning","aria-label":"12 motivations for dimensionality reduction in machine learning permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"1.2 Motivations for dimensionality reduction in machine learning"),"\n",r.createElement(t.p,null,"Dimensionality reduction greatly simplifies downstream modeling tasks. By paring down the dataset into fewer and more expressive features, one can:"),"\n",r.createElement(t.p,null,"• Improve interpretability: Instead of analyzing hundreds or thousands of correlated features, a handful of principal components or factors may be easier to interpret or visualize.",r.createElement(t.br),"\n","• Reduce computational complexity: Many algorithms can have run times that scale poorly with the number of features.",r.createElement(t.br),"\n","• Mitigate overfitting: Fewer features often mean fewer parameters to learn, thus reducing the likelihood of memorizing noise.",r.createElement(t.br),"\n","• Aid in data visualization: Projecting the data into two- or three-dimensional space is one of the most straightforward ways to get a visual sense of structure, outliers, or regional clusters."),"\n",r.createElement(t.p,null,"With PCA specifically, the user obtains new axes — principal components — that are linear combinations of the original variables, orthogonal to each other and explaining progressively smaller portions of the total variance. PCA can be considered a stepping stone for many advanced methods in machine learning and data science, and it continues to influence subsequent techniques such as kernel PCA for nonlinear embeddings, manifold learning approaches, and even autoencoder structures in neural networks."),"\n",r.createElement(t.h2,{id:"2-the-conceptual-landscape-of-dimensionality-reduction",style:{position:"relative"}},r.createElement(t.a,{href:"#2-the-conceptual-landscape-of-dimensionality-reduction","aria-label":"2 the conceptual landscape of dimensionality reduction permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2. The conceptual landscape of dimensionality reduction"),"\n",r.createElement(t.p,null,"Before diving into PCA's minute details, it is instructive to situate PCA within the broader context of dimensionality reduction methods. Although PCA is arguably the most popular technique, there exist many other approaches. Some focus on linear transformations and rely on matrix factorization, while others attempt to preserve local neighborhoods or manifold structures by means of sophisticated nonlinear embeddings."),"\n",r.createElement(t.h3,{id:"21-linear-vs-non-linear-approaches",style:{position:"relative"}},r.createElement(t.a,{href:"#21-linear-vs-non-linear-approaches","aria-label":"21 linear vs non linear approaches permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.1 Linear vs. non-linear approaches"),"\n",r.createElement(t.p,null,"Dimensionality reduction can be roughly categorized into linear and nonlinear methods:"),"\n",r.createElement(t.p,null,"• Linear techniques: These include PCA, Factor Analysis (FA), Linear Discriminant Analysis (LDA), and standard random projection methods. They transform the data using linear maps, typically manifested as matrix transformations.",r.createElement(t.br),"\n","• Nonlinear techniques: Algorithms such as kernel PCA, t-SNE (Maaten & Hinton, JMLR, 2008), UMAP, isomap, and locally linear embedding attempt to preserve manifold structures or distances in a more intricate fashion. By incorporating kernels or specialized distance metrics, these methods can capture more complex relationships that do not align with linear subspaces."),"\n",r.createElement(t.h3,{id:"22-feature-selection-vs-feature-extraction",style:{position:"relative"}},r.createElement(t.a,{href:"#22-feature-selection-vs-feature-extraction","aria-label":"22 feature selection vs feature extraction permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.2 Feature selection vs. feature extraction"),"\n",r.createElement(t.p,null,"It also helps to distinguish between feature selection and feature extraction:"),"\n",r.createElement(t.p,null,"• Feature selection: The technique of identifying and retaining a subset of the original features that hold the highest predictive or explanatory power, leaving them otherwise unchanged.",r.createElement(t.br),"\n","• Feature extraction: The practice of creating new features by combining existing ones, usually through a mathematically informed transformation such as PCA or autoencoders."),"\n",r.createElement(t.p,null,"This article focuses on feature extraction, specifically linear transformations based on covariance structure — i.e., PCA."),"\n",r.createElement(t.h3,{id:"23-manifold-learning-perspective",style:{position:"relative"}},r.createElement(t.a,{href:"#23-manifold-learning-perspective","aria-label":"23 manifold learning perspective permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.3 Manifold learning perspective"),"\n",r.createElement(t.p,null,'Modern viewpoints often treat high-dimensional data as lying (approximately) on or near low-dimensional manifolds. The idea is that, even if we have thousands of coordinates describing a single sample, the "intrinsic dimension" of that sample might actually be quite small. Manifold learning algorithms such as isomap or t-SNE attempt to uncover this lower-dimensional structure. PCA, as a linear method, can be seen as finding the best linear manifold — a plane, or hyperplane, spanning a subspace — that approximates the data under the assumption that linear relationships are a main driver of covariance.'),"\n",r.createElement(t.h2,{id:"3-the-theoretical-foundation-of-pca",style:{position:"relative"}},r.createElement(t.a,{href:"#3-the-theoretical-foundation-of-pca","aria-label":"3 the theoretical foundation of pca permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3. The theoretical foundation of PCA"),"\n",r.createElement(t.p,null,"Principal Component Analysis can be introduced via at least three related perspectives:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Maximize variance"),": Find directions (vectors) in the feature space that possess maximal variance."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Minimize reconstruction error"),": Find the best rank-",r.createElement(s.A,{text:"\\(k\\)"})," linear subspace that allows you to reconstruct the original data."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Orthogonal transformation of the covariance matrix"),": Diagonalize the data's covariance matrix to obtain uncorrelated variables ordered by their variance."),"\n"),"\n",r.createElement(t.p,null,"These three interpretations all converge to the same mathematical machinery — the covariance matrix and its eigen-decomposition (or equivalently, the singular value decomposition of the data matrix)."),"\n",r.createElement(t.h3,{id:"31-covariance-matrix",style:{position:"relative"}},r.createElement(t.a,{href:"#31-covariance-matrix","aria-label":"31 covariance matrix permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.1 Covariance matrix"),"\n",r.createElement(t.p,null,"Consider a dataset of ",r.createElement(s.A,{text:"\\(N\\)"})," samples, each sample represented as a ",r.createElement(s.A,{text:"\\(d\\)"}),"-dimensional vector ",r.createElement(s.A,{text:"\\(x_i \\in \\mathbb{R}^d\\)"}),". Let ",r.createElement(s.A,{text:"\\(X\\)"})," be the ",r.createElement(s.A,{text:"\\(N \\times d\\)"})," data matrix, where each row corresponds to a sample. For simplicity, we assume the data matrix is mean-centered, such that each column has mean zero. Let ",r.createElement(s.A,{text:"\\(\\bar{x}\\)"})," be the sample mean."),"\n",r.createElement(t.p,null,"The covariance matrix ",r.createElement(s.A,{text:"\\( \\Sigma \\)"})," of the data is given by:"),"\n",r.createElement(s.A,{text:"\\[\n\\Sigma = \\frac{1}{N - 1}\\sum_{i=1}^{N} (x_i - \\bar{x})(x_i - \\bar{x})^\\top\n\\]"}),"\n",r.createElement(t.p,null,"In practice, if ",r.createElement(s.A,{text:"\\(X\\)"})," is already mean-centered, ",r.createElement(s.A,{text:"\\(\\Sigma\\)"})," can be estimated by the matrix multiplication:"),"\n",r.createElement(s.A,{text:"\\[\n\\Sigma = \\frac{1}{N - 1} X^\\top X.\n\\]"}),"\n",r.createElement(t.p,null,"Here, ",r.createElement(s.A,{text:"\\(N\\)"})," denotes the number of samples, ",r.createElement(s.A,{text:"\\(d\\)"})," the number of original features, ",r.createElement(s.A,{text:"\\(x_i\\)"})," the ",r.createElement(s.A,{text:"\\(i\\)"}),"-th sample vector, and ",r.createElement(s.A,{text:"\\(\\bar{x}\\)"})," the sample mean. The matrix ",r.createElement(s.A,{text:"\\(X\\)"})," has dimensions ",r.createElement(s.A,{text:"\\(N \\times d\\)"}),"; thus, ",r.createElement(s.A,{text:"\\(X^\\top\\)"})," is ",r.createElement(s.A,{text:"\\(d \\times N\\)"}),". Multiplying ",r.createElement(s.A,{text:"\\(X^\\top X\\)"})," yields a ",r.createElement(s.A,{text:"\\(d \\times d\\)"})," matrix."),"\n",r.createElement(t.h3,{id:"32-eigenvalues-and-eigenvectors",style:{position:"relative"}},r.createElement(t.a,{href:"#32-eigenvalues-and-eigenvectors","aria-label":"32 eigenvalues and eigenvectors permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.2 Eigenvalues and eigenvectors"),"\n",r.createElement(t.p,null,"Once ",r.createElement(s.A,{text:"\\(\\Sigma\\)"})," is computed, PCA seeks to find its eigenvalues and eigenvectors. We denote the eigenvalue–eigenvector pairs by ",r.createElement(s.A,{text:"\\((\\lambda_k, v_k)\\)"})," for ",r.createElement(s.A,{text:"\\(k = 1, 2, \\ldots, d\\)"}),". That is:"),"\n",r.createElement(s.A,{text:"\\[\n\\Sigma \\, v_k = \\lambda_k \\, v_k,\n\\]"}),"\n",r.createElement(t.p,null,"where ",r.createElement(s.A,{text:"\\(\\lambda_k\\)"})," is the ",r.createElement(s.A,{text:"\\(k\\)"}),"-th eigenvalue and ",r.createElement(s.A,{text:"\\(v_k\\)"})," is the corresponding eigenvector. The eigenvalues, which are non-negative because ",r.createElement(s.A,{text:"\\(\\Sigma\\)"})," is positive semi-definite, quantitatively indicate how much variance is captured in each principal component direction. We commonly rank the eigenvalues in descending order:"),"\n",r.createElement(s.A,{text:"\\( \\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_d \\ge 0. \\)"}),"\n",r.createElement(t.p,null,"The eigenvector ",r.createElement(s.A,{text:"\\(v_1\\)"})," corresponding to the largest eigenvalue ",r.createElement(s.A,{text:"\\(\\lambda_1\\)"})," is the first principal component, and it aligns with the direction of greatest variance. The second principal component ",r.createElement(s.A,{text:"\\(v_2\\)"})," is then orthogonal to ",r.createElement(s.A,{text:"\\(v_1\\)"})," and aligns with the direction of the second greatest variance, and so on."),"\n",r.createElement(t.h3,{id:"33-svd-perspective",style:{position:"relative"}},r.createElement(t.a,{href:"#33-svd-perspective","aria-label":"33 svd perspective permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.3 SVD perspective"),"\n",r.createElement(t.p,null,"An alternative but equivalent approach uses Singular Value Decomposition (SVD) of ",r.createElement(s.A,{text:"\\(X\\)"}),". If ",r.createElement(s.A,{text:"\\(X\\)"})," is an ",r.createElement(s.A,{text:"\\(N \\times d\\)"})," mean-centered matrix, the SVD states:"),"\n",r.createElement(s.A,{text:"\\[\nX = U \\, S \\, V^\\top,\n\\]"}),"\n",r.createElement(t.p,null,"where",r.createElement(t.br),"\n","• ",r.createElement(s.A,{text:"\\(U\\)"})," is an ",r.createElement(s.A,{text:"\\(N \\times N\\)"})," orthonormal matrix,",r.createElement(t.br),"\n","• ",r.createElement(s.A,{text:"\\(S\\)"})," is an ",r.createElement(s.A,{text:"\\(N \\times d\\)"})," rectangular diagonal matrix (its diagonal entries are the singular values),",r.createElement(t.br),"\n","• ",r.createElement(s.A,{text:"\\(V\\)"})," is a ",r.createElement(s.A,{text:"\\(d \\times d\\)"})," orthonormal matrix."),"\n",r.createElement(t.p,null,"The columns of ",r.createElement(s.A,{text:"\\(V\\)"})," (or equivalently the rows of ",r.createElement(s.A,{text:"\\(V^\\top\\)"}),") correspond to the eigenvectors of ",r.createElement(s.A,{text:"\\(\\Sigma\\)"}),". For each singular value ",r.createElement(s.A,{text:"\\(\\sigma_k\\)"}),", we have:"),"\n",r.createElement(s.A,{text:"\\( \\sigma_k^2 = \\lambda_k. \\)"}),"\n",r.createElement(t.p,null,"Hence, the left singular vectors in ",r.createElement(s.A,{text:"\\(U\\)"})," form an orthonormal basis for the row space of ",r.createElement(s.A,{text:"\\(X\\)"}),", and the columns of ",r.createElement(s.A,{text:"\\(V\\)"})," form an orthonormal basis for the column space — precisely the directions corresponding to principal components. SVD-based implementations of PCA tend to be numerically stable and often favored in practice."),"\n",r.createElement(t.h3,{id:"34-geometric-interpretation-of-pca",style:{position:"relative"}},r.createElement(t.a,{href:"#34-geometric-interpretation-of-pca","aria-label":"34 geometric interpretation of pca permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.4 Geometric interpretation of PCA"),"\n",r.createElement(t.p,null,"In geometric terms, PCA can be seen as fitting a low-dimensional plane (a linear subspace) that best represents the data in terms of squared Euclidean distance. If one projects the data onto the first ",r.createElement(s.A,{text:"\\(k\\)"})," principal components, the resulting reconstruction error is minimized (in a least-squares sense) among all possible rank-",r.createElement(s.A,{text:"\\(k\\)"})," linear subspaces. This ties into another classic interpretation: PCA seeks the directions that capture the maximum variance, which also correspond to the singular vectors with the largest singular values."),"\n",r.createElement(t.h2,{id:"4-pca-algorithmic-steps",style:{position:"relative"}},r.createElement(t.a,{href:"#4-pca-algorithmic-steps","aria-label":"4 pca algorithmic steps permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4. PCA algorithmic steps"),"\n",r.createElement(t.h3,{id:"41-data-centering",style:{position:"relative"}},r.createElement(t.a,{href:"#41-data-centering","aria-label":"41 data centering permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.1 Data centering"),"\n",r.createElement(t.p,null,"A key preparatory step for PCA is data centering, sometimes combined with feature scaling. Data centering subtracts the mean ",r.createElement(s.A,{text:"\\(\\bar{x}\\)"})," from each sample so that the resulting dataset has zero mean. Without this step, the covariance matrix does not accurately reflect directions of maximum variability around the origin. Additionally, some practitioners standardize each feature to have unit variance before PCA, depending on whether features are measured in different scales and the immediate goals of the analysis."),"\n",r.createElement(t.h3,{id:"42-computing-the-covariance-matrix",style:{position:"relative"}},r.createElement(t.a,{href:"#42-computing-the-covariance-matrix","aria-label":"42 computing the covariance matrix permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.2 Computing the covariance matrix"),"\n",r.createElement(t.p,null,"After centering, an easy route is to form the covariance matrix ",r.createElement(s.A,{text:"\\(\\Sigma\\)"})," by ",r.createElement(s.A,{text:"\\( (1/(N-1)) X^\\top X \\)"}),". For large ",r.createElement(s.A,{text:"\\(d\\)"}),", forming the full matrix might be expensive, but it remains conceptually straightforward. The cost of forming ",r.createElement(s.A,{text:"\\(\\Sigma\\)"})," is on the order of ",r.createElement(s.A,{text:"\\(O(N d^2)\\)"})," if done naively, because one must multiply a ",r.createElement(s.A,{text:"\\(d \\times N\\)"})," matrix with an ",r.createElement(s.A,{text:"\\(N \\times d\\)"})," matrix."),"\n",r.createElement(t.h3,{id:"43-eigen-decomposition-approach",style:{position:"relative"}},r.createElement(t.a,{href:"#43-eigen-decomposition-approach","aria-label":"43 eigen decomposition approach permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.3 Eigen decomposition approach"),"\n",r.createElement(t.p,null,"One can then compute the eigenvalues and eigenvectors of the covariance matrix:"),"\n",r.createElement(s.A,{text:"\\[\n\\Sigma v_k = \\lambda_k v_k, \\quad k = 1,\\dots,d.\n\\]"}),"\n",r.createElement(t.p,null,"The ",r.createElement(s.A,{text:"\\(k\\)"}),"-th principal component is the eigenvector ",r.createElement(s.A,{text:"\\(v_k\\)"})," corresponding to the ",r.createElement(s.A,{text:"\\(k\\)"}),"-th largest eigenvalue ",r.createElement(s.A,{text:"\\(\\lambda_k\\)"}),". If the objective is to retain only ",r.createElement(s.A,{text:"\\(k\\)"})," components, one extracts ",r.createElement(s.A,{text:"\\(\\{v_1, \\ldots, v_k\\}\\)"})," and the corresponding eigenvalues. This approach is direct but might be computationally prohibitive if ",r.createElement(s.A,{text:"\\(d\\)"})," is extremely large."),"\n",r.createElement(t.h3,{id:"44-svd-method",style:{position:"relative"}},r.createElement(t.a,{href:"#44-svd-method","aria-label":"44 svd method permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.4 SVD method"),"\n",r.createElement(t.p,null,"In practice, many PCA implementations revolve around performing an SVD on ",r.createElement(s.A,{text:"\\(X\\)"})," directly:"),"\n",r.createElement(s.A,{text:"\\[\nX = U \\, S \\, V^\\top.\n\\]"}),"\n",r.createElement(t.p,null,"Then the principal components are the columns of ",r.createElement(s.A,{text:"\\(V\\)"}),". The product ",r.createElement(s.A,{text:"\\(X \\, V\\)"})," yields a matrix whose columns are the projections of ",r.createElement(s.A,{text:"\\(X\\)"})," onto the principal components. The SVD approach can be more numerically stable and can be more efficient when ",r.createElement(s.A,{text:"\\(N < d\\)"}),"."),"\n",r.createElement(t.h3,{id:"45-completeness-conditions",style:{position:"relative"}},r.createElement(t.a,{href:"#45-completeness-conditions","aria-label":"45 completeness conditions permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.5 Completeness conditions"),"\n",r.createElement(t.p,null,"Because ",r.createElement(s.A,{text:"\\(\\Sigma\\)"})," is positive semi-definite, all its eigenvalues ",r.createElement(s.A,{text:"\\(\\lambda_k\\)"})," are ",r.createElement(s.A,{text:"\\(\\ge 0\\)"}),", meaning the total variance in the dataset is:"),"\n",r.createElement(s.A,{text:"\\[\n\\text{Var}_{\\text{total}} = \\sum_{k=1}^{d} \\lambda_k.\n\\]"}),"\n",r.createElement(t.p,null,"Thus, when one chooses to project onto the top ",r.createElement(s.A,{text:"\\(k\\)"})," components, the fraction of explained variance is:"),"\n",r.createElement(s.A,{text:"\\[\n\\frac{\\lambda_1 + \\cdots + \\lambda_k}{\\lambda_1 + \\cdots + \\lambda_d}.\n\\]"}),"\n",r.createElement(t.p,null,"These concepts form the bedrock of how one interprets PCA results in both theoretical and practical contexts."),"\n",r.createElement(t.h3,{id:"46-numerical-stability-and-computational-complexity",style:{position:"relative"}},r.createElement(t.a,{href:"#46-numerical-stability-and-computational-complexity","aria-label":"46 numerical stability and computational complexity permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.6 Numerical stability and computational complexity"),"\n",r.createElement(t.p,null,"For moderate sizes (",r.createElement(s.A,{text:"\\(d\\)"})," in the thousands or less, ",r.createElement(s.A,{text:"\\(N\\)"})," in the thousands or tens of thousands), PCA via standard SVD or eigen decomposition is typically tractable using optimized linear algebra libraries (e.g., LAPACK, BLAS, or vendor-optimized routines). However, for truly massive datasets — for instance, when ",r.createElement(s.A,{text:"\\(d\\)"})," can be in the tens or hundreds of thousands — specialized algorithms or approximations like randomized SVD (Halko and gang, SIAM Review, 2011) or incremental PCA become necessary to keep the computations feasible."),"\n",r.createElement(t.h2,{id:"5-implementation-considerations-in-practice",style:{position:"relative"}},r.createElement(t.a,{href:"#5-implementation-considerations-in-practice","aria-label":"5 implementation considerations in practice permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5. Implementation considerations in practice"),"\n",r.createElement(t.h3,{id:"51-pca-using-scikit-learn",style:{position:"relative"}},r.createElement(t.a,{href:"#51-pca-using-scikit-learn","aria-label":"51 pca using scikit learn permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.1 PCA using scikit-learn"),"\n",r.createElement(t.p,null,"A highly popular approach in Python's data science stack is:"),"\n",r.createElement(o.A,{text:'\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\n# Suppose X is an (N x d) NumPy array of training data\n# and we want top k components\nk = 2\npca = PCA(n_components=k)\nX_reduced = pca.fit_transform(X)\n\nprint("Explained variance ratio:", pca.explained_variance_ratio_)\nprint("Principal components shape:", X_reduced.shape)\n'}),"\n",r.createElement(t.p,null,"Here, ",r.createElement(l.A,null,"explained_variance_ratio_")," indicates, for each principal component, the fraction of the total variance captured. The result ",r.createElement(s.A,{text:"\\(X_{\\text{reduced}}\\)"})," is an ",r.createElement(s.A,{text:"\\(N \\times k\\)"})," matrix containing the coordinates of each sample in the reduced ",r.createElement(s.A,{text:"\\(k\\)"}),"-dimensional space."),"\n",r.createElement(t.h3,{id:"52-incremental-pca",style:{position:"relative"}},r.createElement(t.a,{href:"#52-incremental-pca","aria-label":"52 incremental pca permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.2 Incremental PCA"),"\n",r.createElement(t.p,null,"When ",r.createElement(s.A,{text:"\\(X\\)"})," is too large to fit in memory, or when data arrives in streams, incremental PCA processes the data in mini-batches. Each batch updates estimates of the principal components without requiring repeated decomposition of the full dataset. This approach, used frequently in big data contexts, is an important stepping stone toward streaming or online dimensionality reduction."),"\n",r.createElement(t.h3,{id:"53-randomized-pca",style:{position:"relative"}},r.createElement(t.a,{href:"#53-randomized-pca","aria-label":"53 randomized pca permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.3 Randomized PCA"),"\n",r.createElement(t.p,null,"Randomized PCA (Halko and gang, SIAM Review, 2011) is a technique that constructs a random projection of the data into a lower dimensional space. Then it refines this projection iteratively. In many high-dimensional situations, randomized PCA can yield approximate principal components at a fraction of the cost of a full SVD, with surprisingly small errors for many practical datasets. This technique can be crucial in large-scale natural language processing tasks where one might have enormous term-document matrices."),"\n",r.createElement(t.h3,{id:"54-gpu-accelerated-approaches",style:{position:"relative"}},r.createElement(t.a,{href:"#54-gpu-accelerated-approaches","aria-label":"54 gpu accelerated approaches permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.4 GPU-accelerated approaches"),"\n",r.createElement(t.p,null,"Libraries such as RAPIDS (NVIDIA) in Python or specialized HPC solutions implement GPU-accelerated PCA. By performing matrix multiplications on massively parallel architectures, one can handle very large ",r.createElement(s.A,{text:"\\(N\\)"})," or ",r.createElement(s.A,{text:"\\(d\\)"})," more quickly. The speedup can be significant, especially when synergy with other GPU-based data loading and preprocessing steps is realized."),"\n",r.createElement(t.h3,{id:"55-typical-pitfalls-and-data-preprocessing",style:{position:"relative"}},r.createElement(t.a,{href:"#55-typical-pitfalls-and-data-preprocessing","aria-label":"55 typical pitfalls and data preprocessing permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.5 Typical pitfalls and data preprocessing"),"\n",r.createElement(t.p,null,"In real-world projects, one must keep in mind the effect of outliers and the necessity of consistent preprocessing. Outliers can produce disproportionately large effects on covariance estimates. Hence, it may be beneficial to use robust PCA variants or transform the data (e.g., using logarithms when dealing with positive-only values). Additionally, if the features have different scales, standardizing columns to have standard deviation 1 (or to some robust scale measure) can prevent certain features from dominating the variance."),"\n",r.createElement(t.h2,{id:"6-interpreting-results-from-pca",style:{position:"relative"}},r.createElement(t.a,{href:"#6-interpreting-results-from-pca","aria-label":"6 interpreting results from pca permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6. Interpreting results from PCA"),"\n",r.createElement(t.h3,{id:"61-variance-explained",style:{position:"relative"}},r.createElement(t.a,{href:"#61-variance-explained","aria-label":"61 variance explained permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.1 Variance explained"),"\n",r.createElement(t.p,null,"The primary quantity of interest from PCA is how much variance each principal component explains. That fraction of the variance, often graphed as a bar chart or line plot, helps the analyst decide how many components to retain. For instance, if the first two components explain 90% of the variance, that might be sufficiently high to use them alone for many tasks such as visualization or clustering."),"\n",r.createElement(t.h3,{id:"62-scree-plot",style:{position:"relative"}},r.createElement(t.a,{href:"#62-scree-plot","aria-label":"62 scree plot permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.2 Scree plot"),"\n",r.createElement(t.p,null,'A frequently used visual is the "scree plot", which plots ',r.createElement(s.A,{text:"\\(\\lambda_k\\)"})," — or the proportion of variance explained ",r.createElement(s.A,{text:"\\(\\lambda_k / \\sum_j \\lambda_j\\)"})," — as a function of the component index ",r.createElement(s.A,{text:"\\(k\\)"}),'. One typically looks for an "elbow" in the curve, a point beyond which additional components do not yield a marked improvement in explained variance.'),"\n",r.createElement(a,{alt:"Scree plot example",path:"",caption:"An example scree plot showing how consecutive principal components contribute to total variance.",zoom:"false"}),"\n",r.createElement(t.h3,{id:"63-choosing-the-number-of-components",style:{position:"relative"}},r.createElement(t.a,{href:"#63-choosing-the-number-of-components","aria-label":"63 choosing the number of components permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.3 Choosing the number of components"),"\n",r.createElement(t.p,null,"There is no universal formula for picking the correct number of principal components. Some heuristics:"),"\n",r.createElement(t.p,null,"• Retain all components needed to explain a certain threshold (e.g., 95%) of the variance.",r.createElement(t.br),"\n","• Identify the elbow in the scree plot.",r.createElement(t.br),"\n","• Cross-validate performance in downstream tasks (classification, regression) with subsets of components."),"\n",r.createElement(t.p,null,"In practice, the choice can be somewhat subjective, governed by domain knowledge and the sensitivity of the problem to small differences in explained variance."),"\n",r.createElement(t.h3,{id:"64-reconstructing-data-from-principal-components",style:{position:"relative"}},r.createElement(t.a,{href:"#64-reconstructing-data-from-principal-components","aria-label":"64 reconstructing data from principal components permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.4 Reconstructing data from principal components"),"\n",r.createElement(t.p,null,"It is also possible to approximately reconstruct the original data from a reduced set of principal components. Suppose you keep the top ",r.createElement(s.A,{text:"\\(k\\)"})," principal components ",r.createElement(s.A,{text:"\\(v_1, v_2, ..., v_k\\)"}),". The projected representation of a data vector ",r.createElement(s.A,{text:"\\(x\\)"})," is:"),"\n",r.createElement(s.A,{text:"\\[\nz = \\begin{bmatrix}\nv_1^\\top x \\\\\nv_2^\\top x \\\\\n\\vdots \\\\\nv_k^\\top x\n\\end{bmatrix}.\n\\]"}),"\n",r.createElement(t.p,null,"To reconstruct ",r.createElement(s.A,{text:"\\(x\\)"})," (approximately), one uses:"),"\n",r.createElement(s.A,{text:"\\[\n\\hat{x} = \\sum_{m=1}^k (v_m^\\top x)\\, v_m.\n\\]"}),"\n",r.createElement(t.p,null,"This reveals that by truncating to ",r.createElement(s.A,{text:"\\(k\\)"})," components, we are effectively discarding contributions from the remaining ",r.createElement(s.A,{text:"\\(d-k\\)"})," eigenvectors and hence losing some information (variance)."),"\n",r.createElement(t.h3,{id:"65-partial-interpretability",style:{position:"relative"}},r.createElement(t.a,{href:"#65-partial-interpretability","aria-label":"65 partial interpretability permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.5 Partial interpretability"),"\n",r.createElement(t.p,null,"Each principal component is a linear combination of the original features, so it is possible to interpret which original variables have the most weight in each principal component's direction. However, these combinations can be somewhat tricky to parse if many features have moderate loadings. Domain knowledge can aid in naming or describing principal components. For instance, in certain biological datasets, one might see that the first component corresponds to cell size or cell cycle effect, whereas subsequent ones might capture specific gene pathways."),"\n",r.createElement(t.h2,{id:"7-kernel-pca",style:{position:"relative"}},r.createElement(t.a,{href:"#7-kernel-pca","aria-label":"7 kernel pca permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7. Kernel PCA"),"\n",r.createElement(t.p,null,'While PCA is purely linear, kernel PCA extends the basic approach by applying a "kernel trick", a concept widely used in support vector machines. One replaces the explicit features ',r.createElement(s.A,{text:"\\(x_i\\)"})," with implicit feature mappings ",r.createElement(s.A,{text:"\\(\\phi(x_i)\\)"})," in a high (or infinite) dimensional RKHS. Then, PCA is performed in that transformed (kernel) space."),"\n",r.createElement(t.h3,{id:"71-the-kernel-trick",style:{position:"relative"}},r.createElement(t.a,{href:"#71-the-kernel-trick","aria-label":"71 the kernel trick permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.1 The kernel trick"),"\n",r.createElement(t.p,null,"For a given kernel function ",r.createElement(s.A,{text:"\\(k(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j)\\rangle\\)"}),", we only need the kernel matrix ",r.createElement(s.A,{text:"\\(K\\)"}),", whose ",r.createElement(s.A,{text:"\\((i,j)\\)"}),"-entry is ",r.createElement(s.A,{text:"\\(k(x_i, x_j)\\)"}),". The entire machinery — covariance, eigen decomposition, and so forth — can then be carried out in that kernel space, albeit with additional constraints regarding centering the kernel matrix."),"\n",r.createElement(t.h3,{id:"72-rbf-kernel-polynomial-kernel-etc",style:{position:"relative"}},r.createElement(t.a,{href:"#72-rbf-kernel-polynomial-kernel-etc","aria-label":"72 rbf kernel polynomial kernel etc permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.2 RBF kernel, polynomial kernel, etc."),"\n",r.createElement(t.p,null,"Common kernels used in kernel PCA include the RBF (Gaussian) kernel ",r.createElement(s.A,{text:"\\( \\exp(-\\|x_i - x_j\\|^2 / (2\\sigma^2))\\)"}),", polynomial kernels ",r.createElement(s.A,{text:"\\((\\alpha \\, x_i^\\top x_j + c)^p\\)"}),", and others. By choosing different kernels, one can extract nonlinear structure that is not captured by standard linear PCA."),"\n",r.createElement(t.h3,{id:"73-computational-aspects",style:{position:"relative"}},r.createElement(t.a,{href:"#73-computational-aspects","aria-label":"73 computational aspects permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.3 Computational aspects"),"\n",r.createElement(t.p,null,"Kernel PCA requires storing and decomposing an ",r.createElement(s.A,{text:"\\(N \\times N\\)"})," kernel matrix, which becomes expensive for large ",r.createElement(s.A,{text:"\\(N\\)"}),". This is in sharp contrast to linear PCA, where one might only deal with a ",r.createElement(s.A,{text:"\\(d \\times d\\)"})," matrix. Research in approximate kernel methods has led to approaches such as the Nyström method to reduce the computational burden."),"\n",r.createElement(t.h3,{id:"74-limitations-of-kernel-pca",style:{position:"relative"}},r.createElement(t.a,{href:"#74-limitations-of-kernel-pca","aria-label":"74 limitations of kernel pca permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.4 Limitations of kernel PCA"),"\n",r.createElement(t.p,null,"Although kernel PCA can reveal nonlinear structures, it lacks some of the interpretability of linear PCA, since the resulting components exist in a high-dimensional feature space defined implicitly by the kernel. Also, out-of-sample extension (projecting new data points onto the learned components) becomes more complex and typically requires storing or recalculating kernel values with training points."),"\n",r.createElement(t.h2,{id:"8-advanced-transformations-and-related-methods",style:{position:"relative"}},r.createElement(t.a,{href:"#8-advanced-transformations-and-related-methods","aria-label":"8 advanced transformations and related methods permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8. Advanced transformations and related methods"),"\n",r.createElement(t.p,null,"Though PCA is a powerful technique, it is by no means the only method. Depending on the structure of the data and the goals of the analysis, other algorithms may be more appropriate."),"\n",r.createElement(t.h3,{id:"81-lda-linear-discriminant-analysis",style:{position:"relative"}},r.createElement(t.a,{href:"#81-lda-linear-discriminant-analysis","aria-label":"81 lda linear discriminant analysis permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.1 LDA (Linear Discriminant Analysis)"),"\n",r.createElement(t.p,null,"Linear Discriminant Analysis (LDA) is sometimes viewed in the same category as PCA, but it optimizes a fundamentally different criterion: LDA seeks to maximize class separation for labeled data. If the dataset is supervised and the objective is classification, LDA can provide a subspace that makes classes more linearly separable, whereas PCA is purely unsupervised and does not consider class labels."),"\n",r.createElement(t.h3,{id:"82-factor-analysis",style:{position:"relative"}},r.createElement(t.a,{href:"#82-factor-analysis","aria-label":"82 factor analysis permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.2 Factor analysis"),"\n",r.createElement(t.p,null,"In factor analysis, the data are modeled as a linear combination of hidden factors plus noise. Parameters are estimated to capture covariances in a way that might be more interpretable in certain social sciences or psychological contexts, although in practice there is some conceptual overlap with PCA."),"\n",r.createElement(t.h3,{id:"83-ica-independent-component-analysis",style:{position:"relative"}},r.createElement(t.a,{href:"#83-ica-independent-component-analysis","aria-label":"83 ica independent component analysis permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.3 ICA (Independent Component Analysis)"),"\n",r.createElement(t.p,null,'Independent Component Analysis (ICA) is designed to separate mixed signals into statistically independent components. It is especially effective in blind source separation. For example, in the "cocktail party problem" scenario, ICA can separate multiple voices recorded by multiple microphones. Unlike PCA, which focuses on maximizing variance in orthogonal directions, ICA focuses on maximizing non-Gaussianity or measures of statistical independence.'),"\n",r.createElement(t.h3,{id:"84-manifold-learning-approaches",style:{position:"relative"}},r.createElement(t.a,{href:"#84-manifold-learning-approaches","aria-label":"84 manifold learning approaches permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.4 Manifold learning approaches"),"\n",r.createElement(t.p,null,"Algorithms such as isomap, locally linear embedding (LLE), and Laplacian eigenmaps attempt to learn low-dimensional manifolds while preserving local geometry. If data truly lie near a complex manifold, these methods can succeed where linear PCA fails to capture curved structures. However, they can be more fragile, sensitive to hyperparameters (like neighborhood sizes), and challenging to scale."),"\n",r.createElement(t.h3,{id:"85-t-sne",style:{position:"relative"}},r.createElement(t.a,{href:"#85-t-sne","aria-label":"85 t sne permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.5 t-SNE"),"\n",r.createElement(t.p,null,"Developed by Maaten & Hinton (JMLR, 2008), t-Distributed Stochastic Neighbor Embedding (t-SNE) is a nonlinear dimensionality reduction method especially popular for visualizing high-dimensional data in 2D or 3D. t-SNE places a strong emphasis on preserving local neighborhoods, often producing visually meaningful clusters. Its main drawback is that distances in the low-dimensional projection can be somewhat distorted between clusters, and the method can be quite computationally heavy for large datasets. Nonetheless, t-SNE remains widely used in biology, NLP, and other fields for producing intuitive scatter plot visualizations."),"\n",r.createElement(t.h3,{id:"86-umap",style:{position:"relative"}},r.createElement(t.a,{href:"#86-umap","aria-label":"86 umap permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.6 UMAP"),"\n",r.createElement(t.p,null,"UMAP (Uniform Manifold Approximation and Projection) is conceptually related to t-SNE but claims certain theoretical guarantees grounded in Riemannian geometry and fuzzy simplicial sets. It tends to be faster than t-SNE and can sometimes better preserve global structure."),"\n",r.createElement(t.h3,{id:"87-autoencoders-for-dimensionality-reduction",style:{position:"relative"}},r.createElement(t.a,{href:"#87-autoencoders-for-dimensionality-reduction","aria-label":"87 autoencoders for dimensionality reduction permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.7 Autoencoders for dimensionality reduction"),"\n",r.createElement(t.p,null,"Neural network-based autoencoders can perform nonlinear dimensionality reduction. An encoder network compresses data to a latent representation with reduced dimension, while a decoder network reconstructs the original data from that latent space. Techniques like variational autoencoders (VAEs) extend this approach into a probabilistic realm. Autoencoders can produce embeddings that capture complex variance not easily captured in standard PCA."),"\n",r.createElement(t.h2,{id:"9-data-visualization-with-pca",style:{position:"relative"}},r.createElement(t.a,{href:"#9-data-visualization-with-pca","aria-label":"9 data visualization with pca permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9. Data visualization with PCA"),"\n",r.createElement(t.p,null,"PCA provides a convenient tool for visualization if we limit ourselves to two or three principal components."),"\n",r.createElement(t.h3,{id:"91-2d-and-3d-scatter-plots",style:{position:"relative"}},r.createElement(t.a,{href:"#91-2d-and-3d-scatter-plots","aria-label":"91 2d and 3d scatter plots permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9.1 2D and 3D scatter plots"),"\n",r.createElement(t.p,null,"By projecting each ",r.createElement(s.A,{text:"\\(x_i\\)"})," onto the first two principal components, we obtain a 2D scatter plot that frequently reveals clusters or patterns in the data. If the data do not separate clearly in the first two components, exploring combinations like ",r.createElement(s.A,{text:"\\((v_1, v_3)\\)"})," or ",r.createElement(s.A,{text:"\\((v_2, v_3)\\)"})," might bring additional structures to light."),"\n",r.createElement(t.h3,{id:"92-color-coded-pca",style:{position:"relative"}},r.createElement(t.a,{href:"#92-color-coded-pca","aria-label":"92 color coded pca permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9.2 Color-coded PCA"),"\n",r.createElement(t.p,null,"When the dataset comes with categorical labels, one can color the scatter plot points according to class membership. This approach provides a quick check of whether classes are separable in the first few components. Similarly, continuous variables (e.g., a measured response) can be shown through color gradients on the PCA plane to visualize correlation with principal component directions."),"\n",r.createElement(t.h3,{id:"93-synergy-with-cluster-analysis",style:{position:"relative"}},r.createElement(t.a,{href:"#93-synergy-with-cluster-analysis","aria-label":"93 synergy with cluster analysis permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9.3 Synergy with cluster analysis"),"\n",r.createElement(t.p,null,"Sometimes, applying PCA as a pre-processing step followed by clustering (e.g., k-means) can yield simpler cluster boundaries or more intuitive groupings in fewer dimensions. Because PCA can remove noise and reduce complexity, clustering in a lower-dimensional subspace is often more stable."),"\n",r.createElement(a,{alt:"Cluster example",path:"",caption:"Clustering on PCA-reduced data can unveil simplified separation of points.",zoom:"false"}),"\n",r.createElement(t.h3,{id:"94-advanced-eda-with-pca",style:{position:"relative"}},r.createElement(t.a,{href:"#94-advanced-eda-with-pca","aria-label":"94 advanced eda with pca permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"9.4 Advanced EDA with PCA"),"\n",r.createElement(t.p,null,"In complex datasets, especially in fields like genomics, financial time series, or image-based data, plotting the first few principal components can yield significant insights. For instance, outliers often stand out immediately, or subpopulations might form distinct clusters. In these fields, PCA is frequently used for quick exploratory data analysis before diving into more specialized or nonlinear dimension reduction strategies."),"\n",r.createElement(t.h2,{id:"10-real-world-applications-and-concluding-remarks",style:{position:"relative"}},r.createElement(t.a,{href:"#10-real-world-applications-and-concluding-remarks","aria-label":"10 real world applications and concluding remarks permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"10. Real-world applications and concluding remarks"),"\n",r.createElement(t.h3,{id:"101-high-throughput-genomics-data",style:{position:"relative"}},r.createElement(t.a,{href:"#101-high-throughput-genomics-data","aria-label":"101 high throughput genomics data permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"10.1 High-throughput genomics data"),"\n",r.createElement(t.p,null,"PCA remains indispensable in modern computational biology. When dealing with thousands or even hundreds of thousands of measured variables (e.g., gene expression, single-cell RNA-seq data), PCA helps compress the data, remove noise, and highlight major sources of variation, such as different cell types or technical confounders like batch effects."),"\n",r.createElement(t.h3,{id:"102-image-compression",style:{position:"relative"}},r.createElement(t.a,{href:"#102-image-compression","aria-label":"102 image compression permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"10.2 Image compression"),"\n",r.createElement(t.p,null,"When images are vectorized into high-dimensional pixel spaces, PCA can reduce redundancy by discovering principal components that describe major patterns (edges, color gradients, etc.). One can store an image using fewer components and then reconstruct an approximation with acceptable fidelity. Although more specialized methods like JPEG or wavelets are used in practice, PCA-based compression is valuable for teaching the fundamentals of transform-based data compression."),"\n",r.createElement(t.h3,{id:"103-noise-reduction",style:{position:"relative"}},r.createElement(t.a,{href:"#103-noise-reduction","aria-label":"103 noise reduction permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"10.3 Noise reduction"),"\n",r.createElement(t.p,null,"Applying PCA to noisy signals often allows one to discard components corresponding to small eigenvalues (which often align with noise) and preserve the main structure. This principle can apply in signal processing, image denoising, or sensor fusion tasks, leading to a simpler, denoised version of the data."),"\n",r.createElement(t.h3,{id:"104-finance-time-series",style:{position:"relative"}},r.createElement(t.a,{href:"#104-finance-time-series","aria-label":"104 finance time series permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"10.4 Finance time series"),"\n",r.createElement(t.p,null,'In finance, large covariance matrices are common when analyzing the co-movements of hundreds of assets. PCA can be used to identify market factors: the first principal component often corresponds to an overall "market" movement, while subsequent components might capture sector-specific trends or other structural factors. This can guide portfolio construction or risk management by highlighting systematic vs. idiosyncratic risk components.'),"\n",r.createElement(t.h3,{id:"105-concluding-remarks",style:{position:"relative"}},r.createElement(t.a,{href:"#105-concluding-remarks","aria-label":"105 concluding remarks permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"10.5 Concluding remarks"),"\n",r.createElement(t.p,null,"Despite the proliferation of sophisticated manifold learning techniques and deep-learning-based compressions, PCA endures as the go-to reference method for dimensionality reduction. Its geometric clarity, ease of calculation, and interpretability in terms of variance explained make it a firmly entrenched staple of data science educational curricula and real-world pipelines. It also forms the basis for many expansions such as kernel PCA, incremental methods for large-scale problems, and preprocessing for advanced algorithms. Understanding PCA thoroughly — from its linear algebraic underpinnings to practical tips for usage — remains indispensable for any machine learning practitioner or researcher aiming to master data representation and compression."),"\n",r.createElement(t.p,null,"Such justification underscores why, more than a century after its progenitor's earliest insights, PCA remains an essential technique in the data science toolkit. When carefully deployed, it reveals hidden structure in high-dimensional datasets, aids in creating more efficient and robust models, and facilitates domain-specific insights that might otherwise remain obscured in a cloud of raw features."))}var h=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,i.RP)(),e.components);return t?r.createElement(t,e,r.createElement(c,e)):c(e)};var m=a(36710),d=a(58481),p=a.n(d),u=a(36310),f=a(87245),g=a(27042),v=a(59849),b=a(5591),y=a(61122),E=a(9219),x=a(33203),S=a(95751),w=a(94328),A=a(80791),k=a(78137);const H=e=>{let{toc:t}=e;if(!t||!t.items)return null;return r.createElement("nav",{className:A.R},r.createElement("ul",null,t.items.map(((e,t)=>r.createElement("li",{key:t},r.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const a=t.replace("#",""),n=document.getElementById(a);n&&n.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&r.createElement(H,{toc:{items:e.items}}))))))};function C(e){let{data:{mdx:t,allMdx:l,allPostImages:o},children:s}=e;const{frontmatter:c,body:h,tableOfContents:m}=t,d=c.index,v=c.slug.split("/")[1],A=l.nodes.filter((e=>e.frontmatter.slug.includes(`/${v}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),C=A.findIndex((e=>e.frontmatter.index===d)),_=A[C+1],z=A[C-1],M=c.slug.replace(/\/$/,""),V=/[^/]*$/.exec(M)[0],P=`posts/${v}/content/${V}/`,{0:T,1:I}=(0,r.useState)(c.flagWideLayoutByDefault),{0:N,1:L}=(0,r.useState)(!1);var B;(0,r.useEffect)((()=>{L(!0);const e=setTimeout((()=>L(!1)),340);return()=>clearTimeout(e)}),[T]),"adventures"===v?B=E.cb:"research"===v?B=E.Qh:"thoughts"===v&&(B=E.T6);const D=p()(h).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,q=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),a=e%60;return a<=30?`~${t}${a>0?".5":""} h`:`~${t+1} h`}(Math.ceil(D/B)+(c.extraReadTimeMin||0)),j=[{flag:c.flagDraft,component:()=>Promise.all([a.e(3231),a.e(8809)]).then(a.bind(a,28809))},{flag:c.flagMindfuckery,component:()=>Promise.all([a.e(3231),a.e(2471)]).then(a.bind(a,67709))},{flag:c.flagRewrite,component:()=>Promise.all([a.e(3231),a.e(6764)]).then(a.bind(a,62002))},{flag:c.flagOffensive,component:()=>Promise.all([a.e(3231),a.e(2443)]).then(a.bind(a,17681))},{flag:c.flagProfane,component:()=>Promise.all([a.e(3231),a.e(8048)]).then(a.bind(a,53286))},{flag:c.flagMultilingual,component:()=>Promise.all([a.e(3231),a.e(4069)]).then(a.bind(a,78831))},{flag:c.flagUnreliably,component:()=>Promise.all([a.e(3231),a.e(3417)]).then(a.bind(a,8179))},{flag:c.flagPolitical,component:()=>Promise.all([a.e(3231),a.e(5195)]).then(a.bind(a,30433))},{flag:c.flagCognitohazard,component:()=>Promise.all([a.e(3231),a.e(3175)]).then(a.bind(a,8413))},{flag:c.flagHidden,component:()=>Promise.all([a.e(3231),a.e(9556)]).then(a.bind(a,14794))}],{0:O,1:R}=(0,r.useState)([]);return(0,r.useEffect)((()=>{j.forEach((e=>{let{flag:t,component:a}=e;t&&a().then((e=>{R((t=>[].concat((0,n.A)(t),[e.default])))}))}))}),[]),r.createElement(g.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},r.createElement(b.A,{postNumber:c.index,date:c.date,updated:c.updated,readTime:q,difficulty:c.difficultyLevel,title:c.title,desc:c.desc,banner:c.banner,section:v,postKey:V,isMindfuckery:c.flagMindfuckery,mainTag:c.mainTag}),r.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},c.otherTags.map(((e,t)=>r.createElement("span",{key:t,className:`noselect ${k.MW}`,style:{margin:"0 5px 5px 0"}},e)))),r.createElement("div",{className:"postBody"},r.createElement(H,{toc:m})),r.createElement("br"),r.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},r.createElement(g.P.button,{className:`noselect ${w.pb}`,id:w.xG,onClick:()=>{I(!T)},whileTap:{scale:.93}},r.createElement(g.P.div,{className:S.DJ,key:T,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},T?"Switch to default layout":"Switch to wide layout"))),r.createElement("br"),r.createElement("div",{className:"postBody",style:{margin:T?"0 -14%":"",maxWidth:T?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},r.createElement("div",{className:`${w.P_} ${N?w.Xn:w.qG}`},O.map(((e,t)=>r.createElement(e,{key:t}))),c.indexCourse?r.createElement(x.A,{index:c.indexCourse,category:c.courseCategoryName}):"",r.createElement(u.Z.Provider,{value:{images:o.nodes,basePath:P.replace(/\/$/,"")+"/"}},r.createElement(i.xA,{components:{Image:f.A}},s)))),r.createElement(y.A,{nextPost:_,lastPost:z,keyCurrent:V,section:v}))}function _(e){return r.createElement(C,e,r.createElement(h,e))}function z(e){var t,a,n,i,l;let{data:o}=e;const{frontmatter:s}=o.mdx,c=s.titleSEO||s.title,h=s.titleOG||c,d=s.titleTwitter||c,p=s.descSEO||s.desc,u=s.descOG||p,f=s.descTwitter||p,g=s.schemaType||"BlogPosting",b=s.keywordsSEO,y=s.date,E=s.updated||y,x=s.imageOG||(null===(t=s.banner)||void 0===t||null===(a=t.childImageSharp)||void 0===a||null===(n=a.gatsbyImageData)||void 0===n||null===(i=n.images)||void 0===i||null===(l=i.fallback)||void 0===l?void 0:l.src),S=s.imageAltOG||u,w=s.imageTwitter||x,A=s.imageAltTwitter||f,k=s.canonicalURL,H=s.flagHidden||!1,C=s.mainTag||"Posts",_=s.slug.split("/")[1]||"posts",{siteUrl:z}=(0,m.Q)(),M={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:z},{"@type":"ListItem",position:2,name:C,item:`${z}/${s.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${z}${s.slug}`}]};return r.createElement(v.A,{title:c+" - avrtt.blog",titleOG:h,titleTwitter:d,description:p,descriptionOG:u,descriptionTwitter:f,schemaType:g,keywords:b,datePublished:y,dateModified:E,imageOG:x,imageAltOG:S,imageTwitter:w,imageAltTwitter:A,canonicalUrl:k,flagHidden:H,mainTag:C,section:_,type:"article"},r.createElement("script",{type:"application/ld+json"},JSON.stringify(M)))}},96098:function(e,t,a){var n=a(96540),i=a(7978);t.A=e=>{let{text:t}=e;return n.createElement(i.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-js-content-file-path-src-pages-posts-research-dimensionality-reduction-and-pca-mdx-fc5e8c2de38928bde83c.js.map