"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[8818],{26002:function(e,t,a){a.r(t),a.d(t,{Head:function(){return M},PostTemplate:function(){return k},default:function(){return C}});var n=a(54506),i=a(28453),r=a(96540),l=a(16886),s=a(46295),o=a(96098);function c(e){const t=Object.assign({p:"p",h2:"h2",a:"a",span:"span",h3:"h3",ul:"ul",li:"li",strong:"strong",h4:"h4",h5:"h5",ol:"ol",em:"em",hr:"hr"},(0,i.RP)(),e.components),{Image:a}=t;return a||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),r.createElement(r.Fragment,null,"\n",r.createElement("br"),"\n","\n",r.createElement(t.p,null,"Decision trees are among the most intuitive and widely used models in machine learning. They partition data space into smaller and simpler regions, making predictions within each region using straightforward rules. Historically, decision trees date back to the 1960s, where early work on symbolic decision-making systems in AI provided the conceptual basis for tree-like decision structures. Over the decades, famous algorithms such as ",r.createElement(l.A,null,"ID3"),", ",r.createElement(l.A,null,"C4.5")," (Quinlan, J. R., 1986), and ",r.createElement(l.A,null,"CART")," (Breiman and gang, 1984) have further refined how decision trees are constructed, making them highly practical and efficient for both classification and regression."),"\n",r.createElement(t.p,null,"Today, decision trees find application in numerous areas: from marketing (customer segmentation) and finance (credit-risk evaluation) to bioinformatics (gene expression data analysis). They remain a staple of many predictive modeling tasks because of their interpretability: each node in the tree is a yes/no or threshold-based question, which can be easily understood and visualized."),"\n",r.createElement(t.h2,{id:"2-decision-tree-intuition-and-in-depth-review",style:{position:"relative"}},r.createElement(t.a,{href:"#2-decision-tree-intuition-and-in-depth-review","aria-label":"2 decision tree intuition and in depth review permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2. Decision tree intuition and in-depth review"),"\n",r.createElement(t.h3,{id:"21-understanding-tree-based-models",style:{position:"relative"}},r.createElement(t.a,{href:"#21-understanding-tree-based-models","aria-label":"21 understanding tree based models permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.1. Understanding tree-based models"),"\n",r.createElement(t.p,null,"A decision tree is a flowchart-like structure where each internal node represents a test on a feature (for example, whether a feature value exceeds some threshold), each branch represents the outcome of the test, and each leaf node represents a final classification or regression value. Formally, a ",r.createElement(l.A,null,"binary decision tree"),' repeatedly splits the dataset into two subsets according to some "best" rule, until a stopping criterion is met (e.g., no further splits are beneficial, or the tree reaches a maximum depth).'),"\n",r.createElement(t.p,null,"Intuitively, the tree tries to reduce the ",r.createElement(l.A,null,'"mixedness"')," or uncertainty of data at each node. After enough splits, the leaves ideally contain data points that are mostly homogeneous (e.g., points from one class in classification, or points with minimal variance in regression)."),"\n",r.createElement(t.h3,{id:"22-visualizing-a-simple-decision-tree",style:{position:"relative"}},r.createElement(t.a,{href:"#22-visualizing-a-simple-decision-tree","aria-label":"22 visualizing a simple decision tree permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.2. Visualizing a simple decision tree"),"\n",r.createElement(t.p,null,"One way to visualize a simple binary decision tree is to imagine a top-down splitting of your feature space:"),"\n",r.createElement(a,{alt:"Simple decision tree splitting on 2D data",path:"",caption:"A conceptual illustration of a decision tree boundary in 2D space",zoom:"false"}),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,"Each non-leaf node checks whether a feature ",r.createElement(o.A,{text:"\\( x_j \\)"})," is less than or greater than some threshold ",r.createElement(o.A,{text:"\\( t \\)"}),"."),"\n",r.createElement(t.li,null,"Points for which ",r.createElement(o.A,{text:"\\( x_j \\le t \\)"})," go to the left branch; the rest go to the right branch."),"\n",r.createElement(t.li,null,"This process repeats recursively down every branch until stopping conditions are met."),"\n"),"\n",r.createElement(t.h3,{id:"23-advantages-and-limitations",style:{position:"relative"}},r.createElement(t.a,{href:"#23-advantages-and-limitations","aria-label":"23 advantages and limitations permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"2.3. Advantages and limitations"),"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"Advantages")),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(l.A,null,"Interpretability:")," The hierarchical, rule-based structure is easy to interpret compared to many other machine learning models."),"\n",r.createElement(t.li,null,r.createElement(l.A,null,"Non-linearity:")," Decision boundaries can be arbitrarily complex, since a series of orthogonal splits can adapt to complex data patterns."),"\n",r.createElement(t.li,null,r.createElement(l.A,null,"Feature scaling invariance:")," Decision trees work with raw feature values â€” no normalization or standardization is typically required."),"\n",r.createElement(t.li,null,r.createElement(l.A,null,"Mixed feature types:")," Easily handle both numeric and categorical features."),"\n"),"\n",r.createElement(t.p,null,r.createElement(t.strong,null,"Limitations")),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(l.A,null,"Overfitting:")," An unconstrained tree can grow very deep, creating many leaf nodes that fit training data almost perfectly but generalize poorly."),"\n",r.createElement(t.li,null,r.createElement(l.A,null,"Instability:")," Small changes in the training data can produce large changes in the final model structure."),"\n",r.createElement(t.li,null,r.createElement(l.A,null,"Greedy splitting:")," Standard tree-building is based on local heuristics (greedy strategy), which might not yield a globally optimal tree."),"\n"),"\n",r.createElement(t.h2,{id:"3-building-a-decision-tree",style:{position:"relative"}},r.createElement(t.a,{href:"#3-building-a-decision-tree","aria-label":"3 building a decision tree permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3. Building a decision tree"),"\n",r.createElement(t.h3,{id:"31-choosing-a-split-criterion",style:{position:"relative"}},r.createElement(t.a,{href:"#31-choosing-a-split-criterion","aria-label":"31 choosing a split criterion permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.1. Choosing a split criterion"),"\n",r.createElement(t.p,null,"Choosing how to split at each node is crucial. Common criteria include ",r.createElement(l.A,null,"entropy and information gain")," (famously used in ID3/C4.5) and ",r.createElement(l.A,null,"Gini impurity")," (used in CART)."),"\n",r.createElement(t.h4,{id:"entropy-and-information-gain",style:{position:"relative"}},r.createElement(t.a,{href:"#entropy-and-information-gain","aria-label":"entropy and information gain permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Entropy and information gain"),"\n",r.createElement(t.p,null,"Entropy (",r.createElement(o.A,{text:"\\( \\mathrm{H}(S) \\)"}),'), from information theory, is a measure of uncertainty (or "disorder") in a set ',r.createElement(o.A,{text:"\\( S \\)"}),". For a dataset with ",r.createElement(o.A,{text:"\\( k \\)"})," classes, we define the fraction of examples belonging to class ",r.createElement(o.A,{text:"\\( i \\)"})," as ",r.createElement(o.A,{text:"\\( p_i \\)"}),". The entropy is:"),"\n",r.createElement(o.A,{text:"\\[\n\\mathrm{H}(S) = - \\sum_{i=1}^k p_i \\log_2(p_i).\n\\]"}),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(o.A,{text:"\\( p_i \\)"})," is the proportion of samples in class ",r.createElement(o.A,{text:"\\( i \\)"}),"."),"\n",r.createElement(t.li,null,r.createElement(o.A,{text:"\\( \\mathrm{H}(S) \\)"})," is ",r.createElement(l.A,null,"0")," if all elements are of the same class, and maximal if the data is perfectly balanced across all classes."),"\n"),"\n",r.createElement(t.p,null,"When we apply a split that partitions ",r.createElement(o.A,{text:"\\( S \\)"})," into subsets ",r.createElement(o.A,{text:"\\( \\{S_1, S_2, \\dots \\} \\)"}),", the entropy after splitting is a weighted sum of the entropies of each subset. The metric ",r.createElement(l.A,null,"information gain (IG)")," is:"),"\n",r.createElement(o.A,{text:"\\[\n\\mathrm{IG}(S, \\text{split}) = \\mathrm{H}(S) \\;-\\;\\sum_{j} \\frac{|S_j|}{|S|} \\,\\mathrm{H}(S_j).\n\\]"}),"\n",r.createElement(t.p,null,'A larger information gain indicates a better split: you have "removed" more entropy or, equivalently, introduced greater purity.'),"\n",r.createElement(t.h5,{id:"binary-classification-example",style:{position:"relative"}},r.createElement(t.a,{href:"#binary-classification-example","aria-label":"binary classification example permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Binary classification example"),"\n",r.createElement(t.p,null,"Consider a binary classification with classes ",r.createElement(o.A,{text:"\\( \\{0,1\\} \\)"}),". Suppose we split the dataset at threshold ",r.createElement(o.A,{text:"\\( t \\)"})," on some feature. We compute:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,"Entropy of the original set ",r.createElement(o.A,{text:"\\( S \\)"}),"."),"\n",r.createElement(t.li,null,"Entropy of the left subset ",r.createElement(o.A,{text:"\\( S_{\\le t} \\)"})," and the right subset ",r.createElement(o.A,{text:"\\( S_{> t} \\)"}),"."),"\n",r.createElement(t.li,null,"The weighted sum of left/right entropies."),"\n",r.createElement(t.li,null,"The information gain as the difference between the original entropy and the weighted sum."),"\n"),"\n",r.createElement(t.p,null,"We repeat for every possible threshold and pick the threshold that gives the maximum information gain."),"\n",r.createElement(t.h4,{id:"gini-impurity",style:{position:"relative"}},r.createElement(t.a,{href:"#gini-impurity","aria-label":"gini impurity permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Gini impurity"),"\n",r.createElement(t.p,null,"Another common measure is ",r.createElement(l.A,null,"Gini impurity"),". For a dataset with ",r.createElement(o.A,{text:"\\( k \\)"})," classes and class proportions ",r.createElement(o.A,{text:"\\( p_1, p_2, \\ldots, p_k \\)"}),", the Gini impurity is:"),"\n",r.createElement(o.A,{text:"\\( G(S) = 1 - \\sum_{i=1}^{k} p_i^2. \\)"}),"\n",r.createElement(t.p,null,"Equivalently, it can be seen as the probability of misclassifying a sample if we randomly label it according to the class distribution in ",r.createElement(o.A,{text:"\\( S \\)"}),". Like entropy, a lower Gini value means higher purity. Many libraries (such as scikit-learn) default to Gini due to its computational convenience and similar performance to entropy in practice."),"\n",r.createElement(t.h3,{id:"32-stopping-criteria",style:{position:"relative"}},r.createElement(t.a,{href:"#32-stopping-criteria","aria-label":"32 stopping criteria permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.2. Stopping criteria"),"\n",r.createElement(t.p,null,"A naive approach might keep splitting until every leaf contains examples from only one class or until a certain maximum depth is reached. But other practical stopping rules often include:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(l.A,null,"Maximum depth:")," Stop splitting if the tree reaches a specified depth."),"\n",r.createElement(t.li,null,r.createElement(l.A,null,"Minimum samples in a node:")," Require each node to have at least ",r.createElement(o.A,{text:"\\( n_\\text{min} \\)"})," samples to further split."),"\n",r.createElement(t.li,null,r.createElement(l.A,null,"Minimum information gain or impurity reduction:")," If no split yields an improvement beyond a certain threshold, stop."),"\n"),"\n",r.createElement(t.h3,{id:"33-handling-categorical-vs-numerical-features",style:{position:"relative"}},r.createElement(t.a,{href:"#33-handling-categorical-vs-numerical-features","aria-label":"33 handling categorical vs numerical features permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3.3. Handling categorical vs. numerical features"),"\n",r.createElement(t.p,null,"Decision trees can handle both discrete (categorical) and continuous (numeric) variables:"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"For numeric data"),", the split ",r.createElement(o.A,{text:"\\( [ x_j \\le t ] \\)"})," is typical, scanning over potential threshold values."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"For categorical data"),", the tree checks membership in one or more categories or enumerates ways to separate the categories into distinct branches. In practice, this can become computationally intensive for high-cardinality features, so heuristics (e.g., grouping or one-hot-encoding) are common."),"\n"),"\n",r.createElement(t.h2,{id:"4-decision-trees-in-classification-and-regression",style:{position:"relative"}},r.createElement(t.a,{href:"#4-decision-trees-in-classification-and-regression","aria-label":"4 decision trees in classification and regression permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4. Decision trees in classification and regression"),"\n",r.createElement(t.h3,{id:"41-how-decision-trees-handle-classification-tasks",style:{position:"relative"}},r.createElement(t.a,{href:"#41-how-decision-trees-handle-classification-tasks","aria-label":"41 how decision trees handle classification tasks permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.1. How decision trees handle classification tasks"),"\n",r.createElement(t.p,null,"In classification, each leaf outputs a class label â€” often determined by majority voting among the training samples that fall into that leaf. The cost function that drives the splits is usually based on ",r.createElement(l.A,null,"information gain")," or ",r.createElement(l.A,null,"Gini impurity"),", as described above."),"\n",r.createElement(t.h3,{id:"42-adapting-trees-to-regression-problems",style:{position:"relative"}},r.createElement(t.a,{href:"#42-adapting-trees-to-regression-problems","aria-label":"42 adapting trees to regression problems permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.2. Adapting trees to regression problems"),"\n",r.createElement(t.p,null,"For regression, each leaf outputs a numeric value, frequently the mean of the target values of the training samples in that leaf. Instead of using Gini or entropy, the algorithm can use metrics like variance or mean squared error (MSE) to evaluate potential splits."),"\n",r.createElement(t.p,null,"For example, if ",r.createElement(o.A,{text:"\\( S \\)"})," is a subset of training points ",r.createElement(o.A,{text:"\\( \\{(x_i, y_i)\\} \\)"})," in a leaf, a common approach is to minimize:"),"\n",r.createElement(o.A,{text:"\\[\n\\mathrm{MSE}(S) = \\frac{1}{|S|}\\sum_{(x_i, y_i)\\in S}(y_i - \\bar{y})^2,\n\\]"}),"\n",r.createElement(t.p,null,"where ",r.createElement(o.A,{text:"\\( \\bar{y} \\)"})," is the mean target value in ",r.createElement(o.A,{text:"\\( S \\)"}),"."),"\n",r.createElement(t.h3,{id:"43-comparison-of-classification-and-regression-trees",style:{position:"relative"}},r.createElement(t.a,{href:"#43-comparison-of-classification-and-regression-trees","aria-label":"43 comparison of classification and regression trees permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.3. Comparison of classification and regression trees"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Target variable type"),": Classification deals with discrete class labels; regression handles continuous targets."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Split criteria"),": Classification commonly uses impurity-based metrics (entropy, Gini). Regression uses variance reduction (e.g., MSE or MAE) as a measure of split quality."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Leaf predictions"),": Classification leaves predict a class, regression leaves predict a numeric value (often the mean)."),"\n"),"\n",r.createElement(t.h2,{id:"5-pruning-and-optimization",style:{position:"relative"}},r.createElement(t.a,{href:"#5-pruning-and-optimization","aria-label":"5 pruning and optimization permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5. Pruning and optimization"),"\n",r.createElement(t.h3,{id:"51-overfitting-in-decision-trees",style:{position:"relative"}},r.createElement(t.a,{href:"#51-overfitting-in-decision-trees","aria-label":"51 overfitting in decision trees permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.1. Overfitting in decision trees"),"\n",r.createElement(t.p,null,"Because decision trees grow greedily, they can overfit the training set by creating too many splits that capture noise rather than generalizable patterns. The tree might end up with an excessive number of leaves, each covering only a few samples."),"\n",r.createElement(t.h3,{id:"52-post-pruning-vs-pre-pruning",style:{position:"relative"}},r.createElement(t.a,{href:"#52-post-pruning-vs-pre-pruning","aria-label":"52 post pruning vs pre pruning permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.2. Post-pruning vs. pre-pruning"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Pre-pruning")," (early stopping): Halt the tree growth early if splitting further is unlikely to reduce generalization error. Examples include limiting the maximum depth or imposing a minimum number of samples in each node. However, an early stop may discard useful splits."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Post-pruning"),": Allow the tree to grow and then prune back sub-trees if they do not improve performance on a validation set. This often uses a separate ",r.createElement(l.A,null,"validation set")," or cross-validation to verify which sub-trees truly generalize well and which ones overfit."),"\n"),"\n",r.createElement(t.h3,{id:"53-using-validation-sets-or-cross-validation-for-pruning",style:{position:"relative"}},r.createElement(t.a,{href:"#53-using-validation-sets-or-cross-validation-for-pruning","aria-label":"53 using validation sets or cross validation for pruning permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"5.3. Using validation sets or cross-validation for pruning"),"\n",r.createElement(t.p,null,"A common practice is to:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,"Build a large tree."),"\n",r.createElement(t.li,null,"Systematically cut back (prune) sub-trees."),"\n",r.createElement(t.li,null,"Evaluate each pruned version on a validation set or via cross-validation."),"\n",r.createElement(t.li,null,"Select the pruned tree that yields the best validation score."),"\n"),"\n",r.createElement(t.p,null,"This helps manage the bias-variance tradeoff: deeper, more flexible trees have lower bias but higher variance; shallower trees have higher bias but lower variance."),"\n",r.createElement(t.h2,{id:"6-random-forest-and-random-subspace-method-rsm",style:{position:"relative"}},r.createElement(t.a,{href:"#6-random-forest-and-random-subspace-method-rsm","aria-label":"6 random forest and random subspace method rsm permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6. Random forest and random subspace method (RSM)"),"\n",r.createElement(t.p,null,"One of the major breakthroughs in overcoming the limitations of single decision trees was the invention of ",r.createElement(l.A,null,"Random Forests")," (Breiman, 2001). They form an ensemble of multiple decision trees, typically trained via ",r.createElement(l.A,null,"bagging")," (bootstrapped aggregation) and ",r.createElement(l.A,null,"feature randomness")," (random subspace method)."),"\n",r.createElement(t.h3,{id:"61-key-idea-of-random-forests",style:{position:"relative"}},r.createElement(t.a,{href:"#61-key-idea-of-random-forests","aria-label":"61 key idea of random forests permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.1. Key idea of random forests"),"\n",r.createElement(t.p,null,"A random forest is an ensemble of decision trees trained on different bootstrapped subsets of the data. Each tree typically:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,"Is grown to a large depth (often with minimal or no pruning)."),"\n",r.createElement(t.li,null,"Chooses candidate splits from a random subset of features at each node."),"\n"),"\n",r.createElement(t.p,null,"After training, the model aggregates (usually by majority vote in classification or averaging in regression) the predictions from all trees."),"\n",r.createElement(t.h4,{id:"611-bagging-refresher-conceptual",style:{position:"relative"}},r.createElement(t.a,{href:"#611-bagging-refresher-conceptual","aria-label":"611 bagging refresher conceptual permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.1.1. Bagging refresher (conceptual)"),"\n",r.createElement(t.p,null,r.createElement(l.A,null,"Bagging")," means each tree is trained on a ",r.createElement(l.A,null,"bootstrap sample")," â€” a random sample taken with replacement from the training set â€” often as large as the original dataset but containing duplicates and omissions of some samples. This increases the variance among individual trees while decreasing the correlation between them, helping the combined model generalize better."),"\n",r.createElement(t.h4,{id:"612-rsm-explained-in-detail",style:{position:"relative"}},r.createElement(t.a,{href:"#612-rsm-explained-in-detail","aria-label":"612 rsm explained in detail permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.1.2. RSM explained in detail"),"\n",r.createElement(t.p,null,r.createElement(l.A,null,"Random subspace method (RSM)")," randomly selects a subset of features at each split. If a dataset has ",r.createElement(o.A,{text:"\\( d \\)"})," features, we might pick ",r.createElement(o.A,{text:"\\( \\sqrt{d} \\)"})," or ",r.createElement(o.A,{text:"\\( \\log_2(d) \\)"})," features at random for each node. This further diversifies the trees in the ensemble, reducing correlation and improving overall performance."),"\n",r.createElement(t.h4,{id:"613-why-random-forest--rsm--bagging",style:{position:"relative"}},r.createElement(t.a,{href:"#613-why-random-forest--rsm--bagging","aria-label":"613 why random forest  rsm  bagging permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.1.3. Why random forest = RSM + bagging"),"\n",r.createElement(t.p,null,'By combining (1) random sampling of data points (bagging) and (2) random sampling of features (RSM), each tree in the forest is trained on a unique "view" of the dataset. Although each tree can be quite overfit to its subsample, the overall ',r.createElement(l.A,null,"ensemble average")," smooths out the overfitting, usually yielding superior results compared to a single tree."),"\n",r.createElement(t.h3,{id:"62-feature-selection-and-randomness-in-tree-construction",style:{position:"relative"}},r.createElement(t.a,{href:"#62-feature-selection-and-randomness-in-tree-construction","aria-label":"62 feature selection and randomness in tree construction permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.2. Feature selection and randomness in tree construction"),"\n",r.createElement(t.p,null,"A further advantage is that random forests inherently perform a form of feature selection: features less useful for splitting are rarely chosen, while strong predictors tend to produce more informative splits. The randomness introduced (both in data sampling and feature selection) makes random forests resilient to noise in training data and robust in many real-world scenarios."),"\n",r.createElement(t.h2,{id:"7-example-of-a-random-forest-with-bootstrapped-dataset",style:{position:"relative"}},r.createElement(t.a,{href:"#7-example-of-a-random-forest-with-bootstrapped-dataset","aria-label":"7 example of a random forest with bootstrapped dataset permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7. Example of a random forest with bootstrapped dataset"),"\n",r.createElement(t.h3,{id:"71-step-by-step-bootstrapping-process",style:{position:"relative"}},r.createElement(t.a,{href:"#71-step-by-step-bootstrapping-process","aria-label":"71 step by step bootstrapping process permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.1. Step-by-step bootstrapping process"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Original dataset"),": Suppose we have ",r.createElement(o.A,{text:"\\( N \\)"})," training samples."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Bootstrap sample"),": We create a new dataset by randomly picking ",r.createElement(o.A,{text:"\\( N \\)"})," samples from the original, ",r.createElement(t.em,null,"with replacement"),". Some samples may appear multiple times, others not at all."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Train a decision tree"),": Build a tree on that bootstrapped dataset to its full (or near-full) depth."),"\n"),"\n",r.createElement(t.p,null,"We repeat these steps multiple times, generating multiple trees."),"\n",r.createElement(t.h3,{id:"72-building-multiple-trees",style:{position:"relative"}},r.createElement(t.a,{href:"#72-building-multiple-trees","aria-label":"72 building multiple trees permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.2. Building multiple trees"),"\n",r.createElement(t.p,null,"For a random forest, each tree is also restricted in which features it can split on at each node. For instance, each node might only consider ",r.createElement(o.A,{text:"\\( m \\)"})," random features (with ",r.createElement(o.A,{text:"\\( m \\le d \\)"}),", where ",r.createElement(o.A,{text:"\\( d \\)"})," is the total number of features)."),"\n",r.createElement(a,{alt:"Random forest flow",path:"",caption:"Illustration of how random forests build multiple trees from bootstrapped data and random subsets of features",zoom:"false"}),"\n",r.createElement(t.h3,{id:"73-aggregating-predictions-and-interpretation",style:{position:"relative"}},r.createElement(t.a,{href:"#73-aggregating-predictions-and-interpretation","aria-label":"73 aggregating predictions and interpretation permalink",className:"anchor before"},r.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.3. Aggregating predictions and interpretation"),"\n",r.createElement(t.p,null,"To predict a label for a new sample ",r.createElement(o.A,{text:"\\( x^* \\)"}),":"),"\n",r.createElement(t.ul,null,"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Classification"),": each tree votes for a class, and we take the majority vote."),"\n",r.createElement(t.li,null,r.createElement(t.strong,null,"Regression"),": each tree outputs a real value, and we take the mean."),"\n"),"\n",r.createElement(t.p,null,"This ensemble approach typically yields robust performance and is far less prone to overfitting than a single deep decision tree."),"\n",r.createElement(t.p,null,"Below is a short Python example illustrating a random forest using scikit-learn on a simple dataset:"),"\n",r.createElement(s.A,{text:'\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Load data\niris = load_iris()\nX = iris.data  # shape (150, 4)\ny = iris.target\n\n# Train/test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Create a random forest\nclf = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42)\nclf.fit(X_train, y_train)\n\n# Evaluate\naccuracy = clf.score(X_test, y_test)\nprint("Accuracy on test set: ", accuracy)\n'}),"\n",r.createElement(t.p,null,"In this example, each of the 100 trees is trained on a bootstrapped sample of ",r.createElement(l.A,null,"70%")," of the data, and each node chooses from a random subset of features when finding the best split. The final prediction is an ensemble of all trees."),"\n",r.createElement(t.hr),"\n",r.createElement(t.p,null,"Such tree-based ensembles continue to be improved upon in research and practice. But even in their basic form, decision trees and random forests remain powerful, explainable, and often surprisingly accurate methods for a wide range of tasks."))}var m=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,i.RP)(),e.components);return t?r.createElement(t,e,r.createElement(c,e)):c(e)};var h=a(36710),d=a(58481),u=a.n(d),p=a(36310),g=a(87245),f=a(27042),v=a(59849),E=a(5591),b=a(61122),y=a(9219),S=a(33203),w=a(95751),x=a(94328),H=a(80791),_=a(78137);const A=e=>{let{toc:t}=e;if(!t||!t.items)return null;return r.createElement("nav",{className:H.R},r.createElement("ul",null,t.items.map(((e,t)=>r.createElement("li",{key:t},r.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const a=t.replace("#",""),n=document.getElementById(a);n&&n.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&r.createElement(A,{toc:{items:e.items}}))))))};function k(e){let{data:{mdx:t,allMdx:l,allPostImages:s},children:o}=e;const{frontmatter:c,body:m,tableOfContents:h}=t,d=c.index,v=c.slug.split("/")[1],H=l.nodes.filter((e=>e.frontmatter.slug.includes(`/${v}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),k=H.findIndex((e=>e.frontmatter.index===d)),C=H[k+1],M=H[k-1],z=c.slug.replace(/\/$/,""),T=/[^/]*$/.exec(z)[0],V=`posts/${v}/content/${T}/`,{0:I,1:B}=(0,r.useState)(c.flagWideLayoutByDefault),{0:L,1:N}=(0,r.useState)(!1);var P;(0,r.useEffect)((()=>{N(!0);const e=setTimeout((()=>N(!1)),340);return()=>clearTimeout(e)}),[I]),"adventures"===v?P=y.cb:"research"===v?P=y.Qh:"thoughts"===v&&(P=y.T6);const R=u()(m).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,G=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),a=e%60;return a<=30?`~${t}${a>0?".5":""} h`:`~${t+1} h`}(Math.ceil(R/P)+(c.extraReadTimeMin||0)),O=[{flag:c.flagDraft,component:()=>Promise.all([a.e(3231),a.e(8809)]).then(a.bind(a,28809))},{flag:c.flagMindfuckery,component:()=>Promise.all([a.e(3231),a.e(2471)]).then(a.bind(a,67709))},{flag:c.flagRewrite,component:()=>Promise.all([a.e(3231),a.e(6764)]).then(a.bind(a,62002))},{flag:c.flagOffensive,component:()=>Promise.all([a.e(3231),a.e(2443)]).then(a.bind(a,17681))},{flag:c.flagProfane,component:()=>Promise.all([a.e(3231),a.e(8048)]).then(a.bind(a,53286))},{flag:c.flagMultilingual,component:()=>Promise.all([a.e(3231),a.e(4069)]).then(a.bind(a,78831))},{flag:c.flagUnreliably,component:()=>Promise.all([a.e(3231),a.e(3417)]).then(a.bind(a,8179))},{flag:c.flagPolitical,component:()=>Promise.all([a.e(3231),a.e(5195)]).then(a.bind(a,30433))},{flag:c.flagCognitohazard,component:()=>Promise.all([a.e(3231),a.e(3175)]).then(a.bind(a,8413))},{flag:c.flagHidden,component:()=>Promise.all([a.e(3231),a.e(9556)]).then(a.bind(a,14794))}],{0:$,1:j}=(0,r.useState)([]);return(0,r.useEffect)((()=>{O.forEach((e=>{let{flag:t,component:a}=e;t&&a().then((e=>{j((t=>[].concat((0,n.A)(t),[e.default])))}))}))}),[]),r.createElement(f.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},r.createElement(E.A,{postNumber:c.index,date:c.date,updated:c.updated,readTime:G,difficulty:c.difficultyLevel,title:c.title,desc:c.desc,banner:c.banner,section:v,postKey:T,isMindfuckery:c.flagMindfuckery,mainTag:c.mainTag}),r.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},c.otherTags.map(((e,t)=>r.createElement("span",{key:t,className:`noselect ${_.MW}`,style:{margin:"0 5px 5px 0"}},e)))),r.createElement("div",{class:"postBody"},r.createElement(A,{toc:h})),r.createElement("br"),r.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},r.createElement(f.P.button,{class:"noselect",className:x.pb,id:x.xG,onClick:()=>{B(!I)},whileTap:{scale:.93}},r.createElement(f.P.div,{className:w.DJ,key:I,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},I?"Switch to default layout":"Switch to wide layout"))),r.createElement("br"),r.createElement("div",{class:"postBody",style:{margin:I?"0 -14%":"",maxWidth:I?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},r.createElement("div",{className:`${x.P_} ${L?x.Xn:x.qG}`},$.map(((e,t)=>r.createElement(e,{key:t}))),c.indexCourse?r.createElement(S.A,{index:c.indexCourse,category:c.courseCategoryName}):"",r.createElement(p.Z.Provider,{value:{images:s.nodes,basePath:V.replace(/\/$/,"")+"/"}},r.createElement(i.xA,{components:{Image:g.A}},o)))),r.createElement(b.A,{nextPost:C,lastPost:M,keyCurrent:T,section:v}))}function C(e){return r.createElement(k,e,r.createElement(m,e))}function M(e){var t,a,n,i,l;let{data:s}=e;const{frontmatter:o}=s.mdx,c=o.titleSEO||o.title,m=o.titleOG||c,d=o.titleTwitter||c,u=o.descSEO||o.desc,p=o.descOG||u,g=o.descTwitter||u,f=o.schemaType||"BlogPosting",E=o.keywordsSEO,b=o.date,y=o.updated||b,S=o.imageOG||(null===(t=o.banner)||void 0===t||null===(a=t.childImageSharp)||void 0===a||null===(n=a.gatsbyImageData)||void 0===n||null===(i=n.images)||void 0===i||null===(l=i.fallback)||void 0===l?void 0:l.src),w=o.imageAltOG||p,x=o.imageTwitter||S,H=o.imageAltTwitter||g,_=o.canonicalURL,A=o.flagHidden||!1,k=o.mainTag||"Posts",C=o.slug.split("/")[1]||"posts",{siteUrl:M}=(0,h.Q)(),z={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:M},{"@type":"ListItem",position:2,name:k,item:`${M}/${o.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${M}${o.slug}`}]};return r.createElement(v.A,{title:c+" - avrtt.blog",titleOG:m,titleTwitter:d,description:u,descriptionOG:p,descriptionTwitter:g,schemaType:f,keywords:E,datePublished:b,dateModified:y,imageOG:S,imageAltOG:w,imageTwitter:x,imageAltTwitter:H,canonicalUrl:_,flagHidden:A,mainTag:k,section:C,type:"article"},r.createElement("script",{type:"application/ld+json"},JSON.stringify(z)))}},96098:function(e,t,a){var n=a(96540),i=a(7978);t.A=e=>{let{text:t}=e;return n.createElement(i.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-js-content-file-path-src-pages-posts-research-decision-trees-mdx-eb49242b608a73d7a6f1.js.map