"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[2187],{5755:function(e,t,n){n.r(t),n.d(t,{Head:function(){return T},PostTemplate:function(){return z},default:function(){return A}});var a=n(54506),i=n(28453),l=n(96540),r=n(16886),o=n(46295),s=n(96098);function c(e){const t=Object.assign({p:"p",h3:"h3",a:"a",span:"span",ol:"ol",li:"li",strong:"strong",h2:"h2",ul:"ul",hr:"hr"},(0,i.RP)(),e.components),{Image:n}=t;return n||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),l.createElement(l.Fragment,null,"\n",l.createElement("br"),"\n","\n","\n",l.createElement(t.p,null,"Computer vision is a subfield of artificial intelligence and machine learning dedicated to enabling machines to interpret and derive meaningful information from visual data, such as images and videos. At its core, computer vision seeks to mimic aspects of human visual perception and understanding, but it also aims to surpass our capabilities by virtue of speed, consistency, and the ability to leverage massive datasets. In practice, this involves the development of algorithms, models, and pipelines that can detect, classify, localize, segment, and track objects or regions of interest within a visual scene. By transforming raw pixel intensities into higher-level insights, computers can then automate tasks — from recognizing faces to guiding autonomous vehicles — that traditionally required direct human visual expertise."),"\n",l.createElement(t.p,null,"While early theoretical work in computer vision focused on simple edge detection and feature extraction, rapid advances in both hardware (GPUs, specialized AI accelerators) and software (deep learning frameworks, optimized libraries for high-throughput computation) have propelled the field to new frontiers. Today, computer vision technology underpins numerous industrial and consumer applications, driving innovation across multiple sectors and domains."),"\n",l.createElement(t.h3,{id:"historical-context",style:{position:"relative"}},l.createElement(t.a,{href:"#historical-context","aria-label":"historical context permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"historical context"),"\n",l.createElement(t.p,null,"The development of computer vision spans several decades, with its roots in the early pattern recognition and artificial intelligence research of the 1960s and 1970s. During the 1970s, classical edge detection research (e.g., the Sobel filter, Canny edge operator) laid the groundwork for methods that transformed images into sets of meaningful features. Various researchers realized that robust image segmentation and feature extraction were vital to any form of higher-level recognition or scene understanding."),"\n",l.createElement(t.p,null,"In the 1980s, the introduction of neural networks — particularly the work on the Neocognitron and early multilayer perceptrons — began to show promise for visual pattern recognition. However, hardware limitations and the scarcity of large labeled datasets made it challenging to train these models to handle complex real-world data. Over time, new techniques emerged to address these limitations, such as Support Vector Machines (SVMs) for image classification and various feature descriptor methods (SIFT, SURF, HOG) for object detection and recognition. During the 2000s, these methods became standard in practical computer vision pipelines, especially for tasks that required robust matching and recognition in real-world conditions."),"\n",l.createElement(t.p,null,'A watershed moment occurred in 2012 when a deep convolutional neural network (AlexNet) trained on the massive ImageNet dataset achieved a dramatic improvement in image classification performance (Krizhevsky and gang, NIPS 2012). This victory in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) ignited a frenzy of research and innovation, ushering in what is often called the "deep learning era" of computer vision. Successive breakthroughs — VGG, GoogLeNet, ResNet, DenseNet, EfficientNet, and Vision Transformers (ViTs) — have since led to steady improvements in accuracy, speed, and robustness.'),"\n",l.createElement(t.h3,{id:"common-real-world-applications",style:{position:"relative"}},l.createElement(t.a,{href:"#common-real-world-applications","aria-label":"common real world applications permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"common real-world applications"),"\n",l.createElement(t.p,null,"Computer vision's influence spans a vast array of fields, reflecting the ubiquity of visual data in modern life:"),"\n",l.createElement(t.ol,null,"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Autonomous driving"),": Self-driving cars rely on object detection and tracking to identify vehicles, pedestrians, traffic lights, and road signs. Lane detection and free-space segmentation help guide steering and ensure safety."),"\n"),"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Facial recognition"),": Used extensively in security systems, personal device access, surveillance, and social media platforms for tagging and identity verification."),"\n"),"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Medical imaging"),": Radiology, pathology, and other medical disciplines increasingly leverage computer vision to detect abnormalities such as tumors in MRI or CT scans. Automated systems can assist doctors in diagnosis and treatment planning, reducing human error and improving patient outcomes."),"\n"),"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Robotics"),": In industrial settings, computer vision assists robots in tasks like pick-and-place, assembly, and inspection. Mobile robots utilize vision-based simultaneous localization and mapping (SLAM) for navigation and obstacle avoidance."),"\n"),"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Agriculture"),": Drones and remote sensing cameras identify plant diseases, estimate crop yield, and monitor field conditions to optimize resource usage."),"\n"),"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Industrial inspection"),": High-throughput camera systems rapidly detect defects in manufacturing lines, ensuring product quality."),"\n"),"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Augmented reality (AR) and virtual reality (VR)"),": Vision-based understanding of the user's surroundings allows seamless overlay of virtual objects (AR) or immersive environment creation (VR)."),"\n"),"\n"),"\n",l.createElement(t.p,null,"From broad societal applications like retail checkout systems and traffic management, to specialized fields like marine biology (underwater exploration and species identification), the significance of computer vision continues to grow. The field now stands at the forefront of AI research, with new breakthroughs poised to reshape our interaction with machines."),"\n",l.createElement(t.h2,{id:"fundamentals-of-image-processing-and-representation",style:{position:"relative"}},l.createElement(t.a,{href:"#fundamentals-of-image-processing-and-representation","aria-label":"fundamentals of image processing and representation permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Fundamentals of image processing and representation"),"\n",l.createElement(t.h3,{id:"image-formation",style:{position:"relative"}},l.createElement(t.a,{href:"#image-formation","aria-label":"image formation permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"image formation"),"\n",l.createElement(t.p,null,"Understanding how images are formed is essential to computer vision, as it lays the groundwork for how machines interpret visual data. A common simplified model of image formation is the ",l.createElement(r.A,null,"pinhole camera model"),", which conceptualizes how 3D scenes are projected onto a 2D plane (the image sensor)."),"\n",l.createElement(t.p,null,"A pinhole camera, in idealized form, consists of a small aperture through which light rays pass, projecting an inverted view of the scene onto a planar surface. In modern cameras, a lens system replaces the single pinhole, but the geometry of projection remains conceptually similar."),"\n",l.createElement(t.p,null,"We describe the mapping from a 3D point ",l.createElement(s.A,{text:"\\(P = (X, Y, Z)\\)"})," to a 2D image coordinate ",l.createElement(s.A,{text:"\\(p = (x, y)\\)"})," often with the equation:"),"\n",l.createElement(s.A,{text:"\\[\n\\begin{bmatrix}\nx \\\\\ny \\\\\n1\n\\end{bmatrix}\n= K \\, [R \\mid t] \\, \n\\begin{bmatrix}\nX \\\\\nY \\\\\nZ \\\\\n1\n\\end{bmatrix}\n\\]"}),"\n",l.createElement(t.p,null,"Where:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(s.A,{text:"\\(K\\)"})," is the camera intrinsic matrix (focal length, principal point, and skew)."),"\n",l.createElement(t.li,null,l.createElement(s.A,{text:"\\(R\\)"})," is the rotation matrix representing the camera's orientation."),"\n",l.createElement(t.li,null,l.createElement(s.A,{text:"\\(t\\)"})," is the translation vector representing the camera's position in the world."),"\n"),"\n",l.createElement(t.p,null,"These intrinsic and extrinsic parameters define how the camera captures light and projects it onto the image plane. Conceptually, ",l.createElement(s.A,{text:"\\(K\\)"})," captures the geometry of the camera itself, while ",l.createElement(s.A,{text:"\\(R\\)"})," and ",l.createElement(s.A,{text:"\\(t\\)"})," represent how the camera is oriented and placed in the environment."),"\n",l.createElement(t.p,null,"Images themselves are often stored as matrices of pixel values, with each pixel encoding color or intensity information. The pixel array has dimensions of resolution (width and height), and each pixel might store color channels (e.g., red, green, and blue) or a single grayscale value. The resolution and aspect ratio (the ratio of width to height) are fundamental descriptors of an image's geometry, influencing both visual quality and computational cost in processing."),"\n",l.createElement(t.h3,{id:"thresholding-filtering-edge-detection",style:{position:"relative"}},l.createElement(t.a,{href:"#thresholding-filtering-edge-detection","aria-label":"thresholding filtering edge detection permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"thresholding, filtering, edge detection"),"\n",l.createElement(t.p,null,"Before applying complex recognition algorithms, computer vision pipelines commonly employ classic image processing techniques. These methods can enhance features, suppress noise, or facilitate straightforward object segmentation."),"\n",l.createElement(t.ol,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Thresholding"),": A simple but powerful segmentation technique that turns a grayscale or color image into a binary image based on a threshold value. For instance, you might convert each pixel ",l.createElement(s.A,{text:"\\(I(x, y)\\)"})," into:"),"\n"),"\n",l.createElement(s.A,{text:"\\[\nI_{\\text{binary}}(x, y) = \n\\begin{cases}\n1 & \\text{if } I(x, y) \\ge \\tau\\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]"}),"\n",l.createElement(t.p,null,"where ",l.createElement(s.A,{text:"\\(\\tau\\)"})," is a predefined threshold. Otsu's method (Otsu, IEEE Trans. SMC, 1979) automatically selects an optimal threshold by minimizing intra-class intensity variance."),"\n",l.createElement(t.ol,{start:"2"},"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Filtering"),": Convolving an image with specific kernels allows enhancement or suppression of certain spatial frequencies. Common examples include:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Gaussian blur"),": Uses a Gaussian kernel to smooth the image, typically to reduce noise or detail."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Median filter"),": Replaces each pixel with the median of neighboring pixel values, effective against salt-and-pepper noise."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Sharpening filters"),": Enhance edges and fine details by emphasizing high-frequency components."),"\n"),"\n"),"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Edge detection"),": Detecting boundaries in an image is crucial for feature extraction. Examples include:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Sobel operator"),": Computes an approximation of the gradient of intensity, highlighting regions of rapid intensity change."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Canny edge detector")," (Canny, IEEE Trans. PAMI, 1986): A multi-stage algorithm involving gradient calculation, non-maximum suppression, and hysteresis thresholding for robust and thin edges."),"\n"),"\n"),"\n"),"\n",l.createElement(t.p,null,"Edge detection is significant in tasks like object boundary extraction, shape recognition, and image registration. Combined with thresholding or morphological operations, edges provide geometric cues about the underlying structures in an image."),"\n",l.createElement(t.h3,{id:"color-spaces-and-transformations",style:{position:"relative"}},l.createElement(t.a,{href:"#color-spaces-and-transformations","aria-label":"color spaces and transformations permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"color spaces and transformations"),"\n",l.createElement(t.p,null,"In computer vision, it is often beneficial to transform from the standard RGB color space into alternative representations that might simplify or improve specific tasks (e.g., segmentation or color-based feature extraction)."),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"HSV (Hue, Saturation, Value)"),': Separates color into hue (the "color" component), saturation (the intensity of the color), and value (brightness). This is often more aligned with how humans perceive color, making thresholding or segmentation by color more intuitive.'),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Lab color space"),": Decomposes a color into L (lightness) and two chromaticity components a (green-red) and b (blue-yellow). One advantage is its approximate perceptual uniformity — a small Euclidean distance in Lab often corresponds to a small perceived difference in color."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"YCrCb"),": Common in video compression, separates luminance (Y) from chrominance (Cr and Cb) components, which can be processed or compressed differently according to human vision's varying sensitivity."),"\n"),"\n",l.createElement(t.p,null,"Sometimes, tasks like skin detection, fruit ripeness assessment, or specialized segmentation benefit greatly from converting an image to a color space where the relevant features (e.g., color hue) appear more distinct."),"\n",l.createElement(t.h3,{id:"additional-techniques",style:{position:"relative"}},l.createElement(t.a,{href:"#additional-techniques","aria-label":"additional techniques permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"additional techniques"),"\n",l.createElement(t.p,null,"Further essential image manipulations serve as building blocks or pre-processing steps:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Morphological operations"),": In binary images, morphological transformations (e.g., erosion and dilation) can refine or correct the structure of segmented objects."),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Erosion")," shrinks the foreground by stripping away boundary pixels, helpful for removing isolated noise."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Dilation")," expands foreground regions, bridging gaps or holes within objects."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Opening")," (erosion followed by dilation) and ",l.createElement(t.strong,null,"closing")," (dilation followed by erosion) can correct small artifacts or join disconnected parts of objects."),"\n"),"\n"),"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Geometric transformations"),": Rotation, scaling, translation, or more complex transformations like perspective warping. These allow for dataset augmentation or correction of geometric misalignment."),"\n"),"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Histogram equalization"),": Adjusts contrast by redistributing the intensity histogram. A well-known approach is Contrast Limited Adaptive Histogram Equalization (CLAHE), which can locally normalize contrast in different regions of an image without over-amplifying noise."),"\n"),"\n"),"\n",l.createElement(t.p,null,"All these techniques represent valuable steps in a computer vision workflow, often used in tandem with more sophisticated methods for classification or detection. Even in the era of deep learning, traditional image processing remains highly relevant for data preprocessing, augmentation, and interpretability."),"\n",l.createElement(t.h2,{id:"essential-tools-and-libraries",style:{position:"relative"}},l.createElement(t.a,{href:"#essential-tools-and-libraries","aria-label":"essential tools and libraries permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Essential tools and libraries"),"\n",l.createElement(t.h3,{id:"opencv-basics",style:{position:"relative"}},l.createElement(t.a,{href:"#opencv-basics","aria-label":"opencv basics permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"OpenCV basics"),"\n",l.createElement(t.p,null,"OpenCV (Open Source Computer Vision Library) is a foundational library for real-time computer vision. With interfaces in C++, Python, Java, and more, it offers numerous functions for image processing, feature detection, video analysis, and machine learning."),"\n",l.createElement(t.p,null,"A typical Python installation with OpenCV might look like:"),"\n",l.createElement(o.A,{text:"\npip install opencv-python\n"}),"\n",l.createElement(t.p,null,"Once installed, you can read, display, and write images:"),"\n",l.createElement(o.A,{text:'\nimport cv2\n\n# Read an image from disk (file path)\nimage = cv2.imread("my_image.jpg")  \n\n# Display the image in a window\ncv2.imshow("Window Name", image)\ncv2.waitKey(0)  \ncv2.destroyAllWindows()\n\n# Write an image to disk\ncv2.imwrite("output_image.png", image)\n'}),"\n",l.createElement(t.p,null,"OpenCV also supports capturing frames from webcams or video files, plus a rich array of transformations (resizing, cropping, rotating, morphological operations), thresholding, edge detection, and so on."),"\n",l.createElement(t.h3,{id:"other-popular-python-libraries",style:{position:"relative"}},l.createElement(t.a,{href:"#other-popular-python-libraries","aria-label":"other popular python libraries permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"other popular python libraries"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"NumPy"),": The fundamental package for scientific computing with Python. Images in OpenCV are typically stored as NumPy arrays, making it easy to perform low-level array manipulations or to integrate OpenCV with other data science or ML libraries."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Matplotlib"),": A versatile library for plotting and visualizations. Often used in Jupyter notebooks for displaying images inline and generating charts to visualize intermediate results, such as loss curves or detection bounding boxes."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"scikit-image"),": A collection of algorithms for image processing, including advanced techniques for restoration, segmentation, and feature extraction. It sometimes complements or extends what OpenCV offers, especially in areas like morphological filtering and advanced transforms (e.g., Hough transforms)."),"\n"),"\n",l.createElement(t.h3,{id:"additional-frameworks",style:{position:"relative"}},l.createElement(t.a,{href:"#additional-frameworks","aria-label":"additional frameworks permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"additional frameworks"),"\n",l.createElement(t.p,null,"Many tasks in modern computer vision workflows rely on deep learning. Two of the most widespread frameworks are:"),"\n",l.createElement(t.ol,null,"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"PyTorch"),": Developed primarily by Meta AI (formerly Facebook AI Research), PyTorch provides a flexible, Pythonic interface for building neural networks. Its dynamic computation graph has made it extremely popular for research, while support for accelerated training on GPUs or specialized hardware ensures it remains efficient for large-scale production tasks."),"\n"),"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"TensorFlow"),": An end-to-end open-source platform for machine learning created by Google. TensorFlow also has high-level APIs like Keras for streamlined model prototyping, plus TensorBoard for visualization of training metrics. TensorFlow Lite targets mobile and embedded applications."),"\n"),"\n"),"\n",l.createElement(t.p,null,"Using these frameworks for computer vision tasks typically involves the following steps:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"Loading or generating datasets (images, bounding boxes, segmentation masks)."),"\n",l.createElement(t.li,null,"Constructing a neural network architecture (Convolutional Neural Networks, Transformers, etc.)."),"\n",l.createElement(t.li,null,"Defining a loss function and optimization strategy."),"\n",l.createElement(t.li,null,"Training the model on GPUs or specialized accelerators."),"\n",l.createElement(t.li,null,"Evaluating performance on validation and test sets."),"\n",l.createElement(t.li,null,"Deploying the trained model to production (e.g., using TensorFlow Serving, TorchServe, or exporting to an ONNX format)."),"\n"),"\n",l.createElement(t.p,null,"In practice, combining domain-specific libraries (OpenCV, scikit-image) with deep learning frameworks (PyTorch, TensorFlow) yields a powerful environment for tackling a wide range of computer vision challenges."),"\n",l.createElement(t.h2,{id:"deep-learning-for-computer-vision",style:{position:"relative"}},l.createElement(t.a,{href:"#deep-learning-for-computer-vision","aria-label":"deep learning for computer vision permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"deep learning for computer vision"),"\n",l.createElement(t.h3,{id:"convolution-and-pooling-layers",style:{position:"relative"}},l.createElement(t.a,{href:"#convolution-and-pooling-layers","aria-label":"convolution and pooling layers permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"convolution and pooling layers"),"\n",l.createElement(t.p,null,"Convolutional neural networks (CNNs) are central to modern computer vision. Their core operation, the 2D convolution, serves as a feature extractor that learns spatial hierarchies of patterns directly from data. The convolution operation for a single 2D feature map can be expressed as:"),"\n",l.createElement(s.A,{text:"\\[\ny_{k, l} = \\sum_{i}\\sum_{j} X_{k+i, l+j}\\, W_{i, j} + b\n\\]"}),"\n",l.createElement(t.p,null,"where:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(s.A,{text:"\\(X_{k+i, l+j}\\)"})," is the input feature map's pixel (or neuron) value at position ",l.createElement(s.A,{text:"(k+i, l+j)"}),"."),"\n",l.createElement(t.li,null,l.createElement(s.A,{text:"\\(W_{i, j}\\)"})," is the weight of the kernel filter at offset ",l.createElement(s.A,{text:"(i, j)"}),"."),"\n",l.createElement(t.li,null,l.createElement(s.A,{text:"\\(b\\)"})," is the bias term, often included in the operation."),"\n"),"\n",l.createElement(t.p,null,"Key concepts:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Local receptive fields"),": Each filter covers only a small spatial area of the input."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Stride"),": How many pixels the filter window moves each step."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Padding"),": Adding zero (or other) values around the input edges, controlling the spatial dimensionality of the output."),"\n"),"\n",l.createElement(t.p,null,"Pooling layers (max or average pooling) reduce the spatial resolution by aggregating information, thereby reducing the parameter count and controlling overfitting. A max pooling layer with size 2×2 and stride 2, for example, splits the feature map into non-overlapping 2×2 blocks, taking the maximum value in each block."),"\n",l.createElement(t.h3,{id:"batch-normalization",style:{position:"relative"}},l.createElement(t.a,{href:"#batch-normalization","aria-label":"batch normalization permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"batch normalization"),"\n",l.createElement(t.p,null,"Batch normalization normalizes the activations of each layer within a mini-batch. By preventing large shifts in the distribution of intermediate layer outputs, batch normalization:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"Accelerates training by allowing larger learning rates."),"\n",l.createElement(t.li,null,"Stabilizes gradient flow."),"\n",l.createElement(t.li,null,"Acts as a regularizer, often reducing the need for other forms of regularization."),"\n"),"\n",l.createElement(t.p,null,"It can be expressed as:"),"\n",l.createElement(s.A,{text:"\\[\n\\hat{x} = \\frac{x - \\mu_{\\mathcal{B}}}{\\sqrt{\\sigma_{\\mathcal{B}}^{2} + \\epsilon}}, \\quad\ny = \\gamma \\hat{x} + \\beta\n\\]"}),"\n",l.createElement(t.p,null,"Where ",l.createElement(s.A,{text:"\\(x\\)"})," is an activation within the mini-batch ",l.createElement(s.A,{text:"(\\mathcal{B})"}),"; ",l.createElement(s.A,{text:"\\(\\mu_{\\mathcal{B}}\\)"})," and ",l.createElement(s.A,{text:"\\(\\sigma_{\\mathcal{B}}^{2}\\)"})," are the mean and variance within that mini-batch; ",l.createElement(s.A,{text:"\\(\\gamma\\)"})," and ",l.createElement(s.A,{text:"\\(\\beta\\)"})," are trainable parameters that allow the normalized activations to scale and shift."),"\n",l.createElement(t.h3,{id:"depthwise-separable-dw-convolution",style:{position:"relative"}},l.createElement(t.a,{href:"#depthwise-separable-dw-convolution","aria-label":"depthwise separable dw convolution permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"depthwise separable (DW) convolution"),"\n",l.createElement(t.p,null,"A standard 2D convolution simultaneously learns filters for both spatial and cross-channel mixing, leading to a large computational overhead for wide or deep networks. Depthwise separable convolution breaks down the convolution into two parts:"),"\n",l.createElement(t.ol,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Depthwise convolution"),": Applies a single filter to each input channel separately."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Pointwise convolution"),": Combines the separate channel outputs with a 1×1 convolution that mixes them."),"\n"),"\n",l.createElement(t.p,null,"This factorization drastically reduces the number of parameters and multiplications. Used in architectures like MobileNet (Howard and gang, arXiv 2017) and Xception (Chollet, CVPR 2017), depthwise separable convolutions are particularly handy for mobile and embedded vision applications."),"\n",l.createElement(t.h3,{id:"popular-cnn-architectures",style:{position:"relative"}},l.createElement(t.a,{href:"#popular-cnn-architectures","aria-label":"popular cnn architectures permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"popular CNN architectures"),"\n",l.createElement(t.ol,null,"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"LeNet")," (LeCun and gang, 1990s): Pioneered convolutional networks for digit recognition on the MNIST dataset. Uses a few convolution layers followed by fully connected layers."),"\n"),"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"AlexNet")," (Krizhevsky and gang, NIPS 2012): Set a new state of the art on ImageNet classification with deeper architecture and GPU training. Introduced ReLU activations for faster training."),"\n"),"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"VGG")," (Simonyan and Zisserman, ICLR 2015): Demonstrated that deeper networks (16 or 19 layers) with small 3×3 convolutions significantly improve accuracy. However, it was computationally expensive."),"\n"),"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"GoogLeNet / Inception")," (Szegedy and gang, CVPR 2015): Proposed the Inception module that computes multiple filter sizes in parallel and concatenates them, improving efficiency and multi-scale feature extraction."),"\n"),"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"ResNet")," (He and gang, CVPR 2016): Introduced residual connections that allow gradients to flow unimpeded through deeper networks, tackling the vanishing gradient problem. Some ResNet variants exceed 100 layers in depth."),"\n"),"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"DenseNet")," (Huang and gang, CVPR 2017): Connects each layer to every other layer in a dense connectivity pattern, reducing parameter count and improving feature reuse."),"\n"),"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"EfficientNet")," (Tan and Le, ICML 2019): Scales width, depth, and resolution in a principled way to find more efficient model families."),"\n"),"\n"),"\n",l.createElement(t.h3,{id:"training-basics",style:{position:"relative"}},l.createElement(t.a,{href:"#training-basics","aria-label":"training basics permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"training basics"),"\n",l.createElement(t.p,null,"Deep learning relies heavily on efficient optimization methods to adjust network parameters:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Gradient descent")," and ",l.createElement(t.strong,null,"Stochastic Gradient Descent (SGD)")," are the standard."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Momentum"),"-based methods help accelerate training and escape local minima by adding a fraction of the previous update to the current update."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Adam")," blends momentum and RMSProp ideas, adapting the learning rate for each parameter based on first and second moments of gradients."),"\n"),"\n",l.createElement(t.p,null,"During ",l.createElement(t.strong,null,"backpropagation"),", each layer's parameters are updated by computing the gradient of the loss function with respect to those parameters. The process is repeated over many epochs (full passes through the dataset), ideally converging to a local (or global) minimum in the parameter space."),"\n",l.createElement(t.h3,{id:"regularization-and-data-augmentation",style:{position:"relative"}},l.createElement(t.a,{href:"#regularization-and-data-augmentation","aria-label":"regularization and data augmentation permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"regularization and data augmentation"),"\n",l.createElement(t.p,null,"Deep neural networks are prone to overfitting, especially if the training data is limited. Common techniques to address overfitting:"),"\n",l.createElement(t.ol,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Dropout"),": Randomly drops a fraction of neurons (and their connections) during training, preventing co-adaptation of features."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"L2 regularization"),": Penalizes large weights by adding a term ",l.createElement(s.A,{text:"\\(\\lambda ||W||_2^2\\)"})," to the loss function, encouraging smaller parameter values."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Data augmentation"),": Artificially expands the dataset with label-preserving transformations (random crops, flips, rotations, color jitter). Networks exposed to these variants generalize better."),"\n"),"\n",l.createElement(t.h3,{id:"transfer-learning",style:{position:"relative"}},l.createElement(t.a,{href:"#transfer-learning","aria-label":"transfer learning permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"transfer learning"),"\n",l.createElement(t.p,null,"Modern deep networks typically require large datasets and extensive compute resources. Transfer learning addresses this by taking a network pre-trained on a large dataset like ImageNet and fine-tuning its weights on a new, typically smaller dataset. This approach often yields strong performance with significantly reduced training time. You typically freeze the early layers that contain general feature extractors (edges, textures) and adapt the latter layers for the new classification or detection task."),"\n",l.createElement(t.h2,{id:"object-detection-and-beyond",style:{position:"relative"}},l.createElement(t.a,{href:"#object-detection-and-beyond","aria-label":"object detection and beyond permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"object detection and beyond"),"\n",l.createElement(t.h3,{id:"single-shot-networks-vs-two-stage-networks",style:{position:"relative"}},l.createElement(t.a,{href:"#single-shot-networks-vs-two-stage-networks","aria-label":"single shot networks vs two stage networks permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"single-shot networks vs two-stage networks"),"\n",l.createElement(t.p,null,"Object detection extends image classification by localizing objects within the image. Two principal design paradigms exist:"),"\n",l.createElement(t.ol,null,"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Single-stage detectors"),": Predict bounding boxes and class probabilities directly from the feature map in a single pass. Examples:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"YOLO (You Only Look Once)")," (Redmon and gang, CVPR 2016, later variants like YOLOv3, YOLOv5)"),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"SSD (Single Shot Detector)")," (Liu and gang, ECCV 2016)"),"\n"),"\n",l.createElement(t.p,null,"Single-stage detectors can be very fast and are suitable for real-time applications, though they sometimes trade off accuracy for speed."),"\n"),"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Two-stage detectors"),": Use a region proposal mechanism (e.g., RPN in Faster R-CNN) to suggest candidate object regions, and then classify and refine these proposals in a second step. This typically yields higher accuracy but at a computational cost."),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Faster R-CNN")," (Ren and gang, NeurIPS 2015) remains a popular benchmark for many detection tasks."),"\n"),"\n"),"\n"),"\n",l.createElement(t.h3,{id:"focal-loss",style:{position:"relative"}},l.createElement(t.a,{href:"#focal-loss","aria-label":"focal loss permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"focal loss"),"\n",l.createElement(t.p,null,'In problems with class imbalance or tasks where many examples belong to the "background" class (e.g., small objects vs. large background), standard cross-entropy can overemphasize the majority classes. ',l.createElement(t.strong,null,"Focal loss")," (Lin and gang, ICCV 2017) modifies cross-entropy by introducing a factor ",l.createElement(s.A,{text:"\\((1 - p_t)^\\gamma\\)"})," that down-weights easy examples so that the model focuses more on the hard, misclassified ones:"),"\n",l.createElement(s.A,{text:"\\[\n\\text{FL}(p_t) = - \\alpha_t (1 - p_t)^\\gamma \\log(p_t)\n\\]"}),"\n",l.createElement(t.p,null,"where:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(s.A,{text:"\\(p_t\\)"})," is the model's estimated probability for the correct class."),"\n",l.createElement(t.li,null,l.createElement(s.A,{text:"\\(\\alpha_t\\)"})," is a weighting factor for class imbalance."),"\n",l.createElement(t.li,null,l.createElement(s.A,{text:"\\(\\gamma\\)"})," is the focusing parameter that adjusts how heavily the loss penalizes well-classified examples."),"\n"),"\n",l.createElement(t.h3,{id:"retinanet-faster-r-cnn-effdet-and-detr",style:{position:"relative"}},l.createElement(t.a,{href:"#retinanet-faster-r-cnn-effdet-and-detr","aria-label":"retinanet faster r cnn effdet and detr permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"retinaNet, faster r-cnn, effdet, and detr"),"\n",l.createElement(t.p,null,"Beyond YOLO and SSD, several object detection architectures illustrate the evolution of the field:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"RetinaNet"),": A single-stage detector that uses focal loss to handle class imbalance effectively, achieving competitive accuracy with two-stage methods."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Faster R-CNN"),": Often considered the standard two-stage approach, balancing speed and accuracy."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"EffDet")," (EfficientDet, Tan and gang, CVPR 2020): Builds on EfficientNet backbones and a new BiFPN feature pyramid to achieve strong accuracy-speed tradeoffs."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"DETR")," (Carion and gang, ECCV 2020): Introduces transformer-based object detection, removing the need for many hand-crafted components like non-maximum suppression and anchor generation. DETR directly predicts bounding boxes as sets, using the attention mechanism to capture global relationships."),"\n"),"\n",l.createElement(t.h3,{id:"image-segmentation-extensions",style:{position:"relative"}},l.createElement(t.a,{href:"#image-segmentation-extensions","aria-label":"image segmentation extensions permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"image segmentation extensions"),"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Semantic segmentation")," classifies each pixel into a semantic class (e.g., road, building, car). ",l.createElement(t.strong,null,"Instance segmentation")," goes further, distinguishing individual object instances of the same class."),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Mask R-CNN")," (He and gang, ICCV 2017) extends Faster R-CNN by adding a parallel branch that outputs segmentation masks for each detected instance. This architecture is widely used in medical imaging and tasks that need pixel-level understanding, such as robotic grasping of objects with complex shapes."),"\n"),"\n",l.createElement(t.h2,{id:"attention-based-methods",style:{position:"relative"}},l.createElement(t.a,{href:"#attention-based-methods","aria-label":"attention based methods permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"attention-based methods"),"\n",l.createElement(t.h3,{id:"attention-mechanism-in-computer-vision",style:{position:"relative"}},l.createElement(t.a,{href:"#attention-mechanism-in-computer-vision","aria-label":"attention mechanism in computer vision permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"attention mechanism in computer vision"),"\n",l.createElement(t.p,null,'Attention mechanisms were first popularized in natural language processing. However, the concept of letting a model learn "what to focus on" in the input has proven highly useful in computer vision as well. Unlike convolution filters that capture local patterns, self-attention mechanisms can capture long-range dependencies across an entire image or feature map. This ability helps a model learn relationships between distant parts of an image, potentially leading to more robust representations.'),"\n",l.createElement(t.h3,{id:"vision-transformers-vit-and-related-architectures",style:{position:"relative"}},l.createElement(t.a,{href:"#vision-transformers-vit-and-related-architectures","aria-label":"vision transformers vit and related architectures permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"vision transformers (vit) and related architectures"),"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"Vision Transformers (ViT)")," (Dosovitskiy and gang, ICLR 2021) adapt the original transformer architecture from NLP to image recognition. They split the image into a sequence of patches (e.g., 16×16 pixels each), flatten them, and embed them with positional encodings. The transformer blocks then apply multi-head self-attention to these patch embeddings, effectively modeling the entire image as a sequence of tokens."),"\n",l.createElement(t.p,null,"ViT-like architectures have rapidly gained popularity due to their ability to scale with model size and training data. Notable improvements and variants include:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"DeiT")," (Touvron and gang, ICML 2021): Demonstrates data-efficient training strategies for Vision Transformers."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Swin Transformer")," (Liu and gang, ICCV 2021): Uses a hierarchical design with shifted windows, capturing local relationships efficiently while enabling a global receptive field."),"\n"),"\n",l.createElement(t.h3,{id:"hybrid-approaches",style:{position:"relative"}},l.createElement(t.a,{href:"#hybrid-approaches","aria-label":"hybrid approaches permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"hybrid approaches"),"\n",l.createElement(t.p,null,"Some architectures combine CNNs and attention modules, benefiting from the local inductive biases of convolutions and the global modeling power of attention. For example:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"CNN + SE (Squeeze-and-Excitation) blocks")," (Hu and gang, CVPR 2018): Weights channels adaptively based on global context."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"ConViT")," merges convolutional tokens with transformer blocks, seeking an architecture that is both robust to local distortions and capable of capturing long-range interactions."),"\n"),"\n",l.createElement(t.p,null,"These hybrids often appear in detection or segmentation contexts, where multi-scale feature representations (typical of CNNs) merge well with the expressive capacity of attention."),"\n",l.createElement(t.h3,{id:"future-directions",style:{position:"relative"}},l.createElement(t.a,{href:"#future-directions","aria-label":"future directions permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"future directions"),"\n",l.createElement(t.p,null,"Ongoing research explores:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Sparse attention")," patterns to reduce computational cost in large images."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Hierarchical transformers")," that can handle higher resolutions more efficiently."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Multimodal transformers")," integrating language, audio, or other sensors into vision tasks (e.g., text + image for image captioning or visual question answering)."),"\n"),"\n",l.createElement(t.p,null,"The promise of attention-based methods is vast, especially as compute resources grow and training strategies become more refined."),"\n",l.createElement(t.h2,{id:"generative-models-in-computer-vision",style:{position:"relative"}},l.createElement(t.a,{href:"#generative-models-in-computer-vision","aria-label":"generative models in computer vision permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"generative models in computer vision"),"\n",l.createElement(t.h3,{id:"overview-of-generative-models",style:{position:"relative"}},l.createElement(t.a,{href:"#overview-of-generative-models","aria-label":"overview of generative models permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"overview of generative models"),"\n",l.createElement(t.p,null,"Generative models learn to capture the distribution of data. If a model ",l.createElement(s.A,{text:"\\(p_{\\theta}(x)\\)"})," is a good approximation of the real data distribution ",l.createElement(s.A,{text:"\\(p_{\\text{data}}(x)\\)"}),", then sampling from ",l.createElement(s.A,{text:"\\(p_{\\theta}(x)\\)"})," will produce new data points that resemble the real examples. In computer vision, these models enable:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Synthesis of new, realistic images"),"."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Interpolation")," between data points (e.g., generating novel faces)."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Filling in missing data")," (inpainting)."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Domain adaptation")," (sketch to realistic image, summer to winter scenes)."),"\n"),"\n",l.createElement(t.h3,{id:"the-main-difference-between-gan-and-vae",style:{position:"relative"}},l.createElement(t.a,{href:"#the-main-difference-between-gan-and-vae","aria-label":"the main difference between gan and vae permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"the main difference between gan and vae"),"\n",l.createElement(t.p,null,"Two popular generative approaches are ",l.createElement(t.strong,null,"GANs")," (Generative Adversarial Networks) and ",l.createElement(t.strong,null,"VAEs")," (Variational Autoencoders)."),"\n",l.createElement(t.ol,null,"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"GAN")," (Goodfellow and gang, NeurIPS 2014): Trains two networks in an adversarial game. A generator ",l.createElement(s.A,{text:"G"})," tries to produce realistic samples from random noise, while a discriminator ",l.createElement(s.A,{text:"D"})," attempts to distinguish real samples from generated ones. Training aims to converge when ",l.createElement(s.A,{text:"G"})," can consistently fool ",l.createElement(s.A,{text:"D"})," while ",l.createElement(s.A,{text:"D"})," remains accurate on real vs. fake samples."),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,"Typically produce sharper images but can be challenging to stabilize during training."),"\n"),"\n"),"\n",l.createElement(t.li,null,"\n",l.createElement(t.p,null,l.createElement(t.strong,null,"VAE"),": Uses a probabilistic encoder-decoder structure. The encoder maps inputs ",l.createElement(s.A,{text:"\\(x\\)"})," to a latent distribution ",l.createElement(s.A,{text:"\\(q_{\\phi}(z|x)\\)"}),", while the decoder reconstructs ",l.createElement(s.A,{text:"\\(x\\)"})," from latent variable ",l.createElement(s.A,{text:"\\(z\\)"}),". Optimization involves maximizing the Evidence Lower BOund (ELBO):"),"\n",l.createElement(s.A,{text:"\\[\n\\mathcal{L}(\\theta, \\phi; x) = \\mathbb{E}_{z \\sim q_{\\phi}(z|x)}[\\log p_{\\theta}(x|z)] \n- D_{\\text{KL}}\\bigl(q_{\\phi}(z|x) \\parallel p(z)\\bigr)\n\\]"}),"\n",l.createElement(t.p,null,'VAEs tend to produce more "blurry" outputs but have a principled probabilistic foundation and stable training dynamics.'),"\n"),"\n"),"\n",l.createElement(t.h3,{id:"advanced-generative-techniques",style:{position:"relative"}},l.createElement(t.a,{href:"#advanced-generative-techniques","aria-label":"advanced generative techniques permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"advanced generative techniques"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"WGAN (Wasserstein GAN)")," (Arjovsky and gang, ICML 2017): Uses the Earth Mover's distance to improve training stability and measure the quality of samples more meaningfully."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"DCGAN (Deep Convolutional GAN)")," (Radford and gang, ICLR 2016): Applies CNNs in both generator and discriminator, enabling large-scale stable training."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Normalizing flows"),": Models (e.g., RealNVP, Glow) that transform a simple distribution (like a Gaussian) to match the target distribution exactly, allowing exact likelihood calculation."),"\n"),"\n",l.createElement(t.h3,{id:"applications-of-generative-models",style:{position:"relative"}},l.createElement(t.a,{href:"#applications-of-generative-models","aria-label":"applications of generative models permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"applications of generative models"),"\n",l.createElement(t.p,null,"Generative models have shown remarkable potential in various tasks:"),"\n",l.createElement(t.ol,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Image synthesis"),": Generating entirely new images, such as faces or artwork."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Style transfer"),": Combining the content of one image with the style of another. This was popularized by neural style transfer methods, which are partially generative in nature."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Super-resolution"),": Enhancing the resolution of low-quality images (e.g., photo restoration)."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Inpainting"),": Filling in missing or corrupted regions of an image in a visually plausible way."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Domain adaptation"),": CycleGAN, for instance, can map images between two domains (summer ↔ winter landscapes) without needing paired training data."),"\n"),"\n",l.createElement(t.p,null,"Generative modeling continues to evolve rapidly, intersecting with vision transformers, diffusion models, and advanced adversarial setups, which push the frontier of image realism and creative AI outputs."),"\n",l.createElement(t.h2,{id:"advanced-topics-and-future-outlook",style:{position:"relative"}},l.createElement(t.a,{href:"#advanced-topics-and-future-outlook","aria-label":"advanced topics and future outlook permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"advanced topics and future outlook"),"\n",l.createElement(t.h3,{id:"3d-computer-vision-and-depth-estimation",style:{position:"relative"}},l.createElement(t.a,{href:"#3d-computer-vision-and-depth-estimation","aria-label":"3d computer vision and depth estimation permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"3d computer vision and depth estimation"),"\n",l.createElement(t.p,null,"Moving beyond 2D images, 3D computer vision deals with reconstructing, understanding, and manipulating 3D information from multiple images or specialized sensors. Common approaches include:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"LIDAR-based perception"),": Used in autonomous driving to get precise depth measurements."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Structure from Motion (SfM)")," and ",l.createElement(t.strong,null,"multi-view stereo"),": Recovers 3D structures by analyzing correspondences in overlapping images."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Stereo vision"),": Exploits two parallel cameras to estimate disparity maps, which correlate to depth."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"SLAM (Simultaneous Localization and Mapping)"),": Used heavily in robotics and AR/VR. A device or robot incrementally builds a map of an unknown environment while keeping track of its own location."),"\n"),"\n",l.createElement(t.p,null,"3D understanding also includes pose estimation of objects, scene flow (the 3D extension of optical flow), and 3D shape reconstruction, enabling applications in robotics, augmented reality, and digital content creation."),"\n",l.createElement(t.h3,{id:"reinforcement-learning-for-visual-tasks",style:{position:"relative"}},l.createElement(t.a,{href:"#reinforcement-learning-for-visual-tasks","aria-label":"reinforcement learning for visual tasks permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"reinforcement learning for visual tasks"),"\n",l.createElement(t.p,null,"Reinforcement learning (RL) addresses sequential decision-making, where an agent interacts with an environment to maximize cumulative reward. For visual tasks, the agent often receives raw pixel data as input. Achieving robust behavior from high-dimensional visual signals is challenging but has led to successes:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Atari games"),": Agents trained directly from pixel frames can outperform human players (Mnih and gang, Nature 2015)."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Robotics"),": RL can optimize policies for grasping and object manipulation based on camera feeds."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Autonomous navigation"),": Combining RL with computer vision (such as obstacle detection) for mobile robot path planning."),"\n"),"\n",l.createElement(t.h3,{id:"multimodal-learning",style:{position:"relative"}},l.createElement(t.a,{href:"#multimodal-learning","aria-label":"multimodal learning permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"multimodal learning"),"\n",l.createElement(t.p,null,"Modern AI systems increasingly integrate multiple data modalities:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Vision + language"),": E.g., image captioning, visual question answering (VQA). Models must understand an image while also parsing the textual query or generating textual output."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Vision + audio"),": For instance, cross-modal tasks in robotics, where visual cues are combined with auditory signals."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Vision + sensor data"),": Depth sensors, thermal cameras, or radar can supplement RGB cameras to improve perception."),"\n"),"\n",l.createElement(t.p,null,"The synergy between different modalities can yield richer, more robust representations, reflecting the multi-sensory nature of real-world perception."),"\n",l.createElement(t.h3,{id:"additional-areas-of-active-research",style:{position:"relative"}},l.createElement(t.a,{href:"#additional-areas-of-active-research","aria-label":"additional areas of active research permalink",className:"anchor before"},l.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"additional areas of active research"),"\n",l.createElement(t.p,null,"Computer vision is a vibrant field, continually pushing boundaries in domains like:"),"\n",l.createElement(t.ul,null,"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Advanced domain generalization"),": Models that adapt to new domains without fine-tuning."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Long-tail recognition"),": Handling rare classes or examples that appear infrequently in training data."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Federated learning for edge devices"),": Training computer vision models on distributed datasets without centralizing data (protecting privacy)."),"\n",l.createElement(t.li,null,l.createElement(t.strong,null,"Active learning"),": Strategies for selecting the most informative data samples to label, to reduce annotation costs in large-scale vision datasets."),"\n"),"\n",l.createElement(t.p,null,"As computational power grows and new paradigms (e.g., foundation models, large-scale multimodal transformers) become ubiquitous, the future of computer vision promises ever more sophisticated capabilities — from real-time scene understanding to creative synthesis of new visual worlds."),"\n",l.createElement(n,{alt:"Image illustrating a pipeline of modern computer vision tasks",path:"",caption:"A conceptual diagram showing a pipeline of modern computer vision tasks, including image classification, object detection, segmentation, and 3D reconstruction.",zoom:"false"}),"\n",l.createElement(t.hr),"\n",l.createElement(t.p,null,"I hope this long-form introduction to computer vision clarifies not only the fundamental image processing steps and key deep learning architectures, but also highlights the remarkable breadth of tasks and ongoing research in the field. By understanding these foundational elements and tracking cutting-edge developments, readers can confidently navigate the ever-evolving landscape of computer vision."))}var m=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,i.RP)(),e.components);return t?l.createElement(t,e,l.createElement(c,e)):c(e)};var d=n(36710),h=n(58481),u=n.n(h),p=n(36310),g=n(87245),f=n(27042),v=n(59849),E=n(5591),b=n(61122),y=n(9219),w=n(33203),S=n(95751),x=n(94328),k=n(80791),H=n(78137);const C=e=>{let{toc:t}=e;if(!t||!t.items)return null;return l.createElement("nav",{className:k.R},l.createElement("ul",null,t.items.map(((e,t)=>l.createElement("li",{key:t},l.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const n=t.replace("#",""),a=document.getElementById(n);a&&a.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&l.createElement(C,{toc:{items:e.items}}))))))};function z(e){let{data:{mdx:t,allMdx:r,allPostImages:o},children:s}=e;const{frontmatter:c,body:m,tableOfContents:d}=t,h=c.index,v=c.slug.split("/")[1],k=r.nodes.filter((e=>e.frontmatter.slug.includes(`/${v}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),z=k.findIndex((e=>e.frontmatter.index===h)),A=k[z+1],T=k[z-1],V=c.slug.replace(/\/$/,""),M=/[^/]*$/.exec(V)[0],N=`posts/${v}/content/${M}/`,{0:I,1:_}=(0,l.useState)(c.flagWideLayoutByDefault),{0:L,1:P}=(0,l.useState)(!1);var B;(0,l.useEffect)((()=>{P(!0);const e=setTimeout((()=>P(!1)),340);return()=>clearTimeout(e)}),[I]),"adventures"===v?B=y.cb:"research"===v?B=y.Qh:"thoughts"===v&&(B=y.T6);const R=u()(m).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,D=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),n=e%60;return n<=30?`~${t}${n>0?".5":""} h`:`~${t+1} h`}(Math.ceil(R/B)+(c.extraReadTimeMin||0)),O=[{flag:c.flagDraft,component:()=>Promise.all([n.e(3231),n.e(8809)]).then(n.bind(n,28809))},{flag:c.flagMindfuckery,component:()=>Promise.all([n.e(3231),n.e(2471)]).then(n.bind(n,67709))},{flag:c.flagRewrite,component:()=>Promise.all([n.e(3231),n.e(6764)]).then(n.bind(n,62002))},{flag:c.flagOffensive,component:()=>Promise.all([n.e(3231),n.e(2443)]).then(n.bind(n,17681))},{flag:c.flagProfane,component:()=>Promise.all([n.e(3231),n.e(8048)]).then(n.bind(n,53286))},{flag:c.flagMultilingual,component:()=>Promise.all([n.e(3231),n.e(4069)]).then(n.bind(n,78831))},{flag:c.flagUnreliably,component:()=>Promise.all([n.e(3231),n.e(3417)]).then(n.bind(n,8179))},{flag:c.flagPolitical,component:()=>Promise.all([n.e(3231),n.e(5195)]).then(n.bind(n,30433))},{flag:c.flagCognitohazard,component:()=>Promise.all([n.e(3231),n.e(3175)]).then(n.bind(n,8413))},{flag:c.flagHidden,component:()=>Promise.all([n.e(3231),n.e(9556)]).then(n.bind(n,14794))}],{0:j,1:G}=(0,l.useState)([]);return(0,l.useEffect)((()=>{O.forEach((e=>{let{flag:t,component:n}=e;t&&n().then((e=>{G((t=>[].concat((0,a.A)(t),[e.default])))}))}))}),[]),l.createElement(f.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},l.createElement(E.A,{postNumber:c.index,date:c.date,updated:c.updated,readTime:D,difficulty:c.difficultyLevel,title:c.title,desc:c.desc,banner:c.banner,section:v,postKey:M,isMindfuckery:c.flagMindfuckery,mainTag:c.mainTag}),l.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},c.otherTags.map(((e,t)=>l.createElement("span",{key:t,className:`noselect ${H.MW}`,style:{margin:"0 5px 5px 0"}},e)))),l.createElement("div",{className:"postBody"},l.createElement(C,{toc:d})),l.createElement("br"),l.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},l.createElement(f.P.button,{className:`noselect ${x.pb}`,id:x.xG,onClick:()=>{_(!I)},whileTap:{scale:.93}},l.createElement(f.P.div,{className:S.DJ,key:I,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},I?"Switch to default layout":"Switch to wide layout"))),l.createElement("br"),l.createElement("div",{className:"postBody",style:{margin:I?"0 -14%":"",maxWidth:I?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},l.createElement("div",{className:`${x.P_} ${L?x.Xn:x.qG}`},j.map(((e,t)=>l.createElement(e,{key:t}))),c.indexCourse?l.createElement(w.A,{index:c.indexCourse,category:c.courseCategoryName}):"",l.createElement(p.Z.Provider,{value:{images:o.nodes,basePath:N.replace(/\/$/,"")+"/"}},l.createElement(i.xA,{components:{Image:g.A}},s)))),l.createElement(b.A,{nextPost:A,lastPost:T,keyCurrent:M,section:v}))}function A(e){return l.createElement(z,e,l.createElement(m,e))}function T(e){var t,n,a,i,r;let{data:o}=e;const{frontmatter:s}=o.mdx,c=s.titleSEO||s.title,m=s.titleOG||c,h=s.titleTwitter||c,u=s.descSEO||s.desc,p=s.descOG||u,g=s.descTwitter||u,f=s.schemaType||"BlogPosting",E=s.keywordsSEO,b=s.date,y=s.updated||b,w=s.imageOG||(null===(t=s.banner)||void 0===t||null===(n=t.childImageSharp)||void 0===n||null===(a=n.gatsbyImageData)||void 0===a||null===(i=a.images)||void 0===i||null===(r=i.fallback)||void 0===r?void 0:r.src),S=s.imageAltOG||p,x=s.imageTwitter||w,k=s.imageAltTwitter||g,H=s.canonicalURL,C=s.flagHidden||!1,z=s.mainTag||"Posts",A=s.slug.split("/")[1]||"posts",{siteUrl:T}=(0,d.Q)(),V={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:T},{"@type":"ListItem",position:2,name:z,item:`${T}/${s.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${T}${s.slug}`}]};return l.createElement(v.A,{title:c+" - avrtt.blog",titleOG:m,titleTwitter:h,description:u,descriptionOG:p,descriptionTwitter:g,schemaType:f,keywords:E,datePublished:b,dateModified:y,imageOG:w,imageAltOG:S,imageTwitter:x,imageAltTwitter:k,canonicalUrl:H,flagHidden:C,mainTag:z,section:A,type:"article"},l.createElement("script",{type:"application/ld+json"},JSON.stringify(V)))}},96098:function(e,t,n){var a=n(96540),i=n(7978);t.A=e=>{let{text:t}=e;return a.createElement(i.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-js-content-file-path-src-pages-posts-research-intro-to-computer-vision-mdx-7bc7726a703916c5b0a0.js.map