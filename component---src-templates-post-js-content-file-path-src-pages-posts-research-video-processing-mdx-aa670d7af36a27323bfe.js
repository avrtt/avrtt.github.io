"use strict";(self.webpackChunkavrtt_blog=self.webpackChunkavrtt_blog||[]).push([[4531],{3962:function(e,t){t.A="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iaXNvLTg4NTktMSI/Pgo8IS0tIEdlbmVyYXRvcjogQWRvYmUgSWxsdXN0cmF0b3IgMTYuMC4wLCBTVkcgRXhwb3J0IFBsdWctSW4gLiBTVkcgVmVyc2lvbjogNi4wMCBCdWlsZCAwKSAgLS0+CjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+CjxzdmcgdmVyc2lvbj0iMS4xIiBpZD0iQ2FwYV8xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB4PSIwcHgiIHk9IjBweCIKCSB3aWR0aD0iNDE2Ljk3OXB4IiBoZWlnaHQ9IjQxNi45NzlweCIgdmlld0JveD0iMCAwIDQxNi45NzkgNDE2Ljk3OSIgc3R5bGU9ImVuYWJsZS1iYWNrZ3JvdW5kOm5ldyAwIDAgNDE2Ljk3OSA0MTYuOTc5OyIKCSB4bWw6c3BhY2U9InByZXNlcnZlIj4KPGc+Cgk8cGF0aCBkPSJNMzU2LjAwNCw2MS4xNTZjLTgxLjM3LTgxLjQ3LTIxMy4zNzctODEuNTUxLTI5NC44NDgtMC4xODJjLTgxLjQ3LDgxLjM3MS04MS41NTIsMjEzLjM3OS0wLjE4MSwyOTQuODUKCQljODEuMzY5LDgxLjQ3LDIxMy4zNzgsODEuNTUxLDI5NC44NDksMC4xODFDNDM3LjI5MywyNzQuNjM2LDQzNy4zNzUsMTQyLjYyNiwzNTYuMDA0LDYxLjE1NnogTTIzNy42LDM0MC43ODYKCQljMCwzLjIxNy0yLjYwNyw1LjgyMi01LjgyMiw1LjgyMmgtNDYuNTc2Yy0zLjIxNSwwLTUuODIyLTIuNjA1LTUuODIyLTUuODIyVjE2Ny44ODVjMC0zLjIxNywyLjYwNy01LjgyMiw1LjgyMi01LjgyMmg0Ni41NzYKCQljMy4yMTUsMCw1LjgyMiwyLjYwNCw1LjgyMiw1LjgyMlYzNDAuNzg2eiBNMjA4LjQ5LDEzNy45MDFjLTE4LjYxOCwwLTMzLjc2Ni0xNS4xNDYtMzMuNzY2LTMzLjc2NQoJCWMwLTE4LjYxNywxNS4xNDctMzMuNzY2LDMzLjc2Ni0zMy43NjZjMTguNjE5LDAsMzMuNzY2LDE1LjE0OCwzMy43NjYsMzMuNzY2QzI0Mi4yNTYsMTIyLjc1NSwyMjcuMTA3LDEzNy45MDEsMjA4LjQ5LDEzNy45MDF6Ii8+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPGc+CjwvZz4KPC9zdmc+Cg=="},14255:function(e,t,n){n.r(t),n.d(t,{Head:function(){return H},PostTemplate:function(){return M},default:function(){return z}});var a=n(54506),i=n(28453),o=n(96540),r=n(66501),l=n(16886),s=(n(46295),n(96098));function c(e){const t=Object.assign({p:"p",h2:"h2",a:"a",span:"span",h3:"h3",ul:"ul",li:"li",strong:"strong",ol:"ol",hr:"hr"},(0,i.RP)(),e.components),{Image:n}=t;return n||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Image",!0),o.createElement(o.Fragment,null,"\n",o.createElement("br"),"\n","\n",o.createElement(t.p,null,"Video processing is a multifaceted domain of computer vision and machine learning focused on analyzing, enhancing, manipulating, and extracting meaningful information from moving visual media. Although still image processing and video processing share some foundational concepts — such as pixel-level operations, transformations, and feature extraction — videos add the dimension of time. This extra dimension introduces temporal coherence and potentially large volumes of data, creating unique challenges and opportunities that are fundamentally different from the processing of individual static images."),"\n",o.createElement(t.p,null,"The basic structure of a video is a rapid sequence of still images (frames) displayed one after another, typically at a specific frame rate (e.g., 24, 30, or 60 frames per second). This rapid display induces the illusion of continuous motion, much like a flipbook. Techniques that worked well for still images — like convolutional neural networks (CNNs) or even modern Vision Transformers (ViTs) — often need to be adapted to capture and exploit the temporal patterns that arise from frame-to-frame dynamics."),"\n",o.createElement(t.p,null,"In this article, we will dive into the distinct characteristics of videos, analyze their significance in machine learning and data science contexts, and discuss how video differs from image processing. We will also examine the role of temporal continuity, typical approaches for motion estimation, and modern spatiotemporal modeling strategies. Finally, we will cover the most prevalent challenges — ranging from data storage constraints to real-time processing requirements — that practitioners and researchers must address when developing solutions in video processing."),"\n",o.createElement(t.p,null,"Throughout this piece, we will reference several influential research projects and relevant techniques that have shaped the field in recent years (e.g., advanced spatiotemporal modeling with 3D convolutional networks, optical flow estimation, and video transformers). We will highlight how these cutting-edge approaches address core challenges in video understanding, including tasks such as action recognition, object tracking, detection, segmentation, and more. This discussion aims to provide clarity and depth for experienced practitioners looking to expand their theoretical foundations and practical skills in video processing."),"\n",o.createElement(t.h2,{id:"characteristics-of-videos",style:{position:"relative"}},o.createElement(t.a,{href:"#characteristics-of-videos","aria-label":"characteristics of videos permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Characteristics of videos"),"\n",o.createElement(t.p,null,"In contrast to still images, videos have more properties that directly impact both computational and storage requirements. Furthermore, these additional aspects — such as codec types, bitrates, and the consistent illusion of motion — significantly shape how we conduct and optimize machine learning pipelines."),"\n",o.createElement(t.h3,{id:"bitrate",style:{position:"relative"}},o.createElement(t.a,{href:"#bitrate","aria-label":"bitrate permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Bitrate"),"\n",o.createElement(t.p,null,"The term ",o.createElement(l.A,null,"bitrate")," describes the quantity of data required to encode and transmit (or store) one second of a video. It is typically measured in kilobits per second (kbps) or megabits per second (Mbps). Higher bitrates generally preserve more visual information, thus providing clearer images, but at the cost of increased storage requirements and greater bandwidth consumption during streaming. To reduce file sizes, compression methods (lossy or lossless) adjust the bitrate or selectively eliminate certain information."),"\n",o.createElement(t.h3,{id:"codecs",style:{position:"relative"}},o.createElement(t.a,{href:"#codecs","aria-label":"codecs permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Codecs"),"\n",o.createElement(t.p,null,"A ",o.createElement(l.A,null,"codec")," (compressor-decompressor) is a hardware or software component used to compress raw video and decompress it during playback or processing. Codecs come in two primary categories:"),"\n",o.createElement(t.ul,null,"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Lossless codecs:")," These preserve every detail of the video frames. They are used for high-precision tasks (like professional film editing or medical imaging), where losing even small amounts of information is unacceptable. Examples include some specialized codecs used for archiving."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Lossy codecs:")," These remove a fraction of the data in order to reduce file size. In many cases, the removed data is imperceptible to the human eye at typical playback speeds. Examples include H.264/AVC, H.265/HEVC, VP9, and AV1. Machine learning pipelines can also use these codecs but must handle the potential artifacts they introduce (e.g., blockiness, blurred edges)."),"\n"),"\n",o.createElement(t.p,null,"For real-world applications, the choice of codec is a balancing act between video quality, computational overhead, and storage or transmission constraints. Modern machine learning workflows dealing with large-scale data frequently rely on compressed videos to keep storage demands manageable, even though decompression overhead can slow processing pipelines."),"\n",o.createElement(t.h3,{id:"the-illusion-of-motion",style:{position:"relative"}},o.createElement(t.a,{href:"#the-illusion-of-motion","aria-label":"the illusion of motion permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"The illusion of motion"),"\n",o.createElement(t.p,null,"The core difference between still images and video is that a video is perceived as a continuous motion rather than a static snapshot. This arises from the biological phenomenon of ",o.createElement(l.A,null,"persistence of vision"),", in which the human eye and brain hold onto an image briefly after it disappears. When frames are shown in quick succession — generally at a rate of at least 24 frames per second — our visual system interprets the rapid sequence as continuous motion."),"\n",o.createElement(t.p,null,'Whereas still images contain a snapshot in time, videos contain sequences of snapshots that inherently encode object movement and transformations from frame to frame. This spatiotemporal context is crucial for tasks like action recognition, where the difference between "walking" and "running" can be subtle in a single frame but obvious when observing multiple frames in sequence.'),"\n",o.createElement(t.h3,{id:"frame-rate",style:{position:"relative"}},o.createElement(t.a,{href:"#frame-rate","aria-label":"frame rate permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Frame rate"),"\n",o.createElement(t.p,null,o.createElement(l.A,null,"Frame rate"),", typically measured in frames per second (fps), describes how many frames are displayed (or captured) each second. Common frame rates include 24 fps (common in cinema), 30 fps (common in television and general video), and 60 fps (common in high-definition or slow-motion scenarios). Higher frame rates produce smoother motion, but also increase the data size and computational cost. In certain high-speed imaging tasks (e.g., sports analytics or industrial inspection), specialized cameras can capture thousands of frames per second."),"\n",o.createElement(t.h3,{id:"resolution-and-aspect-ratio",style:{position:"relative"}},o.createElement(t.a,{href:"#resolution-and-aspect-ratio","aria-label":"resolution and aspect ratio permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Resolution and aspect ratio"),"\n",o.createElement(t.p,null,"The ",o.createElement(l.A,null,"resolution")," of a video refers to the pixel dimensions of each frame (e.g., 1920×1080 for Full HD). Larger resolutions (e.g., 4K at 3840×2160) offer more detailed visual information but come at the cost of higher storage requirements and processing overhead. Meanwhile, the ",o.createElement(l.A,null,"aspect ratio")," indicates the proportional relationship between width and height (e.g., 16:9 for many modern displays)."),"\n",o.createElement(t.h3,{id:"audio-and-metadata",style:{position:"relative"}},o.createElement(t.a,{href:"#audio-and-metadata","aria-label":"audio and metadata permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Audio and metadata"),"\n",o.createElement(t.p,null,"Although typically overshadowed by the visual dimension, videos also incorporate audio tracks and possibly supplementary metadata (e.g., subtitles, timestamps, or sensor data for augmented reality). In advanced data science applications, audio analysis can be important (e.g., detecting speech, classifying sounds, or combining visual and auditory cues for improved event recognition)."),"\n",o.createElement(t.p,null,"The combination of all these properties — video resolution, frame rate, bitrate, codecs, and possible additional data streams — renders video processing more complex than single-frame processing. As we will see, however, this very complexity enables new challenges and new possibilities for machine learning."),"\n",o.createElement(t.h2,{id:"the-role-of-video-processing-in-machine-learning-and-data-science",style:{position:"relative"}},o.createElement(t.a,{href:"#the-role-of-video-processing-in-machine-learning-and-data-science","aria-label":"the role of video processing in machine learning and data science permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"The role of video processing in machine learning and data science"),"\n",o.createElement(t.p,null,o.createElement(l.A,null,"Video processing")," techniques are central to a wide range of modern machine learning and data science applications. With the explosive growth of video content on the internet (e.g., social media platforms, streaming services, surveillance cameras, autonomous vehicles, industrial automation, etc.), efficiently analyzing, interpreting, and transforming large volumes of video data has become a priority."),"\n",o.createElement(t.p,null,"Several specific tasks highlight the broad utility of video processing in machine learning and data science:"),"\n",o.createElement(t.ul,null,"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Video classification:")," Classifying short clips into categories such as ",o.createElement(r.A,{text:"Examples: sports, cooking, vlogging, etc."}),' or identifying specific activities (e.g., "playing guitar," "dribbling a basketball").'),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Object detection and tracking:")," Identifying and localizing objects frame by frame and keeping track of their trajectories. This is crucial for tasks such as vehicle traffic analysis, surveillance, or autonomous navigation."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Video segmentation:")," Labeling every pixel in each frame according to its semantic category (semantic segmentation), instance identity (instance segmentation), or both. This is used for advanced scene understanding, special effects, and more."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Action recognition:"),' Classifying the activity or action taking place in a clip (e.g., "jumping," "swinging a tennis racket"). Often combined with robust spatiotemporal feature extraction to handle subtle motion cues.'),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Video captioning and summarization:"),' Automatically generating textual descriptions of video content or extracting the "key frames" to produce a condensed representation.'),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Anomaly detection in video:")," Identifying unusual or suspicious events, e.g., in security footage."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Video enhancement and super-resolution:")," Improving video quality by reducing noise, enhancing resolution, or correcting color."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Video-to-video translation and domain adaptation:")," Transforming video from one style to another (e.g., day-to-night transformation, or applying an artistic style)."),"\n"),"\n",o.createElement(t.p,null,"Modern deep learning architectures — especially Vision Transformers (ViTs), spatiotemporal convolutional neural networks (3D CNNs), recurrent neural networks (RNNs), and attention-based models — enable these tasks at large scale, often surpassing traditional feature engineering methods. Recent research has also explored the synergy between large language models and video understanding, leading to advanced multi-modal systems that integrate both textual and visual contexts (e.g., video question-answering or cross-modal retrieval)."),"\n",o.createElement(t.h2,{id:"differences-between-image-and-video-processing",style:{position:"relative"}},o.createElement(t.a,{href:"#differences-between-image-and-video-processing","aria-label":"differences between image and video processing permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Differences between image and video processing"),"\n",o.createElement(t.p,null,"While image processing focuses on static 2D signals, video processing extends into the temporal domain, introducing a host of new concepts and complexities. Below are the major points that set video processing apart from the simpler case of handling individual images."),"\n",o.createElement(t.h3,{id:"41-additional-temporal-axis-in-video-data",style:{position:"relative"}},o.createElement(t.a,{href:"#41-additional-temporal-axis-in-video-data","aria-label":"41 additional temporal axis in video data permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.1 Additional temporal axis in video data"),"\n",o.createElement(t.p,null,"A single image is indexed by two spatial coordinates (e.g., ",o.createElement(s.A,{text:"\\( x \\)"})," and ",o.createElement(s.A,{text:"\\( y \\)"})," ). A video, on the other hand, is indexed by three coordinates — two spatial (",o.createElement(s.A,{text:"\\( x, y \\)"}),") and one temporal (",o.createElement(s.A,{text:"\\( t \\)"}),"). Conceptually, we can think of a video as a function:"),"\n",o.createElement(s.A,{text:"\\[\nV(x, y, t) : \\{(x, y, t) \\mid x \\in [1, W], y \\in [1, H], t \\in [1, T]\\} \\rightarrow \\mathcal{C}\n\\]"}),"\n",o.createElement(t.p,null,"where ",o.createElement(s.A,{text:"\\(W, H\\)"})," denote the frame width and height, ",o.createElement(s.A,{text:"\\(T\\)"})," the total number of frames, and ",o.createElement(s.A,{text:"\\(\\mathcal{C}\\)"})," the color space (e.g., RGB). This temporal dimension is key to the notion of motion and changes in scene content over time."),"\n",o.createElement(t.h3,{id:"42-complexity-from-sequential-frames",style:{position:"relative"}},o.createElement(t.a,{href:"#42-complexity-from-sequential-frames","aria-label":"42 complexity from sequential frames permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.2 Complexity from sequential frames"),"\n",o.createElement(t.p,null,"Because a video is comprised of sequential frames, the model must account for dynamic changes — both subtle (e.g., slight movement of a hand) and large (e.g., sudden scene cuts). Furthermore, the number of frames in even a short clip can be sizable; a 10-second clip at 30 fps yields 300 frames. Naively treating each frame as an independent image can be computationally expensive and overlooks the context provided by adjacent frames. Many advanced methods aim to share or fuse information across frames to improve both computational efficiency and performance."),"\n",o.createElement(t.h3,{id:"43-larger-data-size-and-memory-constraints",style:{position:"relative"}},o.createElement(t.a,{href:"#43-larger-data-size-and-memory-constraints","aria-label":"43 larger data size and memory constraints permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.3 Larger data size and memory constraints"),"\n",o.createElement(t.p,null,"Video data is inherently large because it stacks a series of high-resolution images. Even a compressed video file can quickly balloon in size when stored in raw pixel form or uncompressed streams. This volume of data places heavy demands on GPU memory, CPU processing time, and disk space. Data scientists often must design specialized strategies for reading, buffering, and processing video data (e.g., working with short clips, using streaming techniques, or applying advanced compression/decompression pipelines)."),"\n",o.createElement(t.h3,{id:"44-need-for-spatiotemporal-feature-extraction",style:{position:"relative"}},o.createElement(t.a,{href:"#44-need-for-spatiotemporal-feature-extraction","aria-label":"44 need for spatiotemporal feature extraction permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"4.4 Need for spatiotemporal feature extraction"),"\n",o.createElement(t.p,null,'In image processing, "spatial features" (edges, corners, textures, object shapes, etc.) are sufficient for tasks such as image classification or object detection. In video processing, "temporal features" — the patterns of change from frame to frame — can be equally crucial. Merging these into a unified ',o.createElement(l.A,null,"spatiotemporal feature representation")," is often the crux of successful video recognition models. Methods such as 3D convolutions or self-attention across space and time can explicitly encode how objects and scenes evolve as the video progresses."),"\n",o.createElement(t.h2,{id:"temporal-continuity-in-videos",style:{position:"relative"}},o.createElement(t.a,{href:"#temporal-continuity-in-videos","aria-label":"temporal continuity in videos permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Temporal continuity in videos"),"\n",o.createElement(t.p,null,o.createElement(l.A,null,"Temporal continuity")," describes the fact that consecutive frames in a natural video are often correlated. For instance, objects typically move slightly between consecutive frames, so there is a consistent transformation connecting them. This continuity is fundamental to capturing actions and events that unfold across time. Conversely, abrupt scene changes break this continuity, indicating transitions in the story or environment."),"\n",o.createElement(t.p,null,"Leveraging temporal continuity can improve various applications:"),"\n",o.createElement(t.ul,null,"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Action recognition:")," By analyzing how the visual content shifts over a short time window, we can recognize that a subject is walking, running, or jumping."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Object tracking:")," Tracking depends on the assumption that object positions or appearances in consecutive frames do not drastically change under normal conditions."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Video super-resolution:")," Interpolating or upscaling frames benefits from knowledge of how adjacent frames relate, potentially filling in missing details more robustly."),"\n"),"\n",o.createElement(t.p,null,"Several common strategies exist for handling temporal data in deep learning:"),"\n",o.createElement(t.ul,null,"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Early fusion:")," Stacking frames along the channel dimension (or time dimension) as if they were separate channels."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Late fusion:")," Processing each frame or small set of frames individually before merging higher-level features."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Recurrent approaches:")," Feeding spatiotemporal features into RNNs (e.g., LSTM or GRU) to maintain hidden state across the sequence."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"3D CNNs:")," Extending 2D convolutional filters into the time dimension to capture motion cues."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Spatiotemporal transformers:")," Applying self-attention over both spatial patches and temporal segments to unify contextual information across frames."),"\n"),"\n",o.createElement(t.p,null,"These approaches aim to harness the unique advantage that video data provides: the interplay of space and time that can reveal more complex semantic content than any single static image alone."),"\n",o.createElement(t.h2,{id:"motion-estimation",style:{position:"relative"}},o.createElement(t.a,{href:"#motion-estimation","aria-label":"motion estimation permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Motion estimation"),"\n",o.createElement(t.p,null,o.createElement(l.A,null,"Motion estimation")," is the process of quantifying and tracking movement in consecutive frames. This includes shifts in object position, changes in shape, or transformations in the background. Motion estimation provides not only a tool for analyzing how objects move through the scene but also underpins key aspects of video compression, real-time monitoring, and advanced spatiotemporal tasks."),"\n",o.createElement(t.h3,{id:"61-traditional-approaches-optical-flow-block-matching",style:{position:"relative"}},o.createElement(t.a,{href:"#61-traditional-approaches-optical-flow-block-matching","aria-label":"61 traditional approaches optical flow block matching permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.1 Traditional approaches (optical flow, block matching)"),"\n",o.createElement(t.p,null,"Before the rise of deep learning, classical computer vision techniques for motion estimation were already well developed:"),"\n",o.createElement(t.ul,null,"\n",o.createElement(t.li,null,"\n",o.createElement(t.p,null,o.createElement(t.strong,null,"Optical flow:")," Optical flow methods (e.g., the Horn–Schunck or Lucas–Kanade algorithms) estimate the pixel-wise motion field between two consecutive frames. The ",o.createElement(l.A,null,"optical flow vector")," at each pixel indicates how it moves from one frame to the next."),"\n",o.createElement(t.p,null,"Mathematically, optical flow solves for a velocity field ",o.createElement(s.A,{text:"\\( (u, v) \\)"})," that approximates the change in pixel intensity or color between frames ",o.createElement(s.A,{text:"\\( I(t) \\)"})," and ",o.createElement(s.A,{text:"\\( I(t+1) \\)"}),". For example, the Horn–Schunck approach tries to minimize a global energy function:"),"\n",o.createElement(s.A,{text:"\\[\nE(u, v) = \\iint \\left( \\frac{\\partial I}{\\partial t} + \\frac{\\partial I}{\\partial x}u + \\frac{\\partial I}{\\partial y}v \\right)^2 + \\lambda \\left( \\left|\\nabla u\\right|^2 + \\left|\\nabla v\\right|^2 \\right) \\, dx\\,dy\n\\]"}),"\n",o.createElement(t.p,null,"where ",o.createElement(s.A,{text:"\\( \\left(\\frac{\\partial I}{\\partial x}, \\frac{\\partial I}{\\partial y}, \\frac{\\partial I}{\\partial t}\\right) \\)"})," are the spatiotemporal intensity gradients, ",o.createElement(s.A,{text:"\\( (u, v) \\)"})," is the flow field, and ",o.createElement(s.A,{text:"\\( \\lambda \\)"})," is a regularization parameter that enforces smoothness."),"\n"),"\n",o.createElement(t.li,null,"\n",o.createElement(t.p,null,o.createElement(t.strong,null,"Block matching:")," Videos are split into small rectangular blocks, and each block in the current frame is matched with the most similar block in the next frame. The displacement between matching blocks is interpreted as the motion vector. This technique is popular in video codecs (e.g., MPEG standards) due to its relative simplicity."),"\n"),"\n"),"\n",o.createElement(t.h3,{id:"62-modern-deep-learning-methods-for-motion-tracking",style:{position:"relative"}},o.createElement(t.a,{href:"#62-modern-deep-learning-methods-for-motion-tracking","aria-label":"62 modern deep learning methods for motion tracking permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"6.2 Modern deep learning methods for motion tracking"),"\n",o.createElement(t.p,null,"Deep neural networks have taken motion estimation to new heights by learning robust feature representations directly from training data. Some well-known neural approaches for optical flow and motion tracking include:"),"\n",o.createElement(t.ul,null,"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"FlowNet")," (Fischer and gang, 2015) and ",o.createElement(t.strong,null,"FlowNet2")," (Ilg and gang, 2017): Used CNN architectures to learn optical flow end-to-end from large synthetic and real datasets."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"RAFT")," (Teed & Deng, ECCV 2020): A recurrent all-pairs field transforms approach that refines optical flow estimates iteratively, achieving state-of-the-art accuracy on several benchmarks."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"PWC-Net")," (Sun and gang, CVPR 2018): Builds a pyramid, warping, and cost volume architecture to compute optical flow in a coarse-to-fine manner."),"\n"),"\n",o.createElement(t.p,null,'In addition to explicit flow estimation, many spatiotemporal CNNs or attention-based models implicitly capture motion by analyzing multiple frames in a sliding window. Thus, the network effectively "learns" the concept of flow or other motion cues without directly computing an explicit flow map.'),"\n",o.createElement(t.p,null,"Below is a short code snippet illustrating how one might compute optical flow using OpenCV's (classical) built-in functions in Python:"),"\n",o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">&lt;Code text={`\nimport cv2\nimport numpy as np\n\n# Initialize video capture\ncap = cv2.VideoCapture(\'input_video.mp4\')\n\n# Parameters for Lucas-Kanade optical flow\nlk_params = dict(winSize=(15, 15),\n                 maxLevel=2,\n                 criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n\n# Read the first frame and convert to grayscale\nret, old_frame = cap.read()\nold_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n\n# Detect good feature points to track\nfeature_params = dict(maxCorners=100,\n                      qualityLevel=0.3,\n                      minDistance=7,\n                      blockSize=7)\np0 = cv2.goodFeaturesToTrack(old_gray, mask=None, **feature_params)\n\nmask = np.zeros_like(old_frame)\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n    # Calculate optical flow\n    p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n\n    # Select good points\n    good_new = p1[st == 1]\n    good_old = p0[st == 1]\n\n    # Draw the tracks\n    for i, (new, old) in enumerate(zip(good_new, good_old)):\n        x_new, y_new = new.ravel()\n        x_old, y_old = old.ravel()\n        mask = cv2.line(mask, (x_new, y_new), (x_old, y_old), (0, 255, 0), 2)\n        frame = cv2.circle(frame, (x_new, y_new), 5, (0, 0, 255), -1)\n\n    img = cv2.add(frame, mask)\n    cv2.imshow(\'frame\', img)\n\n    # Update old frame and points\n    old_gray = frame_gray.copy()\n    p0 = good_new.reshape(-1, 1, 2)\n\n    if cv2.waitKey(30) &amp; 0xFF == 27:\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n`}/></code></pre></div>'}}),"\n",o.createElement(t.p,null,'This example uses the Lucas-Kanade method for computing optical flow for a sparse set of "good feature points," illustrating a simple (non-deep-learning) approach to capture motion in a video.'),"\n",o.createElement(t.h2,{id:"approaches-for-video-tokenization-and-modeling",style:{position:"relative"}},o.createElement(t.a,{href:"#approaches-for-video-tokenization-and-modeling","aria-label":"approaches for video tokenization and modeling permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Approaches for video tokenization and modeling"),"\n",o.createElement(t.p,null,'As deep learning frameworks evolve to handle more intricate spatiotemporal tasks, novel ways of "tokenizing" or representing video inputs for neural network processing have been proposed. In general, the objective is to break down a video into discrete elements ("tokens") that a network (e.g., a transformer) can process systematically, retaining the essential information of both spatial and temporal dimensions.'),"\n",o.createElement(t.h3,{id:"71-uniform-frame-sampling",style:{position:"relative"}},o.createElement(t.a,{href:"#71-uniform-frame-sampling","aria-label":"71 uniform frame sampling permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.1 Uniform frame sampling"),"\n",o.createElement(t.p,null,o.createElement(l.A,null,"Uniform frame sampling")," is a straightforward strategy: from a longer video sequence, we sample frames at fixed intervals or a fixed rate (e.g., every nth frame). Each extracted frame is then processed in the same manner as a still image. For a Vision Transformer, this means dividing each frame into non-overlapping patches, flattening them, and projecting them into embeddings. Finally, we concatenate the sequence of frame-level tokens into a single token series for the model."),"\n",o.createElement(t.p,null,"Although it simplifies the pipeline, uniform frame sampling can overlook significant parts of the video (particularly if the frame selection interval is large). It also effectively treats each frame as an independent entity, leaving it up to the Transformer's subsequent attention mechanisms to infer temporal relationships."),"\n",o.createElement(t.h3,{id:"72-tubelet-embedding",style:{position:"relative"}},o.createElement(t.a,{href:"#72-tubelet-embedding","aria-label":"72 tubelet embedding permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.2 Tubelet embedding"),"\n",o.createElement(t.p,null,"Rather than focusing exclusively on spatial patches, ",o.createElement(l.A,null,"tubelet embedding"),' extends the patch-based approach into the temporal dimension. Here, we slice the input volume into spatiotemporal "tubes," each capturing a patch of the video in space and a chunk of frames in time. Flattening and projecting each tube into an embedding can be viewed as a 3D convolution with a kernel shape that covers a certain range of frames.'),"\n",o.createElement(t.p,null,"This approach explicitly encodes local motion patterns, as each tube spans a small temporal window. By merging spatial and temporal information early, tubelet embedding often provides superior results in tasks such as action recognition, where consistent short-term motion features are crucial."),"\n",o.createElement(t.h3,{id:"73-comparing-patch-based-vs-spatiotemporal-approaches",style:{position:"relative"}},o.createElement(t.a,{href:"#73-comparing-patch-based-vs-spatiotemporal-approaches","aria-label":"73 comparing patch based vs spatiotemporal approaches permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.3 Comparing patch-based vs. spatiotemporal approaches"),"\n",o.createElement(t.p,null,"From the vantage point of network design, the key question is: ",o.createElement(t.strong,null,"Should we process frames independently and rely on a large network to learn temporal relationships, or should we encode spatiotemporal patterns early on?")),"\n",o.createElement(t.ul,null,"\n",o.createElement(t.li,null,"\n",o.createElement(t.p,null,o.createElement(t.strong,null,"Patch-based (2D) approach:")),"\n",o.createElement(t.ul,null,"\n",o.createElement(t.li,null,"Pros: Simpler; can be built on top of proven 2D image-based architectures."),"\n",o.createElement(t.li,null,"Cons: Potentially misses fine temporal details; might require more parameters to capture time-dependent features later."),"\n"),"\n"),"\n",o.createElement(t.li,null,"\n",o.createElement(t.p,null,o.createElement(t.strong,null,"Spatiotemporal (3D) approach:")),"\n",o.createElement(t.ul,null,"\n",o.createElement(t.li,null,"Pros: Encodes local motion cues directly, possibly improving performance in tasks heavily reliant on motion."),"\n",o.createElement(t.li,null,"Cons: Increased computational cost and memory usage; more complex to implement."),"\n"),"\n"),"\n"),"\n",o.createElement(t.h3,{id:"74-integration-of-position-and-time-encodings",style:{position:"relative"}},o.createElement(t.a,{href:"#74-integration-of-position-and-time-encodings","aria-label":"74 integration of position and time encodings permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"7.4 Integration of position and time encodings"),"\n",o.createElement(t.p,null,"Transformers require positional encodings to keep track of the original spatial ordering of patches. When extended to videos, we often incorporate not only a 2D position embedding but also a temporal position embedding. For instance, each patch or tube can have a learnable embedding that encodes which frame in the sequence it came from. This helps preserve the ordering of frames and signals the network to attend to tokens with adjacent time positions."),"\n",o.createElement(t.p,null,"Recent research (e.g., Arnab and gang, 2021; Bertasius and gang, 2021) has studied various ways to mix these spatiotemporal embeddings, showing that well-crafted embeddings significantly improve the accuracy of video transformers across tasks like action recognition and detection."),"\n",o.createElement(t.h2,{id:"challenges-in-video-processing",style:{position:"relative"}},o.createElement(t.a,{href:"#challenges-in-video-processing","aria-label":"challenges in video processing permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Challenges in video processing"),"\n",o.createElement(t.p,null,"While video processing opens up unique opportunities to capture and analyze temporal dynamics, it also presents a range of significant challenges that practitioners must address. Below, we discuss some of the most common difficulties and potential strategies for mitigation."),"\n",o.createElement(t.h3,{id:"81-computational-complexity-and-resource-demands",style:{position:"relative"}},o.createElement(t.a,{href:"#81-computational-complexity-and-resource-demands","aria-label":"81 computational complexity and resource demands permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.1 Computational complexity and resource demands"),"\n",o.createElement(t.p,null,"Video-based tasks can be orders of magnitude more computationally expensive than image-based tasks. If an image classification model processes a single 224×224 image, a comparable video classification model might process 32 frames of size 224×224 each, resulting in 32 times more data. Depending on the chosen network architecture (e.g., 3D convolutions or dense transformer blocks), GPU memory usage and training time can become prohibitive."),"\n",o.createElement(t.p,null,"Possible solutions:"),"\n",o.createElement(t.ul,null,"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Shorter clips or frame subsampling:")," Work with small or carefully sampled sequences to reduce computational load."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Lightweight architectures:")," Use more efficient network designs, e.g., mobile or shuffle-based blocks for 3D CNNs, or efficient attention variants for transformers."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Distributed training:")," Parallelize across multiple GPUs or use specialized hardware (TPUs, custom ML accelerators)."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Mixed-precision training:")," Leverage half-precision (FP16) computations to speed up training and reduce memory usage, commonly used in frameworks like PyTorch or TensorFlow."),"\n"),"\n",o.createElement(t.h3,{id:"82-memory-storage-constraints-for-large-scale-video-data",style:{position:"relative"}},o.createElement(t.a,{href:"#82-memory-storage-constraints-for-large-scale-video-data","aria-label":"82 memory storage constraints for large scale video data permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.2 Memory storage constraints for large-scale video data"),"\n",o.createElement(t.p,null,"Storing and processing large video datasets (such as Kinetics, AVA, or Something-Something) can easily run into tens or hundreds of terabytes. Traditional image-based datasets are much smaller, enabling offline processing and random access. For video, one must often consider streaming directly from disk or from a distributed file system. Large volumes of data can strain local disk space and bandwidth."),"\n",o.createElement(t.p,null,"Common mitigation techniques:"),"\n",o.createElement(t.ul,null,"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Cloud-based storage:")," Hosting data on cloud services (AWS S3, Google Cloud Storage) and streaming directly in training clusters."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"On-the-fly decoding:")," Decoding compressed videos frame by frame at training time rather than storing them in raw format."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Dataset-level compression:")," Using efficient codecs to store data, combined with a fast decode pipeline to feed the GPUs."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Subset or curriculum training:")," Pretraining on smaller subsets or frames, followed by fine-tuning on larger sequences if needed."),"\n"),"\n",o.createElement(t.h3,{id:"83-handling-noise-occlusions-and-motion-blur",style:{position:"relative"}},o.createElement(t.a,{href:"#83-handling-noise-occlusions-and-motion-blur","aria-label":"83 handling noise occlusions and motion blur permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.3 Handling noise, occlusions, and motion blur"),"\n",o.createElement(t.p,null,"Due to the temporal dimension, video frames often exhibit additional artifacts like motion blur, abrupt camera movements, partial occlusions, or environmental noise. A robust video processing pipeline must handle these variations. Techniques to address them include:"),"\n",o.createElement(t.ul,null,"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Data augmentation:")," Random cropping, temporal jittering, color jitter, random occlusion, or random slow-motion augmentation."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Advanced denoising or deblurring modules:")," Incorporating dedicated layers or sub-networks (e.g., spatiotemporal autoencoders) to clean up frames before the main task."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Temporal smoothing or gating:")," Weighted smoothing or gating mechanisms that suppress spurious high-frequency noise across consecutive frames."),"\n"),"\n",o.createElement(t.h3,{id:"84-balancing-quality-vs-compression-bitrate-codecs",style:{position:"relative"}},o.createElement(t.a,{href:"#84-balancing-quality-vs-compression-bitrate-codecs","aria-label":"84 balancing quality vs compression bitrate codecs permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.4 Balancing quality vs. compression (bitrate, codecs)"),"\n",o.createElement(t.p,null,"Many real-world applications rely on compressed videos. Yet heavy compression can introduce block artifacts or degrade fine details crucial for tasks like object detection or recognition. The challenge is to find a sweet spot between storage/bandwidth efficiency and minimal information loss."),"\n",o.createElement(t.ul,null,"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Choose specialized codecs:")," Some next-generation codecs (e.g., H.265/HEVC, VP9, AV1) outperform older standards with better compression ratios for a given quality."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Adaptive bitrate streaming:")," Dynamically adjust bitrate based on network conditions or user demands, though this can complicate analysis if the content changes resolution over time."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Codec-aware training:")," In some recent works, training the network with data that reflect the same compression artifacts as the final application scenario can lead to better performance in real-world settings."),"\n"),"\n",o.createElement(t.h3,{id:"85-real-time-vs-offline-processing-considerations",style:{position:"relative"}},o.createElement(t.a,{href:"#85-real-time-vs-offline-processing-considerations","aria-label":"85 real time vs offline processing considerations permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"8.5 Real-time vs. offline processing considerations"),"\n",o.createElement(t.p,null,"Video tasks often need to run in real-time, for instance, in scenarios like live surveillance, robotics, or interactive user experiences. Offline batch processing, in contrast, relaxes time constraints, allowing more complex and thorough analysis."),"\n",o.createElement(t.ul,null,"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Real-time constraints:")," The pipeline must process frames at or above the video's framerate. Efficient models or hardware acceleration become paramount."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Latency vs. accuracy trade-off:")," Real-time applications sometimes compromise slight accuracy for drastically lower latency. For example, using specialized hardware-accelerated inference (e.g., NVIDIA TensorRT or Intel OpenVINO) can significantly reduce inference time."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Edge vs. cloud processing:"),' Some applications require video analysis to occur at the "edge" (e.g., on embedded devices), imposing stringent constraints on model size and inference speed. Others can rely on high-performance cloud services.'),"\n"),"\n",o.createElement(t.p,null,o.createElement(l.A,null,"Video processing")," is an indispensable area within the broader field of machine learning and data science, supporting countless applications and driving innovation in areas such as surveillance, health care, sports analytics, entertainment, and more. However, it also demands careful strategies for data handling, spatiotemporal modeling, computational resources, and robust training methodologies."),"\n",o.createElement("br"),"\n",o.createElement(t.h2,{id:"additional-considerations-and-advanced-perspectives",style:{position:"relative"}},o.createElement(t.a,{href:"#additional-considerations-and-advanced-perspectives","aria-label":"additional considerations and advanced perspectives permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Additional considerations and advanced perspectives"),"\n",o.createElement(t.p,null,"While the preceding sections covered the essential aspects of video processing, there are several advanced directions and research thrusts to be aware of:"),"\n",o.createElement(t.h3,{id:"advanced-spatiotemporal-architectures",style:{position:"relative"}},o.createElement(t.a,{href:"#advanced-spatiotemporal-architectures","aria-label":"advanced spatiotemporal architectures permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Advanced spatiotemporal architectures"),"\n",o.createElement(t.ul,null,"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Two-stream networks (Simonyan & Zisserman, 2014):")," Early approach that processes both RGB frames and optical flow maps in parallel, then fuses their outputs for action recognition."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"3D Convolutional Neural Networks:")," Starting from C3D (Tran and gang, ICCV 2015), I3D (Carreira & Zisserman, CVPR 2017), and R(2+1)D networks (Tran and gang, CVPR 2018), these architectures apply 3D kernels to capture motion cues and have become a staple for video recognition tasks."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"SlowFast networks (Feichtenhofer and gang, ICCV 2019):")," Process the video at two different frame rates to capture both slow semantic context and fast motion details."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Temporal Segment Networks (TSN) (Wang and gang, ECCV 2016):")," Sample frames across different segments of the video and aggregate results to capture long-range temporal structure."),"\n"),"\n",o.createElement(t.h3,{id:"transformer-based-video-models",style:{position:"relative"}},o.createElement(t.a,{href:"#transformer-based-video-models","aria-label":"transformer based video models permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Transformer-based video models"),"\n",o.createElement(t.ul,null,"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"Video Vision Transformers (ViViT)"),": Expands the standard Vision Transformer to video by combining patch embedding with temporal embeddings or 3D patch tokens."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"TimeSformer")," (Bertasius and gang, CVPR 2021): Applies divided space-time attention that factorizes attention across space and time, significantly reducing complexity."),"\n",o.createElement(t.li,null,o.createElement(t.strong,null,"MViT")," (Fan and gang, ICCV 2021): A Multiscale Vision Transformer that progressively reduces spatial resolution while expanding the channel dimension, especially suitable for video."),"\n"),"\n",o.createElement(t.h3,{id:"multi-modal-integration",style:{position:"relative"}},o.createElement(t.a,{href:"#multi-modal-integration","aria-label":"multi modal integration permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Multi-modal integration"),"\n",o.createElement(t.p,null,"Videos often come with additional data such as audio or text (subtitles, metadata). Models that combine multiple data streams — called ",o.createElement(l.A,null,"multi-modal models")," — can outperform single-modal approaches on tasks like video captioning or query-based retrieval."),"\n",o.createElement(t.h3,{id:"self-supervised-and-weakly-supervised-learning",style:{position:"relative"}},o.createElement(t.a,{href:"#self-supervised-and-weakly-supervised-learning","aria-label":"self supervised and weakly supervised learning permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Self-supervised and weakly supervised learning"),"\n",o.createElement(t.p,null,'Because labeling large video datasets can be extremely labor-intensive, research on self-supervised or weakly supervised methods has gained traction. These approaches use data pretext tasks (e.g., "predict the correct ordering of frames," "mask and reconstruct future frames," or "contrast different segments") to learn general representations of motion and appearance without extensive manual labeling.'),"\n",o.createElement(t.h3,{id:"reinforcement-learning-for-video-tasks",style:{position:"relative"}},o.createElement(t.a,{href:"#reinforcement-learning-for-video-tasks","aria-label":"reinforcement learning for video tasks permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Reinforcement learning for video tasks"),"\n",o.createElement(t.p,null,"Some advanced applications, like robotics or autonomous vehicles, require decision making based on continuous video input. Integrating video processing with reinforcement learning methods can yield systems that perceive their environment and make real-time decisions (e.g., controlling a robot's actions in response to observed motion)."),"\n",o.createElement(t.h3,{id:"ethical-and-privacy-concerns",style:{position:"relative"}},o.createElement(t.a,{href:"#ethical-and-privacy-concerns","aria-label":"ethical and privacy concerns permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Ethical and privacy concerns"),"\n",o.createElement(t.p,null,"Video data often contains sensitive information about individuals, locations, or activities. Ethical frameworks and regulatory guidelines (e.g., GDPR in Europe, or other privacy laws) must be considered when collecting, storing, or analyzing large volumes of video content. Techniques such as face anonymization or bounding-box-level obfuscation are sometimes mandated."),"\n",o.createElement("br"),"\n",o.createElement(t.h2,{id:"optional-example-building-a-video-action-recognition-inference-pipeline",style:{position:"relative"}},o.createElement(t.a,{href:"#optional-example-building-a-video-action-recognition-inference-pipeline","aria-label":"optional example building a video action recognition inference pipeline permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"(Optional) Example: Building a video action recognition inference pipeline"),"\n",o.createElement(t.p,null,"Below is a simplified demonstration of how one might put together a video action recognition pipeline using a hypothetical PyTorch 3D CNN model or a spatiotemporal transformer. This snippet focuses only on the fundamental structure of inference, not training:"),"\n",o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">&lt;Code text={`\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as T\nimport cv2\nimport numpy as np\n\n# Suppose we have a pretrained model that takes N frames of size 224x224, 3D input\nclass MockVideoModel(nn.Module):\n    def __init__(self, num_classes=10):\n        super(MockVideoModel, self).__init__()\n        # Mock architecture: just a placeholder\n        self.conv = nn.Conv3d(in_channels=3, out_channels=8, kernel_size=(3,3,3), padding=1)\n        self.pool = nn.AdaptiveAvgPool3d((1,1,1))\n        self.fc = nn.Linear(8, num_classes)\n\n    def forward(self, x):\n        # x shape: (batch, channels=3, frames=N, height=224, width=224)\n        out = self.conv(x)\n        out = self.pool(out)\n        out = out.view(out.size(0), -1)\n        out = self.fc(out)\n        return out\n\nmodel = MockVideoModel(num_classes=5)  # e.g., 5 possible actions\nmodel.eval()\n\n# Example transformation: resizing to 224x224, convert to tensor, etc.\ntransform = T.Compose([\n    T.ToPILImage(),\n    T.Resize((224,224)),\n    T.ToTensor()\n])\n\n# Load video capture\ncap = cv2.VideoCapture(\'test_video.mp4\')\n\nframes = []\nMAX_FRAMES = 16  # example: we\'ll use 16-frame snippets\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    # Convert frame to RGB\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    # Apply transform\n    tensor_frame = transform(frame_rgb)\n    frames.append(tensor_frame)\n    \n    # If we have enough frames, run inference\n    if len(frames) == MAX_FRAMES:\n        # Stack frames along a new dimension -> shape (frames, channels, height, width)\n        clip = torch.stack(frames, dim=0)\n        # Reorder to (channels, frames, height, width)\n        clip = clip.permute(1, 0, 2, 3).unsqueeze(0)  # add batch dim at index 0\n\n        # Inference\n        with torch.no_grad():\n            logits = model(clip)\n            probs = torch.softmax(logits, dim=1)\n        \n        predicted_class = torch.argmax(probs, dim=1)\n        print(f"Predicted class index: {predicted_class.item()}, Probability distribution: {probs.squeeze().tolist()}")\n\n        # Reset frames for next snippet\n        frames = []\n\ncap.release()\n`}/></code></pre></div>'}}),"\n",o.createElement(t.p,null,"In this simple illustration:"),"\n",o.createElement(t.ol,null,"\n",o.createElement(t.li,null,"We read frames from a video in real-time."),"\n",o.createElement(t.li,null,"Each frame is transformed to a consistent resolution and converted into a tensor."),"\n",o.createElement(t.li,null,"Once we have enough frames to form a snippet (in this case, 16 frames), we group them into a single 5D tensor: ",o.createElement(s.A,{text:"\\((B, C, T, H, W)\\)"}),"."),"\n",o.createElement(t.li,null,"We feed it to the model for classification, obtaining a probability distribution over possible actions."),"\n",o.createElement(t.li,null,"The snippet is reset, and the process continues for the next batch of frames."),"\n"),"\n",o.createElement(t.p,null,"Though rudimentary, this example demonstrates a common pipeline for many spatiotemporal models, whether 3D CNN-based or using advanced video transformers."),"\n",o.createElement("br"),"\n",o.createElement(t.h2,{id:"conclusion",style:{position:"relative"}},o.createElement(t.a,{href:"#conclusion","aria-label":"conclusion permalink",className:"anchor before"},o.createElement(t.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Conclusion"),"\n",o.createElement(t.p,null,"Video processing is a dynamic, expansive field that transcends the challenges of standard image-based tasks by incorporating an additional temporal dimension. This extra dimension empowers a host of new applications, ranging from real-time object tracking to complex action recognition and beyond, but it also introduces non-trivial complexities. Handling massive volumes of data, ensuring efficient spatiotemporal feature extraction, and balancing the nuances of compression and quality are only a few of the many hurdles practitioners face."),"\n",o.createElement(t.p,null,"Yet, the progression of techniques — starting from classical optical flow and block matching, moving through 3D CNNs, and arriving at sophisticated spatiotemporal transformer models — demonstrates the ongoing innovation and expanding capabilities in this area. Researchers are increasingly exploring methods that fuse multi-modal signals (audio, text, sensor data) with the visual stream, pushing the boundaries of what can be understood and inferred from video data. Meanwhile, growing emphasis on distributed training, efficient model architectures, and powerful hardware accelerators helps mitigate the formidable computational and storage challenges."),"\n",o.createElement(t.p,null,"For data scientists and machine learning engineers, developing expertise in video processing opens up a multitude of opportunities to drive forward solutions in domains such as surveillance, healthcare, entertainment, robotics, sports analytics, and more. By mastering concepts like motion estimation, spatiotemporal feature extraction, and advanced architectures for deep learning, practitioners can harness the inherent richness of video data — turning raw streams into actionable insights and intelligent systems."),"\n",o.createElement(t.hr),"\n",o.createElement(n,{alt:"Illustration of spatiotemporal representation in a video clip",path:"",caption:"Example depiction of how frames form a 3D volume of (Width × Height × Time) to be processed for tasks like action recognition.",zoom:"false"}),"\n",o.createElement(n,{alt:"Depiction of tubelet embedding for video transformers",path:"",caption:"Diagram showing how a clip is divided into spatiotemporal 'tubes' that capture both space and time information for embedding.",zoom:"false"}))}var m=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,i.RP)(),e.components);return t?o.createElement(t,e,o.createElement(c,e)):c(e)};var d=n(36710),h=n(58481),p=n.n(h),u=n(36310),g=n(87245),f=n(27042),v=n(59849),y=n(5591),b=n(61122),E=n(9219),w=n(33203),S=n(95751),x=n(94328),k=n(80791),C=n(78137);const T=e=>{let{toc:t}=e;if(!t||!t.items)return null;return o.createElement("nav",{className:k.R},o.createElement("ul",null,t.items.map(((e,t)=>o.createElement("li",{key:t},o.createElement("a",{href:e.url,onClick:t=>((e,t)=>{e.preventDefault();const n=t.replace("#",""),a=document.getElementById(n);a&&a.scrollIntoView({behavior:"smooth",block:"start"})})(t,e.url)},e.title),e.items&&o.createElement(T,{toc:{items:e.items}}))))))};function M(e){let{data:{mdx:t,allMdx:r,allPostImages:l},children:s}=e;const{frontmatter:c,body:m,tableOfContents:d}=t,h=c.index,v=c.slug.split("/")[1],k=r.nodes.filter((e=>e.frontmatter.slug.includes(`/${v}/`))).sort(((e,t)=>e.frontmatter.index-t.frontmatter.index)),M=k.findIndex((e=>e.frontmatter.index===h)),z=k[M+1],H=k[M-1],N=c.slug.replace(/\/$/,""),L=/[^/]*$/.exec(N)[0],I=`posts/${v}/content/${L}/`,{0:V,1:_}=(0,o.useState)(c.flagWideLayoutByDefault),{0:j,1:A}=(0,o.useState)(!1);var D;(0,o.useEffect)((()=>{A(!0);const e=setTimeout((()=>A(!1)),340);return()=>clearTimeout(e)}),[V]),"adventures"===v?D=E.cb:"research"===v?D=E.Qh:"thoughts"===v&&(D=E.T6);const B=p()(m).replace(/import .*? from .*?;/g,"").replace(/<.*?>/g,"").replace(/\{\/\*[\s\S]*?\*\/\}/g,"").trim().split(/\s+/).length,P=function(e){if(e<=10)return"~10 min";if(e<=20)return"~20 min";if(e<=30)return"~30 min";if(e<=40)return"~40 min";if(e<=50)return"~50 min";if(e<=60)return"~1 h";const t=Math.floor(e/60),n=e%60;return n<=30?`~${t}${n>0?".5":""} h`:`~${t+1} h`}(Math.ceil(B/D)+(c.extraReadTimeMin||0)),R=[{flag:c.flagDraft,component:()=>Promise.all([n.e(3231),n.e(8809)]).then(n.bind(n,28809))},{flag:c.flagMindfuckery,component:()=>Promise.all([n.e(3231),n.e(2471)]).then(n.bind(n,67709))},{flag:c.flagRewrite,component:()=>Promise.all([n.e(3231),n.e(6764)]).then(n.bind(n,62002))},{flag:c.flagOffensive,component:()=>Promise.all([n.e(3231),n.e(2443)]).then(n.bind(n,17681))},{flag:c.flagProfane,component:()=>Promise.all([n.e(3231),n.e(8048)]).then(n.bind(n,53286))},{flag:c.flagMultilingual,component:()=>Promise.all([n.e(3231),n.e(4069)]).then(n.bind(n,78831))},{flag:c.flagUnreliably,component:()=>Promise.all([n.e(3231),n.e(3417)]).then(n.bind(n,8179))},{flag:c.flagPolitical,component:()=>Promise.all([n.e(3231),n.e(5195)]).then(n.bind(n,30433))},{flag:c.flagCognitohazard,component:()=>Promise.all([n.e(3231),n.e(3175)]).then(n.bind(n,8413))},{flag:c.flagHidden,component:()=>Promise.all([n.e(3231),n.e(9556)]).then(n.bind(n,14794))}],{0:O,1:q}=(0,o.useState)([]);return(0,o.useEffect)((()=>{R.forEach((e=>{let{flag:t,component:n}=e;t&&n().then((e=>{q((t=>[].concat((0,a.A)(t),[e.default])))}))}))}),[]),o.createElement(f.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.15}},o.createElement(y.A,{postNumber:c.index,date:c.date,updated:c.updated,readTime:P,difficulty:c.difficultyLevel,title:c.title,desc:c.desc,banner:c.banner,section:v,postKey:L,isMindfuckery:c.flagMindfuckery,mainTag:c.mainTag}),o.createElement("div",{style:{display:"flex",justifyContent:"flex-end",flexWrap:"wrap",maxWidth:"75%",marginLeft:"auto",paddingRight:"1vw",marginTop:"-6vh",marginBottom:"4vh"}},c.otherTags.map(((e,t)=>o.createElement("span",{key:t,className:`noselect ${C.MW}`,style:{margin:"0 5px 5px 0"}},e)))),o.createElement("div",{class:"postBody"},o.createElement(T,{toc:d})),o.createElement("br"),o.createElement("div",{style:{margin:"0 10% -2vh 30%",textAlign:"right"}},o.createElement(f.P.button,{class:"noselect",className:x.pb,id:x.xG,onClick:()=>{_(!V)},whileTap:{scale:.93}},o.createElement(f.P.div,{className:S.DJ,key:V,initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.3,ease:"easeInOut"}},V?"Switch to default layout":"Switch to wide layout"))),o.createElement("br"),o.createElement("div",{class:"postBody",style:{margin:V?"0 -14%":"",maxWidth:V?"200%":"",transition:"margin 1s ease, max-width 1s ease, padding 1s ease"}},o.createElement("div",{className:`${x.P_} ${j?x.Xn:x.qG}`},O.map(((e,t)=>o.createElement(e,{key:t}))),c.indexCourse?o.createElement(w.A,{index:c.indexCourse,category:c.courseCategoryName}):"",o.createElement(u.Z.Provider,{value:{images:l.nodes,basePath:I.replace(/\/$/,"")+"/"}},o.createElement(i.xA,{components:{Image:g.A}},s)))),o.createElement(b.A,{nextPost:z,lastPost:H,keyCurrent:L,section:v}))}function z(e){return o.createElement(M,e,o.createElement(m,e))}function H(e){var t,n,a,i,r;let{data:l}=e;const{frontmatter:s}=l.mdx,c=s.titleSEO||s.title,m=s.titleOG||c,h=s.titleTwitter||c,p=s.descSEO||s.desc,u=s.descOG||p,g=s.descTwitter||p,f=s.schemaType||"BlogPosting",y=s.keywordsSEO,b=s.date,E=s.updated||b,w=s.imageOG||(null===(t=s.banner)||void 0===t||null===(n=t.childImageSharp)||void 0===n||null===(a=n.gatsbyImageData)||void 0===a||null===(i=a.images)||void 0===i||null===(r=i.fallback)||void 0===r?void 0:r.src),S=s.imageAltOG||u,x=s.imageTwitter||w,k=s.imageAltTwitter||g,C=s.canonicalURL,T=s.flagHidden||!1,M=s.mainTag||"Posts",z=s.slug.split("/")[1]||"posts",{siteUrl:H}=(0,d.Q)(),N={"@context":"https://schema.org","@type":"BreadcrumbList",itemListElement:[{"@type":"ListItem",position:1,name:"Home",item:H},{"@type":"ListItem",position:2,name:M,item:`${H}/${s.slug.split("/")[1]}`},{"@type":"ListItem",position:3,name:c,item:`${H}${s.slug}`}]};return o.createElement(v.A,{title:c+" - avrtt.blog",titleOG:m,titleTwitter:h,description:p,descriptionOG:u,descriptionTwitter:g,schemaType:f,keywords:y,datePublished:b,dateModified:E,imageOG:w,imageAltOG:S,imageTwitter:x,imageAltTwitter:k,canonicalUrl:C,flagHidden:T,mainTag:M,section:z,type:"article"},o.createElement("script",{type:"application/ld+json"},JSON.stringify(N)))}},66501:function(e,t,n){n.d(t,{A:function(){return r}});var a=n(96540),i=n(3962),o="styles-module--tooltiptext--a263b";var r=e=>{let{text:t,isBadge:n=!1}=e;const{0:r,1:l}=(0,a.useState)(!1),s=(0,a.useRef)(null);return(0,a.useEffect)((()=>{function e(e){s.current&&!s.current.contains(e.target)&&l(!1)}return document.addEventListener("click",e),()=>{document.removeEventListener("click",e)}}),[]),a.createElement("span",{className:"styles-module--tooltipWrapper--75ebf",ref:s},a.createElement("img",{id:n?"styles-module--infoBadge--e3d66":"styles-module--info--26c1f",src:i.A,alt:"info",onClick:e=>{e.stopPropagation(),l((e=>!e))}}),a.createElement("span",{className:r?`${o} styles-module--visible--c063c`:o},t))}},96098:function(e,t,n){var a=n(96540),i=n(7978);t.A=e=>{let{text:t}=e;return a.createElement(i.A,null,t)}}}]);
//# sourceMappingURL=component---src-templates-post-js-content-file-path-src-pages-posts-research-video-processing-mdx-aa670d7af36a27323bfe.js.map